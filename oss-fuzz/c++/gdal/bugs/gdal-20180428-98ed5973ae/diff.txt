diff --git a/gdal/gcore/rasterio.cpp b/gdal/gcore/rasterio.cpp
index 81f71fb2b9..d66c49aeb2 100644
--- a/gdal/gcore/rasterio.cpp
+++ b/gdal/gcore/rasterio.cpp
@@ -2832,37 +2832,40 @@ template<> void GDALUnrolledCopy<GByte,2,1>( GByte* CPL_RESTRICT pDest,
                                              const GByte* CPL_RESTRICT pSrc,
                                              int nIters )
 {
-#ifdef HAVE_SSSE3_AT_COMPILE_TIME
-    if( CPLHaveRuntimeSSSE3() )
+    int i = 0;
+    if( nIters > 16 )
     {
-        GDALUnrolledCopy_GByte_2_1_SSSE3(pDest, pSrc, nIters);
-        return;
-    }
+#ifdef HAVE_SSSE3_AT_COMPILE_TIME
+        if( CPLHaveRuntimeSSSE3() )
+        {
+            GDALUnrolledCopy_GByte_2_1_SSSE3(pDest, pSrc, nIters);
+            return;
+        }
 #endif
 
-    int i;
-    const __m128i xmm_mask = _mm_set1_epi16(0xff);
-    // If we were sure that there would always be 1 trailing byte, we could
-    // check against nIters - 15
-    for ( i = 0; i < nIters - 16; i += 16 )
-    {
-        __m128i xmm0 = _mm_loadu_si128( reinterpret_cast<__m128i const*>(pSrc + 0) );
-        __m128i xmm1 = _mm_loadu_si128( reinterpret_cast<__m128i const*>(pSrc + 16) );
-        // Set higher 8bit of each int16 packed word to 0
-        xmm0 = _mm_and_si128(xmm0, xmm_mask);
-        xmm1 = _mm_and_si128(xmm1, xmm_mask);
-        // Pack int16 to uint8
-        xmm0 = _mm_packus_epi16(xmm0, xmm0);
-        xmm1 = _mm_packus_epi16(xmm1, xmm1);
-
-        // Move 64 lower bits of xmm1 to 64 upper bits of xmm0
-        xmm1 = _mm_slli_si128(xmm1, 8);
-        xmm0 = _mm_or_si128(xmm0, xmm1);
-
-        // Store result
-        _mm_storeu_si128( reinterpret_cast<__m128i*>(pDest + i), xmm0);
-
-        pSrc += 2 * 16;
+        const __m128i xmm_mask = _mm_set1_epi16(0xff);
+        // If we were sure that there would always be 1 trailing byte, we could
+        // check against nIters - 15
+        for ( ; i < nIters - 16; i += 16 )
+        {
+            __m128i xmm0 = _mm_loadu_si128( reinterpret_cast<__m128i const*>(pSrc + 0) );
+            __m128i xmm1 = _mm_loadu_si128( reinterpret_cast<__m128i const*>(pSrc + 16) );
+            // Set higher 8bit of each int16 packed word to 0
+            xmm0 = _mm_and_si128(xmm0, xmm_mask);
+            xmm1 = _mm_and_si128(xmm1, xmm_mask);
+            // Pack int16 to uint8
+            xmm0 = _mm_packus_epi16(xmm0, xmm0);
+            xmm1 = _mm_packus_epi16(xmm1, xmm1);
+
+            // Move 64 lower bits of xmm1 to 64 upper bits of xmm0
+            xmm1 = _mm_slli_si128(xmm1, 8);
+            xmm0 = _mm_or_si128(xmm0, xmm1);
+
+            // Store result
+            _mm_storeu_si128( reinterpret_cast<__m128i*>(pDest + i), xmm0);
+
+            pSrc += 2 * 16;
+        }
     }
     for( ; i < nIters; i++ )
     {
@@ -2878,7 +2881,7 @@ template<> void GDALUnrolledCopy<GByte,3,1>( GByte* CPL_RESTRICT pDest,
                                              const GByte* CPL_RESTRICT pSrc,
                                              int nIters )
 {
-    if( CPLHaveRuntimeSSSE3() )
+    if( nIters > 16 && CPLHaveRuntimeSSSE3() )
     {
         GDALUnrolledCopy_GByte_3_1_SSSE3(pDest, pSrc, nIters);
     }
@@ -2894,47 +2897,50 @@ template<> void GDALUnrolledCopy<GByte,4,1>( GByte* CPL_RESTRICT pDest,
                                              const GByte* CPL_RESTRICT pSrc,
                                              int nIters )
 {
-#ifdef HAVE_SSSE3_AT_COMPILE_TIME
-    if( CPLHaveRuntimeSSSE3() )
+    int i = 0;
+    if( nIters > 16 )
     {
-        GDALUnrolledCopy_GByte_4_1_SSSE3(pDest, pSrc, nIters);
-        return;
-    }
+#ifdef HAVE_SSSE3_AT_COMPILE_TIME
+        if( CPLHaveRuntimeSSSE3() )
+        {
+            GDALUnrolledCopy_GByte_4_1_SSSE3(pDest, pSrc, nIters);
+            return;
+        }
 #endif
 
-    int i;
-    const __m128i xmm_mask = _mm_set1_epi32(0xff);
-    // If we were sure that there would always be 3 trailing bytes, we could
-    // check against nIters - 15
-    for ( i = 0; i < nIters - 16; i += 16 )
-    {
-        __m128i xmm0 = _mm_loadu_si128( reinterpret_cast<__m128i const*> (pSrc + 0) );
-        __m128i xmm1 = _mm_loadu_si128( reinterpret_cast<__m128i const*> (pSrc + 16) );
-        __m128i xmm2 = _mm_loadu_si128( reinterpret_cast<__m128i const*> (pSrc + 32) );
-        __m128i xmm3 = _mm_loadu_si128( reinterpret_cast<__m128i const*> (pSrc + 48) );
-        // Set higher 24bit of each int32 packed word to 0
-        xmm0 = _mm_and_si128(xmm0, xmm_mask);
-        xmm1 = _mm_and_si128(xmm1, xmm_mask);
-        xmm2 = _mm_and_si128(xmm2, xmm_mask);
-        xmm3 = _mm_and_si128(xmm3, xmm_mask);
-        // Pack int32 to int16
-        xmm0 = _mm_packs_epi32(xmm0, xmm0);
-        xmm1 = _mm_packs_epi32(xmm1, xmm1);
-        xmm2 = _mm_packs_epi32(xmm2, xmm2);
-        xmm3 = _mm_packs_epi32(xmm3, xmm3);
-        // Pack int16 to uint8
-        xmm0 = _mm_packus_epi16(xmm0, xmm0);
-        xmm1 = _mm_packus_epi16(xmm1, xmm1);
-        xmm2 = _mm_packus_epi16(xmm2, xmm2);
-        xmm3 = _mm_packus_epi16(xmm3, xmm3);
-
-        // Store lower 32 bit word
-        GDALCopyXMMToInt32(xmm0, pDest + i + 0);
-        GDALCopyXMMToInt32(xmm1, pDest + i + 4);
-        GDALCopyXMMToInt32(xmm2, pDest + i + 8);
-        GDALCopyXMMToInt32(xmm3, pDest + i + 12);
-
-        pSrc += 4 * 16;
+        const __m128i xmm_mask = _mm_set1_epi32(0xff);
+        // If we were sure that there would always be 3 trailing bytes, we could
+        // check against nIters - 15
+        for ( ; i < nIters - 16; i += 16 )
+        {
+            __m128i xmm0 = _mm_loadu_si128( reinterpret_cast<__m128i const*> (pSrc + 0) );
+            __m128i xmm1 = _mm_loadu_si128( reinterpret_cast<__m128i const*> (pSrc + 16) );
+            __m128i xmm2 = _mm_loadu_si128( reinterpret_cast<__m128i const*> (pSrc + 32) );
+            __m128i xmm3 = _mm_loadu_si128( reinterpret_cast<__m128i const*> (pSrc + 48) );
+            // Set higher 24bit of each int32 packed word to 0
+            xmm0 = _mm_and_si128(xmm0, xmm_mask);
+            xmm1 = _mm_and_si128(xmm1, xmm_mask);
+            xmm2 = _mm_and_si128(xmm2, xmm_mask);
+            xmm3 = _mm_and_si128(xmm3, xmm_mask);
+            // Pack int32 to int16
+            xmm0 = _mm_packs_epi32(xmm0, xmm0);
+            xmm1 = _mm_packs_epi32(xmm1, xmm1);
+            xmm2 = _mm_packs_epi32(xmm2, xmm2);
+            xmm3 = _mm_packs_epi32(xmm3, xmm3);
+            // Pack int16 to uint8
+            xmm0 = _mm_packus_epi16(xmm0, xmm0);
+            xmm1 = _mm_packus_epi16(xmm1, xmm1);
+            xmm2 = _mm_packus_epi16(xmm2, xmm2);
+            xmm3 = _mm_packus_epi16(xmm3, xmm3);
+
+            // Store lower 32 bit word
+            GDALCopyXMMToInt32(xmm0, pDest + i + 0);
+            GDALCopyXMMToInt32(xmm1, pDest + i + 4);
+            GDALCopyXMMToInt32(xmm2, pDest + i + 8);
+            GDALCopyXMMToInt32(xmm3, pDest + i + 12);
+
+            pSrc += 4 * 16;
+        }
     }
     for( ; i < nIters; i++ )
     {
