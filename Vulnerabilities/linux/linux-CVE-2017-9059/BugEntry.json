{"buggy_code": ["/*\n * linux/fs/lockd/svc.c\n *\n * This is the central lockd service.\n *\n * FIXME: Separate the lockd NFS server functionality from the lockd NFS\n * \t  client functionality. Oh why didn't Sun create two separate\n *\t  services in the first place?\n *\n * Authors:\tOlaf Kirch (okir@monad.swb.de)\n *\n * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>\n */\n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/sysctl.h>\n#include <linux/moduleparam.h>\n\n#include <linux/sched/signal.h>\n#include <linux/errno.h>\n#include <linux/in.h>\n#include <linux/uio.h>\n#include <linux/smp.h>\n#include <linux/mutex.h>\n#include <linux/kthread.h>\n#include <linux/freezer.h>\n#include <linux/inetdevice.h>\n\n#include <linux/sunrpc/types.h>\n#include <linux/sunrpc/stats.h>\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/svc.h>\n#include <linux/sunrpc/svcsock.h>\n#include <linux/sunrpc/svc_xprt.h>\n#include <net/ip.h>\n#include <net/addrconf.h>\n#include <net/ipv6.h>\n#include <linux/lockd/lockd.h>\n#include <linux/nfs.h>\n\n#include \"netns.h\"\n#include \"procfs.h\"\n\n#define NLMDBG_FACILITY\t\tNLMDBG_SVC\n#define LOCKD_BUFSIZE\t\t(1024 + NLMSVC_XDRSIZE)\n#define ALLOWED_SIGS\t\t(sigmask(SIGKILL))\n\nstatic struct svc_program\tnlmsvc_program;\n\nconst struct nlmsvc_binding\t*nlmsvc_ops;\nEXPORT_SYMBOL_GPL(nlmsvc_ops);\n\nstatic DEFINE_MUTEX(nlmsvc_mutex);\nstatic unsigned int\t\tnlmsvc_users;\nstatic struct task_struct\t*nlmsvc_task;\nstatic struct svc_rqst\t\t*nlmsvc_rqst;\nunsigned long\t\t\tnlmsvc_timeout;\n\nunsigned int lockd_net_id;\n\n/*\n * These can be set at insmod time (useful for NFS as root filesystem),\n * and also changed through the sysctl interface.  -- Jamie Lokier, Aug 2003\n */\nstatic unsigned long\t\tnlm_grace_period;\nstatic unsigned long\t\tnlm_timeout = LOCKD_DFLT_TIMEO;\nstatic int\t\t\tnlm_udpport, nlm_tcpport;\n\n/* RLIM_NOFILE defaults to 1024. That seems like a reasonable default here. */\nstatic unsigned int\t\tnlm_max_connections = 1024;\n\n/*\n * Constants needed for the sysctl interface.\n */\nstatic const unsigned long\tnlm_grace_period_min = 0;\nstatic const unsigned long\tnlm_grace_period_max = 240;\nstatic const unsigned long\tnlm_timeout_min = 3;\nstatic const unsigned long\tnlm_timeout_max = 20;\nstatic const int\t\tnlm_port_min = 0, nlm_port_max = 65535;\n\n#ifdef CONFIG_SYSCTL\nstatic struct ctl_table_header * nlm_sysctl_table;\n#endif\n\nstatic unsigned long get_lockd_grace_period(void)\n{\n\t/* Note: nlm_timeout should always be nonzero */\n\tif (nlm_grace_period)\n\t\treturn roundup(nlm_grace_period, nlm_timeout) * HZ;\n\telse\n\t\treturn nlm_timeout * 5 * HZ;\n}\n\nstatic void grace_ender(struct work_struct *grace)\n{\n\tstruct delayed_work *dwork = to_delayed_work(grace);\n\tstruct lockd_net *ln = container_of(dwork, struct lockd_net,\n\t\t\t\t\t    grace_period_end);\n\n\tlocks_end_grace(&ln->lockd_manager);\n}\n\nstatic void set_grace_period(struct net *net)\n{\n\tunsigned long grace_period = get_lockd_grace_period();\n\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n\n\tlocks_start_grace(net, &ln->lockd_manager);\n\tcancel_delayed_work_sync(&ln->grace_period_end);\n\tschedule_delayed_work(&ln->grace_period_end, grace_period);\n}\n\nstatic void restart_grace(void)\n{\n\tif (nlmsvc_ops) {\n\t\tstruct net *net = &init_net;\n\t\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n\n\t\tcancel_delayed_work_sync(&ln->grace_period_end);\n\t\tlocks_end_grace(&ln->lockd_manager);\n\t\tnlmsvc_invalidate_all();\n\t\tset_grace_period(net);\n\t}\n}\n\n/*\n * This is the lockd kernel thread\n */\nstatic int\nlockd(void *vrqstp)\n{\n\tint\t\terr = 0;\n\tstruct svc_rqst *rqstp = vrqstp;\n\n\t/* try_to_freeze() is called from svc_recv() */\n\tset_freezable();\n\n\t/* Allow SIGKILL to tell lockd to drop all of its locks */\n\tallow_signal(SIGKILL);\n\n\tdprintk(\"NFS locking service started (ver \" LOCKD_VERSION \").\\n\");\n\n\t/*\n\t * The main request loop. We don't terminate until the last\n\t * NFS mount or NFS daemon has gone away.\n\t */\n\twhile (!kthread_should_stop()) {\n\t\tlong timeout = MAX_SCHEDULE_TIMEOUT;\n\t\tRPC_IFDEBUG(char buf[RPC_MAX_ADDRBUFLEN]);\n\n\t\t/* update sv_maxconn if it has changed */\n\t\trqstp->rq_server->sv_maxconn = nlm_max_connections;\n\n\t\tif (signalled()) {\n\t\t\tflush_signals(current);\n\t\t\trestart_grace();\n\t\t\tcontinue;\n\t\t}\n\n\t\ttimeout = nlmsvc_retry_blocked();\n\n\t\t/*\n\t\t * Find a socket with data available and call its\n\t\t * recvfrom routine.\n\t\t */\n\t\terr = svc_recv(rqstp, timeout);\n\t\tif (err == -EAGAIN || err == -EINTR)\n\t\t\tcontinue;\n\t\tdprintk(\"lockd: request from %s\\n\",\n\t\t\t\tsvc_print_addr(rqstp, buf, sizeof(buf)));\n\n\t\tsvc_process(rqstp);\n\t}\n\tflush_signals(current);\n\tif (nlmsvc_ops)\n\t\tnlmsvc_invalidate_all();\n\tnlm_shutdown_hosts();\n\treturn 0;\n}\n\nstatic int create_lockd_listener(struct svc_serv *serv, const char *name,\n\t\t\t\t struct net *net, const int family,\n\t\t\t\t const unsigned short port)\n{\n\tstruct svc_xprt *xprt;\n\n\txprt = svc_find_xprt(serv, name, net, family, 0);\n\tif (xprt == NULL)\n\t\treturn svc_create_xprt(serv, name, net, family, port,\n\t\t\t\t\t\tSVC_SOCK_DEFAULTS);\n\tsvc_xprt_put(xprt);\n\treturn 0;\n}\n\nstatic int create_lockd_family(struct svc_serv *serv, struct net *net,\n\t\t\t       const int family)\n{\n\tint err;\n\n\terr = create_lockd_listener(serv, \"udp\", net, family, nlm_udpport);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn create_lockd_listener(serv, \"tcp\", net, family, nlm_tcpport);\n}\n\n/*\n * Ensure there are active UDP and TCP listeners for lockd.\n *\n * Even if we have only TCP NFS mounts and/or TCP NFSDs, some\n * local services (such as rpc.statd) still require UDP, and\n * some NFS servers do not yet support NLM over TCP.\n *\n * Returns zero if all listeners are available; otherwise a\n * negative errno value is returned.\n */\nstatic int make_socks(struct svc_serv *serv, struct net *net)\n{\n\tstatic int warned;\n\tint err;\n\n\terr = create_lockd_family(serv, net, PF_INET);\n\tif (err < 0)\n\t\tgoto out_err;\n\n\terr = create_lockd_family(serv, net, PF_INET6);\n\tif (err < 0 && err != -EAFNOSUPPORT)\n\t\tgoto out_err;\n\n\twarned = 0;\n\treturn 0;\n\nout_err:\n\tif (warned++ == 0)\n\t\tprintk(KERN_WARNING\n\t\t\t\"lockd_up: makesock failed, error=%d\\n\", err);\n\tsvc_shutdown_net(serv, net);\n\treturn err;\n}\n\nstatic int lockd_up_net(struct svc_serv *serv, struct net *net)\n{\n\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n\tint error;\n\n\tif (ln->nlmsvc_users++)\n\t\treturn 0;\n\n\terror = svc_bind(serv, net);\n\tif (error)\n\t\tgoto err_bind;\n\n\terror = make_socks(serv, net);\n\tif (error < 0)\n\t\tgoto err_bind;\n\tset_grace_period(net);\n\tdprintk(\"lockd_up_net: per-net data created; net=%p\\n\", net);\n\treturn 0;\n\nerr_bind:\n\tln->nlmsvc_users--;\n\treturn error;\n}\n\nstatic void lockd_down_net(struct svc_serv *serv, struct net *net)\n{\n\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n\n\tif (ln->nlmsvc_users) {\n\t\tif (--ln->nlmsvc_users == 0) {\n\t\t\tnlm_shutdown_hosts_net(net);\n\t\t\tcancel_delayed_work_sync(&ln->grace_period_end);\n\t\t\tlocks_end_grace(&ln->lockd_manager);\n\t\t\tsvc_shutdown_net(serv, net);\n\t\t\tdprintk(\"lockd_down_net: per-net data destroyed; net=%p\\n\", net);\n\t\t}\n\t} else {\n\t\tprintk(KERN_ERR \"lockd_down_net: no users! task=%p, net=%p\\n\",\n\t\t\t\tnlmsvc_task, net);\n\t\tBUG();\n\t}\n}\n\nstatic int lockd_inetaddr_event(struct notifier_block *this,\n\tunsigned long event, void *ptr)\n{\n\tstruct in_ifaddr *ifa = (struct in_ifaddr *)ptr;\n\tstruct sockaddr_in sin;\n\n\tif (event != NETDEV_DOWN)\n\t\tgoto out;\n\n\tif (nlmsvc_rqst) {\n\t\tdprintk(\"lockd_inetaddr_event: removed %pI4\\n\",\n\t\t\t&ifa->ifa_local);\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = ifa->ifa_local;\n\t\tsvc_age_temp_xprts_now(nlmsvc_rqst->rq_server,\n\t\t\t(struct sockaddr *)&sin);\n\t}\n\nout:\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block lockd_inetaddr_notifier = {\n\t.notifier_call = lockd_inetaddr_event,\n};\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic int lockd_inet6addr_event(struct notifier_block *this,\n\tunsigned long event, void *ptr)\n{\n\tstruct inet6_ifaddr *ifa = (struct inet6_ifaddr *)ptr;\n\tstruct sockaddr_in6 sin6;\n\n\tif (event != NETDEV_DOWN)\n\t\tgoto out;\n\n\tif (nlmsvc_rqst) {\n\t\tdprintk(\"lockd_inet6addr_event: removed %pI6\\n\", &ifa->addr);\n\t\tsin6.sin6_family = AF_INET6;\n\t\tsin6.sin6_addr = ifa->addr;\n\t\tif (ipv6_addr_type(&sin6.sin6_addr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tsin6.sin6_scope_id = ifa->idev->dev->ifindex;\n\t\tsvc_age_temp_xprts_now(nlmsvc_rqst->rq_server,\n\t\t\t(struct sockaddr *)&sin6);\n\t}\n\nout:\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block lockd_inet6addr_notifier = {\n\t.notifier_call = lockd_inet6addr_event,\n};\n#endif\n\nstatic void lockd_unregister_notifiers(void)\n{\n\tunregister_inetaddr_notifier(&lockd_inetaddr_notifier);\n#if IS_ENABLED(CONFIG_IPV6)\n\tunregister_inet6addr_notifier(&lockd_inet6addr_notifier);\n#endif\n}\n\nstatic void lockd_svc_exit_thread(void)\n{\n\tlockd_unregister_notifiers();\n\tsvc_exit_thread(nlmsvc_rqst);\n}\n\nstatic int lockd_start_svc(struct svc_serv *serv)\n{\n\tint error;\n\n\tif (nlmsvc_rqst)\n\t\treturn 0;\n\n\t/*\n\t * Create the kernel thread and wait for it to start.\n\t */\n\tnlmsvc_rqst = svc_prepare_thread(serv, &serv->sv_pools[0], NUMA_NO_NODE);\n\tif (IS_ERR(nlmsvc_rqst)) {\n\t\terror = PTR_ERR(nlmsvc_rqst);\n\t\tprintk(KERN_WARNING\n\t\t\t\"lockd_up: svc_rqst allocation failed, error=%d\\n\",\n\t\t\terror);\n\t\tgoto out_rqst;\n\t}\n\n\tsvc_sock_update_bufs(serv);\n\tserv->sv_maxconn = nlm_max_connections;\n\n\tnlmsvc_task = kthread_create(lockd, nlmsvc_rqst, \"%s\", serv->sv_name);\n\tif (IS_ERR(nlmsvc_task)) {\n\t\terror = PTR_ERR(nlmsvc_task);\n\t\tprintk(KERN_WARNING\n\t\t\t\"lockd_up: kthread_run failed, error=%d\\n\", error);\n\t\tgoto out_task;\n\t}\n\tnlmsvc_rqst->rq_task = nlmsvc_task;\n\twake_up_process(nlmsvc_task);\n\n\tdprintk(\"lockd_up: service started\\n\");\n\treturn 0;\n\nout_task:\n\tlockd_svc_exit_thread();\n\tnlmsvc_task = NULL;\nout_rqst:\n\tnlmsvc_rqst = NULL;\n\treturn error;\n}\n\nstatic struct svc_serv_ops lockd_sv_ops = {\n\t.svo_shutdown\t\t= svc_rpcb_cleanup,\n\t.svo_enqueue_xprt\t= svc_xprt_do_enqueue,\n};\n\nstatic struct svc_serv *lockd_create_svc(void)\n{\n\tstruct svc_serv *serv;\n\n\t/*\n\t * Check whether we're already up and running.\n\t */\n\tif (nlmsvc_rqst) {\n\t\t/*\n\t\t * Note: increase service usage, because later in case of error\n\t\t * svc_destroy() will be called.\n\t\t */\n\t\tsvc_get(nlmsvc_rqst->rq_server);\n\t\treturn nlmsvc_rqst->rq_server;\n\t}\n\n\t/*\n\t * Sanity check: if there's no pid,\n\t * we should be the first user ...\n\t */\n\tif (nlmsvc_users)\n\t\tprintk(KERN_WARNING\n\t\t\t\"lockd_up: no pid, %d users??\\n\", nlmsvc_users);\n\n\tif (!nlm_timeout)\n\t\tnlm_timeout = LOCKD_DFLT_TIMEO;\n\tnlmsvc_timeout = nlm_timeout * HZ;\n\n\tserv = svc_create(&nlmsvc_program, LOCKD_BUFSIZE, &lockd_sv_ops);\n\tif (!serv) {\n\t\tprintk(KERN_WARNING \"lockd_up: create service failed\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tregister_inetaddr_notifier(&lockd_inetaddr_notifier);\n#if IS_ENABLED(CONFIG_IPV6)\n\tregister_inet6addr_notifier(&lockd_inet6addr_notifier);\n#endif\n\tdprintk(\"lockd_up: service created\\n\");\n\treturn serv;\n}\n\n/*\n * Bring up the lockd process if it's not already up.\n */\nint lockd_up(struct net *net)\n{\n\tstruct svc_serv *serv;\n\tint error;\n\n\tmutex_lock(&nlmsvc_mutex);\n\n\tserv = lockd_create_svc();\n\tif (IS_ERR(serv)) {\n\t\terror = PTR_ERR(serv);\n\t\tgoto err_create;\n\t}\n\n\terror = lockd_up_net(serv, net);\n\tif (error < 0)\n\t\tgoto err_net;\n\n\terror = lockd_start_svc(serv);\n\tif (error < 0)\n\t\tgoto err_start;\n\n\tnlmsvc_users++;\n\t/*\n\t * Note: svc_serv structures have an initial use count of 1,\n\t * so we exit through here on both success and failure.\n\t */\nerr_put:\n\tsvc_destroy(serv);\nerr_create:\n\tmutex_unlock(&nlmsvc_mutex);\n\treturn error;\n\nerr_start:\n\tlockd_down_net(serv, net);\nerr_net:\n\tlockd_unregister_notifiers();\n\tgoto err_put;\n}\nEXPORT_SYMBOL_GPL(lockd_up);\n\n/*\n * Decrement the user count and bring down lockd if we're the last.\n */\nvoid\nlockd_down(struct net *net)\n{\n\tmutex_lock(&nlmsvc_mutex);\n\tlockd_down_net(nlmsvc_rqst->rq_server, net);\n\tif (nlmsvc_users) {\n\t\tif (--nlmsvc_users)\n\t\t\tgoto out;\n\t} else {\n\t\tprintk(KERN_ERR \"lockd_down: no users! task=%p\\n\",\n\t\t\tnlmsvc_task);\n\t\tBUG();\n\t}\n\n\tif (!nlmsvc_task) {\n\t\tprintk(KERN_ERR \"lockd_down: no lockd running.\\n\");\n\t\tBUG();\n\t}\n\tkthread_stop(nlmsvc_task);\n\tdprintk(\"lockd_down: service stopped\\n\");\n\tlockd_svc_exit_thread();\n\tdprintk(\"lockd_down: service destroyed\\n\");\n\tnlmsvc_task = NULL;\n\tnlmsvc_rqst = NULL;\nout:\n\tmutex_unlock(&nlmsvc_mutex);\n}\nEXPORT_SYMBOL_GPL(lockd_down);\n\n#ifdef CONFIG_SYSCTL\n\n/*\n * Sysctl parameters (same as module parameters, different interface).\n */\n\nstatic struct ctl_table nlm_sysctls[] = {\n\t{\n\t\t.procname\t= \"nlm_grace_period\",\n\t\t.data\t\t= &nlm_grace_period,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t\t.extra1\t\t= (unsigned long *) &nlm_grace_period_min,\n\t\t.extra2\t\t= (unsigned long *) &nlm_grace_period_max,\n\t},\n\t{\n\t\t.procname\t= \"nlm_timeout\",\n\t\t.data\t\t= &nlm_timeout,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t\t.extra1\t\t= (unsigned long *) &nlm_timeout_min,\n\t\t.extra2\t\t= (unsigned long *) &nlm_timeout_max,\n\t},\n\t{\n\t\t.procname\t= \"nlm_udpport\",\n\t\t.data\t\t= &nlm_udpport,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= (int *) &nlm_port_min,\n\t\t.extra2\t\t= (int *) &nlm_port_max,\n\t},\n\t{\n\t\t.procname\t= \"nlm_tcpport\",\n\t\t.data\t\t= &nlm_tcpport,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= (int *) &nlm_port_min,\n\t\t.extra2\t\t= (int *) &nlm_port_max,\n\t},\n\t{\n\t\t.procname\t= \"nsm_use_hostnames\",\n\t\t.data\t\t= &nsm_use_hostnames,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"nsm_local_state\",\n\t\t.data\t\t= &nsm_local_state,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{ }\n};\n\nstatic struct ctl_table nlm_sysctl_dir[] = {\n\t{\n\t\t.procname\t= \"nfs\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= nlm_sysctls,\n\t},\n\t{ }\n};\n\nstatic struct ctl_table nlm_sysctl_root[] = {\n\t{\n\t\t.procname\t= \"fs\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= nlm_sysctl_dir,\n\t},\n\t{ }\n};\n\n#endif\t/* CONFIG_SYSCTL */\n\n/*\n * Module (and sysfs) parameters.\n */\n\n#define param_set_min_max(name, type, which_strtol, min, max)\t\t\\\nstatic int param_set_##name(const char *val, struct kernel_param *kp)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tchar *endp;\t\t\t\t\t\t\t\\\n\t__typeof__(type) num = which_strtol(val, &endp, 0);\t\t\\\n\tif (endp == val || *endp || num < (min) || num > (max))\t\t\\\n\t\treturn -EINVAL;\t\t\t\t\t\t\\\n\t*((type *) kp->arg) = num;\t\t\t\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\n\nstatic inline int is_callback(u32 proc)\n{\n\treturn proc == NLMPROC_GRANTED\n\t\t|| proc == NLMPROC_GRANTED_MSG\n\t\t|| proc == NLMPROC_TEST_RES\n\t\t|| proc == NLMPROC_LOCK_RES\n\t\t|| proc == NLMPROC_CANCEL_RES\n\t\t|| proc == NLMPROC_UNLOCK_RES\n\t\t|| proc == NLMPROC_NSM_NOTIFY;\n}\n\n\nstatic int lockd_authenticate(struct svc_rqst *rqstp)\n{\n\trqstp->rq_client = NULL;\n\tswitch (rqstp->rq_authop->flavour) {\n\t\tcase RPC_AUTH_NULL:\n\t\tcase RPC_AUTH_UNIX:\n\t\t\tif (rqstp->rq_proc == 0)\n\t\t\t\treturn SVC_OK;\n\t\t\tif (is_callback(rqstp->rq_proc)) {\n\t\t\t\t/* Leave it to individual procedures to\n\t\t\t\t * call nlmsvc_lookup_host(rqstp)\n\t\t\t\t */\n\t\t\t\treturn SVC_OK;\n\t\t\t}\n\t\t\treturn svc_set_client(rqstp);\n\t}\n\treturn SVC_DENIED;\n}\n\n\nparam_set_min_max(port, int, simple_strtol, 0, 65535)\nparam_set_min_max(grace_period, unsigned long, simple_strtoul,\n\t\t  nlm_grace_period_min, nlm_grace_period_max)\nparam_set_min_max(timeout, unsigned long, simple_strtoul,\n\t\t  nlm_timeout_min, nlm_timeout_max)\n\nMODULE_AUTHOR(\"Olaf Kirch <okir@monad.swb.de>\");\nMODULE_DESCRIPTION(\"NFS file locking service version \" LOCKD_VERSION \".\");\nMODULE_LICENSE(\"GPL\");\n\nmodule_param_call(nlm_grace_period, param_set_grace_period, param_get_ulong,\n\t\t  &nlm_grace_period, 0644);\nmodule_param_call(nlm_timeout, param_set_timeout, param_get_ulong,\n\t\t  &nlm_timeout, 0644);\nmodule_param_call(nlm_udpport, param_set_port, param_get_int,\n\t\t  &nlm_udpport, 0644);\nmodule_param_call(nlm_tcpport, param_set_port, param_get_int,\n\t\t  &nlm_tcpport, 0644);\nmodule_param(nsm_use_hostnames, bool, 0644);\nmodule_param(nlm_max_connections, uint, 0644);\n\nstatic int lockd_init_net(struct net *net)\n{\n\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n\n\tINIT_DELAYED_WORK(&ln->grace_period_end, grace_ender);\n\tINIT_LIST_HEAD(&ln->lockd_manager.list);\n\tln->lockd_manager.block_opens = false;\n\tINIT_LIST_HEAD(&ln->nsm_handles);\n\treturn 0;\n}\n\nstatic void lockd_exit_net(struct net *net)\n{\n}\n\nstatic struct pernet_operations lockd_net_ops = {\n\t.init = lockd_init_net,\n\t.exit = lockd_exit_net,\n\t.id = &lockd_net_id,\n\t.size = sizeof(struct lockd_net),\n};\n\n\n/*\n * Initialising and terminating the module.\n */\n\nstatic int __init init_nlm(void)\n{\n\tint err;\n\n#ifdef CONFIG_SYSCTL\n\terr = -ENOMEM;\n\tnlm_sysctl_table = register_sysctl_table(nlm_sysctl_root);\n\tif (nlm_sysctl_table == NULL)\n\t\tgoto err_sysctl;\n#endif\n\terr = register_pernet_subsys(&lockd_net_ops);\n\tif (err)\n\t\tgoto err_pernet;\n\n\terr = lockd_create_procfs();\n\tif (err)\n\t\tgoto err_procfs;\n\n\treturn 0;\n\nerr_procfs:\n\tunregister_pernet_subsys(&lockd_net_ops);\nerr_pernet:\n#ifdef CONFIG_SYSCTL\n\tunregister_sysctl_table(nlm_sysctl_table);\nerr_sysctl:\n#endif\n\treturn err;\n}\n\nstatic void __exit exit_nlm(void)\n{\n\t/* FIXME: delete all NLM clients */\n\tnlm_shutdown_hosts();\n\tlockd_remove_procfs();\n\tunregister_pernet_subsys(&lockd_net_ops);\n#ifdef CONFIG_SYSCTL\n\tunregister_sysctl_table(nlm_sysctl_table);\n#endif\n}\n\nmodule_init(init_nlm);\nmodule_exit(exit_nlm);\n\n/*\n * Define NLM program and procedures\n */\nstatic struct svc_version\tnlmsvc_version1 = {\n\t\t.vs_vers\t= 1,\n\t\t.vs_nproc\t= 17,\n\t\t.vs_proc\t= nlmsvc_procedures,\n\t\t.vs_xdrsize\t= NLMSVC_XDRSIZE,\n};\nstatic struct svc_version\tnlmsvc_version3 = {\n\t\t.vs_vers\t= 3,\n\t\t.vs_nproc\t= 24,\n\t\t.vs_proc\t= nlmsvc_procedures,\n\t\t.vs_xdrsize\t= NLMSVC_XDRSIZE,\n};\n#ifdef CONFIG_LOCKD_V4\nstatic struct svc_version\tnlmsvc_version4 = {\n\t\t.vs_vers\t= 4,\n\t\t.vs_nproc\t= 24,\n\t\t.vs_proc\t= nlmsvc_procedures4,\n\t\t.vs_xdrsize\t= NLMSVC_XDRSIZE,\n};\n#endif\nstatic struct svc_version *\tnlmsvc_version[] = {\n\t[1] = &nlmsvc_version1,\n\t[3] = &nlmsvc_version3,\n#ifdef CONFIG_LOCKD_V4\n\t[4] = &nlmsvc_version4,\n#endif\n};\n\nstatic struct svc_stat\t\tnlmsvc_stats;\n\n#define NLM_NRVERS\tARRAY_SIZE(nlmsvc_version)\nstatic struct svc_program\tnlmsvc_program = {\n\t.pg_prog\t\t= NLM_PROGRAM,\t\t/* program number */\n\t.pg_nvers\t\t= NLM_NRVERS,\t\t/* number of entries in nlmsvc_version */\n\t.pg_vers\t\t= nlmsvc_version,\t/* version table */\n\t.pg_name\t\t= \"lockd\",\t\t/* service name */\n\t.pg_class\t\t= \"nfsd\",\t\t/* share authentication with nfsd */\n\t.pg_stats\t\t= &nlmsvc_stats,\t/* stats table */\n\t.pg_authenticate = &lockd_authenticate\t/* export authentication */\n};\n", "/*\n * linux/fs/lockd/svclock.c\n *\n * Handling of server-side locks, mostly of the blocked variety.\n * This is the ugliest part of lockd because we tread on very thin ice.\n * GRANT and CANCEL calls may get stuck, meet in mid-flight, etc.\n * IMNSHO introducing the grant callback into the NLM protocol was one\n * of the worst ideas Sun ever had. Except maybe for the idea of doing\n * NFS file locking at all.\n *\n * I'm trying hard to avoid race conditions by protecting most accesses\n * to a file's list of blocked locks through a semaphore. The global\n * list of blocked locks is not protected in this fashion however.\n * Therefore, some functions (such as the RPC callback for the async grant\n * call) move blocked locks towards the head of the list *while some other\n * process might be traversing it*. This should not be a problem in\n * practice, because this will only cause functions traversing the list\n * to visit some blocks twice.\n *\n * Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>\n */\n\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/svc_xprt.h>\n#include <linux/lockd/nlm.h>\n#include <linux/lockd/lockd.h>\n#include <linux/kthread.h>\n\n#define NLMDBG_FACILITY\t\tNLMDBG_SVCLOCK\n\n#ifdef CONFIG_LOCKD_V4\n#define nlm_deadlock\tnlm4_deadlock\n#else\n#define nlm_deadlock\tnlm_lck_denied\n#endif\n\nstatic void nlmsvc_release_block(struct nlm_block *block);\nstatic void\tnlmsvc_insert_block(struct nlm_block *block, unsigned long);\nstatic void\tnlmsvc_remove_block(struct nlm_block *block);\n\nstatic int nlmsvc_setgrantargs(struct nlm_rqst *call, struct nlm_lock *lock);\nstatic void nlmsvc_freegrantargs(struct nlm_rqst *call);\nstatic const struct rpc_call_ops nlmsvc_grant_ops;\n\n/*\n * The list of blocked locks to retry\n */\nstatic LIST_HEAD(nlm_blocked);\nstatic DEFINE_SPINLOCK(nlm_blocked_lock);\n\n#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)\nstatic const char *nlmdbg_cookie2a(const struct nlm_cookie *cookie)\n{\n\t/*\n\t * We can get away with a static buffer because this is only called\n\t * from lockd, which is single-threaded.\n\t */\n\tstatic char buf[2*NLM_MAXCOOKIELEN+1];\n\tunsigned int i, len = sizeof(buf);\n\tchar *p = buf;\n\n\tlen--;\t/* allow for trailing \\0 */\n\tif (len < 3)\n\t\treturn \"???\";\n\tfor (i = 0 ; i < cookie->len ; i++) {\n\t\tif (len < 2) {\n\t\t\tstrcpy(p-3, \"...\");\n\t\t\tbreak;\n\t\t}\n\t\tsprintf(p, \"%02x\", cookie->data[i]);\n\t\tp += 2;\n\t\tlen -= 2;\n\t}\n\t*p = '\\0';\n\n\treturn buf;\n}\n#endif\n\n/*\n * Insert a blocked lock into the global list\n */\nstatic void\nnlmsvc_insert_block_locked(struct nlm_block *block, unsigned long when)\n{\n\tstruct nlm_block *b;\n\tstruct list_head *pos;\n\n\tdprintk(\"lockd: nlmsvc_insert_block(%p, %ld)\\n\", block, when);\n\tif (list_empty(&block->b_list)) {\n\t\tkref_get(&block->b_count);\n\t} else {\n\t\tlist_del_init(&block->b_list);\n\t}\n\n\tpos = &nlm_blocked;\n\tif (when != NLM_NEVER) {\n\t\tif ((when += jiffies) == NLM_NEVER)\n\t\t\twhen ++;\n\t\tlist_for_each(pos, &nlm_blocked) {\n\t\t\tb = list_entry(pos, struct nlm_block, b_list);\n\t\t\tif (time_after(b->b_when,when) || b->b_when == NLM_NEVER)\n\t\t\t\tbreak;\n\t\t}\n\t\t/* On normal exit from the loop, pos == &nlm_blocked,\n\t\t * so we will be adding to the end of the list - good\n\t\t */\n\t}\n\n\tlist_add_tail(&block->b_list, pos);\n\tblock->b_when = when;\n}\n\nstatic void nlmsvc_insert_block(struct nlm_block *block, unsigned long when)\n{\n\tspin_lock(&nlm_blocked_lock);\n\tnlmsvc_insert_block_locked(block, when);\n\tspin_unlock(&nlm_blocked_lock);\n}\n\n/*\n * Remove a block from the global list\n */\nstatic inline void\nnlmsvc_remove_block(struct nlm_block *block)\n{\n\tif (!list_empty(&block->b_list)) {\n\t\tspin_lock(&nlm_blocked_lock);\n\t\tlist_del_init(&block->b_list);\n\t\tspin_unlock(&nlm_blocked_lock);\n\t\tnlmsvc_release_block(block);\n\t}\n}\n\n/*\n * Find a block for a given lock\n */\nstatic struct nlm_block *\nnlmsvc_lookup_block(struct nlm_file *file, struct nlm_lock *lock)\n{\n\tstruct nlm_block\t*block;\n\tstruct file_lock\t*fl;\n\n\tdprintk(\"lockd: nlmsvc_lookup_block f=%p pd=%d %Ld-%Ld ty=%d\\n\",\n\t\t\t\tfile, lock->fl.fl_pid,\n\t\t\t\t(long long)lock->fl.fl_start,\n\t\t\t\t(long long)lock->fl.fl_end, lock->fl.fl_type);\n\tlist_for_each_entry(block, &nlm_blocked, b_list) {\n\t\tfl = &block->b_call->a_args.lock.fl;\n\t\tdprintk(\"lockd: check f=%p pd=%d %Ld-%Ld ty=%d cookie=%s\\n\",\n\t\t\t\tblock->b_file, fl->fl_pid,\n\t\t\t\t(long long)fl->fl_start,\n\t\t\t\t(long long)fl->fl_end, fl->fl_type,\n\t\t\t\tnlmdbg_cookie2a(&block->b_call->a_args.cookie));\n\t\tif (block->b_file == file && nlm_compare_locks(fl, &lock->fl)) {\n\t\t\tkref_get(&block->b_count);\n\t\t\treturn block;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic inline int nlm_cookie_match(struct nlm_cookie *a, struct nlm_cookie *b)\n{\n\tif (a->len != b->len)\n\t\treturn 0;\n\tif (memcmp(a->data, b->data, a->len))\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * Find a block with a given NLM cookie.\n */\nstatic inline struct nlm_block *\nnlmsvc_find_block(struct nlm_cookie *cookie)\n{\n\tstruct nlm_block *block;\n\n\tlist_for_each_entry(block, &nlm_blocked, b_list) {\n\t\tif (nlm_cookie_match(&block->b_call->a_args.cookie,cookie))\n\t\t\tgoto found;\n\t}\n\n\treturn NULL;\n\nfound:\n\tdprintk(\"nlmsvc_find_block(%s): block=%p\\n\", nlmdbg_cookie2a(cookie), block);\n\tkref_get(&block->b_count);\n\treturn block;\n}\n\n/*\n * Create a block and initialize it.\n *\n * Note: we explicitly set the cookie of the grant reply to that of\n * the blocked lock request. The spec explicitly mentions that the client\n * should _not_ rely on the callback containing the same cookie as the\n * request, but (as I found out later) that's because some implementations\n * do just this. Never mind the standards comittees, they support our\n * logging industries.\n *\n * 10 years later: I hope we can safely ignore these old and broken\n * clients by now. Let's fix this so we can uniquely identify an incoming\n * GRANTED_RES message by cookie, without having to rely on the client's IP\n * address. --okir\n */\nstatic struct nlm_block *\nnlmsvc_create_block(struct svc_rqst *rqstp, struct nlm_host *host,\n\t\t    struct nlm_file *file, struct nlm_lock *lock,\n\t\t    struct nlm_cookie *cookie)\n{\n\tstruct nlm_block\t*block;\n\tstruct nlm_rqst\t\t*call = NULL;\n\n\tcall = nlm_alloc_call(host);\n\tif (call == NULL)\n\t\treturn NULL;\n\n\t/* Allocate memory for block, and initialize arguments */\n\tblock = kzalloc(sizeof(*block), GFP_KERNEL);\n\tif (block == NULL)\n\t\tgoto failed;\n\tkref_init(&block->b_count);\n\tINIT_LIST_HEAD(&block->b_list);\n\tINIT_LIST_HEAD(&block->b_flist);\n\n\tif (!nlmsvc_setgrantargs(call, lock))\n\t\tgoto failed_free;\n\n\t/* Set notifier function for VFS, and init args */\n\tcall->a_args.lock.fl.fl_flags |= FL_SLEEP;\n\tcall->a_args.lock.fl.fl_lmops = &nlmsvc_lock_operations;\n\tnlmclnt_next_cookie(&call->a_args.cookie);\n\n\tdprintk(\"lockd: created block %p...\\n\", block);\n\n\t/* Create and initialize the block */\n\tblock->b_daemon = rqstp->rq_server;\n\tblock->b_host   = host;\n\tblock->b_file   = file;\n\tfile->f_count++;\n\n\t/* Add to file's list of blocks */\n\tlist_add(&block->b_flist, &file->f_blocks);\n\n\t/* Set up RPC arguments for callback */\n\tblock->b_call = call;\n\tcall->a_flags   = RPC_TASK_ASYNC;\n\tcall->a_block = block;\n\n\treturn block;\n\nfailed_free:\n\tkfree(block);\nfailed:\n\tnlmsvc_release_call(call);\n\treturn NULL;\n}\n\n/*\n * Delete a block.\n * It is the caller's responsibility to check whether the file\n * can be closed hereafter.\n */\nstatic int nlmsvc_unlink_block(struct nlm_block *block)\n{\n\tint status;\n\tdprintk(\"lockd: unlinking block %p...\\n\", block);\n\n\t/* Remove block from list */\n\tstatus = posix_unblock_lock(&block->b_call->a_args.lock.fl);\n\tnlmsvc_remove_block(block);\n\treturn status;\n}\n\nstatic void nlmsvc_free_block(struct kref *kref)\n{\n\tstruct nlm_block *block = container_of(kref, struct nlm_block, b_count);\n\tstruct nlm_file\t\t*file = block->b_file;\n\n\tdprintk(\"lockd: freeing block %p...\\n\", block);\n\n\t/* Remove block from file's list of blocks */\n\tlist_del_init(&block->b_flist);\n\tmutex_unlock(&file->f_mutex);\n\n\tnlmsvc_freegrantargs(block->b_call);\n\tnlmsvc_release_call(block->b_call);\n\tnlm_release_file(block->b_file);\n\tkfree(block);\n}\n\nstatic void nlmsvc_release_block(struct nlm_block *block)\n{\n\tif (block != NULL)\n\t\tkref_put_mutex(&block->b_count, nlmsvc_free_block, &block->b_file->f_mutex);\n}\n\n/*\n * Loop over all blocks and delete blocks held by\n * a matching host.\n */\nvoid nlmsvc_traverse_blocks(struct nlm_host *host,\n\t\t\tstruct nlm_file *file,\n\t\t\tnlm_host_match_fn_t match)\n{\n\tstruct nlm_block *block, *next;\n\nrestart:\n\tmutex_lock(&file->f_mutex);\n\tlist_for_each_entry_safe(block, next, &file->f_blocks, b_flist) {\n\t\tif (!match(block->b_host, host))\n\t\t\tcontinue;\n\t\t/* Do not destroy blocks that are not on\n\t\t * the global retry list - why? */\n\t\tif (list_empty(&block->b_list))\n\t\t\tcontinue;\n\t\tkref_get(&block->b_count);\n\t\tmutex_unlock(&file->f_mutex);\n\t\tnlmsvc_unlink_block(block);\n\t\tnlmsvc_release_block(block);\n\t\tgoto restart;\n\t}\n\tmutex_unlock(&file->f_mutex);\n}\n\n/*\n * Initialize arguments for GRANTED call. The nlm_rqst structure\n * has been cleared already.\n */\nstatic int nlmsvc_setgrantargs(struct nlm_rqst *call, struct nlm_lock *lock)\n{\n\tlocks_copy_lock(&call->a_args.lock.fl, &lock->fl);\n\tmemcpy(&call->a_args.lock.fh, &lock->fh, sizeof(call->a_args.lock.fh));\n\tcall->a_args.lock.caller = utsname()->nodename;\n\tcall->a_args.lock.oh.len = lock->oh.len;\n\n\t/* set default data area */\n\tcall->a_args.lock.oh.data = call->a_owner;\n\tcall->a_args.lock.svid = lock->fl.fl_pid;\n\n\tif (lock->oh.len > NLMCLNT_OHSIZE) {\n\t\tvoid *data = kmalloc(lock->oh.len, GFP_KERNEL);\n\t\tif (!data)\n\t\t\treturn 0;\n\t\tcall->a_args.lock.oh.data = (u8 *) data;\n\t}\n\n\tmemcpy(call->a_args.lock.oh.data, lock->oh.data, lock->oh.len);\n\treturn 1;\n}\n\nstatic void nlmsvc_freegrantargs(struct nlm_rqst *call)\n{\n\tif (call->a_args.lock.oh.data != call->a_owner)\n\t\tkfree(call->a_args.lock.oh.data);\n\n\tlocks_release_private(&call->a_args.lock.fl);\n}\n\n/*\n * Deferred lock request handling for non-blocking lock\n */\nstatic __be32\nnlmsvc_defer_lock_rqst(struct svc_rqst *rqstp, struct nlm_block *block)\n{\n\t__be32 status = nlm_lck_denied_nolocks;\n\n\tblock->b_flags |= B_QUEUED;\n\n\tnlmsvc_insert_block(block, NLM_TIMEOUT);\n\n\tblock->b_cache_req = &rqstp->rq_chandle;\n\tif (rqstp->rq_chandle.defer) {\n\t\tblock->b_deferred_req =\n\t\t\trqstp->rq_chandle.defer(block->b_cache_req);\n\t\tif (block->b_deferred_req != NULL)\n\t\t\tstatus = nlm_drop_reply;\n\t}\n\tdprintk(\"lockd: nlmsvc_defer_lock_rqst block %p flags %d status %d\\n\",\n\t\tblock, block->b_flags, ntohl(status));\n\n\treturn status;\n}\n\n/*\n * Attempt to establish a lock, and if it can't be granted, block it\n * if required.\n */\n__be32\nnlmsvc_lock(struct svc_rqst *rqstp, struct nlm_file *file,\n\t    struct nlm_host *host, struct nlm_lock *lock, int wait,\n\t    struct nlm_cookie *cookie, int reclaim)\n{\n\tstruct nlm_block\t*block = NULL;\n\tint\t\t\terror;\n\t__be32\t\t\tret;\n\n\tdprintk(\"lockd: nlmsvc_lock(%s/%ld, ty=%d, pi=%d, %Ld-%Ld, bl=%d)\\n\",\n\t\t\t\tfile_inode(file->f_file)->i_sb->s_id,\n\t\t\t\tfile_inode(file->f_file)->i_ino,\n\t\t\t\tlock->fl.fl_type, lock->fl.fl_pid,\n\t\t\t\t(long long)lock->fl.fl_start,\n\t\t\t\t(long long)lock->fl.fl_end,\n\t\t\t\twait);\n\n\t/* Lock file against concurrent access */\n\tmutex_lock(&file->f_mutex);\n\t/* Get existing block (in case client is busy-waiting)\n\t * or create new block\n\t */\n\tblock = nlmsvc_lookup_block(file, lock);\n\tif (block == NULL) {\n\t\tblock = nlmsvc_create_block(rqstp, host, file, lock, cookie);\n\t\tret = nlm_lck_denied_nolocks;\n\t\tif (block == NULL)\n\t\t\tgoto out;\n\t\tlock = &block->b_call->a_args.lock;\n\t} else\n\t\tlock->fl.fl_flags &= ~FL_SLEEP;\n\n\tif (block->b_flags & B_QUEUED) {\n\t\tdprintk(\"lockd: nlmsvc_lock deferred block %p flags %d\\n\",\n\t\t\t\t\t\t\tblock, block->b_flags);\n\t\tif (block->b_granted) {\n\t\t\tnlmsvc_unlink_block(block);\n\t\t\tret = nlm_granted;\n\t\t\tgoto out;\n\t\t}\n\t\tif (block->b_flags & B_TIMED_OUT) {\n\t\t\tnlmsvc_unlink_block(block);\n\t\t\tret = nlm_lck_denied;\n\t\t\tgoto out;\n\t\t}\n\t\tret = nlm_drop_reply;\n\t\tgoto out;\n\t}\n\n\tif (locks_in_grace(SVC_NET(rqstp)) && !reclaim) {\n\t\tret = nlm_lck_denied_grace_period;\n\t\tgoto out;\n\t}\n\tif (reclaim && !locks_in_grace(SVC_NET(rqstp))) {\n\t\tret = nlm_lck_denied_grace_period;\n\t\tgoto out;\n\t}\n\n\tif (!wait)\n\t\tlock->fl.fl_flags &= ~FL_SLEEP;\n\terror = vfs_lock_file(file->f_file, F_SETLK, &lock->fl, NULL);\n\tlock->fl.fl_flags &= ~FL_SLEEP;\n\n\tdprintk(\"lockd: vfs_lock_file returned %d\\n\", error);\n\tswitch (error) {\n\t\tcase 0:\n\t\t\tret = nlm_granted;\n\t\t\tgoto out;\n\t\tcase -EAGAIN:\n\t\t\t/*\n\t\t\t * If this is a blocking request for an\n\t\t\t * already pending lock request then we need\n\t\t\t * to put it back on lockd's block list\n\t\t\t */\n\t\t\tif (wait)\n\t\t\t\tbreak;\n\t\t\tret = nlm_lck_denied;\n\t\t\tgoto out;\n\t\tcase FILE_LOCK_DEFERRED:\n\t\t\tif (wait)\n\t\t\t\tbreak;\n\t\t\t/* Filesystem lock operation is in progress\n\t\t\t   Add it to the queue waiting for callback */\n\t\t\tret = nlmsvc_defer_lock_rqst(rqstp, block);\n\t\t\tgoto out;\n\t\tcase -EDEADLK:\n\t\t\tret = nlm_deadlock;\n\t\t\tgoto out;\n\t\tdefault:\t\t\t/* includes ENOLCK */\n\t\t\tret = nlm_lck_denied_nolocks;\n\t\t\tgoto out;\n\t}\n\n\tret = nlm_lck_blocked;\n\n\t/* Append to list of blocked */\n\tnlmsvc_insert_block(block, NLM_NEVER);\nout:\n\tmutex_unlock(&file->f_mutex);\n\tnlmsvc_release_block(block);\n\tdprintk(\"lockd: nlmsvc_lock returned %u\\n\", ret);\n\treturn ret;\n}\n\n/*\n * Test for presence of a conflicting lock.\n */\n__be32\nnlmsvc_testlock(struct svc_rqst *rqstp, struct nlm_file *file,\n\t\tstruct nlm_host *host, struct nlm_lock *lock,\n\t\tstruct nlm_lock *conflock, struct nlm_cookie *cookie)\n{\n\tint\t\t\terror;\n\t__be32\t\t\tret;\n\n\tdprintk(\"lockd: nlmsvc_testlock(%s/%ld, ty=%d, %Ld-%Ld)\\n\",\n\t\t\t\tfile_inode(file->f_file)->i_sb->s_id,\n\t\t\t\tfile_inode(file->f_file)->i_ino,\n\t\t\t\tlock->fl.fl_type,\n\t\t\t\t(long long)lock->fl.fl_start,\n\t\t\t\t(long long)lock->fl.fl_end);\n\n\tif (locks_in_grace(SVC_NET(rqstp))) {\n\t\tret = nlm_lck_denied_grace_period;\n\t\tgoto out;\n\t}\n\n\terror = vfs_test_lock(file->f_file, &lock->fl);\n\tif (error) {\n\t\t/* We can't currently deal with deferred test requests */\n\t\tif (error == FILE_LOCK_DEFERRED)\n\t\t\tWARN_ON_ONCE(1);\n\n\t\tret = nlm_lck_denied_nolocks;\n\t\tgoto out;\n\t}\n\n\tif (lock->fl.fl_type == F_UNLCK) {\n\t\tret = nlm_granted;\n\t\tgoto out;\n\t}\n\n\tdprintk(\"lockd: conflicting lock(ty=%d, %Ld-%Ld)\\n\",\n\t\tlock->fl.fl_type, (long long)lock->fl.fl_start,\n\t\t(long long)lock->fl.fl_end);\n\tconflock->caller = \"somehost\";\t/* FIXME */\n\tconflock->len = strlen(conflock->caller);\n\tconflock->oh.len = 0;\t\t/* don't return OH info */\n\tconflock->svid = lock->fl.fl_pid;\n\tconflock->fl.fl_type = lock->fl.fl_type;\n\tconflock->fl.fl_start = lock->fl.fl_start;\n\tconflock->fl.fl_end = lock->fl.fl_end;\n\tlocks_release_private(&lock->fl);\n\tret = nlm_lck_denied;\nout:\n\treturn ret;\n}\n\n/*\n * Remove a lock.\n * This implies a CANCEL call: We send a GRANT_MSG, the client replies\n * with a GRANT_RES call which gets lost, and calls UNLOCK immediately\n * afterwards. In this case the block will still be there, and hence\n * must be removed.\n */\n__be32\nnlmsvc_unlock(struct net *net, struct nlm_file *file, struct nlm_lock *lock)\n{\n\tint\terror;\n\n\tdprintk(\"lockd: nlmsvc_unlock(%s/%ld, pi=%d, %Ld-%Ld)\\n\",\n\t\t\t\tfile_inode(file->f_file)->i_sb->s_id,\n\t\t\t\tfile_inode(file->f_file)->i_ino,\n\t\t\t\tlock->fl.fl_pid,\n\t\t\t\t(long long)lock->fl.fl_start,\n\t\t\t\t(long long)lock->fl.fl_end);\n\n\t/* First, cancel any lock that might be there */\n\tnlmsvc_cancel_blocked(net, file, lock);\n\n\tlock->fl.fl_type = F_UNLCK;\n\terror = vfs_lock_file(file->f_file, F_SETLK, &lock->fl, NULL);\n\n\treturn (error < 0)? nlm_lck_denied_nolocks : nlm_granted;\n}\n\n/*\n * Cancel a previously blocked request.\n *\n * A cancel request always overrides any grant that may currently\n * be in progress.\n * The calling procedure must check whether the file can be closed.\n */\n__be32\nnlmsvc_cancel_blocked(struct net *net, struct nlm_file *file, struct nlm_lock *lock)\n{\n\tstruct nlm_block\t*block;\n\tint status = 0;\n\n\tdprintk(\"lockd: nlmsvc_cancel(%s/%ld, pi=%d, %Ld-%Ld)\\n\",\n\t\t\t\tfile_inode(file->f_file)->i_sb->s_id,\n\t\t\t\tfile_inode(file->f_file)->i_ino,\n\t\t\t\tlock->fl.fl_pid,\n\t\t\t\t(long long)lock->fl.fl_start,\n\t\t\t\t(long long)lock->fl.fl_end);\n\n\tif (locks_in_grace(net))\n\t\treturn nlm_lck_denied_grace_period;\n\n\tmutex_lock(&file->f_mutex);\n\tblock = nlmsvc_lookup_block(file, lock);\n\tmutex_unlock(&file->f_mutex);\n\tif (block != NULL) {\n\t\tvfs_cancel_lock(block->b_file->f_file,\n\t\t\t\t&block->b_call->a_args.lock.fl);\n\t\tstatus = nlmsvc_unlink_block(block);\n\t\tnlmsvc_release_block(block);\n\t}\n\treturn status ? nlm_lck_denied : nlm_granted;\n}\n\n/*\n * This is a callback from the filesystem for VFS file lock requests.\n * It will be used if lm_grant is defined and the filesystem can not\n * respond to the request immediately.\n * For SETLK or SETLKW request it will get the local posix lock.\n * In all cases it will move the block to the head of nlm_blocked q where\n * nlmsvc_retry_blocked() can send back a reply for SETLKW or revisit the\n * deferred rpc for GETLK and SETLK.\n */\nstatic void\nnlmsvc_update_deferred_block(struct nlm_block *block, int result)\n{\n\tblock->b_flags |= B_GOT_CALLBACK;\n\tif (result == 0)\n\t\tblock->b_granted = 1;\n\telse\n\t\tblock->b_flags |= B_TIMED_OUT;\n}\n\nstatic int nlmsvc_grant_deferred(struct file_lock *fl, int result)\n{\n\tstruct nlm_block *block;\n\tint rc = -ENOENT;\n\n\tspin_lock(&nlm_blocked_lock);\n\tlist_for_each_entry(block, &nlm_blocked, b_list) {\n\t\tif (nlm_compare_locks(&block->b_call->a_args.lock.fl, fl)) {\n\t\t\tdprintk(\"lockd: nlmsvc_notify_blocked block %p flags %d\\n\",\n\t\t\t\t\t\t\tblock, block->b_flags);\n\t\t\tif (block->b_flags & B_QUEUED) {\n\t\t\t\tif (block->b_flags & B_TIMED_OUT) {\n\t\t\t\t\trc = -ENOLCK;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnlmsvc_update_deferred_block(block, result);\n\t\t\t} else if (result == 0)\n\t\t\t\tblock->b_granted = 1;\n\n\t\t\tnlmsvc_insert_block_locked(block, 0);\n\t\t\tsvc_wake_up(block->b_daemon);\n\t\t\trc = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&nlm_blocked_lock);\n\tif (rc == -ENOENT)\n\t\tprintk(KERN_WARNING \"lockd: grant for unknown block\\n\");\n\treturn rc;\n}\n\n/*\n * Unblock a blocked lock request. This is a callback invoked from the\n * VFS layer when a lock on which we blocked is removed.\n *\n * This function doesn't grant the blocked lock instantly, but rather moves\n * the block to the head of nlm_blocked where it can be picked up by lockd.\n */\nstatic void\nnlmsvc_notify_blocked(struct file_lock *fl)\n{\n\tstruct nlm_block\t*block;\n\n\tdprintk(\"lockd: VFS unblock notification for block %p\\n\", fl);\n\tspin_lock(&nlm_blocked_lock);\n\tlist_for_each_entry(block, &nlm_blocked, b_list) {\n\t\tif (nlm_compare_locks(&block->b_call->a_args.lock.fl, fl)) {\n\t\t\tnlmsvc_insert_block_locked(block, 0);\n\t\t\tspin_unlock(&nlm_blocked_lock);\n\t\t\tsvc_wake_up(block->b_daemon);\n\t\t\treturn;\n\t\t}\n\t}\n\tspin_unlock(&nlm_blocked_lock);\n\tprintk(KERN_WARNING \"lockd: notification for unknown block!\\n\");\n}\n\nstatic int nlmsvc_same_owner(struct file_lock *fl1, struct file_lock *fl2)\n{\n\treturn fl1->fl_owner == fl2->fl_owner && fl1->fl_pid == fl2->fl_pid;\n}\n\n/*\n * Since NLM uses two \"keys\" for tracking locks, we need to hash them down\n * to one for the blocked_hash. Here, we're just xor'ing the host address\n * with the pid in order to create a key value for picking a hash bucket.\n */\nstatic unsigned long\nnlmsvc_owner_key(struct file_lock *fl)\n{\n\treturn (unsigned long)fl->fl_owner ^ (unsigned long)fl->fl_pid;\n}\n\nconst struct lock_manager_operations nlmsvc_lock_operations = {\n\t.lm_compare_owner = nlmsvc_same_owner,\n\t.lm_owner_key = nlmsvc_owner_key,\n\t.lm_notify = nlmsvc_notify_blocked,\n\t.lm_grant = nlmsvc_grant_deferred,\n};\n\n/*\n * Try to claim a lock that was previously blocked.\n *\n * Note that we use both the RPC_GRANTED_MSG call _and_ an async\n * RPC thread when notifying the client. This seems like overkill...\n * Here's why:\n *  -\twe don't want to use a synchronous RPC thread, otherwise\n *\twe might find ourselves hanging on a dead portmapper.\n *  -\tSome lockd implementations (e.g. HP) don't react to\n *\tRPC_GRANTED calls; they seem to insist on RPC_GRANTED_MSG calls.\n */\nstatic void\nnlmsvc_grant_blocked(struct nlm_block *block)\n{\n\tstruct nlm_file\t\t*file = block->b_file;\n\tstruct nlm_lock\t\t*lock = &block->b_call->a_args.lock;\n\tint\t\t\terror;\n\tloff_t\t\t\tfl_start, fl_end;\n\n\tdprintk(\"lockd: grant blocked lock %p\\n\", block);\n\n\tkref_get(&block->b_count);\n\n\t/* Unlink block request from list */\n\tnlmsvc_unlink_block(block);\n\n\t/* If b_granted is true this means we've been here before.\n\t * Just retry the grant callback, possibly refreshing the RPC\n\t * binding */\n\tif (block->b_granted) {\n\t\tnlm_rebind_host(block->b_host);\n\t\tgoto callback;\n\t}\n\n\t/* Try the lock operation again */\n\t/* vfs_lock_file() can mangle fl_start and fl_end, but we need\n\t * them unchanged for the GRANT_MSG\n\t */\n\tlock->fl.fl_flags |= FL_SLEEP;\n\tfl_start = lock->fl.fl_start;\n\tfl_end = lock->fl.fl_end;\n\terror = vfs_lock_file(file->f_file, F_SETLK, &lock->fl, NULL);\n\tlock->fl.fl_flags &= ~FL_SLEEP;\n\tlock->fl.fl_start = fl_start;\n\tlock->fl.fl_end = fl_end;\n\n\tswitch (error) {\n\tcase 0:\n\t\tbreak;\n\tcase FILE_LOCK_DEFERRED:\n\t\tdprintk(\"lockd: lock still blocked error %d\\n\", error);\n\t\tnlmsvc_insert_block(block, NLM_NEVER);\n\t\tnlmsvc_release_block(block);\n\t\treturn;\n\tdefault:\n\t\tprintk(KERN_WARNING \"lockd: unexpected error %d in %s!\\n\",\n\t\t\t\t-error, __func__);\n\t\tnlmsvc_insert_block(block, 10 * HZ);\n\t\tnlmsvc_release_block(block);\n\t\treturn;\n\t}\n\ncallback:\n\t/* Lock was granted by VFS. */\n\tdprintk(\"lockd: GRANTing blocked lock.\\n\");\n\tblock->b_granted = 1;\n\n\t/* keep block on the list, but don't reattempt until the RPC\n\t * completes or the submission fails\n\t */\n\tnlmsvc_insert_block(block, NLM_NEVER);\n\n\t/* Call the client -- use a soft RPC task since nlmsvc_retry_blocked\n\t * will queue up a new one if this one times out\n\t */\n\terror = nlm_async_call(block->b_call, NLMPROC_GRANTED_MSG,\n\t\t\t\t&nlmsvc_grant_ops);\n\n\t/* RPC submission failed, wait a bit and retry */\n\tif (error < 0)\n\t\tnlmsvc_insert_block(block, 10 * HZ);\n}\n\n/*\n * This is the callback from the RPC layer when the NLM_GRANTED_MSG\n * RPC call has succeeded or timed out.\n * Like all RPC callbacks, it is invoked by the rpciod process, so it\n * better not sleep. Therefore, we put the blocked lock on the nlm_blocked\n * chain once more in order to have it removed by lockd itself (which can\n * then sleep on the file semaphore without disrupting e.g. the nfs client).\n */\nstatic void nlmsvc_grant_callback(struct rpc_task *task, void *data)\n{\n\tstruct nlm_rqst\t\t*call = data;\n\tstruct nlm_block\t*block = call->a_block;\n\tunsigned long\t\ttimeout;\n\n\tdprintk(\"lockd: GRANT_MSG RPC callback\\n\");\n\n\tspin_lock(&nlm_blocked_lock);\n\t/* if the block is not on a list at this point then it has\n\t * been invalidated. Don't try to requeue it.\n\t *\n\t * FIXME: it's possible that the block is removed from the list\n\t * after this check but before the nlmsvc_insert_block. In that\n\t * case it will be added back. Perhaps we need better locking\n\t * for nlm_blocked?\n\t */\n\tif (list_empty(&block->b_list))\n\t\tgoto out;\n\n\t/* Technically, we should down the file semaphore here. Since we\n\t * move the block towards the head of the queue only, no harm\n\t * can be done, though. */\n\tif (task->tk_status < 0) {\n\t\t/* RPC error: Re-insert for retransmission */\n\t\ttimeout = 10 * HZ;\n\t} else {\n\t\t/* Call was successful, now wait for client callback */\n\t\ttimeout = 60 * HZ;\n\t}\n\tnlmsvc_insert_block_locked(block, timeout);\n\tsvc_wake_up(block->b_daemon);\nout:\n\tspin_unlock(&nlm_blocked_lock);\n}\n\n/*\n * FIXME: nlmsvc_release_block() grabs a mutex.  This is not allowed for an\n * .rpc_release rpc_call_op\n */\nstatic void nlmsvc_grant_release(void *data)\n{\n\tstruct nlm_rqst\t\t*call = data;\n\tnlmsvc_release_block(call->a_block);\n}\n\nstatic const struct rpc_call_ops nlmsvc_grant_ops = {\n\t.rpc_call_done = nlmsvc_grant_callback,\n\t.rpc_release = nlmsvc_grant_release,\n};\n\n/*\n * We received a GRANT_RES callback. Try to find the corresponding\n * block.\n */\nvoid\nnlmsvc_grant_reply(struct nlm_cookie *cookie, __be32 status)\n{\n\tstruct nlm_block\t*block;\n\n\tdprintk(\"grant_reply: looking for cookie %x, s=%d \\n\",\n\t\t*(unsigned int *)(cookie->data), status);\n\tif (!(block = nlmsvc_find_block(cookie)))\n\t\treturn;\n\n\tif (block) {\n\t\tif (status == nlm_lck_denied_grace_period) {\n\t\t\t/* Try again in a couple of seconds */\n\t\t\tnlmsvc_insert_block(block, 10 * HZ);\n\t\t} else {\n\t\t\t/* Lock is now held by client, or has been rejected.\n\t\t\t * In both cases, the block should be removed. */\n\t\t\tnlmsvc_unlink_block(block);\n\t\t}\n\t}\n\tnlmsvc_release_block(block);\n}\n\n/* Helper function to handle retry of a deferred block.\n * If it is a blocking lock, call grant_blocked.\n * For a non-blocking lock or test lock, revisit the request.\n */\nstatic void\nretry_deferred_block(struct nlm_block *block)\n{\n\tif (!(block->b_flags & B_GOT_CALLBACK))\n\t\tblock->b_flags |= B_TIMED_OUT;\n\tnlmsvc_insert_block(block, NLM_TIMEOUT);\n\tdprintk(\"revisit block %p flags %d\\n\",\tblock, block->b_flags);\n\tif (block->b_deferred_req) {\n\t\tblock->b_deferred_req->revisit(block->b_deferred_req, 0);\n\t\tblock->b_deferred_req = NULL;\n\t}\n}\n\n/*\n * Retry all blocked locks that have been notified. This is where lockd\n * picks up locks that can be granted, or grant notifications that must\n * be retransmitted.\n */\nunsigned long\nnlmsvc_retry_blocked(void)\n{\n\tunsigned long\ttimeout = MAX_SCHEDULE_TIMEOUT;\n\tstruct nlm_block *block;\n\n\tspin_lock(&nlm_blocked_lock);\n\twhile (!list_empty(&nlm_blocked) && !kthread_should_stop()) {\n\t\tblock = list_entry(nlm_blocked.next, struct nlm_block, b_list);\n\n\t\tif (block->b_when == NLM_NEVER)\n\t\t\tbreak;\n\t\tif (time_after(block->b_when, jiffies)) {\n\t\t\ttimeout = block->b_when - jiffies;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&nlm_blocked_lock);\n\n\t\tdprintk(\"nlmsvc_retry_blocked(%p, when=%ld)\\n\",\n\t\t\tblock, block->b_when);\n\t\tif (block->b_flags & B_QUEUED) {\n\t\t\tdprintk(\"nlmsvc_retry_blocked delete block (%p, granted=%d, flags=%d)\\n\",\n\t\t\t\tblock, block->b_granted, block->b_flags);\n\t\t\tretry_deferred_block(block);\n\t\t} else\n\t\t\tnlmsvc_grant_blocked(block);\n\t\tspin_lock(&nlm_blocked_lock);\n\t}\n\tspin_unlock(&nlm_blocked_lock);\n\n\treturn timeout;\n}\n", "/*\n * linux/fs/nfs/callback.c\n *\n * Copyright (C) 2004 Trond Myklebust\n *\n * NFSv4 callback handling\n */\n\n#include <linux/completion.h>\n#include <linux/ip.h>\n#include <linux/module.h>\n#include <linux/sched/signal.h>\n#include <linux/sunrpc/svc.h>\n#include <linux/sunrpc/svcsock.h>\n#include <linux/nfs_fs.h>\n#include <linux/errno.h>\n#include <linux/mutex.h>\n#include <linux/freezer.h>\n#include <linux/kthread.h>\n#include <linux/sunrpc/svcauth_gss.h>\n#include <linux/sunrpc/bc_xprt.h>\n\n#include <net/inet_sock.h>\n\n#include \"nfs4_fs.h\"\n#include \"callback.h\"\n#include \"internal.h\"\n#include \"netns.h\"\n\n#define NFSDBG_FACILITY NFSDBG_CALLBACK\n\nstruct nfs_callback_data {\n\tunsigned int users;\n\tstruct svc_serv *serv;\n};\n\nstatic struct nfs_callback_data nfs_callback_info[NFS4_MAX_MINOR_VERSION + 1];\nstatic DEFINE_MUTEX(nfs_callback_mutex);\nstatic struct svc_program nfs4_callback_program;\n\nstatic int nfs4_callback_up_net(struct svc_serv *serv, struct net *net)\n{\n\tint ret;\n\tstruct nfs_net *nn = net_generic(net, nfs_net_id);\n\n\tret = svc_create_xprt(serv, \"tcp\", net, PF_INET,\n\t\t\t\tnfs_callback_set_tcpport, SVC_SOCK_ANONYMOUS);\n\tif (ret <= 0)\n\t\tgoto out_err;\n\tnn->nfs_callback_tcpport = ret;\n\tdprintk(\"NFS: Callback listener port = %u (af %u, net %p)\\n\",\n\t\t\tnn->nfs_callback_tcpport, PF_INET, net);\n\n\tret = svc_create_xprt(serv, \"tcp\", net, PF_INET6,\n\t\t\t\tnfs_callback_set_tcpport, SVC_SOCK_ANONYMOUS);\n\tif (ret > 0) {\n\t\tnn->nfs_callback_tcpport6 = ret;\n\t\tdprintk(\"NFS: Callback listener port = %u (af %u, net %p)\\n\",\n\t\t\t\tnn->nfs_callback_tcpport6, PF_INET6, net);\n\t} else if (ret != -EAFNOSUPPORT)\n\t\tgoto out_err;\n\treturn 0;\n\nout_err:\n\treturn (ret) ? ret : -ENOMEM;\n}\n\n/*\n * This is the NFSv4 callback kernel thread.\n */\nstatic int\nnfs4_callback_svc(void *vrqstp)\n{\n\tint err;\n\tstruct svc_rqst *rqstp = vrqstp;\n\n\tset_freezable();\n\n\twhile (!kthread_should_stop()) {\n\t\t/*\n\t\t * Listen for a request on the socket\n\t\t */\n\t\terr = svc_recv(rqstp, MAX_SCHEDULE_TIMEOUT);\n\t\tif (err == -EAGAIN || err == -EINTR)\n\t\t\tcontinue;\n\t\tsvc_process(rqstp);\n\t}\n\treturn 0;\n}\n\n#if defined(CONFIG_NFS_V4_1)\n/*\n * The callback service for NFSv4.1 callbacks\n */\nstatic int\nnfs41_callback_svc(void *vrqstp)\n{\n\tstruct svc_rqst *rqstp = vrqstp;\n\tstruct svc_serv *serv = rqstp->rq_server;\n\tstruct rpc_rqst *req;\n\tint error;\n\tDEFINE_WAIT(wq);\n\n\tset_freezable();\n\n\twhile (!kthread_should_stop()) {\n\t\tif (try_to_freeze())\n\t\t\tcontinue;\n\n\t\tprepare_to_wait(&serv->sv_cb_waitq, &wq, TASK_INTERRUPTIBLE);\n\t\tspin_lock_bh(&serv->sv_cb_lock);\n\t\tif (!list_empty(&serv->sv_cb_list)) {\n\t\t\treq = list_first_entry(&serv->sv_cb_list,\n\t\t\t\t\tstruct rpc_rqst, rq_bc_list);\n\t\t\tlist_del(&req->rq_bc_list);\n\t\t\tspin_unlock_bh(&serv->sv_cb_lock);\n\t\t\tfinish_wait(&serv->sv_cb_waitq, &wq);\n\t\t\tdprintk(\"Invoking bc_svc_process()\\n\");\n\t\t\terror = bc_svc_process(serv, req, rqstp);\n\t\t\tdprintk(\"bc_svc_process() returned w/ error code= %d\\n\",\n\t\t\t\terror);\n\t\t} else {\n\t\t\tspin_unlock_bh(&serv->sv_cb_lock);\n\t\t\tschedule();\n\t\t\tfinish_wait(&serv->sv_cb_waitq, &wq);\n\t\t}\n\t\tflush_signals(current);\n\t}\n\treturn 0;\n}\n\nstatic inline void nfs_callback_bc_serv(u32 minorversion, struct rpc_xprt *xprt,\n\t\tstruct svc_serv *serv)\n{\n\tif (minorversion)\n\t\t/*\n\t\t * Save the svc_serv in the transport so that it can\n\t\t * be referenced when the session backchannel is initialized\n\t\t */\n\t\txprt->bc_serv = serv;\n}\n#else\nstatic inline void nfs_callback_bc_serv(u32 minorversion, struct rpc_xprt *xprt,\n\t\tstruct svc_serv *serv)\n{\n}\n#endif /* CONFIG_NFS_V4_1 */\n\nstatic int nfs_callback_start_svc(int minorversion, struct rpc_xprt *xprt,\n\t\t\t\t  struct svc_serv *serv)\n{\n\tint nrservs = nfs_callback_nr_threads;\n\tint ret;\n\n\tnfs_callback_bc_serv(minorversion, xprt, serv);\n\n\tif (nrservs < NFS4_MIN_NR_CALLBACK_THREADS)\n\t\tnrservs = NFS4_MIN_NR_CALLBACK_THREADS;\n\n\tif (serv->sv_nrthreads-1 == nrservs)\n\t\treturn 0;\n\n\tret = serv->sv_ops->svo_setup(serv, NULL, nrservs);\n\tif (ret) {\n\t\tserv->sv_ops->svo_setup(serv, NULL, 0);\n\t\treturn ret;\n\t}\n\tdprintk(\"nfs_callback_up: service started\\n\");\n\treturn 0;\n}\n\nstatic void nfs_callback_down_net(u32 minorversion, struct svc_serv *serv, struct net *net)\n{\n\tstruct nfs_net *nn = net_generic(net, nfs_net_id);\n\n\tif (--nn->cb_users[minorversion])\n\t\treturn;\n\n\tdprintk(\"NFS: destroy per-net callback data; net=%p\\n\", net);\n\tsvc_shutdown_net(serv, net);\n}\n\nstatic int nfs_callback_up_net(int minorversion, struct svc_serv *serv,\n\t\t\t       struct net *net, struct rpc_xprt *xprt)\n{\n\tstruct nfs_net *nn = net_generic(net, nfs_net_id);\n\tint ret;\n\n\tif (nn->cb_users[minorversion]++)\n\t\treturn 0;\n\n\tdprintk(\"NFS: create per-net callback data; net=%p\\n\", net);\n\n\tret = svc_bind(serv, net);\n\tif (ret < 0) {\n\t\tprintk(KERN_WARNING \"NFS: bind callback service failed\\n\");\n\t\tgoto err_bind;\n\t}\n\n\tret = -EPROTONOSUPPORT;\n\tif (!IS_ENABLED(CONFIG_NFS_V4_1) || minorversion == 0)\n\t\tret = nfs4_callback_up_net(serv, net);\n\telse if (xprt->ops->bc_up)\n\t\tret = xprt->ops->bc_up(serv, net);\n\n\tif (ret < 0) {\n\t\tprintk(KERN_ERR \"NFS: callback service start failed\\n\");\n\t\tgoto err_socks;\n\t}\n\treturn 0;\n\nerr_socks:\n\tsvc_rpcb_cleanup(serv, net);\nerr_bind:\n\tnn->cb_users[minorversion]--;\n\tdprintk(\"NFS: Couldn't create callback socket: err = %d; \"\n\t\t\t\"net = %p\\n\", ret, net);\n\treturn ret;\n}\n\nstatic struct svc_serv_ops nfs40_cb_sv_ops = {\n\t.svo_function\t\t= nfs4_callback_svc,\n\t.svo_enqueue_xprt\t= svc_xprt_do_enqueue,\n\t.svo_setup\t\t= svc_set_num_threads,\n\t.svo_module\t\t= THIS_MODULE,\n};\n#if defined(CONFIG_NFS_V4_1)\nstatic struct svc_serv_ops nfs41_cb_sv_ops = {\n\t.svo_function\t\t= nfs41_callback_svc,\n\t.svo_enqueue_xprt\t= svc_xprt_do_enqueue,\n\t.svo_setup\t\t= svc_set_num_threads,\n\t.svo_module\t\t= THIS_MODULE,\n};\n\nstatic struct svc_serv_ops *nfs4_cb_sv_ops[] = {\n\t[0] = &nfs40_cb_sv_ops,\n\t[1] = &nfs41_cb_sv_ops,\n};\n#else\nstatic struct svc_serv_ops *nfs4_cb_sv_ops[] = {\n\t[0] = &nfs40_cb_sv_ops,\n\t[1] = NULL,\n};\n#endif\n\nstatic struct svc_serv *nfs_callback_create_svc(int minorversion)\n{\n\tstruct nfs_callback_data *cb_info = &nfs_callback_info[minorversion];\n\tstruct svc_serv *serv;\n\tstruct svc_serv_ops *sv_ops;\n\n\t/*\n\t * Check whether we're already up and running.\n\t */\n\tif (cb_info->serv) {\n\t\t/*\n\t\t * Note: increase service usage, because later in case of error\n\t\t * svc_destroy() will be called.\n\t\t */\n\t\tsvc_get(cb_info->serv);\n\t\treturn cb_info->serv;\n\t}\n\n\tswitch (minorversion) {\n\tcase 0:\n\t\tsv_ops = nfs4_cb_sv_ops[0];\n\t\tbreak;\n\tdefault:\n\t\tsv_ops = nfs4_cb_sv_ops[1];\n\t}\n\n\tif (sv_ops == NULL)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\t/*\n\t * Sanity check: if there's no task,\n\t * we should be the first user ...\n\t */\n\tif (cb_info->users)\n\t\tprintk(KERN_WARNING \"nfs_callback_create_svc: no kthread, %d users??\\n\",\n\t\t\tcb_info->users);\n\n\tserv = svc_create(&nfs4_callback_program, NFS4_CALLBACK_BUFSIZE, sv_ops);\n\tif (!serv) {\n\t\tprintk(KERN_ERR \"nfs_callback_create_svc: create service failed\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tcb_info->serv = serv;\n\t/* As there is only one thread we need to over-ride the\n\t * default maximum of 80 connections\n\t */\n\tserv->sv_maxconn = 1024;\n\tdprintk(\"nfs_callback_create_svc: service created\\n\");\n\treturn serv;\n}\n\n/*\n * Bring up the callback thread if it is not already up.\n */\nint nfs_callback_up(u32 minorversion, struct rpc_xprt *xprt)\n{\n\tstruct svc_serv *serv;\n\tstruct nfs_callback_data *cb_info = &nfs_callback_info[minorversion];\n\tint ret;\n\tstruct net *net = xprt->xprt_net;\n\n\tmutex_lock(&nfs_callback_mutex);\n\n\tserv = nfs_callback_create_svc(minorversion);\n\tif (IS_ERR(serv)) {\n\t\tret = PTR_ERR(serv);\n\t\tgoto err_create;\n\t}\n\n\tret = nfs_callback_up_net(minorversion, serv, net, xprt);\n\tif (ret < 0)\n\t\tgoto err_net;\n\n\tret = nfs_callback_start_svc(minorversion, xprt, serv);\n\tif (ret < 0)\n\t\tgoto err_start;\n\n\tcb_info->users++;\n\t/*\n\t * svc_create creates the svc_serv with sv_nrthreads == 1, and then\n\t * svc_prepare_thread increments that. So we need to call svc_destroy\n\t * on both success and failure so that the refcount is 1 when the\n\t * thread exits.\n\t */\nerr_net:\n\tif (!cb_info->users)\n\t\tcb_info->serv = NULL;\n\tsvc_destroy(serv);\nerr_create:\n\tmutex_unlock(&nfs_callback_mutex);\n\treturn ret;\n\nerr_start:\n\tnfs_callback_down_net(minorversion, serv, net);\n\tdprintk(\"NFS: Couldn't create server thread; err = %d\\n\", ret);\n\tgoto err_net;\n}\n\n/*\n * Kill the callback thread if it's no longer being used.\n */\nvoid nfs_callback_down(int minorversion, struct net *net)\n{\n\tstruct nfs_callback_data *cb_info = &nfs_callback_info[minorversion];\n\tstruct svc_serv *serv;\n\n\tmutex_lock(&nfs_callback_mutex);\n\tserv = cb_info->serv;\n\tnfs_callback_down_net(minorversion, serv, net);\n\tcb_info->users--;\n\tif (cb_info->users == 0) {\n\t\tsvc_get(serv);\n\t\tserv->sv_ops->svo_setup(serv, NULL, 0);\n\t\tsvc_destroy(serv);\n\t\tdprintk(\"nfs_callback_down: service destroyed\\n\");\n\t\tcb_info->serv = NULL;\n\t}\n\tmutex_unlock(&nfs_callback_mutex);\n}\n\n/* Boolean check of RPC_AUTH_GSS principal */\nint\ncheck_gss_callback_principal(struct nfs_client *clp, struct svc_rqst *rqstp)\n{\n\tchar *p = rqstp->rq_cred.cr_principal;\n\n\tif (rqstp->rq_authop->flavour != RPC_AUTH_GSS)\n\t\treturn 1;\n\n\t/* No RPC_AUTH_GSS on NFSv4.1 back channel yet */\n\tif (clp->cl_minorversion != 0)\n\t\treturn 0;\n\t/*\n\t * It might just be a normal user principal, in which case\n\t * userspace won't bother to tell us the name at all.\n\t */\n\tif (p == NULL)\n\t\treturn 0;\n\n\t/*\n\t * Did we get the acceptor from userland during the SETCLIENID\n\t * negotiation?\n\t */\n\tif (clp->cl_acceptor)\n\t\treturn !strcmp(p, clp->cl_acceptor);\n\n\t/*\n\t * Otherwise try to verify it using the cl_hostname. Note that this\n\t * doesn't work if a non-canonical hostname was used in the devname.\n\t */\n\n\t/* Expect a GSS_C_NT_HOSTBASED_NAME like \"nfs@serverhostname\" */\n\n\tif (memcmp(p, \"nfs@\", 4) != 0)\n\t\treturn 0;\n\tp += 4;\n\tif (strcmp(p, clp->cl_hostname) != 0)\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * pg_authenticate method for nfsv4 callback threads.\n *\n * The authflavor has been negotiated, so an incorrect flavor is a server\n * bug. Deny packets with incorrect authflavor.\n *\n * All other checking done after NFS decoding where the nfs_client can be\n * found in nfs4_callback_compound\n */\nstatic int nfs_callback_authenticate(struct svc_rqst *rqstp)\n{\n\tswitch (rqstp->rq_authop->flavour) {\n\tcase RPC_AUTH_NULL:\n\t\tif (rqstp->rq_proc != CB_NULL)\n\t\t\treturn SVC_DENIED;\n\t\tbreak;\n\tcase RPC_AUTH_GSS:\n\t\t/* No RPC_AUTH_GSS support yet in NFSv4.1 */\n\t\t if (svc_is_backchannel(rqstp))\n\t\t\treturn SVC_DENIED;\n\t}\n\treturn SVC_OK;\n}\n\n/*\n * Define NFS4 callback program\n */\nstatic struct svc_version *nfs4_callback_version[] = {\n\t[1] = &nfs4_callback_version1,\n\t[4] = &nfs4_callback_version4,\n};\n\nstatic struct svc_stat nfs4_callback_stats;\n\nstatic struct svc_program nfs4_callback_program = {\n\t.pg_prog = NFS4_CALLBACK,\t\t\t/* RPC service number */\n\t.pg_nvers = ARRAY_SIZE(nfs4_callback_version),\t/* Number of entries */\n\t.pg_vers = nfs4_callback_version,\t\t/* version table */\n\t.pg_name = \"NFSv4 callback\",\t\t\t/* service name */\n\t.pg_class = \"nfs\",\t\t\t\t/* authentication class */\n\t.pg_stats = &nfs4_callback_stats,\n\t.pg_authenticate = nfs_callback_authenticate,\n};\n", "/*\n * XDR support for nfsd/protocol version 3.\n *\n * Copyright (C) 1995, 1996, 1997 Olaf Kirch <okir@monad.swb.de>\n *\n * 2003-08-09 Jamie Lokier: Use htonl() for nanoseconds, not htons()!\n */\n\n#include <linux/namei.h>\n#include <linux/sunrpc/svc_xprt.h>\n#include \"xdr3.h\"\n#include \"auth.h\"\n#include \"netns.h\"\n#include \"vfs.h\"\n\n#define NFSDDBG_FACILITY\t\tNFSDDBG_XDR\n\n\n/*\n * Mapping of S_IF* types to NFS file types\n */\nstatic u32\tnfs3_ftypes[] = {\n\tNF3NON,  NF3FIFO, NF3CHR, NF3BAD,\n\tNF3DIR,  NF3BAD,  NF3BLK, NF3BAD,\n\tNF3REG,  NF3BAD,  NF3LNK, NF3BAD,\n\tNF3SOCK, NF3BAD,  NF3LNK, NF3BAD,\n};\n\n/*\n * XDR functions for basic NFS types\n */\nstatic __be32 *\nencode_time3(__be32 *p, struct timespec *time)\n{\n\t*p++ = htonl((u32) time->tv_sec); *p++ = htonl(time->tv_nsec);\n\treturn p;\n}\n\nstatic __be32 *\ndecode_time3(__be32 *p, struct timespec *time)\n{\n\ttime->tv_sec = ntohl(*p++);\n\ttime->tv_nsec = ntohl(*p++);\n\treturn p;\n}\n\nstatic __be32 *\ndecode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\tunsigned int size;\n\tfh_init(fhp, NFS3_FHSIZE);\n\tsize = ntohl(*p++);\n\tif (size > NFS3_FHSIZE)\n\t\treturn NULL;\n\n\tmemcpy(&fhp->fh_handle.fh_base, p, size);\n\tfhp->fh_handle.fh_size = size;\n\treturn p + XDR_QUADLEN(size);\n}\n\n/* Helper function for NFSv3 ACL code */\n__be32 *nfs3svc_decode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\treturn decode_fh(p, fhp);\n}\n\nstatic __be32 *\nencode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\tunsigned int size = fhp->fh_handle.fh_size;\n\t*p++ = htonl(size);\n\tif (size) p[XDR_QUADLEN(size)-1]=0;\n\tmemcpy(p, &fhp->fh_handle.fh_base, size);\n\treturn p + XDR_QUADLEN(size);\n}\n\n/*\n * Decode a file name and make sure that the path contains\n * no slashes or null bytes.\n */\nstatic __be32 *\ndecode_filename(__be32 *p, char **namp, unsigned int *lenp)\n{\n\tchar\t\t*name;\n\tunsigned int\ti;\n\n\tif ((p = xdr_decode_string_inplace(p, namp, lenp, NFS3_MAXNAMLEN)) != NULL) {\n\t\tfor (i = 0, name = *namp; i < *lenp; i++, name++) {\n\t\t\tif (*name == '\\0' || *name == '/')\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn p;\n}\n\nstatic __be32 *\ndecode_sattr3(__be32 *p, struct iattr *iap)\n{\n\tu32\ttmp;\n\n\tiap->ia_valid = 0;\n\n\tif (*p++) {\n\t\tiap->ia_valid |= ATTR_MODE;\n\t\tiap->ia_mode = ntohl(*p++);\n\t}\n\tif (*p++) {\n\t\tiap->ia_uid = make_kuid(&init_user_ns, ntohl(*p++));\n\t\tif (uid_valid(iap->ia_uid))\n\t\t\tiap->ia_valid |= ATTR_UID;\n\t}\n\tif (*p++) {\n\t\tiap->ia_gid = make_kgid(&init_user_ns, ntohl(*p++));\n\t\tif (gid_valid(iap->ia_gid))\n\t\t\tiap->ia_valid |= ATTR_GID;\n\t}\n\tif (*p++) {\n\t\tu64\tnewsize;\n\n\t\tiap->ia_valid |= ATTR_SIZE;\n\t\tp = xdr_decode_hyper(p, &newsize);\n\t\tiap->ia_size = min_t(u64, newsize, NFS_OFFSET_MAX);\n\t}\n\tif ((tmp = ntohl(*p++)) == 1) {\t/* set to server time */\n\t\tiap->ia_valid |= ATTR_ATIME;\n\t} else if (tmp == 2) {\t\t/* set to client time */\n\t\tiap->ia_valid |= ATTR_ATIME | ATTR_ATIME_SET;\n\t\tiap->ia_atime.tv_sec = ntohl(*p++);\n\t\tiap->ia_atime.tv_nsec = ntohl(*p++);\n\t}\n\tif ((tmp = ntohl(*p++)) == 1) {\t/* set to server time */\n\t\tiap->ia_valid |= ATTR_MTIME;\n\t} else if (tmp == 2) {\t\t/* set to client time */\n\t\tiap->ia_valid |= ATTR_MTIME | ATTR_MTIME_SET;\n\t\tiap->ia_mtime.tv_sec = ntohl(*p++);\n\t\tiap->ia_mtime.tv_nsec = ntohl(*p++);\n\t}\n\treturn p;\n}\n\nstatic __be32 *encode_fsid(__be32 *p, struct svc_fh *fhp)\n{\n\tu64 f;\n\tswitch(fsid_source(fhp)) {\n\tdefault:\n\tcase FSIDSOURCE_DEV:\n\t\tp = xdr_encode_hyper(p, (u64)huge_encode_dev\n\t\t\t\t     (fhp->fh_dentry->d_sb->s_dev));\n\t\tbreak;\n\tcase FSIDSOURCE_FSID:\n\t\tp = xdr_encode_hyper(p, (u64) fhp->fh_export->ex_fsid);\n\t\tbreak;\n\tcase FSIDSOURCE_UUID:\n\t\tf = ((u64*)fhp->fh_export->ex_uuid)[0];\n\t\tf ^= ((u64*)fhp->fh_export->ex_uuid)[1];\n\t\tp = xdr_encode_hyper(p, f);\n\t\tbreak;\n\t}\n\treturn p;\n}\n\nstatic __be32 *\nencode_fattr3(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp,\n\t      struct kstat *stat)\n{\n\t*p++ = htonl(nfs3_ftypes[(stat->mode & S_IFMT) >> 12]);\n\t*p++ = htonl((u32) (stat->mode & S_IALLUGO));\n\t*p++ = htonl((u32) stat->nlink);\n\t*p++ = htonl((u32) from_kuid(&init_user_ns, stat->uid));\n\t*p++ = htonl((u32) from_kgid(&init_user_ns, stat->gid));\n\tif (S_ISLNK(stat->mode) && stat->size > NFS3_MAXPATHLEN) {\n\t\tp = xdr_encode_hyper(p, (u64) NFS3_MAXPATHLEN);\n\t} else {\n\t\tp = xdr_encode_hyper(p, (u64) stat->size);\n\t}\n\tp = xdr_encode_hyper(p, ((u64)stat->blocks) << 9);\n\t*p++ = htonl((u32) MAJOR(stat->rdev));\n\t*p++ = htonl((u32) MINOR(stat->rdev));\n\tp = encode_fsid(p, fhp);\n\tp = xdr_encode_hyper(p, stat->ino);\n\tp = encode_time3(p, &stat->atime);\n\tp = encode_time3(p, &stat->mtime);\n\tp = encode_time3(p, &stat->ctime);\n\n\treturn p;\n}\n\nstatic __be32 *\nencode_saved_post_attr(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp)\n{\n\t/* Attributes to follow */\n\t*p++ = xdr_one;\n\treturn encode_fattr3(rqstp, p, fhp, &fhp->fh_post_attr);\n}\n\n/*\n * Encode post-operation attributes.\n * The inode may be NULL if the call failed because of a stale file\n * handle. In this case, no attributes are returned.\n */\nstatic __be32 *\nencode_post_op_attr(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp)\n{\n\tstruct dentry *dentry = fhp->fh_dentry;\n\tif (dentry && d_really_is_positive(dentry)) {\n\t        __be32 err;\n\t\tstruct kstat stat;\n\n\t\terr = fh_getattr(fhp, &stat);\n\t\tif (!err) {\n\t\t\t*p++ = xdr_one;\t\t/* attributes follow */\n\t\t\tlease_get_mtime(d_inode(dentry), &stat.mtime);\n\t\t\treturn encode_fattr3(rqstp, p, fhp, &stat);\n\t\t}\n\t}\n\t*p++ = xdr_zero;\n\treturn p;\n}\n\n/* Helper for NFSv3 ACLs */\n__be32 *\nnfs3svc_encode_post_op_attr(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp)\n{\n\treturn encode_post_op_attr(rqstp, p, fhp);\n}\n\n/*\n * Enocde weak cache consistency data\n */\nstatic __be32 *\nencode_wcc_data(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp)\n{\n\tstruct dentry\t*dentry = fhp->fh_dentry;\n\n\tif (dentry && d_really_is_positive(dentry) && fhp->fh_post_saved) {\n\t\tif (fhp->fh_pre_saved) {\n\t\t\t*p++ = xdr_one;\n\t\t\tp = xdr_encode_hyper(p, (u64) fhp->fh_pre_size);\n\t\t\tp = encode_time3(p, &fhp->fh_pre_mtime);\n\t\t\tp = encode_time3(p, &fhp->fh_pre_ctime);\n\t\t} else {\n\t\t\t*p++ = xdr_zero;\n\t\t}\n\t\treturn encode_saved_post_attr(rqstp, p, fhp);\n\t}\n\t/* no pre- or post-attrs */\n\t*p++ = xdr_zero;\n\treturn encode_post_op_attr(rqstp, p, fhp);\n}\n\n/*\n * Fill in the post_op attr for the wcc data\n */\nvoid fill_post_wcc(struct svc_fh *fhp)\n{\n\t__be32 err;\n\n\tif (fhp->fh_post_saved)\n\t\tprintk(\"nfsd: inode locked twice during operation.\\n\");\n\n\terr = fh_getattr(fhp, &fhp->fh_post_attr);\n\tfhp->fh_post_change = d_inode(fhp->fh_dentry)->i_version;\n\tif (err) {\n\t\tfhp->fh_post_saved = false;\n\t\t/* Grab the ctime anyway - set_change_info might use it */\n\t\tfhp->fh_post_attr.ctime = d_inode(fhp->fh_dentry)->i_ctime;\n\t} else\n\t\tfhp->fh_post_saved = true;\n}\n\n/*\n * XDR decode functions\n */\nint\nnfs3svc_decode_fhandle(struct svc_rqst *rqstp, __be32 *p, struct nfsd_fhandle *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_sattrargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_sattrargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = decode_sattr3(p, &args->attrs);\n\n\tif ((args->check_guard = ntohl(*p++)) != 0) { \n\t\tstruct timespec time; \n\t\tp = decode_time3(p, &time);\n\t\targs->guardtime = time.tv_sec;\n\t}\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_diropargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_diropargs *args)\n{\n\tif (!(p = decode_fh(p, &args->fh))\n\t || !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_accessargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_accessargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\targs->access = ntohl(*p++);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_readargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readargs *args)\n{\n\tunsigned int len;\n\tint v;\n\tu32 max_blocksize = svc_max_payload(rqstp);\n\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = xdr_decode_hyper(p, &args->offset);\n\n\targs->count = ntohl(*p++);\n\tlen = min(args->count, max_blocksize);\n\n\t/* set up the kvec */\n\tv=0;\n\twhile (len > 0) {\n\t\tstruct page *p = *(rqstp->rq_next_page++);\n\n\t\trqstp->rq_vec[v].iov_base = page_address(p);\n\t\trqstp->rq_vec[v].iov_len = min_t(unsigned int, len, PAGE_SIZE);\n\t\tlen -= rqstp->rq_vec[v].iov_len;\n\t\tv++;\n\t}\n\targs->vlen = v;\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_writeargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_writeargs *args)\n{\n\tunsigned int len, v, hdr, dlen;\n\tu32 max_blocksize = svc_max_payload(rqstp);\n\tstruct kvec *head = rqstp->rq_arg.head;\n\tstruct kvec *tail = rqstp->rq_arg.tail;\n\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = xdr_decode_hyper(p, &args->offset);\n\n\targs->count = ntohl(*p++);\n\targs->stable = ntohl(*p++);\n\tlen = args->len = ntohl(*p++);\n\tif ((void *)p > head->iov_base + head->iov_len)\n\t\treturn 0;\n\t/*\n\t * The count must equal the amount of data passed.\n\t */\n\tif (args->count != args->len)\n\t\treturn 0;\n\n\t/*\n\t * Check to make sure that we got the right number of\n\t * bytes.\n\t */\n\thdr = (void*)p - head->iov_base;\n\tdlen = head->iov_len + rqstp->rq_arg.page_len + tail->iov_len - hdr;\n\t/*\n\t * Round the length of the data which was specified up to\n\t * the next multiple of XDR units and then compare that\n\t * against the length which was actually received.\n\t * Note that when RPCSEC/GSS (for example) is used, the\n\t * data buffer can be padded so dlen might be larger\n\t * than required.  It must never be smaller.\n\t */\n\tif (dlen < XDR_QUADLEN(len)*4)\n\t\treturn 0;\n\n\tif (args->count > max_blocksize) {\n\t\targs->count = max_blocksize;\n\t\tlen = args->len = max_blocksize;\n\t}\n\trqstp->rq_vec[0].iov_base = (void*)p;\n\trqstp->rq_vec[0].iov_len = head->iov_len - hdr;\n\tv = 0;\n\twhile (len > rqstp->rq_vec[v].iov_len) {\n\t\tlen -= rqstp->rq_vec[v].iov_len;\n\t\tv++;\n\t\trqstp->rq_vec[v].iov_base = page_address(rqstp->rq_pages[v]);\n\t\trqstp->rq_vec[v].iov_len = PAGE_SIZE;\n\t}\n\trqstp->rq_vec[v].iov_len = len;\n\targs->vlen = v + 1;\n\treturn 1;\n}\n\nint\nnfs3svc_decode_createargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_createargs *args)\n{\n\tif (!(p = decode_fh(p, &args->fh))\n\t || !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\n\tswitch (args->createmode = ntohl(*p++)) {\n\tcase NFS3_CREATE_UNCHECKED:\n\tcase NFS3_CREATE_GUARDED:\n\t\tp = decode_sattr3(p, &args->attrs);\n\t\tbreak;\n\tcase NFS3_CREATE_EXCLUSIVE:\n\t\targs->verf = p;\n\t\tp += 2;\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treturn xdr_argsize_check(rqstp, p);\n}\nint\nnfs3svc_decode_mkdirargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_createargs *args)\n{\n\tif (!(p = decode_fh(p, &args->fh)) ||\n\t    !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\tp = decode_sattr3(p, &args->attrs);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_symlinkargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_symlinkargs *args)\n{\n\tunsigned int len, avail;\n\tchar *old, *new;\n\tstruct kvec *vec;\n\n\tif (!(p = decode_fh(p, &args->ffh)) ||\n\t    !(p = decode_filename(p, &args->fname, &args->flen))\n\t\t)\n\t\treturn 0;\n\tp = decode_sattr3(p, &args->attrs);\n\n\t/* now decode the pathname, which might be larger than the first page.\n\t * As we have to check for nul's anyway, we copy it into a new page\n\t * This page appears in the rq_res.pages list, but as pages_len is always\n\t * 0, it won't get in the way\n\t */\n\tlen = ntohl(*p++);\n\tif (len == 0 || len > NFS3_MAXPATHLEN || len >= PAGE_SIZE)\n\t\treturn 0;\n\targs->tname = new = page_address(*(rqstp->rq_next_page++));\n\targs->tlen = len;\n\t/* first copy and check from the first page */\n\told = (char*)p;\n\tvec = &rqstp->rq_arg.head[0];\n\tif ((void *)old > vec->iov_base + vec->iov_len)\n\t\treturn 0;\n\tavail = vec->iov_len - (old - (char*)vec->iov_base);\n\twhile (len && avail && *old) {\n\t\t*new++ = *old++;\n\t\tlen--;\n\t\tavail--;\n\t}\n\t/* now copy next page if there is one */\n\tif (len && !avail && rqstp->rq_arg.page_len) {\n\t\tavail = min_t(unsigned int, rqstp->rq_arg.page_len, PAGE_SIZE);\n\t\told = page_address(rqstp->rq_arg.pages[0]);\n\t}\n\twhile (len && avail && *old) {\n\t\t*new++ = *old++;\n\t\tlen--;\n\t\tavail--;\n\t}\n\t*new = '\\0';\n\tif (len)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nint\nnfs3svc_decode_mknodargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_mknodargs *args)\n{\n\tif (!(p = decode_fh(p, &args->fh))\n\t || !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\n\targs->ftype = ntohl(*p++);\n\n\tif (args->ftype == NF3BLK  || args->ftype == NF3CHR\n\t || args->ftype == NF3SOCK || args->ftype == NF3FIFO)\n\t\tp = decode_sattr3(p, &args->attrs);\n\n\tif (args->ftype == NF3BLK || args->ftype == NF3CHR) {\n\t\targs->major = ntohl(*p++);\n\t\targs->minor = ntohl(*p++);\n\t}\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_renameargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_renameargs *args)\n{\n\tif (!(p = decode_fh(p, &args->ffh))\n\t || !(p = decode_filename(p, &args->fname, &args->flen))\n\t || !(p = decode_fh(p, &args->tfh))\n\t || !(p = decode_filename(p, &args->tname, &args->tlen)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_readlinkargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readlinkargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\targs->buffer = page_address(*(rqstp->rq_next_page++));\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_linkargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_linkargs *args)\n{\n\tif (!(p = decode_fh(p, &args->ffh))\n\t || !(p = decode_fh(p, &args->tfh))\n\t || !(p = decode_filename(p, &args->tname, &args->tlen)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_readdirargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readdirargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = xdr_decode_hyper(p, &args->cookie);\n\targs->verf   = p; p += 2;\n\targs->dircount = ~0;\n\targs->count  = ntohl(*p++);\n\targs->count  = min_t(u32, args->count, PAGE_SIZE);\n\targs->buffer = page_address(*(rqstp->rq_next_page++));\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_readdirplusargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readdirargs *args)\n{\n\tint len;\n\tu32 max_blocksize = svc_max_payload(rqstp);\n\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = xdr_decode_hyper(p, &args->cookie);\n\targs->verf     = p; p += 2;\n\targs->dircount = ntohl(*p++);\n\targs->count    = ntohl(*p++);\n\n\tlen = args->count = min(args->count, max_blocksize);\n\twhile (len > 0) {\n\t\tstruct page *p = *(rqstp->rq_next_page++);\n\t\tif (!args->buffer)\n\t\t\targs->buffer = page_address(p);\n\t\tlen -= PAGE_SIZE;\n\t}\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_commitargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_commitargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = xdr_decode_hyper(p, &args->offset);\n\targs->count = ntohl(*p++);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\n/*\n * XDR encode functions\n */\n/*\n * There must be an encoding function for void results so svc_process\n * will work properly.\n */\nint\nnfs3svc_encode_voidres(struct svc_rqst *rqstp, __be32 *p, void *dummy)\n{\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* GETATTR */\nint\nnfs3svc_encode_attrstat(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_attrstat *resp)\n{\n\tif (resp->status == 0) {\n\t\tlease_get_mtime(d_inode(resp->fh.fh_dentry),\n\t\t\t\t&resp->stat.mtime);\n\t\tp = encode_fattr3(rqstp, p, &resp->fh, &resp->stat);\n\t}\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* SETATTR, REMOVE, RMDIR */\nint\nnfs3svc_encode_wccstat(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_attrstat *resp)\n{\n\tp = encode_wcc_data(rqstp, p, &resp->fh);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* LOOKUP */\nint\nnfs3svc_encode_diropres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_diropres *resp)\n{\n\tif (resp->status == 0) {\n\t\tp = encode_fh(p, &resp->fh);\n\t\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\t}\n\tp = encode_post_op_attr(rqstp, p, &resp->dirfh);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* ACCESS */\nint\nnfs3svc_encode_accessres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_accessres *resp)\n{\n\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\tif (resp->status == 0)\n\t\t*p++ = htonl(resp->access);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* READLINK */\nint\nnfs3svc_encode_readlinkres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readlinkres *resp)\n{\n\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(resp->len);\n\t\txdr_ressize_check(rqstp, p);\n\t\trqstp->rq_res.page_len = resp->len;\n\t\tif (resp->len & 3) {\n\t\t\t/* need to pad the tail */\n\t\t\trqstp->rq_res.tail[0].iov_base = p;\n\t\t\t*p = 0;\n\t\t\trqstp->rq_res.tail[0].iov_len = 4 - (resp->len&3);\n\t\t}\n\t\treturn 1;\n\t} else\n\t\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* READ */\nint\nnfs3svc_encode_readres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readres *resp)\n{\n\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(resp->count);\n\t\t*p++ = htonl(resp->eof);\n\t\t*p++ = htonl(resp->count);\t/* xdr opaque count */\n\t\txdr_ressize_check(rqstp, p);\n\t\t/* now update rqstp->rq_res to reflect data as well */\n\t\trqstp->rq_res.page_len = resp->count;\n\t\tif (resp->count & 3) {\n\t\t\t/* need to pad the tail */\n\t\t\trqstp->rq_res.tail[0].iov_base = p;\n\t\t\t*p = 0;\n\t\t\trqstp->rq_res.tail[0].iov_len = 4 - (resp->count & 3);\n\t\t}\n\t\treturn 1;\n\t} else\n\t\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* WRITE */\nint\nnfs3svc_encode_writeres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_writeres *resp)\n{\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tp = encode_wcc_data(rqstp, p, &resp->fh);\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(resp->count);\n\t\t*p++ = htonl(resp->committed);\n\t\t*p++ = htonl(nn->nfssvc_boot.tv_sec);\n\t\t*p++ = htonl(nn->nfssvc_boot.tv_usec);\n\t}\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* CREATE, MKDIR, SYMLINK, MKNOD */\nint\nnfs3svc_encode_createres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_diropres *resp)\n{\n\tif (resp->status == 0) {\n\t\t*p++ = xdr_one;\n\t\tp = encode_fh(p, &resp->fh);\n\t\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\t}\n\tp = encode_wcc_data(rqstp, p, &resp->dirfh);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* RENAME */\nint\nnfs3svc_encode_renameres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_renameres *resp)\n{\n\tp = encode_wcc_data(rqstp, p, &resp->ffh);\n\tp = encode_wcc_data(rqstp, p, &resp->tfh);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* LINK */\nint\nnfs3svc_encode_linkres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_linkres *resp)\n{\n\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\tp = encode_wcc_data(rqstp, p, &resp->tfh);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* READDIR */\nint\nnfs3svc_encode_readdirres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readdirres *resp)\n{\n\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\n\tif (resp->status == 0) {\n\t\t/* stupid readdir cookie */\n\t\tmemcpy(p, resp->verf, 8); p += 2;\n\t\txdr_ressize_check(rqstp, p);\n\t\tif (rqstp->rq_res.head[0].iov_len + (2<<2) > PAGE_SIZE)\n\t\t\treturn 1; /*No room for trailer */\n\t\trqstp->rq_res.page_len = (resp->count) << 2;\n\n\t\t/* add the 'tail' to the end of the 'head' page - page 0. */\n\t\trqstp->rq_res.tail[0].iov_base = p;\n\t\t*p++ = 0;\t\t/* no more entries */\n\t\t*p++ = htonl(resp->common.err == nfserr_eof);\n\t\trqstp->rq_res.tail[0].iov_len = 2<<2;\n\t\treturn 1;\n\t} else\n\t\treturn xdr_ressize_check(rqstp, p);\n}\n\nstatic __be32 *\nencode_entry_baggage(struct nfsd3_readdirres *cd, __be32 *p, const char *name,\n\t     int namlen, u64 ino)\n{\n\t*p++ = xdr_one;\t\t\t\t /* mark entry present */\n\tp    = xdr_encode_hyper(p, ino);\t /* file id */\n\tp    = xdr_encode_array(p, name, namlen);/* name length & name */\n\n\tcd->offset = p;\t\t\t\t/* remember pointer */\n\tp = xdr_encode_hyper(p, NFS_OFFSET_MAX);/* offset of next entry */\n\n\treturn p;\n}\n\nstatic __be32\ncompose_entry_fh(struct nfsd3_readdirres *cd, struct svc_fh *fhp,\n\t\t const char *name, int namlen, u64 ino)\n{\n\tstruct svc_export\t*exp;\n\tstruct dentry\t\t*dparent, *dchild;\n\t__be32 rv = nfserr_noent;\n\n\tdparent = cd->fh.fh_dentry;\n\texp  = cd->fh.fh_export;\n\n\tif (isdotent(name, namlen)) {\n\t\tif (namlen == 2) {\n\t\t\tdchild = dget_parent(dparent);\n\t\t\t/* filesystem root - cannot return filehandle for \"..\" */\n\t\t\tif (dchild == dparent)\n\t\t\t\tgoto out;\n\t\t} else\n\t\t\tdchild = dget(dparent);\n\t} else\n\t\tdchild = lookup_one_len_unlocked(name, dparent, namlen);\n\tif (IS_ERR(dchild))\n\t\treturn rv;\n\tif (d_mountpoint(dchild))\n\t\tgoto out;\n\tif (d_really_is_negative(dchild))\n\t\tgoto out;\n\tif (dchild->d_inode->i_ino != ino)\n\t\tgoto out;\n\trv = fh_compose(fhp, exp, dchild, &cd->fh);\nout:\n\tdput(dchild);\n\treturn rv;\n}\n\nstatic __be32 *encode_entryplus_baggage(struct nfsd3_readdirres *cd, __be32 *p, const char *name, int namlen, u64 ino)\n{\n\tstruct svc_fh\t*fh = &cd->scratch;\n\t__be32 err;\n\n\tfh_init(fh, NFS3_FHSIZE);\n\terr = compose_entry_fh(cd, fh, name, namlen, ino);\n\tif (err) {\n\t\t*p++ = 0;\n\t\t*p++ = 0;\n\t\tgoto out;\n\t}\n\tp = encode_post_op_attr(cd->rqstp, p, fh);\n\t*p++ = xdr_one;\t\t\t/* yes, a file handle follows */\n\tp = encode_fh(p, fh);\nout:\n\tfh_put(fh);\n\treturn p;\n}\n\n/*\n * Encode a directory entry. This one works for both normal readdir\n * and readdirplus.\n * The normal readdir reply requires 2 (fileid) + 1 (stringlen)\n * + string + 2 (cookie) + 1 (next) words, i.e. 6 + strlen.\n * \n * The readdirplus baggage is 1+21 words for post_op_attr, plus the\n * file handle.\n */\n\n#define NFS3_ENTRY_BAGGAGE\t(2 + 1 + 2 + 1)\n#define NFS3_ENTRYPLUS_BAGGAGE\t(1 + 21 + 1 + (NFS3_FHSIZE >> 2))\nstatic int\nencode_entry(struct readdir_cd *ccd, const char *name, int namlen,\n\t     loff_t offset, u64 ino, unsigned int d_type, int plus)\n{\n\tstruct nfsd3_readdirres *cd = container_of(ccd, struct nfsd3_readdirres,\n\t\t       \t\t\t\t\tcommon);\n\t__be32\t\t*p = cd->buffer;\n\tcaddr_t\t\tcurr_page_addr = NULL;\n\tstruct page **\tpage;\n\tint\t\tslen;\t\t/* string (name) length */\n\tint\t\telen;\t\t/* estimated entry length in words */\n\tint\t\tnum_entry_words = 0;\t/* actual number of words */\n\n\tif (cd->offset) {\n\t\tu64 offset64 = offset;\n\n\t\tif (unlikely(cd->offset1)) {\n\t\t\t/* we ended up with offset on a page boundary */\n\t\t\t*cd->offset = htonl(offset64 >> 32);\n\t\t\t*cd->offset1 = htonl(offset64 & 0xffffffff);\n\t\t\tcd->offset1 = NULL;\n\t\t} else {\n\t\t\txdr_encode_hyper(cd->offset, offset64);\n\t\t}\n\t}\n\n\t/*\n\tdprintk(\"encode_entry(%.*s @%ld%s)\\n\",\n\t\tnamlen, name, (long) offset, plus? \" plus\" : \"\");\n\t */\n\n\t/* truncate filename if too long */\n\tnamlen = min(namlen, NFS3_MAXNAMLEN);\n\n\tslen = XDR_QUADLEN(namlen);\n\telen = slen + NFS3_ENTRY_BAGGAGE\n\t\t+ (plus? NFS3_ENTRYPLUS_BAGGAGE : 0);\n\n\tif (cd->buflen < elen) {\n\t\tcd->common.err = nfserr_toosmall;\n\t\treturn -EINVAL;\n\t}\n\n\t/* determine which page in rq_respages[] we are currently filling */\n\tfor (page = cd->rqstp->rq_respages + 1;\n\t\t\t\tpage < cd->rqstp->rq_next_page; page++) {\n\t\tcurr_page_addr = page_address(*page);\n\n\t\tif (((caddr_t)cd->buffer >= curr_page_addr) &&\n\t\t    ((caddr_t)cd->buffer <  curr_page_addr + PAGE_SIZE))\n\t\t\tbreak;\n\t}\n\n\tif ((caddr_t)(cd->buffer + elen) < (curr_page_addr + PAGE_SIZE)) {\n\t\t/* encode entry in current page */\n\n\t\tp = encode_entry_baggage(cd, p, name, namlen, ino);\n\n\t\tif (plus)\n\t\t\tp = encode_entryplus_baggage(cd, p, name, namlen, ino);\n\t\tnum_entry_words = p - cd->buffer;\n\t} else if (*(page+1) != NULL) {\n\t\t/* temporarily encode entry into next page, then move back to\n\t\t * current and next page in rq_respages[] */\n\t\t__be32 *p1, *tmp;\n\t\tint len1, len2;\n\n\t\t/* grab next page for temporary storage of entry */\n\t\tp1 = tmp = page_address(*(page+1));\n\n\t\tp1 = encode_entry_baggage(cd, p1, name, namlen, ino);\n\n\t\tif (plus)\n\t\t\tp1 = encode_entryplus_baggage(cd, p1, name, namlen, ino);\n\n\t\t/* determine entry word length and lengths to go in pages */\n\t\tnum_entry_words = p1 - tmp;\n\t\tlen1 = curr_page_addr + PAGE_SIZE - (caddr_t)cd->buffer;\n\t\tif ((num_entry_words << 2) < len1) {\n\t\t\t/* the actual number of words in the entry is less\n\t\t\t * than elen and can still fit in the current page\n\t\t\t */\n\t\t\tmemmove(p, tmp, num_entry_words << 2);\n\t\t\tp += num_entry_words;\n\n\t\t\t/* update offset */\n\t\t\tcd->offset = cd->buffer + (cd->offset - tmp);\n\t\t} else {\n\t\t\tunsigned int offset_r = (cd->offset - tmp) << 2;\n\n\t\t\t/* update pointer to offset location.\n\t\t\t * This is a 64bit quantity, so we need to\n\t\t\t * deal with 3 cases:\n\t\t\t *  -\tentirely in first page\n\t\t\t *  -\tentirely in second page\n\t\t\t *  -\t4 bytes in each page\n\t\t\t */\n\t\t\tif (offset_r + 8 <= len1) {\n\t\t\t\tcd->offset = p + (cd->offset - tmp);\n\t\t\t} else if (offset_r >= len1) {\n\t\t\t\tcd->offset -= len1 >> 2;\n\t\t\t} else {\n\t\t\t\t/* sitting on the fence */\n\t\t\t\tBUG_ON(offset_r != len1 - 4);\n\t\t\t\tcd->offset = p + (cd->offset - tmp);\n\t\t\t\tcd->offset1 = tmp;\n\t\t\t}\n\n\t\t\tlen2 = (num_entry_words << 2) - len1;\n\n\t\t\t/* move from temp page to current and next pages */\n\t\t\tmemmove(p, tmp, len1);\n\t\t\tmemmove(tmp, (caddr_t)tmp+len1, len2);\n\n\t\t\tp = tmp + (len2 >> 2);\n\t\t}\n\t}\n\telse {\n\t\tcd->common.err = nfserr_toosmall;\n\t\treturn -EINVAL;\n\t}\n\n\tcd->buflen -= num_entry_words;\n\tcd->buffer = p;\n\tcd->common.err = nfs_ok;\n\treturn 0;\n\n}\n\nint\nnfs3svc_encode_entry(void *cd, const char *name,\n\t\t     int namlen, loff_t offset, u64 ino, unsigned int d_type)\n{\n\treturn encode_entry(cd, name, namlen, offset, ino, d_type, 0);\n}\n\nint\nnfs3svc_encode_entry_plus(void *cd, const char *name,\n\t\t\t  int namlen, loff_t offset, u64 ino,\n\t\t\t  unsigned int d_type)\n{\n\treturn encode_entry(cd, name, namlen, offset, ino, d_type, 1);\n}\n\n/* FSSTAT */\nint\nnfs3svc_encode_fsstatres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_fsstatres *resp)\n{\n\tstruct kstatfs\t*s = &resp->stats;\n\tu64\t\tbs = s->f_bsize;\n\n\t*p++ = xdr_zero;\t/* no post_op_attr */\n\n\tif (resp->status == 0) {\n\t\tp = xdr_encode_hyper(p, bs * s->f_blocks);\t/* total bytes */\n\t\tp = xdr_encode_hyper(p, bs * s->f_bfree);\t/* free bytes */\n\t\tp = xdr_encode_hyper(p, bs * s->f_bavail);\t/* user available bytes */\n\t\tp = xdr_encode_hyper(p, s->f_files);\t/* total inodes */\n\t\tp = xdr_encode_hyper(p, s->f_ffree);\t/* free inodes */\n\t\tp = xdr_encode_hyper(p, s->f_ffree);\t/* user available inodes */\n\t\t*p++ = htonl(resp->invarsec);\t/* mean unchanged time */\n\t}\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* FSINFO */\nint\nnfs3svc_encode_fsinfores(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_fsinfores *resp)\n{\n\t*p++ = xdr_zero;\t/* no post_op_attr */\n\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(resp->f_rtmax);\n\t\t*p++ = htonl(resp->f_rtpref);\n\t\t*p++ = htonl(resp->f_rtmult);\n\t\t*p++ = htonl(resp->f_wtmax);\n\t\t*p++ = htonl(resp->f_wtpref);\n\t\t*p++ = htonl(resp->f_wtmult);\n\t\t*p++ = htonl(resp->f_dtpref);\n\t\tp = xdr_encode_hyper(p, resp->f_maxfilesize);\n\t\t*p++ = xdr_one;\n\t\t*p++ = xdr_zero;\n\t\t*p++ = htonl(resp->f_properties);\n\t}\n\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* PATHCONF */\nint\nnfs3svc_encode_pathconfres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_pathconfres *resp)\n{\n\t*p++ = xdr_zero;\t/* no post_op_attr */\n\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(resp->p_link_max);\n\t\t*p++ = htonl(resp->p_name_max);\n\t\t*p++ = htonl(resp->p_no_trunc);\n\t\t*p++ = htonl(resp->p_chown_restricted);\n\t\t*p++ = htonl(resp->p_case_insensitive);\n\t\t*p++ = htonl(resp->p_case_preserving);\n\t}\n\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* COMMIT */\nint\nnfs3svc_encode_commitres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_commitres *resp)\n{\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tp = encode_wcc_data(rqstp, p, &resp->fh);\n\t/* Write verifier */\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(nn->nfssvc_boot.tv_sec);\n\t\t*p++ = htonl(nn->nfssvc_boot.tv_usec);\n\t}\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/*\n * XDR release functions\n */\nint\nnfs3svc_release_fhandle(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_attrstat *resp)\n{\n\tfh_put(&resp->fh);\n\treturn 1;\n}\n\nint\nnfs3svc_release_fhandle2(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_fhandle_pair *resp)\n{\n\tfh_put(&resp->fh1);\n\tfh_put(&resp->fh2);\n\treturn 1;\n}\n", "/*\n *  Server-side procedures for NFSv4.\n *\n *  Copyright (c) 2002 The Regents of the University of Michigan.\n *  All rights reserved.\n *\n *  Kendrick Smith <kmsmith@umich.edu>\n *  Andy Adamson   <andros@umich.edu>\n *\n *  Redistribution and use in source and binary forms, with or without\n *  modification, are permitted provided that the following conditions\n *  are met:\n *\n *  1. Redistributions of source code must retain the above copyright\n *     notice, this list of conditions and the following disclaimer.\n *  2. Redistributions in binary form must reproduce the above copyright\n *     notice, this list of conditions and the following disclaimer in the\n *     documentation and/or other materials provided with the distribution.\n *  3. Neither the name of the University nor the names of its\n *     contributors may be used to endorse or promote products derived\n *     from this software without specific prior written permission.\n *\n *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED\n *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n *  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE\n *  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n *  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n *  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n *  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n *  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n *  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n *  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n#include <linux/file.h>\n#include <linux/falloc.h>\n#include <linux/slab.h>\n\n#include \"idmap.h\"\n#include \"cache.h\"\n#include \"xdr4.h\"\n#include \"vfs.h\"\n#include \"current_stateid.h\"\n#include \"netns.h\"\n#include \"acl.h\"\n#include \"pnfs.h\"\n#include \"trace.h\"\n\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n#include <linux/security.h>\n\nstatic inline void\nnfsd4_security_inode_setsecctx(struct svc_fh *resfh, struct xdr_netobj *label, u32 *bmval)\n{\n\tstruct inode *inode = d_inode(resfh->fh_dentry);\n\tint status;\n\n\tinode_lock(inode);\n\tstatus = security_inode_setsecctx(resfh->fh_dentry,\n\t\tlabel->data, label->len);\n\tinode_unlock(inode);\n\n\tif (status)\n\t\t/*\n\t\t * XXX: We should really fail the whole open, but we may\n\t\t * already have created a new file, so it may be too\n\t\t * late.  For now this seems the least of evils:\n\t\t */\n\t\tbmval[2] &= ~FATTR4_WORD2_SECURITY_LABEL;\n\n\treturn;\n}\n#else\nstatic inline void\nnfsd4_security_inode_setsecctx(struct svc_fh *resfh, struct xdr_netobj *label, u32 *bmval)\n{ }\n#endif\n\n#define NFSDDBG_FACILITY\t\tNFSDDBG_PROC\n\nstatic u32 nfsd_attrmask[] = {\n\tNFSD_WRITEABLE_ATTRS_WORD0,\n\tNFSD_WRITEABLE_ATTRS_WORD1,\n\tNFSD_WRITEABLE_ATTRS_WORD2\n};\n\nstatic u32 nfsd41_ex_attrmask[] = {\n\tNFSD_SUPPATTR_EXCLCREAT_WORD0,\n\tNFSD_SUPPATTR_EXCLCREAT_WORD1,\n\tNFSD_SUPPATTR_EXCLCREAT_WORD2\n};\n\nstatic __be32\ncheck_attr_support(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t   u32 *bmval, u32 *writable)\n{\n\tstruct dentry *dentry = cstate->current_fh.fh_dentry;\n\tstruct svc_export *exp = cstate->current_fh.fh_export;\n\n\tif (!nfsd_attrs_supported(cstate->minorversion, bmval))\n\t\treturn nfserr_attrnotsupp;\n\tif ((bmval[0] & FATTR4_WORD0_ACL) && !IS_POSIXACL(d_inode(dentry)))\n\t\treturn nfserr_attrnotsupp;\n\tif ((bmval[2] & FATTR4_WORD2_SECURITY_LABEL) &&\n\t\t\t!(exp->ex_flags & NFSEXP_SECURITY_LABEL))\n\t\treturn nfserr_attrnotsupp;\n\tif (writable && !bmval_is_subset(bmval, writable))\n\t\treturn nfserr_inval;\n\tif (writable && (bmval[2] & FATTR4_WORD2_MODE_UMASK) &&\n\t\t\t(bmval[1] & FATTR4_WORD1_MODE))\n\t\treturn nfserr_inval;\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_check_open_attributes(struct svc_rqst *rqstp,\n\tstruct nfsd4_compound_state *cstate, struct nfsd4_open *open)\n{\n\t__be32 status = nfs_ok;\n\n\tif (open->op_create == NFS4_OPEN_CREATE) {\n\t\tif (open->op_createmode == NFS4_CREATE_UNCHECKED\n\t\t    || open->op_createmode == NFS4_CREATE_GUARDED)\n\t\t\tstatus = check_attr_support(rqstp, cstate,\n\t\t\t\t\topen->op_bmval, nfsd_attrmask);\n\t\telse if (open->op_createmode == NFS4_CREATE_EXCLUSIVE4_1)\n\t\t\tstatus = check_attr_support(rqstp, cstate,\n\t\t\t\t\topen->op_bmval, nfsd41_ex_attrmask);\n\t}\n\n\treturn status;\n}\n\nstatic int\nis_create_with_attrs(struct nfsd4_open *open)\n{\n\treturn open->op_create == NFS4_OPEN_CREATE\n\t\t&& (open->op_createmode == NFS4_CREATE_UNCHECKED\n\t\t    || open->op_createmode == NFS4_CREATE_GUARDED\n\t\t    || open->op_createmode == NFS4_CREATE_EXCLUSIVE4_1);\n}\n\n/*\n * if error occurs when setting the acl, just clear the acl bit\n * in the returned attr bitmap.\n */\nstatic void\ndo_set_nfs4_acl(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tstruct nfs4_acl *acl, u32 *bmval)\n{\n\t__be32 status;\n\n\tstatus = nfsd4_set_nfs4_acl(rqstp, fhp, acl);\n\tif (status)\n\t\t/*\n\t\t * We should probably fail the whole open at this point,\n\t\t * but we've already created the file, so it's too late;\n\t\t * So this seems the least of evils:\n\t\t */\n\t\tbmval[0] &= ~FATTR4_WORD0_ACL;\n}\n\nstatic inline void\nfh_dup2(struct svc_fh *dst, struct svc_fh *src)\n{\n\tfh_put(dst);\n\tdget(src->fh_dentry);\n\tif (src->fh_export)\n\t\texp_get(src->fh_export);\n\t*dst = *src;\n}\n\nstatic __be32\ndo_open_permission(struct svc_rqst *rqstp, struct svc_fh *current_fh, struct nfsd4_open *open, int accmode)\n{\n\t__be32 status;\n\n\tif (open->op_truncate &&\n\t\t!(open->op_share_access & NFS4_SHARE_ACCESS_WRITE))\n\t\treturn nfserr_inval;\n\n\taccmode |= NFSD_MAY_READ_IF_EXEC;\n\n\tif (open->op_share_access & NFS4_SHARE_ACCESS_READ)\n\t\taccmode |= NFSD_MAY_READ;\n\tif (open->op_share_access & NFS4_SHARE_ACCESS_WRITE)\n\t\taccmode |= (NFSD_MAY_WRITE | NFSD_MAY_TRUNC);\n\tif (open->op_share_deny & NFS4_SHARE_DENY_READ)\n\t\taccmode |= NFSD_MAY_WRITE;\n\n\tstatus = fh_verify(rqstp, current_fh, S_IFREG, accmode);\n\n\treturn status;\n}\n\nstatic __be32 nfsd_check_obj_isreg(struct svc_fh *fh)\n{\n\tumode_t mode = d_inode(fh->fh_dentry)->i_mode;\n\n\tif (S_ISREG(mode))\n\t\treturn nfs_ok;\n\tif (S_ISDIR(mode))\n\t\treturn nfserr_isdir;\n\t/*\n\t * Using err_symlink as our catch-all case may look odd; but\n\t * there's no other obvious error for this case in 4.0, and we\n\t * happen to know that it will cause the linux v4 client to do\n\t * the right thing on attempts to open something other than a\n\t * regular file.\n\t */\n\treturn nfserr_symlink;\n}\n\nstatic void nfsd4_set_open_owner_reply_cache(struct nfsd4_compound_state *cstate, struct nfsd4_open *open, struct svc_fh *resfh)\n{\n\tif (nfsd4_has_session(cstate))\n\t\treturn;\n\tfh_copy_shallow(&open->op_openowner->oo_owner.so_replay.rp_openfh,\n\t\t\t&resfh->fh_handle);\n}\n\nstatic __be32\ndo_open_lookup(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_open *open, struct svc_fh **resfh)\n{\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\tint accmode;\n\t__be32 status;\n\n\t*resfh = kmalloc(sizeof(struct svc_fh), GFP_KERNEL);\n\tif (!*resfh)\n\t\treturn nfserr_jukebox;\n\tfh_init(*resfh, NFS4_FHSIZE);\n\topen->op_truncate = 0;\n\n\tif (open->op_create) {\n\t\t/* FIXME: check session persistence and pnfs flags.\n\t\t * The nfsv4.1 spec requires the following semantics:\n\t\t *\n\t\t * Persistent   | pNFS   | Server REQUIRED | Client Allowed\n\t\t * Reply Cache  | server |                 |\n\t\t * -------------+--------+-----------------+--------------------\n\t\t * no           | no     | EXCLUSIVE4_1    | EXCLUSIVE4_1\n\t\t *              |        |                 | (SHOULD)\n\t\t *              |        | and EXCLUSIVE4  | or EXCLUSIVE4\n\t\t *              |        |                 | (SHOULD NOT)\n\t\t * no           | yes    | EXCLUSIVE4_1    | EXCLUSIVE4_1\n\t\t * yes          | no     | GUARDED4        | GUARDED4\n\t\t * yes          | yes    | GUARDED4        | GUARDED4\n\t\t */\n\n\t\t/*\n\t\t * Note: create modes (UNCHECKED,GUARDED...) are the same\n\t\t * in NFSv4 as in v3 except EXCLUSIVE4_1.\n\t\t */\n\t\tstatus = do_nfsd_create(rqstp, current_fh, open->op_fname.data,\n\t\t\t\t\topen->op_fname.len, &open->op_iattr,\n\t\t\t\t\t*resfh, open->op_createmode,\n\t\t\t\t\t(u32 *)open->op_verf.data,\n\t\t\t\t\t&open->op_truncate, &open->op_created);\n\n\t\tif (!status && open->op_label.len)\n\t\t\tnfsd4_security_inode_setsecctx(*resfh, &open->op_label, open->op_bmval);\n\n\t\t/*\n\t\t * Following rfc 3530 14.2.16, and rfc 5661 18.16.4\n\t\t * use the returned bitmask to indicate which attributes\n\t\t * we used to store the verifier:\n\t\t */\n\t\tif (nfsd_create_is_exclusive(open->op_createmode) && status == 0)\n\t\t\topen->op_bmval[1] |= (FATTR4_WORD1_TIME_ACCESS |\n\t\t\t\t\t\tFATTR4_WORD1_TIME_MODIFY);\n\t} else\n\t\t/*\n\t\t * Note this may exit with the parent still locked.\n\t\t * We will hold the lock until nfsd4_open's final\n\t\t * lookup, to prevent renames or unlinks until we've had\n\t\t * a chance to an acquire a delegation if appropriate.\n\t\t */\n\t\tstatus = nfsd_lookup(rqstp, current_fh,\n\t\t\t\t     open->op_fname.data, open->op_fname.len, *resfh);\n\tif (status)\n\t\tgoto out;\n\tstatus = nfsd_check_obj_isreg(*resfh);\n\tif (status)\n\t\tgoto out;\n\n\tif (is_create_with_attrs(open) && open->op_acl != NULL)\n\t\tdo_set_nfs4_acl(rqstp, *resfh, open->op_acl, open->op_bmval);\n\n\tnfsd4_set_open_owner_reply_cache(cstate, open, *resfh);\n\taccmode = NFSD_MAY_NOP;\n\tif (open->op_created ||\n\t\t\topen->op_claim_type == NFS4_OPEN_CLAIM_DELEGATE_CUR)\n\t\taccmode |= NFSD_MAY_OWNER_OVERRIDE;\n\tstatus = do_open_permission(rqstp, *resfh, open, accmode);\n\tset_change_info(&open->op_cinfo, current_fh);\nout:\n\treturn status;\n}\n\nstatic __be32\ndo_open_fhandle(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_open *open)\n{\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\t__be32 status;\n\tint accmode = 0;\n\n\t/* We don't know the target directory, and therefore can not\n\t* set the change info\n\t*/\n\n\tmemset(&open->op_cinfo, 0, sizeof(struct nfsd4_change_info));\n\n\tnfsd4_set_open_owner_reply_cache(cstate, open, current_fh);\n\n\topen->op_truncate = (open->op_iattr.ia_valid & ATTR_SIZE) &&\n\t\t(open->op_iattr.ia_size == 0);\n\t/*\n\t * In the delegation case, the client is telling us about an\n\t * open that it *already* performed locally, some time ago.  We\n\t * should let it succeed now if possible.\n\t *\n\t * In the case of a CLAIM_FH open, on the other hand, the client\n\t * may be counting on us to enforce permissions (the Linux 4.1\n\t * client uses this for normal opens, for example).\n\t */\n\tif (open->op_claim_type == NFS4_OPEN_CLAIM_DELEG_CUR_FH)\n\t\taccmode = NFSD_MAY_OWNER_OVERRIDE;\n\n\tstatus = do_open_permission(rqstp, current_fh, open, accmode);\n\n\treturn status;\n}\n\nstatic void\ncopy_clientid(clientid_t *clid, struct nfsd4_session *session)\n{\n\tstruct nfsd4_sessionid *sid =\n\t\t\t(struct nfsd4_sessionid *)session->se_sessionid.data;\n\n\tclid->cl_boot = sid->clientid.cl_boot;\n\tclid->cl_id = sid->clientid.cl_id;\n}\n\nstatic __be32\nnfsd4_open(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t   struct nfsd4_open *open)\n{\n\t__be32 status;\n\tstruct svc_fh *resfh = NULL;\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_open filename %.*s op_openowner %p\\n\",\n\t\t(int)open->op_fname.len, open->op_fname.data,\n\t\topen->op_openowner);\n\n\t/* This check required by spec. */\n\tif (open->op_create && open->op_claim_type != NFS4_OPEN_CLAIM_NULL)\n\t\treturn nfserr_inval;\n\n\topen->op_created = 0;\n\t/*\n\t * RFC5661 18.51.3\n\t * Before RECLAIM_COMPLETE done, server should deny new lock\n\t */\n\tif (nfsd4_has_session(cstate) &&\n\t    !test_bit(NFSD4_CLIENT_RECLAIM_COMPLETE,\n\t\t      &cstate->session->se_client->cl_flags) &&\n\t    open->op_claim_type != NFS4_OPEN_CLAIM_PREVIOUS)\n\t\treturn nfserr_grace;\n\n\tif (nfsd4_has_session(cstate))\n\t\tcopy_clientid(&open->op_clientid, cstate->session);\n\n\t/* check seqid for replay. set nfs4_owner */\n\tstatus = nfsd4_process_open1(cstate, open, nn);\n\tif (status == nfserr_replay_me) {\n\t\tstruct nfs4_replay *rp = &open->op_openowner->oo_owner.so_replay;\n\t\tfh_put(&cstate->current_fh);\n\t\tfh_copy_shallow(&cstate->current_fh.fh_handle,\n\t\t\t\t&rp->rp_openfh);\n\t\tstatus = fh_verify(rqstp, &cstate->current_fh, 0, NFSD_MAY_NOP);\n\t\tif (status)\n\t\t\tdprintk(\"nfsd4_open: replay failed\"\n\t\t\t\t\" restoring previous filehandle\\n\");\n\t\telse\n\t\t\tstatus = nfserr_replay_me;\n\t}\n\tif (status)\n\t\tgoto out;\n\tif (open->op_xdr_error) {\n\t\tstatus = open->op_xdr_error;\n\t\tgoto out;\n\t}\n\n\tstatus = nfsd4_check_open_attributes(rqstp, cstate, open);\n\tif (status)\n\t\tgoto out;\n\n\t/* Openowner is now set, so sequence id will get bumped.  Now we need\n\t * these checks before we do any creates: */\n\tstatus = nfserr_grace;\n\tif (opens_in_grace(net) && open->op_claim_type != NFS4_OPEN_CLAIM_PREVIOUS)\n\t\tgoto out;\n\tstatus = nfserr_no_grace;\n\tif (!opens_in_grace(net) && open->op_claim_type == NFS4_OPEN_CLAIM_PREVIOUS)\n\t\tgoto out;\n\n\tswitch (open->op_claim_type) {\n\t\tcase NFS4_OPEN_CLAIM_DELEGATE_CUR:\n\t\tcase NFS4_OPEN_CLAIM_NULL:\n\t\t\tstatus = do_open_lookup(rqstp, cstate, open, &resfh);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase NFS4_OPEN_CLAIM_PREVIOUS:\n\t\t\tstatus = nfs4_check_open_reclaim(&open->op_clientid,\n\t\t\t\t\t\t\t cstate, nn);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t\topen->op_openowner->oo_flags |= NFS4_OO_CONFIRMED;\n\t\tcase NFS4_OPEN_CLAIM_FH:\n\t\tcase NFS4_OPEN_CLAIM_DELEG_CUR_FH:\n\t\t\tstatus = do_open_fhandle(rqstp, cstate, open);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t\tresfh = &cstate->current_fh;\n\t\t\tbreak;\n\t\tcase NFS4_OPEN_CLAIM_DELEG_PREV_FH:\n             \tcase NFS4_OPEN_CLAIM_DELEGATE_PREV:\n\t\t\tdprintk(\"NFSD: unsupported OPEN claim type %d\\n\",\n\t\t\t\topen->op_claim_type);\n\t\t\tstatus = nfserr_notsupp;\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\tdprintk(\"NFSD: Invalid OPEN claim type %d\\n\",\n\t\t\t\topen->op_claim_type);\n\t\t\tstatus = nfserr_inval;\n\t\t\tgoto out;\n\t}\n\t/*\n\t * nfsd4_process_open2() does the actual opening of the file.  If\n\t * successful, it (1) truncates the file if open->op_truncate was\n\t * set, (2) sets open->op_stateid, (3) sets open->op_delegation.\n\t */\n\tstatus = nfsd4_process_open2(rqstp, resfh, open);\n\tWARN(status && open->op_created,\n\t     \"nfsd4_process_open2 failed to open newly-created file! status=%u\\n\",\n\t     be32_to_cpu(status));\nout:\n\tif (resfh && resfh != &cstate->current_fh) {\n\t\tfh_dup2(&cstate->current_fh, resfh);\n\t\tfh_put(resfh);\n\t\tkfree(resfh);\n\t}\n\tnfsd4_cleanup_open_state(cstate, open);\n\tnfsd4_bump_seqid(cstate, status);\n\treturn status;\n}\n\n/*\n * OPEN is the only seqid-mutating operation whose decoding can fail\n * with a seqid-mutating error (specifically, decoding of user names in\n * the attributes).  Therefore we have to do some processing to look up\n * the stateowner so that we can bump the seqid.\n */\nstatic __be32 nfsd4_open_omfg(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_op *op)\n{\n\tstruct nfsd4_open *open = (struct nfsd4_open *)&op->u;\n\n\tif (!seqid_mutating_err(ntohl(op->status)))\n\t\treturn op->status;\n\tif (nfsd4_has_session(cstate))\n\t\treturn op->status;\n\topen->op_xdr_error = op->status;\n\treturn nfsd4_open(rqstp, cstate, open);\n}\n\n/*\n * filehandle-manipulating ops.\n */\nstatic __be32\nnfsd4_getfh(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct svc_fh **getfh)\n{\n\tif (!cstate->current_fh.fh_dentry)\n\t\treturn nfserr_nofilehandle;\n\n\t*getfh = &cstate->current_fh;\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_putfh(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct nfsd4_putfh *putfh)\n{\n\tfh_put(&cstate->current_fh);\n\tcstate->current_fh.fh_handle.fh_size = putfh->pf_fhlen;\n\tmemcpy(&cstate->current_fh.fh_handle.fh_base, putfh->pf_fhval,\n\t       putfh->pf_fhlen);\n\treturn fh_verify(rqstp, &cstate->current_fh, 0, NFSD_MAY_BYPASS_GSS);\n}\n\nstatic __be32\nnfsd4_putrootfh(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tvoid *arg)\n{\n\t__be32 status;\n\n\tfh_put(&cstate->current_fh);\n\tstatus = exp_pseudoroot(rqstp, &cstate->current_fh);\n\treturn status;\n}\n\nstatic __be32\nnfsd4_restorefh(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tvoid *arg)\n{\n\tif (!cstate->save_fh.fh_dentry)\n\t\treturn nfserr_restorefh;\n\n\tfh_dup2(&cstate->current_fh, &cstate->save_fh);\n\tif (HAS_STATE_ID(cstate, SAVED_STATE_ID_FLAG)) {\n\t\tmemcpy(&cstate->current_stateid, &cstate->save_stateid, sizeof(stateid_t));\n\t\tSET_STATE_ID(cstate, CURRENT_STATE_ID_FLAG);\n\t}\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_savefh(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     void *arg)\n{\n\tif (!cstate->current_fh.fh_dentry)\n\t\treturn nfserr_nofilehandle;\n\n\tfh_dup2(&cstate->save_fh, &cstate->current_fh);\n\tif (HAS_STATE_ID(cstate, CURRENT_STATE_ID_FLAG)) {\n\t\tmemcpy(&cstate->save_stateid, &cstate->current_stateid, sizeof(stateid_t));\n\t\tSET_STATE_ID(cstate, SAVED_STATE_ID_FLAG);\n\t}\n\treturn nfs_ok;\n}\n\n/*\n * misc nfsv4 ops\n */\nstatic __be32\nnfsd4_access(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_access *access)\n{\n\tif (access->ac_req_access & ~NFS3_ACCESS_FULL)\n\t\treturn nfserr_inval;\n\n\taccess->ac_resp_access = access->ac_req_access;\n\treturn nfsd_access(rqstp, &cstate->current_fh, &access->ac_resp_access,\n\t\t\t   &access->ac_supported);\n}\n\nstatic void gen_boot_verifier(nfs4_verifier *verifier, struct net *net)\n{\n\t__be32 verf[2];\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\t/*\n\t * This is opaque to client, so no need to byte-swap. Use\n\t * __force to keep sparse happy\n\t */\n\tverf[0] = (__force __be32)nn->nfssvc_boot.tv_sec;\n\tverf[1] = (__force __be32)nn->nfssvc_boot.tv_usec;\n\tmemcpy(verifier->data, verf, sizeof(verifier->data));\n}\n\nstatic __be32\nnfsd4_commit(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_commit *commit)\n{\n\tgen_boot_verifier(&commit->co_verf, SVC_NET(rqstp));\n\treturn nfsd_commit(rqstp, &cstate->current_fh, commit->co_offset,\n\t\t\t     commit->co_count);\n}\n\nstatic __be32\nnfsd4_create(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_create *create)\n{\n\tstruct svc_fh resfh;\n\t__be32 status;\n\tdev_t rdev;\n\n\tfh_init(&resfh, NFS4_FHSIZE);\n\n\tstatus = fh_verify(rqstp, &cstate->current_fh, S_IFDIR, NFSD_MAY_NOP);\n\tif (status)\n\t\treturn status;\n\n\tstatus = check_attr_support(rqstp, cstate, create->cr_bmval,\n\t\t\t\t    nfsd_attrmask);\n\tif (status)\n\t\treturn status;\n\n\tswitch (create->cr_type) {\n\tcase NF4LNK:\n\t\tstatus = nfsd_symlink(rqstp, &cstate->current_fh,\n\t\t\t\t      create->cr_name, create->cr_namelen,\n\t\t\t\t      create->cr_data, &resfh);\n\t\tbreak;\n\n\tcase NF4BLK:\n\t\trdev = MKDEV(create->cr_specdata1, create->cr_specdata2);\n\t\tif (MAJOR(rdev) != create->cr_specdata1 ||\n\t\t    MINOR(rdev) != create->cr_specdata2)\n\t\t\treturn nfserr_inval;\n\t\tstatus = nfsd_create(rqstp, &cstate->current_fh,\n\t\t\t\t     create->cr_name, create->cr_namelen,\n\t\t\t\t     &create->cr_iattr, S_IFBLK, rdev, &resfh);\n\t\tbreak;\n\n\tcase NF4CHR:\n\t\trdev = MKDEV(create->cr_specdata1, create->cr_specdata2);\n\t\tif (MAJOR(rdev) != create->cr_specdata1 ||\n\t\t    MINOR(rdev) != create->cr_specdata2)\n\t\t\treturn nfserr_inval;\n\t\tstatus = nfsd_create(rqstp, &cstate->current_fh,\n\t\t\t\t     create->cr_name, create->cr_namelen,\n\t\t\t\t     &create->cr_iattr,S_IFCHR, rdev, &resfh);\n\t\tbreak;\n\n\tcase NF4SOCK:\n\t\tstatus = nfsd_create(rqstp, &cstate->current_fh,\n\t\t\t\t     create->cr_name, create->cr_namelen,\n\t\t\t\t     &create->cr_iattr, S_IFSOCK, 0, &resfh);\n\t\tbreak;\n\n\tcase NF4FIFO:\n\t\tstatus = nfsd_create(rqstp, &cstate->current_fh,\n\t\t\t\t     create->cr_name, create->cr_namelen,\n\t\t\t\t     &create->cr_iattr, S_IFIFO, 0, &resfh);\n\t\tbreak;\n\n\tcase NF4DIR:\n\t\tcreate->cr_iattr.ia_valid &= ~ATTR_SIZE;\n\t\tstatus = nfsd_create(rqstp, &cstate->current_fh,\n\t\t\t\t     create->cr_name, create->cr_namelen,\n\t\t\t\t     &create->cr_iattr, S_IFDIR, 0, &resfh);\n\t\tbreak;\n\n\tdefault:\n\t\tstatus = nfserr_badtype;\n\t}\n\n\tif (status)\n\t\tgoto out;\n\n\tif (create->cr_label.len)\n\t\tnfsd4_security_inode_setsecctx(&resfh, &create->cr_label, create->cr_bmval);\n\n\tif (create->cr_acl != NULL)\n\t\tdo_set_nfs4_acl(rqstp, &resfh, create->cr_acl,\n\t\t\t\tcreate->cr_bmval);\n\n\tfh_unlock(&cstate->current_fh);\n\tset_change_info(&create->cr_cinfo, &cstate->current_fh);\n\tfh_dup2(&cstate->current_fh, &resfh);\nout:\n\tfh_put(&resfh);\n\treturn status;\n}\n\nstatic __be32\nnfsd4_getattr(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_getattr *getattr)\n{\n\t__be32 status;\n\n\tstatus = fh_verify(rqstp, &cstate->current_fh, 0, NFSD_MAY_NOP);\n\tif (status)\n\t\treturn status;\n\n\tif (getattr->ga_bmval[1] & NFSD_WRITEONLY_ATTRS_WORD1)\n\t\treturn nfserr_inval;\n\n\tgetattr->ga_bmval[0] &= nfsd_suppattrs[cstate->minorversion][0];\n\tgetattr->ga_bmval[1] &= nfsd_suppattrs[cstate->minorversion][1];\n\tgetattr->ga_bmval[2] &= nfsd_suppattrs[cstate->minorversion][2];\n\n\tgetattr->ga_fhp = &cstate->current_fh;\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_link(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t   struct nfsd4_link *link)\n{\n\t__be32 status = nfserr_nofilehandle;\n\n\tif (!cstate->save_fh.fh_dentry)\n\t\treturn status;\n\tstatus = nfsd_link(rqstp, &cstate->current_fh,\n\t\t\t   link->li_name, link->li_namelen, &cstate->save_fh);\n\tif (!status)\n\t\tset_change_info(&link->li_cinfo, &cstate->current_fh);\n\treturn status;\n}\n\nstatic __be32 nfsd4_do_lookupp(struct svc_rqst *rqstp, struct svc_fh *fh)\n{\n\tstruct svc_fh tmp_fh;\n\t__be32 ret;\n\n\tfh_init(&tmp_fh, NFS4_FHSIZE);\n\tret = exp_pseudoroot(rqstp, &tmp_fh);\n\tif (ret)\n\t\treturn ret;\n\tif (tmp_fh.fh_dentry == fh->fh_dentry) {\n\t\tfh_put(&tmp_fh);\n\t\treturn nfserr_noent;\n\t}\n\tfh_put(&tmp_fh);\n\treturn nfsd_lookup(rqstp, fh, \"..\", 2, fh);\n}\n\nstatic __be32\nnfsd4_lookupp(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      void *arg)\n{\n\treturn nfsd4_do_lookupp(rqstp, &cstate->current_fh);\n}\n\nstatic __be32\nnfsd4_lookup(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_lookup *lookup)\n{\n\treturn nfsd_lookup(rqstp, &cstate->current_fh,\n\t\t\t   lookup->lo_name, lookup->lo_len,\n\t\t\t   &cstate->current_fh);\n}\n\nstatic __be32\nnfsd4_read(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t   struct nfsd4_read *read)\n{\n\t__be32 status;\n\n\tread->rd_filp = NULL;\n\tif (read->rd_offset >= OFFSET_MAX)\n\t\treturn nfserr_inval;\n\n\t/*\n\t * If we do a zero copy read, then a client will see read data\n\t * that reflects the state of the file *after* performing the\n\t * following compound.\n\t *\n\t * To ensure proper ordering, we therefore turn off zero copy if\n\t * the client wants us to do more in this compound:\n\t */\n\tif (!nfsd4_last_compound_op(rqstp))\n\t\tclear_bit(RQ_SPLICE_OK, &rqstp->rq_flags);\n\n\t/* check stateid */\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t&read->rd_stateid, RD_STATE,\n\t\t\t\t\t&read->rd_filp, &read->rd_tmp_file);\n\tif (status) {\n\t\tdprintk(\"NFSD: nfsd4_read: couldn't process stateid!\\n\");\n\t\tgoto out;\n\t}\n\tstatus = nfs_ok;\nout:\n\tread->rd_rqstp = rqstp;\n\tread->rd_fhp = &cstate->current_fh;\n\treturn status;\n}\n\nstatic __be32\nnfsd4_readdir(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_readdir *readdir)\n{\n\tu64 cookie = readdir->rd_cookie;\n\tstatic const nfs4_verifier zeroverf;\n\n\t/* no need to check permission - this will be done in nfsd_readdir() */\n\n\tif (readdir->rd_bmval[1] & NFSD_WRITEONLY_ATTRS_WORD1)\n\t\treturn nfserr_inval;\n\n\treaddir->rd_bmval[0] &= nfsd_suppattrs[cstate->minorversion][0];\n\treaddir->rd_bmval[1] &= nfsd_suppattrs[cstate->minorversion][1];\n\treaddir->rd_bmval[2] &= nfsd_suppattrs[cstate->minorversion][2];\n\n\tif ((cookie == 1) || (cookie == 2) ||\n\t    (cookie == 0 && memcmp(readdir->rd_verf.data, zeroverf.data, NFS4_VERIFIER_SIZE)))\n\t\treturn nfserr_bad_cookie;\n\n\treaddir->rd_rqstp = rqstp;\n\treaddir->rd_fhp = &cstate->current_fh;\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_readlink(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t       struct nfsd4_readlink *readlink)\n{\n\treadlink->rl_rqstp = rqstp;\n\treadlink->rl_fhp = &cstate->current_fh;\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_remove(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_remove *remove)\n{\n\t__be32 status;\n\n\tif (opens_in_grace(SVC_NET(rqstp)))\n\t\treturn nfserr_grace;\n\tstatus = nfsd_unlink(rqstp, &cstate->current_fh, 0,\n\t\t\t     remove->rm_name, remove->rm_namelen);\n\tif (!status) {\n\t\tfh_unlock(&cstate->current_fh);\n\t\tset_change_info(&remove->rm_cinfo, &cstate->current_fh);\n\t}\n\treturn status;\n}\n\nstatic __be32\nnfsd4_rename(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_rename *rename)\n{\n\t__be32 status = nfserr_nofilehandle;\n\n\tif (!cstate->save_fh.fh_dentry)\n\t\treturn status;\n\tif (opens_in_grace(SVC_NET(rqstp)) &&\n\t\t!(cstate->save_fh.fh_export->ex_flags & NFSEXP_NOSUBTREECHECK))\n\t\treturn nfserr_grace;\n\tstatus = nfsd_rename(rqstp, &cstate->save_fh, rename->rn_sname,\n\t\t\t     rename->rn_snamelen, &cstate->current_fh,\n\t\t\t     rename->rn_tname, rename->rn_tnamelen);\n\tif (status)\n\t\treturn status;\n\tset_change_info(&rename->rn_sinfo, &cstate->current_fh);\n\tset_change_info(&rename->rn_tinfo, &cstate->save_fh);\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_secinfo(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_secinfo *secinfo)\n{\n\tstruct svc_export *exp;\n\tstruct dentry *dentry;\n\t__be32 err;\n\n\terr = fh_verify(rqstp, &cstate->current_fh, S_IFDIR, NFSD_MAY_EXEC);\n\tif (err)\n\t\treturn err;\n\terr = nfsd_lookup_dentry(rqstp, &cstate->current_fh,\n\t\t\t\t    secinfo->si_name, secinfo->si_namelen,\n\t\t\t\t    &exp, &dentry);\n\tif (err)\n\t\treturn err;\n\tfh_unlock(&cstate->current_fh);\n\tif (d_really_is_negative(dentry)) {\n\t\texp_put(exp);\n\t\terr = nfserr_noent;\n\t} else\n\t\tsecinfo->si_exp = exp;\n\tdput(dentry);\n\tif (cstate->minorversion)\n\t\t/* See rfc 5661 section 2.6.3.1.1.8 */\n\t\tfh_put(&cstate->current_fh);\n\treturn err;\n}\n\nstatic __be32\nnfsd4_secinfo_no_name(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_secinfo_no_name *sin)\n{\n\t__be32 err;\n\n\tswitch (sin->sin_style) {\n\tcase NFS4_SECINFO_STYLE4_CURRENT_FH:\n\t\tbreak;\n\tcase NFS4_SECINFO_STYLE4_PARENT:\n\t\terr = nfsd4_do_lookupp(rqstp, &cstate->current_fh);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\tdefault:\n\t\treturn nfserr_inval;\n\t}\n\n\tsin->sin_exp = exp_get(cstate->current_fh.fh_export);\n\tfh_put(&cstate->current_fh);\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_setattr(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_setattr *setattr)\n{\n\t__be32 status = nfs_ok;\n\tint err;\n\n\tif (setattr->sa_iattr.ia_valid & ATTR_SIZE) {\n\t\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate,\n\t\t\t\t&cstate->current_fh, &setattr->sa_stateid,\n\t\t\t\tWR_STATE, NULL, NULL);\n\t\tif (status) {\n\t\t\tdprintk(\"NFSD: nfsd4_setattr: couldn't process stateid!\\n\");\n\t\t\treturn status;\n\t\t}\n\t}\n\terr = fh_want_write(&cstate->current_fh);\n\tif (err)\n\t\treturn nfserrno(err);\n\tstatus = nfs_ok;\n\n\tstatus = check_attr_support(rqstp, cstate, setattr->sa_bmval,\n\t\t\t\t    nfsd_attrmask);\n\tif (status)\n\t\tgoto out;\n\n\tif (setattr->sa_acl != NULL)\n\t\tstatus = nfsd4_set_nfs4_acl(rqstp, &cstate->current_fh,\n\t\t\t\t\t    setattr->sa_acl);\n\tif (status)\n\t\tgoto out;\n\tif (setattr->sa_label.len)\n\t\tstatus = nfsd4_set_nfs4_label(rqstp, &cstate->current_fh,\n\t\t\t\t&setattr->sa_label);\n\tif (status)\n\t\tgoto out;\n\tstatus = nfsd_setattr(rqstp, &cstate->current_fh, &setattr->sa_iattr,\n\t\t\t\t0, (time_t)0);\nout:\n\tfh_drop_write(&cstate->current_fh);\n\treturn status;\n}\n\nstatic int fill_in_write_vector(struct kvec *vec, struct nfsd4_write *write)\n{\n        int i = 1;\n        int buflen = write->wr_buflen;\n\n        vec[0].iov_base = write->wr_head.iov_base;\n        vec[0].iov_len = min_t(int, buflen, write->wr_head.iov_len);\n        buflen -= vec[0].iov_len;\n\n        while (buflen) {\n                vec[i].iov_base = page_address(write->wr_pagelist[i - 1]);\n                vec[i].iov_len = min_t(int, PAGE_SIZE, buflen);\n                buflen -= vec[i].iov_len;\n                i++;\n        }\n        return i;\n}\n\nstatic __be32\nnfsd4_write(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct nfsd4_write *write)\n{\n\tstateid_t *stateid = &write->wr_stateid;\n\tstruct file *filp = NULL;\n\t__be32 status = nfs_ok;\n\tunsigned long cnt;\n\tint nvecs;\n\n\tif (write->wr_offset >= OFFSET_MAX)\n\t\treturn nfserr_inval;\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t\tstateid, WR_STATE, &filp, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: nfsd4_write: couldn't process stateid!\\n\");\n\t\treturn status;\n\t}\n\n\tcnt = write->wr_buflen;\n\twrite->wr_how_written = write->wr_stable_how;\n\tgen_boot_verifier(&write->wr_verifier, SVC_NET(rqstp));\n\n\tnvecs = fill_in_write_vector(rqstp->rq_vec, write);\n\tWARN_ON_ONCE(nvecs > ARRAY_SIZE(rqstp->rq_vec));\n\n\tstatus = nfsd_vfs_write(rqstp, &cstate->current_fh, filp,\n\t\t\t\twrite->wr_offset, rqstp->rq_vec, nvecs, &cnt,\n\t\t\t\twrite->wr_how_written);\n\tfput(filp);\n\n\twrite->wr_bytes_written = cnt;\n\n\treturn status;\n}\n\nstatic __be32\nnfsd4_verify_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t  stateid_t *src_stateid, struct file **src,\n\t\t  stateid_t *dst_stateid, struct file **dst)\n{\n\t__be32 status;\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->save_fh,\n\t\t\t\t\t    src_stateid, RD_STATE, src, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: %s: couldn't process src stateid!\\n\", __func__);\n\t\tgoto out;\n\t}\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t    dst_stateid, WR_STATE, dst, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: %s: couldn't process dst stateid!\\n\", __func__);\n\t\tgoto out_put_src;\n\t}\n\n\t/* fix up for NFS-specific error code */\n\tif (!S_ISREG(file_inode(*src)->i_mode) ||\n\t    !S_ISREG(file_inode(*dst)->i_mode)) {\n\t\tstatus = nfserr_wrong_type;\n\t\tgoto out_put_dst;\n\t}\n\nout:\n\treturn status;\nout_put_dst:\n\tfput(*dst);\nout_put_src:\n\tfput(*src);\n\tgoto out;\n}\n\nstatic __be32\nnfsd4_clone(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_clone *clone)\n{\n\tstruct file *src, *dst;\n\t__be32 status;\n\n\tstatus = nfsd4_verify_copy(rqstp, cstate, &clone->cl_src_stateid, &src,\n\t\t\t\t   &clone->cl_dst_stateid, &dst);\n\tif (status)\n\t\tgoto out;\n\n\tstatus = nfsd4_clone_file_range(src, clone->cl_src_pos,\n\t\t\tdst, clone->cl_dst_pos, clone->cl_count);\n\n\tfput(dst);\n\tfput(src);\nout:\n\treturn status;\n}\n\nstatic __be32\nnfsd4_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_copy *copy)\n{\n\tstruct file *src, *dst;\n\t__be32 status;\n\tssize_t bytes;\n\n\tstatus = nfsd4_verify_copy(rqstp, cstate, &copy->cp_src_stateid, &src,\n\t\t\t\t   &copy->cp_dst_stateid, &dst);\n\tif (status)\n\t\tgoto out;\n\n\tbytes = nfsd_copy_file_range(src, copy->cp_src_pos,\n\t\t\tdst, copy->cp_dst_pos, copy->cp_count);\n\n\tif (bytes < 0)\n\t\tstatus = nfserrno(bytes);\n\telse {\n\t\tcopy->cp_res.wr_bytes_written = bytes;\n\t\tcopy->cp_res.wr_stable_how = NFS_UNSTABLE;\n\t\tcopy->cp_consecutive = 1;\n\t\tcopy->cp_synchronous = 1;\n\t\tgen_boot_verifier(&copy->cp_res.wr_verifier, SVC_NET(rqstp));\n\t\tstatus = nfs_ok;\n\t}\n\n\tfput(src);\n\tfput(dst);\nout:\n\treturn status;\n}\n\nstatic __be32\nnfsd4_fallocate(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_fallocate *fallocate, int flags)\n{\n\t__be32 status = nfserr_notsupp;\n\tstruct file *file;\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t    &fallocate->falloc_stateid,\n\t\t\t\t\t    WR_STATE, &file, NULL);\n\tif (status != nfs_ok) {\n\t\tdprintk(\"NFSD: nfsd4_fallocate: couldn't process stateid!\\n\");\n\t\treturn status;\n\t}\n\n\tstatus = nfsd4_vfs_fallocate(rqstp, &cstate->current_fh, file,\n\t\t\t\t     fallocate->falloc_offset,\n\t\t\t\t     fallocate->falloc_length,\n\t\t\t\t     flags);\n\tfput(file);\n\treturn status;\n}\n\nstatic __be32\nnfsd4_allocate(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t       struct nfsd4_fallocate *fallocate)\n{\n\treturn nfsd4_fallocate(rqstp, cstate, fallocate, 0);\n}\n\nstatic __be32\nnfsd4_deallocate(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t struct nfsd4_fallocate *fallocate)\n{\n\treturn nfsd4_fallocate(rqstp, cstate, fallocate,\n\t\t\t       FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE);\n}\n\nstatic __be32\nnfsd4_seek(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_seek *seek)\n{\n\tint whence;\n\t__be32 status;\n\tstruct file *file;\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t    &seek->seek_stateid,\n\t\t\t\t\t    RD_STATE, &file, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: nfsd4_seek: couldn't process stateid!\\n\");\n\t\treturn status;\n\t}\n\n\tswitch (seek->seek_whence) {\n\tcase NFS4_CONTENT_DATA:\n\t\twhence = SEEK_DATA;\n\t\tbreak;\n\tcase NFS4_CONTENT_HOLE:\n\t\twhence = SEEK_HOLE;\n\t\tbreak;\n\tdefault:\n\t\tstatus = nfserr_union_notsupp;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Note:  This call does change file->f_pos, but nothing in NFSD\n\t *        should ever file->f_pos.\n\t */\n\tseek->seek_pos = vfs_llseek(file, seek->seek_offset, whence);\n\tif (seek->seek_pos < 0)\n\t\tstatus = nfserrno(seek->seek_pos);\n\telse if (seek->seek_pos >= i_size_read(file_inode(file)))\n\t\tseek->seek_eof = true;\n\nout:\n\tfput(file);\n\treturn status;\n}\n\n/* This routine never returns NFS_OK!  If there are no other errors, it\n * will return NFSERR_SAME or NFSERR_NOT_SAME depending on whether the\n * attributes matched.  VERIFY is implemented by mapping NFSERR_SAME\n * to NFS_OK after the call; NVERIFY by mapping NFSERR_NOT_SAME to NFS_OK.\n */\nstatic __be32\n_nfsd4_verify(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_verify *verify)\n{\n\t__be32 *buf, *p;\n\tint count;\n\t__be32 status;\n\n\tstatus = fh_verify(rqstp, &cstate->current_fh, 0, NFSD_MAY_NOP);\n\tif (status)\n\t\treturn status;\n\n\tstatus = check_attr_support(rqstp, cstate, verify->ve_bmval, NULL);\n\tif (status)\n\t\treturn status;\n\n\tif ((verify->ve_bmval[0] & FATTR4_WORD0_RDATTR_ERROR)\n\t    || (verify->ve_bmval[1] & NFSD_WRITEONLY_ATTRS_WORD1))\n\t\treturn nfserr_inval;\n\tif (verify->ve_attrlen & 3)\n\t\treturn nfserr_inval;\n\n\t/* count in words:\n\t *   bitmap_len(1) + bitmap(2) + attr_len(1) = 4\n\t */\n\tcount = 4 + (verify->ve_attrlen >> 2);\n\tbuf = kmalloc(count << 2, GFP_KERNEL);\n\tif (!buf)\n\t\treturn nfserr_jukebox;\n\n\tp = buf;\n\tstatus = nfsd4_encode_fattr_to_buf(&p, count, &cstate->current_fh,\n\t\t\t\t    cstate->current_fh.fh_export,\n\t\t\t\t    cstate->current_fh.fh_dentry,\n\t\t\t\t    verify->ve_bmval,\n\t\t\t\t    rqstp, 0);\n\t/*\n\t * If nfsd4_encode_fattr() ran out of space, assume that's because\n\t * the attributes are longer (hence different) than those given:\n\t */\n\tif (status == nfserr_resource)\n\t\tstatus = nfserr_not_same;\n\tif (status)\n\t\tgoto out_kfree;\n\n\t/* skip bitmap */\n\tp = buf + 1 + ntohl(buf[0]);\n\tstatus = nfserr_not_same;\n\tif (ntohl(*p++) != verify->ve_attrlen)\n\t\tgoto out_kfree;\n\tif (!memcmp(p, verify->ve_attrval, verify->ve_attrlen))\n\t\tstatus = nfserr_same;\n\nout_kfree:\n\tkfree(buf);\n\treturn status;\n}\n\nstatic __be32\nnfsd4_nverify(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_verify *verify)\n{\n\t__be32 status;\n\n\tstatus = _nfsd4_verify(rqstp, cstate, verify);\n\treturn status == nfserr_not_same ? nfs_ok : status;\n}\n\nstatic __be32\nnfsd4_verify(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_verify *verify)\n{\n\t__be32 status;\n\n\tstatus = _nfsd4_verify(rqstp, cstate, verify);\n\treturn status == nfserr_same ? nfs_ok : status;\n}\n\n#ifdef CONFIG_NFSD_PNFS\nstatic const struct nfsd4_layout_ops *\nnfsd4_layout_verify(struct svc_export *exp, unsigned int layout_type)\n{\n\tif (!exp->ex_layout_types) {\n\t\tdprintk(\"%s: export does not support pNFS\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\tif (!(exp->ex_layout_types & (1 << layout_type))) {\n\t\tdprintk(\"%s: layout type %d not supported\\n\",\n\t\t\t__func__, layout_type);\n\t\treturn NULL;\n\t}\n\n\treturn nfsd4_layout_ops[layout_type];\n}\n\nstatic __be32\nnfsd4_getdeviceinfo(struct svc_rqst *rqstp,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_getdeviceinfo *gdp)\n{\n\tconst struct nfsd4_layout_ops *ops;\n\tstruct nfsd4_deviceid_map *map;\n\tstruct svc_export *exp;\n\t__be32 nfserr;\n\n\tdprintk(\"%s: layout_type %u dev_id [0x%llx:0x%x] maxcnt %u\\n\",\n\t       __func__,\n\t       gdp->gd_layout_type,\n\t       gdp->gd_devid.fsid_idx, gdp->gd_devid.generation,\n\t       gdp->gd_maxcount);\n\n\tmap = nfsd4_find_devid_map(gdp->gd_devid.fsid_idx);\n\tif (!map) {\n\t\tdprintk(\"%s: couldn't find device ID to export mapping!\\n\",\n\t\t\t__func__);\n\t\treturn nfserr_noent;\n\t}\n\n\texp = rqst_exp_find(rqstp, map->fsid_type, map->fsid);\n\tif (IS_ERR(exp)) {\n\t\tdprintk(\"%s: could not find device id\\n\", __func__);\n\t\treturn nfserr_noent;\n\t}\n\n\tnfserr = nfserr_layoutunavailable;\n\tops = nfsd4_layout_verify(exp, gdp->gd_layout_type);\n\tif (!ops)\n\t\tgoto out;\n\n\tnfserr = nfs_ok;\n\tif (gdp->gd_maxcount != 0) {\n\t\tnfserr = ops->proc_getdeviceinfo(exp->ex_path.mnt->mnt_sb,\n\t\t\t\trqstp, cstate->session->se_client, gdp);\n\t}\n\n\tgdp->gd_notify_types &= ops->notify_types;\nout:\n\texp_put(exp);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_layoutget(struct svc_rqst *rqstp,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_layoutget *lgp)\n{\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\tconst struct nfsd4_layout_ops *ops;\n\tstruct nfs4_layout_stateid *ls;\n\t__be32 nfserr;\n\tint accmode;\n\n\tswitch (lgp->lg_seg.iomode) {\n\tcase IOMODE_READ:\n\t\taccmode = NFSD_MAY_READ;\n\t\tbreak;\n\tcase IOMODE_RW:\n\t\taccmode = NFSD_MAY_READ | NFSD_MAY_WRITE;\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"%s: invalid iomode %d\\n\",\n\t\t\t__func__, lgp->lg_seg.iomode);\n\t\tnfserr = nfserr_badiomode;\n\t\tgoto out;\n\t}\n\n\tnfserr = fh_verify(rqstp, current_fh, 0, accmode);\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfserr_layoutunavailable;\n\tops = nfsd4_layout_verify(current_fh->fh_export, lgp->lg_layout_type);\n\tif (!ops)\n\t\tgoto out;\n\n\t/*\n\t * Verify minlength and range as per RFC5661:\n\t *  o  If loga_length is less than loga_minlength,\n\t *     the metadata server MUST return NFS4ERR_INVAL.\n\t *  o  If the sum of loga_offset and loga_minlength exceeds\n\t *     NFS4_UINT64_MAX, and loga_minlength is not\n\t *     NFS4_UINT64_MAX, the error NFS4ERR_INVAL MUST result.\n\t *  o  If the sum of loga_offset and loga_length exceeds\n\t *     NFS4_UINT64_MAX, and loga_length is not NFS4_UINT64_MAX,\n\t *     the error NFS4ERR_INVAL MUST result.\n\t */\n\tnfserr = nfserr_inval;\n\tif (lgp->lg_seg.length < lgp->lg_minlength ||\n\t    (lgp->lg_minlength != NFS4_MAX_UINT64 &&\n\t     lgp->lg_minlength > NFS4_MAX_UINT64 - lgp->lg_seg.offset) ||\n\t    (lgp->lg_seg.length != NFS4_MAX_UINT64 &&\n\t     lgp->lg_seg.length > NFS4_MAX_UINT64 - lgp->lg_seg.offset))\n\t\tgoto out;\n\tif (lgp->lg_seg.length == 0)\n\t\tgoto out;\n\n\tnfserr = nfsd4_preprocess_layout_stateid(rqstp, cstate, &lgp->lg_sid,\n\t\t\t\t\t\ttrue, lgp->lg_layout_type, &ls);\n\tif (nfserr) {\n\t\ttrace_layout_get_lookup_fail(&lgp->lg_sid);\n\t\tgoto out;\n\t}\n\n\tnfserr = nfserr_recallconflict;\n\tif (atomic_read(&ls->ls_stid.sc_file->fi_lo_recalls))\n\t\tgoto out_put_stid;\n\n\tnfserr = ops->proc_layoutget(d_inode(current_fh->fh_dentry),\n\t\t\t\t     current_fh, lgp);\n\tif (nfserr)\n\t\tgoto out_put_stid;\n\n\tnfserr = nfsd4_insert_layout(lgp, ls);\n\nout_put_stid:\n\tmutex_unlock(&ls->ls_mutex);\n\tnfs4_put_stid(&ls->ls_stid);\nout:\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_layoutcommit(struct svc_rqst *rqstp,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_layoutcommit *lcp)\n{\n\tconst struct nfsd4_layout_seg *seg = &lcp->lc_seg;\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\tconst struct nfsd4_layout_ops *ops;\n\tloff_t new_size = lcp->lc_last_wr + 1;\n\tstruct inode *inode;\n\tstruct nfs4_layout_stateid *ls;\n\t__be32 nfserr;\n\n\tnfserr = fh_verify(rqstp, current_fh, 0, NFSD_MAY_WRITE);\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfserr_layoutunavailable;\n\tops = nfsd4_layout_verify(current_fh->fh_export, lcp->lc_layout_type);\n\tif (!ops)\n\t\tgoto out;\n\tinode = d_inode(current_fh->fh_dentry);\n\n\tnfserr = nfserr_inval;\n\tif (new_size <= seg->offset) {\n\t\tdprintk(\"pnfsd: last write before layout segment\\n\");\n\t\tgoto out;\n\t}\n\tif (new_size > seg->offset + seg->length) {\n\t\tdprintk(\"pnfsd: last write beyond layout segment\\n\");\n\t\tgoto out;\n\t}\n\tif (!lcp->lc_newoffset && new_size > i_size_read(inode)) {\n\t\tdprintk(\"pnfsd: layoutcommit beyond EOF\\n\");\n\t\tgoto out;\n\t}\n\n\tnfserr = nfsd4_preprocess_layout_stateid(rqstp, cstate, &lcp->lc_sid,\n\t\t\t\t\t\tfalse, lcp->lc_layout_type,\n\t\t\t\t\t\t&ls);\n\tif (nfserr) {\n\t\ttrace_layout_commit_lookup_fail(&lcp->lc_sid);\n\t\t/* fixup error code as per RFC5661 */\n\t\tif (nfserr == nfserr_bad_stateid)\n\t\t\tnfserr = nfserr_badlayout;\n\t\tgoto out;\n\t}\n\n\t/* LAYOUTCOMMIT does not require any serialization */\n\tmutex_unlock(&ls->ls_mutex);\n\n\tif (new_size > i_size_read(inode)) {\n\t\tlcp->lc_size_chg = 1;\n\t\tlcp->lc_newsize = new_size;\n\t} else {\n\t\tlcp->lc_size_chg = 0;\n\t}\n\n\tnfserr = ops->proc_layoutcommit(inode, lcp);\n\tnfs4_put_stid(&ls->ls_stid);\nout:\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_layoutreturn(struct svc_rqst *rqstp,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_layoutreturn *lrp)\n{\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\t__be32 nfserr;\n\n\tnfserr = fh_verify(rqstp, current_fh, 0, NFSD_MAY_NOP);\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfserr_layoutunavailable;\n\tif (!nfsd4_layout_verify(current_fh->fh_export, lrp->lr_layout_type))\n\t\tgoto out;\n\n\tswitch (lrp->lr_seg.iomode) {\n\tcase IOMODE_READ:\n\tcase IOMODE_RW:\n\tcase IOMODE_ANY:\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"%s: invalid iomode %d\\n\", __func__,\n\t\t\tlrp->lr_seg.iomode);\n\t\tnfserr = nfserr_inval;\n\t\tgoto out;\n\t}\n\n\tswitch (lrp->lr_return_type) {\n\tcase RETURN_FILE:\n\t\tnfserr = nfsd4_return_file_layouts(rqstp, cstate, lrp);\n\t\tbreak;\n\tcase RETURN_FSID:\n\tcase RETURN_ALL:\n\t\tnfserr = nfsd4_return_client_layouts(rqstp, cstate, lrp);\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"%s: invalid return_type %d\\n\", __func__,\n\t\t\tlrp->lr_return_type);\n\t\tnfserr = nfserr_inval;\n\t\tbreak;\n\t}\nout:\n\treturn nfserr;\n}\n#endif /* CONFIG_NFSD_PNFS */\n\n/*\n * NULL call.\n */\nstatic __be32\nnfsd4_proc_null(struct svc_rqst *rqstp, void *argp, void *resp)\n{\n\treturn nfs_ok;\n}\n\nstatic inline void nfsd4_increment_op_stats(u32 opnum)\n{\n\tif (opnum >= FIRST_NFS4_OP && opnum <= LAST_NFS4_OP)\n\t\tnfsdstats.nfs4_opcount[opnum]++;\n}\n\ntypedef __be32(*nfsd4op_func)(struct svc_rqst *, struct nfsd4_compound_state *,\n\t\t\t      void *);\ntypedef u32(*nfsd4op_rsize)(struct svc_rqst *, struct nfsd4_op *op);\ntypedef void(*stateid_setter)(struct nfsd4_compound_state *, void *);\ntypedef void(*stateid_getter)(struct nfsd4_compound_state *, void *);\n\nenum nfsd4_op_flags {\n\tALLOWED_WITHOUT_FH = 1 << 0,\t/* No current filehandle required */\n\tALLOWED_ON_ABSENT_FS = 1 << 1,\t/* ops processed on absent fs */\n\tALLOWED_AS_FIRST_OP = 1 << 2,\t/* ops reqired first in compound */\n\t/* For rfc 5661 section 2.6.3.1.1: */\n\tOP_HANDLES_WRONGSEC = 1 << 3,\n\tOP_IS_PUTFH_LIKE = 1 << 4,\n\t/*\n\t * These are the ops whose result size we estimate before\n\t * encoding, to avoid performing an op then not being able to\n\t * respond or cache a response.  This includes writes and setattrs\n\t * as well as the operations usually called \"nonidempotent\":\n\t */\n\tOP_MODIFIES_SOMETHING = 1 << 5,\n\t/*\n\t * Cache compounds containing these ops in the xid-based drc:\n\t * We use the DRC for compounds containing non-idempotent\n\t * operations, *except* those that are 4.1-specific (since\n\t * sessions provide their own EOS), and except for stateful\n\t * operations other than setclientid and setclientid_confirm\n\t * (since sequence numbers provide EOS for open, lock, etc in\n\t * the v4.0 case).\n\t */\n\tOP_CACHEME = 1 << 6,\n\t/*\n\t * These are ops which clear current state id.\n\t */\n\tOP_CLEAR_STATEID = 1 << 7,\n};\n\nstruct nfsd4_operation {\n\tnfsd4op_func op_func;\n\tu32 op_flags;\n\tchar *op_name;\n\t/* Try to get response size before operation */\n\tnfsd4op_rsize op_rsize_bop;\n\tstateid_getter op_get_currentstateid;\n\tstateid_setter op_set_currentstateid;\n};\n\nstatic struct nfsd4_operation nfsd4_ops[];\n\nstatic const char *nfsd4_op_name(unsigned opnum);\n\n/*\n * Enforce NFSv4.1 COMPOUND ordering rules:\n *\n * Also note, enforced elsewhere:\n *\t- SEQUENCE other than as first op results in\n *\t  NFS4ERR_SEQUENCE_POS. (Enforced in nfsd4_sequence().)\n *\t- BIND_CONN_TO_SESSION must be the only op in its compound.\n *\t  (Enforced in nfsd4_bind_conn_to_session().)\n *\t- DESTROY_SESSION must be the final operation in a compound, if\n *\t  sessionid's in SEQUENCE and DESTROY_SESSION are the same.\n *\t  (Enforced in nfsd4_destroy_session().)\n */\nstatic __be32 nfs41_check_op_ordering(struct nfsd4_compoundargs *args)\n{\n\tstruct nfsd4_op *op = &args->ops[0];\n\n\t/* These ordering requirements don't apply to NFSv4.0: */\n\tif (args->minorversion == 0)\n\t\treturn nfs_ok;\n\t/* This is weird, but OK, not our problem: */\n\tif (args->opcnt == 0)\n\t\treturn nfs_ok;\n\tif (op->status == nfserr_op_illegal)\n\t\treturn nfs_ok;\n\tif (!(nfsd4_ops[op->opnum].op_flags & ALLOWED_AS_FIRST_OP))\n\t\treturn nfserr_op_not_in_session;\n\tif (op->opnum == OP_SEQUENCE)\n\t\treturn nfs_ok;\n\tif (args->opcnt != 1)\n\t\treturn nfserr_not_only_op;\n\treturn nfs_ok;\n}\n\nstatic inline struct nfsd4_operation *OPDESC(struct nfsd4_op *op)\n{\n\treturn &nfsd4_ops[op->opnum];\n}\n\nbool nfsd4_cache_this_op(struct nfsd4_op *op)\n{\n\tif (op->opnum == OP_ILLEGAL)\n\t\treturn false;\n\treturn OPDESC(op)->op_flags & OP_CACHEME;\n}\n\nstatic bool need_wrongsec_check(struct svc_rqst *rqstp)\n{\n\tstruct nfsd4_compoundres *resp = rqstp->rq_resp;\n\tstruct nfsd4_compoundargs *argp = rqstp->rq_argp;\n\tstruct nfsd4_op *this = &argp->ops[resp->opcnt - 1];\n\tstruct nfsd4_op *next = &argp->ops[resp->opcnt];\n\tstruct nfsd4_operation *thisd;\n\tstruct nfsd4_operation *nextd;\n\n\tthisd = OPDESC(this);\n\t/*\n\t * Most ops check wronsec on our own; only the putfh-like ops\n\t * have special rules.\n\t */\n\tif (!(thisd->op_flags & OP_IS_PUTFH_LIKE))\n\t\treturn false;\n\t/*\n\t * rfc 5661 2.6.3.1.1.6: don't bother erroring out a\n\t * put-filehandle operation if we're not going to use the\n\t * result:\n\t */\n\tif (argp->opcnt == resp->opcnt)\n\t\treturn false;\n\tif (next->opnum == OP_ILLEGAL)\n\t\treturn false;\n\tnextd = OPDESC(next);\n\t/*\n\t * Rest of 2.6.3.1.1: certain operations will return WRONGSEC\n\t * errors themselves as necessary; others should check for them\n\t * now:\n\t */\n\treturn !(nextd->op_flags & OP_HANDLES_WRONGSEC);\n}\n\nstatic void svcxdr_init_encode(struct svc_rqst *rqstp,\n\t\t\t       struct nfsd4_compoundres *resp)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct xdr_buf *buf = &rqstp->rq_res;\n\tstruct kvec *head = buf->head;\n\n\txdr->buf = buf;\n\txdr->iov = head;\n\txdr->p   = head->iov_base + head->iov_len;\n\txdr->end = head->iov_base + PAGE_SIZE - rqstp->rq_auth_slack;\n\t/* Tail and page_len should be zero at this point: */\n\tbuf->len = buf->head[0].iov_len;\n\txdr->scratch.iov_len = 0;\n\txdr->page_ptr = buf->pages - 1;\n\tbuf->buflen = PAGE_SIZE * (1 + rqstp->rq_page_end - buf->pages)\n\t\t- rqstp->rq_auth_slack;\n}\n\n/*\n * COMPOUND call.\n */\nstatic __be32\nnfsd4_proc_compound(struct svc_rqst *rqstp,\n\t\t    struct nfsd4_compoundargs *args,\n\t\t    struct nfsd4_compoundres *resp)\n{\n\tstruct nfsd4_op\t*op;\n\tstruct nfsd4_operation *opdesc;\n\tstruct nfsd4_compound_state *cstate = &resp->cstate;\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\tstruct svc_fh *save_fh = &cstate->save_fh;\n\t__be32\t\tstatus;\n\n\tsvcxdr_init_encode(rqstp, resp);\n\tresp->tagp = resp->xdr.p;\n\t/* reserve space for: taglen, tag, and opcnt */\n\txdr_reserve_space(&resp->xdr, 8 + args->taglen);\n\tresp->taglen = args->taglen;\n\tresp->tag = args->tag;\n\tresp->rqstp = rqstp;\n\tcstate->minorversion = args->minorversion;\n\tfh_init(current_fh, NFS4_FHSIZE);\n\tfh_init(save_fh, NFS4_FHSIZE);\n\t/*\n\t * Don't use the deferral mechanism for NFSv4; compounds make it\n\t * too hard to avoid non-idempotency problems.\n\t */\n\tclear_bit(RQ_USEDEFERRAL, &rqstp->rq_flags);\n\n\t/*\n\t * According to RFC3010, this takes precedence over all other errors.\n\t */\n\tstatus = nfserr_minor_vers_mismatch;\n\tif (nfsd_minorversion(args->minorversion, NFSD_TEST) <= 0)\n\t\tgoto out;\n\n\tstatus = nfs41_check_op_ordering(args);\n\tif (status) {\n\t\top = &args->ops[0];\n\t\top->status = status;\n\t\tgoto encode_op;\n\t}\n\n\twhile (!status && resp->opcnt < args->opcnt) {\n\t\top = &args->ops[resp->opcnt++];\n\n\t\tdprintk(\"nfsv4 compound op #%d/%d: %d (%s)\\n\",\n\t\t\tresp->opcnt, args->opcnt, op->opnum,\n\t\t\tnfsd4_op_name(op->opnum));\n\t\t/*\n\t\t * The XDR decode routines may have pre-set op->status;\n\t\t * for example, if there is a miscellaneous XDR error\n\t\t * it will be set to nfserr_bad_xdr.\n\t\t */\n\t\tif (op->status) {\n\t\t\tif (op->opnum == OP_OPEN)\n\t\t\t\top->status = nfsd4_open_omfg(rqstp, cstate, op);\n\t\t\tgoto encode_op;\n\t\t}\n\n\t\topdesc = OPDESC(op);\n\n\t\tif (!current_fh->fh_dentry) {\n\t\t\tif (!(opdesc->op_flags & ALLOWED_WITHOUT_FH)) {\n\t\t\t\top->status = nfserr_nofilehandle;\n\t\t\t\tgoto encode_op;\n\t\t\t}\n\t\t} else if (current_fh->fh_export->ex_fslocs.migrated &&\n\t\t\t  !(opdesc->op_flags & ALLOWED_ON_ABSENT_FS)) {\n\t\t\top->status = nfserr_moved;\n\t\t\tgoto encode_op;\n\t\t}\n\n\t\tfh_clear_wcc(current_fh);\n\n\t\t/* If op is non-idempotent */\n\t\tif (opdesc->op_flags & OP_MODIFIES_SOMETHING) {\n\t\t\t/*\n\t\t\t * Don't execute this op if we couldn't encode a\n\t\t\t * succesful reply:\n\t\t\t */\n\t\t\tu32 plen = opdesc->op_rsize_bop(rqstp, op);\n\t\t\t/*\n\t\t\t * Plus if there's another operation, make sure\n\t\t\t * we'll have space to at least encode an error:\n\t\t\t */\n\t\t\tif (resp->opcnt < args->opcnt)\n\t\t\t\tplen += COMPOUND_ERR_SLACK_SPACE;\n\t\t\top->status = nfsd4_check_resp_size(resp, plen);\n\t\t}\n\n\t\tif (op->status)\n\t\t\tgoto encode_op;\n\n\t\tif (opdesc->op_get_currentstateid)\n\t\t\topdesc->op_get_currentstateid(cstate, &op->u);\n\t\top->status = opdesc->op_func(rqstp, cstate, &op->u);\n\n\t\tif (!op->status) {\n\t\t\tif (opdesc->op_set_currentstateid)\n\t\t\t\topdesc->op_set_currentstateid(cstate, &op->u);\n\n\t\t\tif (opdesc->op_flags & OP_CLEAR_STATEID)\n\t\t\t\tclear_current_stateid(cstate);\n\n\t\t\tif (need_wrongsec_check(rqstp))\n\t\t\t\top->status = check_nfsd_access(current_fh->fh_export, rqstp);\n\t\t}\n\nencode_op:\n\t\t/* Only from SEQUENCE */\n\t\tif (cstate->status == nfserr_replay_cache) {\n\t\t\tdprintk(\"%s NFS4.1 replay from cache\\n\", __func__);\n\t\t\tstatus = op->status;\n\t\t\tgoto out;\n\t\t}\n\t\tif (op->status == nfserr_replay_me) {\n\t\t\top->replay = &cstate->replay_owner->so_replay;\n\t\t\tnfsd4_encode_replay(&resp->xdr, op);\n\t\t\tstatus = op->status = op->replay->rp_status;\n\t\t} else {\n\t\t\tnfsd4_encode_operation(resp, op);\n\t\t\tstatus = op->status;\n\t\t}\n\n\t\tdprintk(\"nfsv4 compound op %p opcnt %d #%d: %d: status %d\\n\",\n\t\t\targs->ops, args->opcnt, resp->opcnt, op->opnum,\n\t\t\tbe32_to_cpu(status));\n\n\t\tnfsd4_cstate_clear_replay(cstate);\n\t\tnfsd4_increment_op_stats(op->opnum);\n\t}\n\n\tcstate->status = status;\n\tfh_put(current_fh);\n\tfh_put(save_fh);\n\tBUG_ON(cstate->replay_owner);\nout:\n\t/* Reset deferral mechanism for RPC deferrals */\n\tset_bit(RQ_USEDEFERRAL, &rqstp->rq_flags);\n\tdprintk(\"nfsv4 compound returned %d\\n\", ntohl(status));\n\treturn status;\n}\n\n#define op_encode_hdr_size\t\t(2)\n#define op_encode_stateid_maxsz\t\t(XDR_QUADLEN(NFS4_STATEID_SIZE))\n#define op_encode_verifier_maxsz\t(XDR_QUADLEN(NFS4_VERIFIER_SIZE))\n#define op_encode_change_info_maxsz\t(5)\n#define nfs4_fattr_bitmap_maxsz\t\t(4)\n\n/* We'll fall back on returning no lockowner if run out of space: */\n#define op_encode_lockowner_maxsz\t(0)\n#define op_encode_lock_denied_maxsz\t(8 + op_encode_lockowner_maxsz)\n\n#define nfs4_owner_maxsz\t\t(1 + XDR_QUADLEN(IDMAP_NAMESZ))\n\n#define op_encode_ace_maxsz\t\t(3 + nfs4_owner_maxsz)\n#define op_encode_delegation_maxsz\t(1 + op_encode_stateid_maxsz + 1 + \\\n\t\t\t\t\t op_encode_ace_maxsz)\n\n#define op_encode_channel_attrs_maxsz\t(6 + 1 + 1)\n\nstatic inline u32 nfsd4_only_status_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_status_stateid_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_stateid_maxsz)* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_access_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\t/* ac_supported, ac_resp_access */\n\treturn (op_encode_hdr_size + 2)* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_commit_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_verifier_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_create_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_change_info_maxsz\n\t\t+ nfs4_fattr_bitmap_maxsz) * sizeof(__be32);\n}\n\n/*\n * Note since this is an idempotent operation we won't insist on failing\n * the op prematurely if the estimate is too large.  We may turn off splice\n * reads unnecessarily.\n */\nstatic inline u32 nfsd4_getattr_rsize(struct svc_rqst *rqstp,\n\t\t\t\t      struct nfsd4_op *op)\n{\n\tu32 *bmap = op->u.getattr.ga_bmval;\n\tu32 bmap0 = bmap[0], bmap1 = bmap[1], bmap2 = bmap[2];\n\tu32 ret = 0;\n\n\tif (bmap0 & FATTR4_WORD0_ACL)\n\t\treturn svc_max_payload(rqstp);\n\tif (bmap0 & FATTR4_WORD0_FS_LOCATIONS)\n\t\treturn svc_max_payload(rqstp);\n\n\tif (bmap1 & FATTR4_WORD1_OWNER) {\n\t\tret += IDMAP_NAMESZ + 4;\n\t\tbmap1 &= ~FATTR4_WORD1_OWNER;\n\t}\n\tif (bmap1 & FATTR4_WORD1_OWNER_GROUP) {\n\t\tret += IDMAP_NAMESZ + 4;\n\t\tbmap1 &= ~FATTR4_WORD1_OWNER_GROUP;\n\t}\n\tif (bmap0 & FATTR4_WORD0_FILEHANDLE) {\n\t\tret += NFS4_FHSIZE + 4;\n\t\tbmap0 &= ~FATTR4_WORD0_FILEHANDLE;\n\t}\n\tif (bmap2 & FATTR4_WORD2_SECURITY_LABEL) {\n\t\tret += NFS4_MAXLABELLEN + 12;\n\t\tbmap2 &= ~FATTR4_WORD2_SECURITY_LABEL;\n\t}\n\t/*\n\t * Largest of remaining attributes are 16 bytes (e.g.,\n\t * supported_attributes)\n\t */\n\tret += 16 * (hweight32(bmap0) + hweight32(bmap1) + hweight32(bmap2));\n\t/* bitmask, length */\n\tret += 20;\n\treturn ret;\n}\n\nstatic inline u32 nfsd4_getfh_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 1) * sizeof(__be32) + NFS4_FHSIZE;\n}\n\nstatic inline u32 nfsd4_link_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_change_info_maxsz)\n\t\t* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_lock_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_lock_denied_maxsz)\n\t\t* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_open_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_stateid_maxsz\n\t\t+ op_encode_change_info_maxsz + 1\n\t\t+ nfs4_fattr_bitmap_maxsz\n\t\t+ op_encode_delegation_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_read_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\tu32 maxcount = 0, rlen = 0;\n\n\tmaxcount = svc_max_payload(rqstp);\n\trlen = min(op->u.read.rd_length, maxcount);\n\n\treturn (op_encode_hdr_size + 2 + XDR_QUADLEN(rlen)) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_readdir_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\tu32 maxcount = 0, rlen = 0;\n\n\tmaxcount = svc_max_payload(rqstp);\n\trlen = min(op->u.readdir.rd_maxcount, maxcount);\n\n\treturn (op_encode_hdr_size + op_encode_verifier_maxsz +\n\t\tXDR_QUADLEN(rlen)) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_readlink_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 1) * sizeof(__be32) + PAGE_SIZE;\n}\n\nstatic inline u32 nfsd4_remove_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_change_info_maxsz)\n\t\t* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_rename_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_change_info_maxsz\n\t\t+ op_encode_change_info_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_sequence_rsize(struct svc_rqst *rqstp,\n\t\t\t\t       struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size\n\t\t+ XDR_QUADLEN(NFS4_MAX_SESSIONID_LEN) + 5) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_test_stateid_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 1 + op->u.test_stateid.ts_num_ids)\n\t\t* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_setattr_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + nfs4_fattr_bitmap_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_secinfo_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + RPC_AUTH_MAXFLAVOR *\n\t\t(4 + XDR_QUADLEN(GSS_OID_MAX_LEN))) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_setclientid_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 2 + XDR_QUADLEN(NFS4_VERIFIER_SIZE)) *\n\t\t\t\t\t\t\t\tsizeof(__be32);\n}\n\nstatic inline u32 nfsd4_write_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 2 + op_encode_verifier_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_exchange_id_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 2 + 1 + /* eir_clientid, eir_sequenceid */\\\n\t\t1 + 1 + /* eir_flags, spr_how */\\\n\t\t4 + /* spo_must_enforce & _allow with bitmap */\\\n\t\t2 + /*eir_server_owner.so_minor_id */\\\n\t\t/* eir_server_owner.so_major_id<> */\\\n\t\tXDR_QUADLEN(NFS4_OPAQUE_LIMIT) + 1 +\\\n\t\t/* eir_server_scope<> */\\\n\t\tXDR_QUADLEN(NFS4_OPAQUE_LIMIT) + 1 +\\\n\t\t1 + /* eir_server_impl_id array length */\\\n\t\t0 /* ignored eir_server_impl_id contents */) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_bind_conn_to_session_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + \\\n\t\tXDR_QUADLEN(NFS4_MAX_SESSIONID_LEN) + /* bctsr_sessid */\\\n\t\t2 /* bctsr_dir, use_conn_in_rdma_mode */) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_create_session_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + \\\n\t\tXDR_QUADLEN(NFS4_MAX_SESSIONID_LEN) + /* sessionid */\\\n\t\t2 + /* csr_sequence, csr_flags */\\\n\t\top_encode_channel_attrs_maxsz + \\\n\t\top_encode_channel_attrs_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_copy_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size +\n\t\t1 /* wr_callback */ +\n\t\top_encode_stateid_maxsz /* wr_callback */ +\n\t\t2 /* wr_count */ +\n\t\t1 /* wr_committed */ +\n\t\top_encode_verifier_maxsz +\n\t\t1 /* cr_consecutive */ +\n\t\t1 /* cr_synchronous */) * sizeof(__be32);\n}\n\n#ifdef CONFIG_NFSD_PNFS\nstatic inline u32 nfsd4_getdeviceinfo_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\tu32 maxcount = 0, rlen = 0;\n\n\tmaxcount = svc_max_payload(rqstp);\n\trlen = min(op->u.getdeviceinfo.gd_maxcount, maxcount);\n\n\treturn (op_encode_hdr_size +\n\t\t1 /* gd_layout_type*/ +\n\t\tXDR_QUADLEN(rlen) +\n\t\t2 /* gd_notify_types */) * sizeof(__be32);\n}\n\n/*\n * At this stage we don't really know what layout driver will handle the request,\n * so we need to define an arbitrary upper bound here.\n */\n#define MAX_LAYOUT_SIZE\t\t128\nstatic inline u32 nfsd4_layoutget_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size +\n\t\t1 /* logr_return_on_close */ +\n\t\top_encode_stateid_maxsz +\n\t\t1 /* nr of layouts */ +\n\t\tMAX_LAYOUT_SIZE) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_layoutcommit_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size +\n\t\t1 /* locr_newsize */ +\n\t\t2 /* ns_size */) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_layoutreturn_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size +\n\t\t1 /* lrs_stateid */ +\n\t\top_encode_stateid_maxsz) * sizeof(__be32);\n}\n#endif /* CONFIG_NFSD_PNFS */\n\n\nstatic inline u32 nfsd4_seek_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 3) * sizeof(__be32);\n}\n\nstatic struct nfsd4_operation nfsd4_ops[] = {\n\t[OP_ACCESS] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_access,\n\t\t.op_name = \"OP_ACCESS\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_access_rsize,\n\t},\n\t[OP_CLOSE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_close,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_CLOSE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_status_stateid_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_closestateid,\n\t\t.op_set_currentstateid = (stateid_setter)nfsd4_set_closestateid,\n\t},\n\t[OP_COMMIT] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_commit,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_COMMIT\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_commit_rsize,\n\t},\n\t[OP_CREATE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_create,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_CREATE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_create_rsize,\n\t},\n\t[OP_DELEGRETURN] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_delegreturn,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_DELEGRETURN\",\n\t\t.op_rsize_bop = nfsd4_only_status_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_delegreturnstateid,\n\t},\n\t[OP_GETATTR] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_getattr,\n\t\t.op_flags = ALLOWED_ON_ABSENT_FS,\n\t\t.op_rsize_bop = nfsd4_getattr_rsize,\n\t\t.op_name = \"OP_GETATTR\",\n\t},\n\t[OP_GETFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_getfh,\n\t\t.op_name = \"OP_GETFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_getfh_rsize,\n\t},\n\t[OP_LINK] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_link,\n\t\t.op_flags = ALLOWED_ON_ABSENT_FS | OP_MODIFIES_SOMETHING\n\t\t\t\t| OP_CACHEME,\n\t\t.op_name = \"OP_LINK\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_link_rsize,\n\t},\n\t[OP_LOCK] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_lock,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_LOCK\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_lock_rsize,\n\t\t.op_set_currentstateid = (stateid_setter)nfsd4_set_lockstateid,\n\t},\n\t[OP_LOCKT] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_lockt,\n\t\t.op_name = \"OP_LOCKT\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_lock_rsize,\n\t},\n\t[OP_LOCKU] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_locku,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_LOCKU\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_status_stateid_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_lockustateid,\n\t},\n\t[OP_LOOKUP] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_lookup,\n\t\t.op_flags = OP_HANDLES_WRONGSEC | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_LOOKUP\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_LOOKUPP] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_lookupp,\n\t\t.op_flags = OP_HANDLES_WRONGSEC | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_LOOKUPP\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_NVERIFY] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_nverify,\n\t\t.op_name = \"OP_NVERIFY\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_OPEN] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_open,\n\t\t.op_flags = OP_HANDLES_WRONGSEC | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_OPEN\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_open_rsize,\n\t\t.op_set_currentstateid = (stateid_setter)nfsd4_set_openstateid,\n\t},\n\t[OP_OPEN_CONFIRM] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_open_confirm,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_OPEN_CONFIRM\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_status_stateid_rsize,\n\t},\n\t[OP_OPEN_DOWNGRADE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_open_downgrade,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_OPEN_DOWNGRADE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_status_stateid_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_opendowngradestateid,\n\t\t.op_set_currentstateid = (stateid_setter)nfsd4_set_opendowngradestateid,\n\t},\n\t[OP_PUTFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_putfh,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_IS_PUTFH_LIKE | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_PUTFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_PUTPUBFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_putrootfh,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_IS_PUTFH_LIKE | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_PUTPUBFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_PUTROOTFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_putrootfh,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_IS_PUTFH_LIKE | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_PUTROOTFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_READ] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_read,\n\t\t.op_name = \"OP_READ\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_read_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_readstateid,\n\t},\n\t[OP_READDIR] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_readdir,\n\t\t.op_name = \"OP_READDIR\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_readdir_rsize,\n\t},\n\t[OP_READLINK] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_readlink,\n\t\t.op_name = \"OP_READLINK\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_readlink_rsize,\n\t},\n\t[OP_REMOVE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_remove,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_REMOVE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_remove_rsize,\n\t},\n\t[OP_RENAME] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_rename,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_RENAME\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_rename_rsize,\n\t},\n\t[OP_RENEW] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_renew,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_RENEW\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\n\t},\n\t[OP_RESTOREFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_restorefh,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_IS_PUTFH_LIKE | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_RESTOREFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_SAVEFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_savefh,\n\t\t.op_flags = OP_HANDLES_WRONGSEC | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_SAVEFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_SECINFO] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_secinfo,\n\t\t.op_flags = OP_HANDLES_WRONGSEC,\n\t\t.op_name = \"OP_SECINFO\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_secinfo_rsize,\n\t},\n\t[OP_SETATTR] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_setattr,\n\t\t.op_name = \"OP_SETATTR\",\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_setattr_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_setattrstateid,\n\t},\n\t[OP_SETCLIENTID] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_setclientid,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_SETCLIENTID\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_setclientid_rsize,\n\t},\n\t[OP_SETCLIENTID_CONFIRM] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_setclientid_confirm,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_SETCLIENTID_CONFIRM\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_VERIFY] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_verify,\n\t\t.op_name = \"OP_VERIFY\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_WRITE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_write,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_WRITE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_write_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_writestateid,\n\t},\n\t[OP_RELEASE_LOCKOWNER] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_release_lockowner,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_RELEASE_LOCKOWNER\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\n\t/* NFSv4.1 operations */\n\t[OP_EXCHANGE_ID] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_exchange_id,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_EXCHANGE_ID\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_exchange_id_rsize,\n\t},\n\t[OP_BACKCHANNEL_CTL] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_backchannel_ctl,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_BACKCHANNEL_CTL\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_BIND_CONN_TO_SESSION] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_bind_conn_to_session,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_BIND_CONN_TO_SESSION\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_bind_conn_to_session_rsize,\n\t},\n\t[OP_CREATE_SESSION] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_create_session,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_CREATE_SESSION\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_create_session_rsize,\n\t},\n\t[OP_DESTROY_SESSION] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_destroy_session,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_DESTROY_SESSION\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_SEQUENCE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_sequence,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP,\n\t\t.op_name = \"OP_SEQUENCE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_sequence_rsize,\n\t},\n\t[OP_DESTROY_CLIENTID] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_destroy_clientid,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_DESTROY_CLIENTID\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_RECLAIM_COMPLETE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_reclaim_complete,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_RECLAIM_COMPLETE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_SECINFO_NO_NAME] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_secinfo_no_name,\n\t\t.op_flags = OP_HANDLES_WRONGSEC,\n\t\t.op_name = \"OP_SECINFO_NO_NAME\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_secinfo_rsize,\n\t},\n\t[OP_TEST_STATEID] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_test_stateid,\n\t\t.op_flags = ALLOWED_WITHOUT_FH,\n\t\t.op_name = \"OP_TEST_STATEID\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_test_stateid_rsize,\n\t},\n\t[OP_FREE_STATEID] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_free_stateid,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_FREE_STATEID\",\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_freestateid,\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n#ifdef CONFIG_NFSD_PNFS\n\t[OP_GETDEVICEINFO] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_getdeviceinfo,\n\t\t.op_flags = ALLOWED_WITHOUT_FH,\n\t\t.op_name = \"OP_GETDEVICEINFO\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_getdeviceinfo_rsize,\n\t},\n\t[OP_LAYOUTGET] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_layoutget,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_LAYOUTGET\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_layoutget_rsize,\n\t},\n\t[OP_LAYOUTCOMMIT] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_layoutcommit,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_LAYOUTCOMMIT\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_layoutcommit_rsize,\n\t},\n\t[OP_LAYOUTRETURN] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_layoutreturn,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_LAYOUTRETURN\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_layoutreturn_rsize,\n\t},\n#endif /* CONFIG_NFSD_PNFS */\n\n\t/* NFSv4.2 operations */\n\t[OP_ALLOCATE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_allocate,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_ALLOCATE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_DEALLOCATE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_deallocate,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_DEALLOCATE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_CLONE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_clone,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_CLONE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_COPY] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_copy,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_COPY\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_copy_rsize,\n\t},\n\t[OP_SEEK] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_seek,\n\t\t.op_name = \"OP_SEEK\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_seek_rsize,\n\t},\n};\n\n/**\n * nfsd4_spo_must_allow - Determine if the compound op contains an\n * operation that is allowed to be sent with machine credentials\n *\n * @rqstp: a pointer to the struct svc_rqst\n *\n * Checks to see if the compound contains a spo_must_allow op\n * and confirms that it was sent with the proper machine creds.\n */\n\nbool nfsd4_spo_must_allow(struct svc_rqst *rqstp)\n{\n\tstruct nfsd4_compoundres *resp = rqstp->rq_resp;\n\tstruct nfsd4_compoundargs *argp = rqstp->rq_argp;\n\tstruct nfsd4_op *this = &argp->ops[resp->opcnt - 1];\n\tstruct nfsd4_compound_state *cstate = &resp->cstate;\n\tstruct nfs4_op_map *allow = &cstate->clp->cl_spo_must_allow;\n\tu32 opiter;\n\n\tif (!cstate->minorversion)\n\t\treturn false;\n\n\tif (cstate->spo_must_allowed == true)\n\t\treturn true;\n\n\topiter = resp->opcnt;\n\twhile (opiter < argp->opcnt) {\n\t\tthis = &argp->ops[opiter++];\n\t\tif (test_bit(this->opnum, allow->u.longs) &&\n\t\t\tcstate->clp->cl_mach_cred &&\n\t\t\tnfsd4_mach_creds_match(cstate->clp, rqstp)) {\n\t\t\tcstate->spo_must_allowed = true;\n\t\t\treturn true;\n\t\t}\n\t}\n\tcstate->spo_must_allowed = false;\n\treturn false;\n}\n\nint nfsd4_max_reply(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\tif (op->opnum == OP_ILLEGAL || op->status == nfserr_notsupp)\n\t\treturn op_encode_hdr_size * sizeof(__be32);\n\n\tBUG_ON(OPDESC(op)->op_rsize_bop == NULL);\n\treturn OPDESC(op)->op_rsize_bop(rqstp, op);\n}\n\nvoid warn_on_nonidempotent_op(struct nfsd4_op *op)\n{\n\tif (OPDESC(op)->op_flags & OP_MODIFIES_SOMETHING) {\n\t\tpr_err(\"unable to encode reply to nonidempotent op %d (%s)\\n\",\n\t\t\top->opnum, nfsd4_op_name(op->opnum));\n\t\tWARN_ON_ONCE(1);\n\t}\n}\n\nstatic const char *nfsd4_op_name(unsigned opnum)\n{\n\tif (opnum < ARRAY_SIZE(nfsd4_ops))\n\t\treturn nfsd4_ops[opnum].op_name;\n\treturn \"unknown_operation\";\n}\n\n#define nfsd4_voidres\t\t\tnfsd4_voidargs\nstruct nfsd4_voidargs { int dummy; };\n\nstatic struct svc_procedure\t\tnfsd_procedures4[2] = {\n\t[NFSPROC4_NULL] = {\n\t\t.pc_func = (svc_procfunc) nfsd4_proc_null,\n\t\t.pc_encode = (kxdrproc_t) nfs4svc_encode_voidres,\n\t\t.pc_argsize = sizeof(struct nfsd4_voidargs),\n\t\t.pc_ressize = sizeof(struct nfsd4_voidres),\n\t\t.pc_cachetype = RC_NOCACHE,\n\t\t.pc_xdrressize = 1,\n\t},\n\t[NFSPROC4_COMPOUND] = {\n\t\t.pc_func = (svc_procfunc) nfsd4_proc_compound,\n\t\t.pc_decode = (kxdrproc_t) nfs4svc_decode_compoundargs,\n\t\t.pc_encode = (kxdrproc_t) nfs4svc_encode_compoundres,\n\t\t.pc_argsize = sizeof(struct nfsd4_compoundargs),\n\t\t.pc_ressize = sizeof(struct nfsd4_compoundres),\n\t\t.pc_release = nfsd4_release_compoundargs,\n\t\t.pc_cachetype = RC_NOCACHE,\n\t\t.pc_xdrressize = NFSD_BUFSIZE/4,\n\t},\n};\n\nstruct svc_version\tnfsd_version4 = {\n\t.vs_vers\t\t= 4,\n\t.vs_nproc\t\t= 2,\n\t.vs_proc\t\t= nfsd_procedures4,\n\t.vs_dispatch\t\t= nfsd_dispatch,\n\t.vs_xdrsize\t\t= NFS4_SVC_XDRSIZE,\n\t.vs_rpcb_optnl\t\t= true,\n\t.vs_need_cong_ctrl\t= true,\n};\n\n/*\n * Local variables:\n *  c-basic-offset: 8\n * End:\n */\n", "/*\n*  Copyright (c) 2001 The Regents of the University of Michigan.\n*  All rights reserved.\n*\n*  Kendrick Smith <kmsmith@umich.edu>\n*  Andy Adamson <kandros@umich.edu>\n*\n*  Redistribution and use in source and binary forms, with or without\n*  modification, are permitted provided that the following conditions\n*  are met:\n*\n*  1. Redistributions of source code must retain the above copyright\n*     notice, this list of conditions and the following disclaimer.\n*  2. Redistributions in binary form must reproduce the above copyright\n*     notice, this list of conditions and the following disclaimer in the\n*     documentation and/or other materials provided with the distribution.\n*  3. Neither the name of the University nor the names of its\n*     contributors may be used to endorse or promote products derived\n*     from this software without specific prior written permission.\n*\n*  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED\n*  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n*  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n*  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE\n*  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n*  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n*  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n*  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n*  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n*  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n*  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n*\n*/\n\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <linux/namei.h>\n#include <linux/swap.h>\n#include <linux/pagemap.h>\n#include <linux/ratelimit.h>\n#include <linux/sunrpc/svcauth_gss.h>\n#include <linux/sunrpc/addr.h>\n#include <linux/jhash.h>\n#include \"xdr4.h\"\n#include \"xdr4cb.h\"\n#include \"vfs.h\"\n#include \"current_stateid.h\"\n\n#include \"netns.h\"\n#include \"pnfs.h\"\n\n#define NFSDDBG_FACILITY                NFSDDBG_PROC\n\n#define all_ones {{~0,~0},~0}\nstatic const stateid_t one_stateid = {\n\t.si_generation = ~0,\n\t.si_opaque = all_ones,\n};\nstatic const stateid_t zero_stateid = {\n\t/* all fields zero */\n};\nstatic const stateid_t currentstateid = {\n\t.si_generation = 1,\n};\n\nstatic u64 current_sessionid = 1;\n\n#define ZERO_STATEID(stateid) (!memcmp((stateid), &zero_stateid, sizeof(stateid_t)))\n#define ONE_STATEID(stateid)  (!memcmp((stateid), &one_stateid, sizeof(stateid_t)))\n#define CURRENT_STATEID(stateid) (!memcmp((stateid), &currentstateid, sizeof(stateid_t)))\n\n/* forward declarations */\nstatic bool check_for_locks(struct nfs4_file *fp, struct nfs4_lockowner *lowner);\nstatic void nfs4_free_ol_stateid(struct nfs4_stid *stid);\n\n/* Locking: */\n\n/*\n * Currently used for the del_recall_lru and file hash table.  In an\n * effort to decrease the scope of the client_mutex, this spinlock may\n * eventually cover more:\n */\nstatic DEFINE_SPINLOCK(state_lock);\n\n/*\n * A waitqueue for all in-progress 4.0 CLOSE operations that are waiting for\n * the refcount on the open stateid to drop.\n */\nstatic DECLARE_WAIT_QUEUE_HEAD(close_wq);\n\nstatic struct kmem_cache *openowner_slab;\nstatic struct kmem_cache *lockowner_slab;\nstatic struct kmem_cache *file_slab;\nstatic struct kmem_cache *stateid_slab;\nstatic struct kmem_cache *deleg_slab;\nstatic struct kmem_cache *odstate_slab;\n\nstatic void free_session(struct nfsd4_session *);\n\nstatic const struct nfsd4_callback_ops nfsd4_cb_recall_ops;\nstatic const struct nfsd4_callback_ops nfsd4_cb_notify_lock_ops;\n\nstatic bool is_session_dead(struct nfsd4_session *ses)\n{\n\treturn ses->se_flags & NFS4_SESSION_DEAD;\n}\n\nstatic __be32 mark_session_dead_locked(struct nfsd4_session *ses, int ref_held_by_me)\n{\n\tif (atomic_read(&ses->se_ref) > ref_held_by_me)\n\t\treturn nfserr_jukebox;\n\tses->se_flags |= NFS4_SESSION_DEAD;\n\treturn nfs_ok;\n}\n\nstatic bool is_client_expired(struct nfs4_client *clp)\n{\n\treturn clp->cl_time == 0;\n}\n\nstatic __be32 get_client_locked(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tif (is_client_expired(clp))\n\t\treturn nfserr_expired;\n\tatomic_inc(&clp->cl_refcount);\n\treturn nfs_ok;\n}\n\n/* must be called under the client_lock */\nstatic inline void\nrenew_client_locked(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tif (is_client_expired(clp)) {\n\t\tWARN_ON(1);\n\t\tprintk(\"%s: client (clientid %08x/%08x) already expired\\n\",\n\t\t\t__func__,\n\t\t\tclp->cl_clientid.cl_boot,\n\t\t\tclp->cl_clientid.cl_id);\n\t\treturn;\n\t}\n\n\tdprintk(\"renewing client (clientid %08x/%08x)\\n\",\n\t\t\tclp->cl_clientid.cl_boot,\n\t\t\tclp->cl_clientid.cl_id);\n\tlist_move_tail(&clp->cl_lru, &nn->client_lru);\n\tclp->cl_time = get_seconds();\n}\n\nstatic void put_client_renew_locked(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tif (!atomic_dec_and_test(&clp->cl_refcount))\n\t\treturn;\n\tif (!is_client_expired(clp))\n\t\trenew_client_locked(clp);\n}\n\nstatic void put_client_renew(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tif (!atomic_dec_and_lock(&clp->cl_refcount, &nn->client_lock))\n\t\treturn;\n\tif (!is_client_expired(clp))\n\t\trenew_client_locked(clp);\n\tspin_unlock(&nn->client_lock);\n}\n\nstatic __be32 nfsd4_get_session_locked(struct nfsd4_session *ses)\n{\n\t__be32 status;\n\n\tif (is_session_dead(ses))\n\t\treturn nfserr_badsession;\n\tstatus = get_client_locked(ses->se_client);\n\tif (status)\n\t\treturn status;\n\tatomic_inc(&ses->se_ref);\n\treturn nfs_ok;\n}\n\nstatic void nfsd4_put_session_locked(struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tif (atomic_dec_and_test(&ses->se_ref) && is_session_dead(ses))\n\t\tfree_session(ses);\n\tput_client_renew_locked(clp);\n}\n\nstatic void nfsd4_put_session(struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tspin_lock(&nn->client_lock);\n\tnfsd4_put_session_locked(ses);\n\tspin_unlock(&nn->client_lock);\n}\n\nstatic struct nfsd4_blocked_lock *\nfind_blocked_lock(struct nfs4_lockowner *lo, struct knfsd_fh *fh,\n\t\t\tstruct nfsd_net *nn)\n{\n\tstruct nfsd4_blocked_lock *cur, *found = NULL;\n\n\tspin_lock(&nn->blocked_locks_lock);\n\tlist_for_each_entry(cur, &lo->lo_blocked, nbl_list) {\n\t\tif (fh_match(fh, &cur->nbl_fh)) {\n\t\t\tlist_del_init(&cur->nbl_list);\n\t\t\tlist_del_init(&cur->nbl_lru);\n\t\t\tfound = cur;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&nn->blocked_locks_lock);\n\tif (found)\n\t\tposix_unblock_lock(&found->nbl_lock);\n\treturn found;\n}\n\nstatic struct nfsd4_blocked_lock *\nfind_or_allocate_block(struct nfs4_lockowner *lo, struct knfsd_fh *fh,\n\t\t\tstruct nfsd_net *nn)\n{\n\tstruct nfsd4_blocked_lock *nbl;\n\n\tnbl = find_blocked_lock(lo, fh, nn);\n\tif (!nbl) {\n\t\tnbl= kmalloc(sizeof(*nbl), GFP_KERNEL);\n\t\tif (nbl) {\n\t\t\tfh_copy_shallow(&nbl->nbl_fh, fh);\n\t\t\tlocks_init_lock(&nbl->nbl_lock);\n\t\t\tnfsd4_init_cb(&nbl->nbl_cb, lo->lo_owner.so_client,\n\t\t\t\t\t&nfsd4_cb_notify_lock_ops,\n\t\t\t\t\tNFSPROC4_CLNT_CB_NOTIFY_LOCK);\n\t\t}\n\t}\n\treturn nbl;\n}\n\nstatic void\nfree_blocked_lock(struct nfsd4_blocked_lock *nbl)\n{\n\tlocks_release_private(&nbl->nbl_lock);\n\tkfree(nbl);\n}\n\nstatic int\nnfsd4_cb_notify_lock_done(struct nfsd4_callback *cb, struct rpc_task *task)\n{\n\t/*\n\t * Since this is just an optimization, we don't try very hard if it\n\t * turns out not to succeed. We'll requeue it on NFS4ERR_DELAY, and\n\t * just quit trying on anything else.\n\t */\n\tswitch (task->tk_status) {\n\tcase -NFS4ERR_DELAY:\n\t\trpc_delay(task, 1 * HZ);\n\t\treturn 0;\n\tdefault:\n\t\treturn 1;\n\t}\n}\n\nstatic void\nnfsd4_cb_notify_lock_release(struct nfsd4_callback *cb)\n{\n\tstruct nfsd4_blocked_lock\t*nbl = container_of(cb,\n\t\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_cb);\n\n\tfree_blocked_lock(nbl);\n}\n\nstatic const struct nfsd4_callback_ops nfsd4_cb_notify_lock_ops = {\n\t.done\t\t= nfsd4_cb_notify_lock_done,\n\t.release\t= nfsd4_cb_notify_lock_release,\n};\n\nstatic inline struct nfs4_stateowner *\nnfs4_get_stateowner(struct nfs4_stateowner *sop)\n{\n\tatomic_inc(&sop->so_count);\n\treturn sop;\n}\n\nstatic int\nsame_owner_str(struct nfs4_stateowner *sop, struct xdr_netobj *owner)\n{\n\treturn (sop->so_owner.len == owner->len) &&\n\t\t0 == memcmp(sop->so_owner.data, owner->data, owner->len);\n}\n\nstatic struct nfs4_openowner *\nfind_openstateowner_str_locked(unsigned int hashval, struct nfsd4_open *open,\n\t\t\tstruct nfs4_client *clp)\n{\n\tstruct nfs4_stateowner *so;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_for_each_entry(so, &clp->cl_ownerstr_hashtbl[hashval],\n\t\t\t    so_strhash) {\n\t\tif (!so->so_is_open_owner)\n\t\t\tcontinue;\n\t\tif (same_owner_str(so, &open->op_owner))\n\t\t\treturn openowner(nfs4_get_stateowner(so));\n\t}\n\treturn NULL;\n}\n\nstatic struct nfs4_openowner *\nfind_openstateowner_str(unsigned int hashval, struct nfsd4_open *open,\n\t\t\tstruct nfs4_client *clp)\n{\n\tstruct nfs4_openowner *oo;\n\n\tspin_lock(&clp->cl_lock);\n\too = find_openstateowner_str_locked(hashval, open, clp);\n\tspin_unlock(&clp->cl_lock);\n\treturn oo;\n}\n\nstatic inline u32\nopaque_hashval(const void *ptr, int nbytes)\n{\n\tunsigned char *cptr = (unsigned char *) ptr;\n\n\tu32 x = 0;\n\twhile (nbytes--) {\n\t\tx *= 37;\n\t\tx += *cptr++;\n\t}\n\treturn x;\n}\n\nstatic void nfsd4_free_file_rcu(struct rcu_head *rcu)\n{\n\tstruct nfs4_file *fp = container_of(rcu, struct nfs4_file, fi_rcu);\n\n\tkmem_cache_free(file_slab, fp);\n}\n\nvoid\nput_nfs4_file(struct nfs4_file *fi)\n{\n\tmight_lock(&state_lock);\n\n\tif (atomic_dec_and_lock(&fi->fi_ref, &state_lock)) {\n\t\thlist_del_rcu(&fi->fi_hash);\n\t\tspin_unlock(&state_lock);\n\t\tWARN_ON_ONCE(!list_empty(&fi->fi_clnt_odstate));\n\t\tWARN_ON_ONCE(!list_empty(&fi->fi_delegations));\n\t\tcall_rcu(&fi->fi_rcu, nfsd4_free_file_rcu);\n\t}\n}\n\nstatic struct file *\n__nfs4_get_fd(struct nfs4_file *f, int oflag)\n{\n\tif (f->fi_fds[oflag])\n\t\treturn get_file(f->fi_fds[oflag]);\n\treturn NULL;\n}\n\nstatic struct file *\nfind_writeable_file_locked(struct nfs4_file *f)\n{\n\tstruct file *ret;\n\n\tlockdep_assert_held(&f->fi_lock);\n\n\tret = __nfs4_get_fd(f, O_WRONLY);\n\tif (!ret)\n\t\tret = __nfs4_get_fd(f, O_RDWR);\n\treturn ret;\n}\n\nstatic struct file *\nfind_writeable_file(struct nfs4_file *f)\n{\n\tstruct file *ret;\n\n\tspin_lock(&f->fi_lock);\n\tret = find_writeable_file_locked(f);\n\tspin_unlock(&f->fi_lock);\n\n\treturn ret;\n}\n\nstatic struct file *find_readable_file_locked(struct nfs4_file *f)\n{\n\tstruct file *ret;\n\n\tlockdep_assert_held(&f->fi_lock);\n\n\tret = __nfs4_get_fd(f, O_RDONLY);\n\tif (!ret)\n\t\tret = __nfs4_get_fd(f, O_RDWR);\n\treturn ret;\n}\n\nstatic struct file *\nfind_readable_file(struct nfs4_file *f)\n{\n\tstruct file *ret;\n\n\tspin_lock(&f->fi_lock);\n\tret = find_readable_file_locked(f);\n\tspin_unlock(&f->fi_lock);\n\n\treturn ret;\n}\n\nstruct file *\nfind_any_file(struct nfs4_file *f)\n{\n\tstruct file *ret;\n\n\tspin_lock(&f->fi_lock);\n\tret = __nfs4_get_fd(f, O_RDWR);\n\tif (!ret) {\n\t\tret = __nfs4_get_fd(f, O_WRONLY);\n\t\tif (!ret)\n\t\t\tret = __nfs4_get_fd(f, O_RDONLY);\n\t}\n\tspin_unlock(&f->fi_lock);\n\treturn ret;\n}\n\nstatic atomic_long_t num_delegations;\nunsigned long max_delegations;\n\n/*\n * Open owner state (share locks)\n */\n\n/* hash tables for lock and open owners */\n#define OWNER_HASH_BITS              8\n#define OWNER_HASH_SIZE             (1 << OWNER_HASH_BITS)\n#define OWNER_HASH_MASK             (OWNER_HASH_SIZE - 1)\n\nstatic unsigned int ownerstr_hashval(struct xdr_netobj *ownername)\n{\n\tunsigned int ret;\n\n\tret = opaque_hashval(ownername->data, ownername->len);\n\treturn ret & OWNER_HASH_MASK;\n}\n\n/* hash table for nfs4_file */\n#define FILE_HASH_BITS                   8\n#define FILE_HASH_SIZE                  (1 << FILE_HASH_BITS)\n\nstatic unsigned int nfsd_fh_hashval(struct knfsd_fh *fh)\n{\n\treturn jhash2(fh->fh_base.fh_pad, XDR_QUADLEN(fh->fh_size), 0);\n}\n\nstatic unsigned int file_hashval(struct knfsd_fh *fh)\n{\n\treturn nfsd_fh_hashval(fh) & (FILE_HASH_SIZE - 1);\n}\n\nstatic struct hlist_head file_hashtbl[FILE_HASH_SIZE];\n\nstatic void\n__nfs4_file_get_access(struct nfs4_file *fp, u32 access)\n{\n\tlockdep_assert_held(&fp->fi_lock);\n\n\tif (access & NFS4_SHARE_ACCESS_WRITE)\n\t\tatomic_inc(&fp->fi_access[O_WRONLY]);\n\tif (access & NFS4_SHARE_ACCESS_READ)\n\t\tatomic_inc(&fp->fi_access[O_RDONLY]);\n}\n\nstatic __be32\nnfs4_file_get_access(struct nfs4_file *fp, u32 access)\n{\n\tlockdep_assert_held(&fp->fi_lock);\n\n\t/* Does this access mode make sense? */\n\tif (access & ~NFS4_SHARE_ACCESS_BOTH)\n\t\treturn nfserr_inval;\n\n\t/* Does it conflict with a deny mode already set? */\n\tif ((access & fp->fi_share_deny) != 0)\n\t\treturn nfserr_share_denied;\n\n\t__nfs4_file_get_access(fp, access);\n\treturn nfs_ok;\n}\n\nstatic __be32 nfs4_file_check_deny(struct nfs4_file *fp, u32 deny)\n{\n\t/* Common case is that there is no deny mode. */\n\tif (deny) {\n\t\t/* Does this deny mode make sense? */\n\t\tif (deny & ~NFS4_SHARE_DENY_BOTH)\n\t\t\treturn nfserr_inval;\n\n\t\tif ((deny & NFS4_SHARE_DENY_READ) &&\n\t\t    atomic_read(&fp->fi_access[O_RDONLY]))\n\t\t\treturn nfserr_share_denied;\n\n\t\tif ((deny & NFS4_SHARE_DENY_WRITE) &&\n\t\t    atomic_read(&fp->fi_access[O_WRONLY]))\n\t\t\treturn nfserr_share_denied;\n\t}\n\treturn nfs_ok;\n}\n\nstatic void __nfs4_file_put_access(struct nfs4_file *fp, int oflag)\n{\n\tmight_lock(&fp->fi_lock);\n\n\tif (atomic_dec_and_lock(&fp->fi_access[oflag], &fp->fi_lock)) {\n\t\tstruct file *f1 = NULL;\n\t\tstruct file *f2 = NULL;\n\n\t\tswap(f1, fp->fi_fds[oflag]);\n\t\tif (atomic_read(&fp->fi_access[1 - oflag]) == 0)\n\t\t\tswap(f2, fp->fi_fds[O_RDWR]);\n\t\tspin_unlock(&fp->fi_lock);\n\t\tif (f1)\n\t\t\tfput(f1);\n\t\tif (f2)\n\t\t\tfput(f2);\n\t}\n}\n\nstatic void nfs4_file_put_access(struct nfs4_file *fp, u32 access)\n{\n\tWARN_ON_ONCE(access & ~NFS4_SHARE_ACCESS_BOTH);\n\n\tif (access & NFS4_SHARE_ACCESS_WRITE)\n\t\t__nfs4_file_put_access(fp, O_WRONLY);\n\tif (access & NFS4_SHARE_ACCESS_READ)\n\t\t__nfs4_file_put_access(fp, O_RDONLY);\n}\n\n/*\n * Allocate a new open/delegation state counter. This is needed for\n * pNFS for proper return on close semantics.\n *\n * Note that we only allocate it for pNFS-enabled exports, otherwise\n * all pointers to struct nfs4_clnt_odstate are always NULL.\n */\nstatic struct nfs4_clnt_odstate *\nalloc_clnt_odstate(struct nfs4_client *clp)\n{\n\tstruct nfs4_clnt_odstate *co;\n\n\tco = kmem_cache_zalloc(odstate_slab, GFP_KERNEL);\n\tif (co) {\n\t\tco->co_client = clp;\n\t\tatomic_set(&co->co_odcount, 1);\n\t}\n\treturn co;\n}\n\nstatic void\nhash_clnt_odstate_locked(struct nfs4_clnt_odstate *co)\n{\n\tstruct nfs4_file *fp = co->co_file;\n\n\tlockdep_assert_held(&fp->fi_lock);\n\tlist_add(&co->co_perfile, &fp->fi_clnt_odstate);\n}\n\nstatic inline void\nget_clnt_odstate(struct nfs4_clnt_odstate *co)\n{\n\tif (co)\n\t\tatomic_inc(&co->co_odcount);\n}\n\nstatic void\nput_clnt_odstate(struct nfs4_clnt_odstate *co)\n{\n\tstruct nfs4_file *fp;\n\n\tif (!co)\n\t\treturn;\n\n\tfp = co->co_file;\n\tif (atomic_dec_and_lock(&co->co_odcount, &fp->fi_lock)) {\n\t\tlist_del(&co->co_perfile);\n\t\tspin_unlock(&fp->fi_lock);\n\n\t\tnfsd4_return_all_file_layouts(co->co_client, fp);\n\t\tkmem_cache_free(odstate_slab, co);\n\t}\n}\n\nstatic struct nfs4_clnt_odstate *\nfind_or_hash_clnt_odstate(struct nfs4_file *fp, struct nfs4_clnt_odstate *new)\n{\n\tstruct nfs4_clnt_odstate *co;\n\tstruct nfs4_client *cl;\n\n\tif (!new)\n\t\treturn NULL;\n\n\tcl = new->co_client;\n\n\tspin_lock(&fp->fi_lock);\n\tlist_for_each_entry(co, &fp->fi_clnt_odstate, co_perfile) {\n\t\tif (co->co_client == cl) {\n\t\t\tget_clnt_odstate(co);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tco = new;\n\tco->co_file = fp;\n\thash_clnt_odstate_locked(new);\nout:\n\tspin_unlock(&fp->fi_lock);\n\treturn co;\n}\n\nstruct nfs4_stid *nfs4_alloc_stid(struct nfs4_client *cl, struct kmem_cache *slab,\n\t\t\t\t  void (*sc_free)(struct nfs4_stid *))\n{\n\tstruct nfs4_stid *stid;\n\tint new_id;\n\n\tstid = kmem_cache_zalloc(slab, GFP_KERNEL);\n\tif (!stid)\n\t\treturn NULL;\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock(&cl->cl_lock);\n\tnew_id = idr_alloc_cyclic(&cl->cl_stateids, stid, 0, 0, GFP_NOWAIT);\n\tspin_unlock(&cl->cl_lock);\n\tidr_preload_end();\n\tif (new_id < 0)\n\t\tgoto out_free;\n\n\tstid->sc_free = sc_free;\n\tstid->sc_client = cl;\n\tstid->sc_stateid.si_opaque.so_id = new_id;\n\tstid->sc_stateid.si_opaque.so_clid = cl->cl_clientid;\n\t/* Will be incremented before return to client: */\n\tatomic_set(&stid->sc_count, 1);\n\tspin_lock_init(&stid->sc_lock);\n\n\t/*\n\t * It shouldn't be a problem to reuse an opaque stateid value.\n\t * I don't think it is for 4.1.  But with 4.0 I worry that, for\n\t * example, a stray write retransmission could be accepted by\n\t * the server when it should have been rejected.  Therefore,\n\t * adopt a trick from the sctp code to attempt to maximize the\n\t * amount of time until an id is reused, by ensuring they always\n\t * \"increase\" (mod INT_MAX):\n\t */\n\treturn stid;\nout_free:\n\tkmem_cache_free(slab, stid);\n\treturn NULL;\n}\n\nstatic struct nfs4_ol_stateid * nfs4_alloc_open_stateid(struct nfs4_client *clp)\n{\n\tstruct nfs4_stid *stid;\n\n\tstid = nfs4_alloc_stid(clp, stateid_slab, nfs4_free_ol_stateid);\n\tif (!stid)\n\t\treturn NULL;\n\n\treturn openlockstateid(stid);\n}\n\nstatic void nfs4_free_deleg(struct nfs4_stid *stid)\n{\n\tkmem_cache_free(deleg_slab, stid);\n\tatomic_long_dec(&num_delegations);\n}\n\n/*\n * When we recall a delegation, we should be careful not to hand it\n * out again straight away.\n * To ensure this we keep a pair of bloom filters ('new' and 'old')\n * in which the filehandles of recalled delegations are \"stored\".\n * If a filehandle appear in either filter, a delegation is blocked.\n * When a delegation is recalled, the filehandle is stored in the \"new\"\n * filter.\n * Every 30 seconds we swap the filters and clear the \"new\" one,\n * unless both are empty of course.\n *\n * Each filter is 256 bits.  We hash the filehandle to 32bit and use the\n * low 3 bytes as hash-table indices.\n *\n * 'blocked_delegations_lock', which is always taken in block_delegations(),\n * is used to manage concurrent access.  Testing does not need the lock\n * except when swapping the two filters.\n */\nstatic DEFINE_SPINLOCK(blocked_delegations_lock);\nstatic struct bloom_pair {\n\tint\tentries, old_entries;\n\ttime_t\tswap_time;\n\tint\tnew; /* index into 'set' */\n\tDECLARE_BITMAP(set[2], 256);\n} blocked_delegations;\n\nstatic int delegation_blocked(struct knfsd_fh *fh)\n{\n\tu32 hash;\n\tstruct bloom_pair *bd = &blocked_delegations;\n\n\tif (bd->entries == 0)\n\t\treturn 0;\n\tif (seconds_since_boot() - bd->swap_time > 30) {\n\t\tspin_lock(&blocked_delegations_lock);\n\t\tif (seconds_since_boot() - bd->swap_time > 30) {\n\t\t\tbd->entries -= bd->old_entries;\n\t\t\tbd->old_entries = bd->entries;\n\t\t\tmemset(bd->set[bd->new], 0,\n\t\t\t       sizeof(bd->set[0]));\n\t\t\tbd->new = 1-bd->new;\n\t\t\tbd->swap_time = seconds_since_boot();\n\t\t}\n\t\tspin_unlock(&blocked_delegations_lock);\n\t}\n\thash = jhash(&fh->fh_base, fh->fh_size, 0);\n\tif (test_bit(hash&255, bd->set[0]) &&\n\t    test_bit((hash>>8)&255, bd->set[0]) &&\n\t    test_bit((hash>>16)&255, bd->set[0]))\n\t\treturn 1;\n\n\tif (test_bit(hash&255, bd->set[1]) &&\n\t    test_bit((hash>>8)&255, bd->set[1]) &&\n\t    test_bit((hash>>16)&255, bd->set[1]))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void block_delegations(struct knfsd_fh *fh)\n{\n\tu32 hash;\n\tstruct bloom_pair *bd = &blocked_delegations;\n\n\thash = jhash(&fh->fh_base, fh->fh_size, 0);\n\n\tspin_lock(&blocked_delegations_lock);\n\t__set_bit(hash&255, bd->set[bd->new]);\n\t__set_bit((hash>>8)&255, bd->set[bd->new]);\n\t__set_bit((hash>>16)&255, bd->set[bd->new]);\n\tif (bd->entries == 0)\n\t\tbd->swap_time = seconds_since_boot();\n\tbd->entries += 1;\n\tspin_unlock(&blocked_delegations_lock);\n}\n\nstatic struct nfs4_delegation *\nalloc_init_deleg(struct nfs4_client *clp, struct svc_fh *current_fh,\n\t\t struct nfs4_clnt_odstate *odstate)\n{\n\tstruct nfs4_delegation *dp;\n\tlong n;\n\n\tdprintk(\"NFSD alloc_init_deleg\\n\");\n\tn = atomic_long_inc_return(&num_delegations);\n\tif (n < 0 || n > max_delegations)\n\t\tgoto out_dec;\n\tif (delegation_blocked(&current_fh->fh_handle))\n\t\tgoto out_dec;\n\tdp = delegstateid(nfs4_alloc_stid(clp, deleg_slab, nfs4_free_deleg));\n\tif (dp == NULL)\n\t\tgoto out_dec;\n\n\t/*\n\t * delegation seqid's are never incremented.  The 4.1 special\n\t * meaning of seqid 0 isn't meaningful, really, but let's avoid\n\t * 0 anyway just for consistency and use 1:\n\t */\n\tdp->dl_stid.sc_stateid.si_generation = 1;\n\tINIT_LIST_HEAD(&dp->dl_perfile);\n\tINIT_LIST_HEAD(&dp->dl_perclnt);\n\tINIT_LIST_HEAD(&dp->dl_recall_lru);\n\tdp->dl_clnt_odstate = odstate;\n\tget_clnt_odstate(odstate);\n\tdp->dl_type = NFS4_OPEN_DELEGATE_READ;\n\tdp->dl_retries = 1;\n\tnfsd4_init_cb(&dp->dl_recall, dp->dl_stid.sc_client,\n\t\t      &nfsd4_cb_recall_ops, NFSPROC4_CLNT_CB_RECALL);\n\treturn dp;\nout_dec:\n\tatomic_long_dec(&num_delegations);\n\treturn NULL;\n}\n\nvoid\nnfs4_put_stid(struct nfs4_stid *s)\n{\n\tstruct nfs4_file *fp = s->sc_file;\n\tstruct nfs4_client *clp = s->sc_client;\n\n\tmight_lock(&clp->cl_lock);\n\n\tif (!atomic_dec_and_lock(&s->sc_count, &clp->cl_lock)) {\n\t\twake_up_all(&close_wq);\n\t\treturn;\n\t}\n\tidr_remove(&clp->cl_stateids, s->sc_stateid.si_opaque.so_id);\n\tspin_unlock(&clp->cl_lock);\n\ts->sc_free(s);\n\tif (fp)\n\t\tput_nfs4_file(fp);\n}\n\nvoid\nnfs4_inc_and_copy_stateid(stateid_t *dst, struct nfs4_stid *stid)\n{\n\tstateid_t *src = &stid->sc_stateid;\n\n\tspin_lock(&stid->sc_lock);\n\tif (unlikely(++src->si_generation == 0))\n\t\tsrc->si_generation = 1;\n\tmemcpy(dst, src, sizeof(*dst));\n\tspin_unlock(&stid->sc_lock);\n}\n\nstatic void nfs4_put_deleg_lease(struct nfs4_file *fp)\n{\n\tstruct file *filp = NULL;\n\n\tspin_lock(&fp->fi_lock);\n\tif (fp->fi_deleg_file && --fp->fi_delegees == 0)\n\t\tswap(filp, fp->fi_deleg_file);\n\tspin_unlock(&fp->fi_lock);\n\n\tif (filp) {\n\t\tvfs_setlease(filp, F_UNLCK, NULL, (void **)&fp);\n\t\tfput(filp);\n\t}\n}\n\nvoid nfs4_unhash_stid(struct nfs4_stid *s)\n{\n\ts->sc_type = 0;\n}\n\n/**\n * nfs4_get_existing_delegation - Discover if this delegation already exists\n * @clp:     a pointer to the nfs4_client we're granting a delegation to\n * @fp:      a pointer to the nfs4_file we're granting a delegation on\n *\n * Return:\n *      On success: NULL if an existing delegation was not found.\n *\n *      On error: -EAGAIN if one was previously granted to this nfs4_client\n *                 for this nfs4_file.\n *\n */\n\nstatic int\nnfs4_get_existing_delegation(struct nfs4_client *clp, struct nfs4_file *fp)\n{\n\tstruct nfs4_delegation *searchdp = NULL;\n\tstruct nfs4_client *searchclp = NULL;\n\n\tlockdep_assert_held(&state_lock);\n\tlockdep_assert_held(&fp->fi_lock);\n\n\tlist_for_each_entry(searchdp, &fp->fi_delegations, dl_perfile) {\n\t\tsearchclp = searchdp->dl_stid.sc_client;\n\t\tif (clp == searchclp) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/**\n * hash_delegation_locked - Add a delegation to the appropriate lists\n * @dp:     a pointer to the nfs4_delegation we are adding.\n * @fp:     a pointer to the nfs4_file we're granting a delegation on\n *\n * Return:\n *      On success: NULL if the delegation was successfully hashed.\n *\n *      On error: -EAGAIN if one was previously granted to this\n *                 nfs4_client for this nfs4_file. Delegation is not hashed.\n *\n */\n\nstatic int\nhash_delegation_locked(struct nfs4_delegation *dp, struct nfs4_file *fp)\n{\n\tint status;\n\tstruct nfs4_client *clp = dp->dl_stid.sc_client;\n\n\tlockdep_assert_held(&state_lock);\n\tlockdep_assert_held(&fp->fi_lock);\n\n\tstatus = nfs4_get_existing_delegation(clp, fp);\n\tif (status)\n\t\treturn status;\n\t++fp->fi_delegees;\n\tatomic_inc(&dp->dl_stid.sc_count);\n\tdp->dl_stid.sc_type = NFS4_DELEG_STID;\n\tlist_add(&dp->dl_perfile, &fp->fi_delegations);\n\tlist_add(&dp->dl_perclnt, &clp->cl_delegations);\n\treturn 0;\n}\n\nstatic bool\nunhash_delegation_locked(struct nfs4_delegation *dp)\n{\n\tstruct nfs4_file *fp = dp->dl_stid.sc_file;\n\n\tlockdep_assert_held(&state_lock);\n\n\tif (list_empty(&dp->dl_perfile))\n\t\treturn false;\n\n\tdp->dl_stid.sc_type = NFS4_CLOSED_DELEG_STID;\n\t/* Ensure that deleg break won't try to requeue it */\n\t++dp->dl_time;\n\tspin_lock(&fp->fi_lock);\n\tlist_del_init(&dp->dl_perclnt);\n\tlist_del_init(&dp->dl_recall_lru);\n\tlist_del_init(&dp->dl_perfile);\n\tspin_unlock(&fp->fi_lock);\n\treturn true;\n}\n\nstatic void destroy_delegation(struct nfs4_delegation *dp)\n{\n\tbool unhashed;\n\n\tspin_lock(&state_lock);\n\tunhashed = unhash_delegation_locked(dp);\n\tspin_unlock(&state_lock);\n\tif (unhashed) {\n\t\tput_clnt_odstate(dp->dl_clnt_odstate);\n\t\tnfs4_put_deleg_lease(dp->dl_stid.sc_file);\n\t\tnfs4_put_stid(&dp->dl_stid);\n\t}\n}\n\nstatic void revoke_delegation(struct nfs4_delegation *dp)\n{\n\tstruct nfs4_client *clp = dp->dl_stid.sc_client;\n\n\tWARN_ON(!list_empty(&dp->dl_recall_lru));\n\n\tput_clnt_odstate(dp->dl_clnt_odstate);\n\tnfs4_put_deleg_lease(dp->dl_stid.sc_file);\n\n\tif (clp->cl_minorversion == 0)\n\t\tnfs4_put_stid(&dp->dl_stid);\n\telse {\n\t\tdp->dl_stid.sc_type = NFS4_REVOKED_DELEG_STID;\n\t\tspin_lock(&clp->cl_lock);\n\t\tlist_add(&dp->dl_recall_lru, &clp->cl_revoked);\n\t\tspin_unlock(&clp->cl_lock);\n\t}\n}\n\n/* \n * SETCLIENTID state \n */\n\nstatic unsigned int clientid_hashval(u32 id)\n{\n\treturn id & CLIENT_HASH_MASK;\n}\n\nstatic unsigned int clientstr_hashval(const char *name)\n{\n\treturn opaque_hashval(name, 8) & CLIENT_HASH_MASK;\n}\n\n/*\n * We store the NONE, READ, WRITE, and BOTH bits separately in the\n * st_{access,deny}_bmap field of the stateid, in order to track not\n * only what share bits are currently in force, but also what\n * combinations of share bits previous opens have used.  This allows us\n * to enforce the recommendation of rfc 3530 14.2.19 that the server\n * return an error if the client attempt to downgrade to a combination\n * of share bits not explicable by closing some of its previous opens.\n *\n * XXX: This enforcement is actually incomplete, since we don't keep\n * track of access/deny bit combinations; so, e.g., we allow:\n *\n *\tOPEN allow read, deny write\n *\tOPEN allow both, deny none\n *\tDOWNGRADE allow read, deny none\n *\n * which we should reject.\n */\nstatic unsigned int\nbmap_to_share_mode(unsigned long bmap) {\n\tint i;\n\tunsigned int access = 0;\n\n\tfor (i = 1; i < 4; i++) {\n\t\tif (test_bit(i, &bmap))\n\t\t\taccess |= i;\n\t}\n\treturn access;\n}\n\n/* set share access for a given stateid */\nstatic inline void\nset_access(u32 access, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << access;\n\n\tWARN_ON_ONCE(access > NFS4_SHARE_ACCESS_BOTH);\n\tstp->st_access_bmap |= mask;\n}\n\n/* clear share access for a given stateid */\nstatic inline void\nclear_access(u32 access, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << access;\n\n\tWARN_ON_ONCE(access > NFS4_SHARE_ACCESS_BOTH);\n\tstp->st_access_bmap &= ~mask;\n}\n\n/* test whether a given stateid has access */\nstatic inline bool\ntest_access(u32 access, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << access;\n\n\treturn (bool)(stp->st_access_bmap & mask);\n}\n\n/* set share deny for a given stateid */\nstatic inline void\nset_deny(u32 deny, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << deny;\n\n\tWARN_ON_ONCE(deny > NFS4_SHARE_DENY_BOTH);\n\tstp->st_deny_bmap |= mask;\n}\n\n/* clear share deny for a given stateid */\nstatic inline void\nclear_deny(u32 deny, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << deny;\n\n\tWARN_ON_ONCE(deny > NFS4_SHARE_DENY_BOTH);\n\tstp->st_deny_bmap &= ~mask;\n}\n\n/* test whether a given stateid is denying specific access */\nstatic inline bool\ntest_deny(u32 deny, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << deny;\n\n\treturn (bool)(stp->st_deny_bmap & mask);\n}\n\nstatic int nfs4_access_to_omode(u32 access)\n{\n\tswitch (access & NFS4_SHARE_ACCESS_BOTH) {\n\tcase NFS4_SHARE_ACCESS_READ:\n\t\treturn O_RDONLY;\n\tcase NFS4_SHARE_ACCESS_WRITE:\n\t\treturn O_WRONLY;\n\tcase NFS4_SHARE_ACCESS_BOTH:\n\t\treturn O_RDWR;\n\t}\n\tWARN_ON_ONCE(1);\n\treturn O_RDONLY;\n}\n\n/*\n * A stateid that had a deny mode associated with it is being released\n * or downgraded. Recalculate the deny mode on the file.\n */\nstatic void\nrecalculate_deny_mode(struct nfs4_file *fp)\n{\n\tstruct nfs4_ol_stateid *stp;\n\n\tspin_lock(&fp->fi_lock);\n\tfp->fi_share_deny = 0;\n\tlist_for_each_entry(stp, &fp->fi_stateids, st_perfile)\n\t\tfp->fi_share_deny |= bmap_to_share_mode(stp->st_deny_bmap);\n\tspin_unlock(&fp->fi_lock);\n}\n\nstatic void\nreset_union_bmap_deny(u32 deny, struct nfs4_ol_stateid *stp)\n{\n\tint i;\n\tbool change = false;\n\n\tfor (i = 1; i < 4; i++) {\n\t\tif ((i & deny) != i) {\n\t\t\tchange = true;\n\t\t\tclear_deny(i, stp);\n\t\t}\n\t}\n\n\t/* Recalculate per-file deny mode if there was a change */\n\tif (change)\n\t\trecalculate_deny_mode(stp->st_stid.sc_file);\n}\n\n/* release all access and file references for a given stateid */\nstatic void\nrelease_all_access(struct nfs4_ol_stateid *stp)\n{\n\tint i;\n\tstruct nfs4_file *fp = stp->st_stid.sc_file;\n\n\tif (fp && stp->st_deny_bmap != 0)\n\t\trecalculate_deny_mode(fp);\n\n\tfor (i = 1; i < 4; i++) {\n\t\tif (test_access(i, stp))\n\t\t\tnfs4_file_put_access(stp->st_stid.sc_file, i);\n\t\tclear_access(i, stp);\n\t}\n}\n\nstatic inline void nfs4_free_stateowner(struct nfs4_stateowner *sop)\n{\n\tkfree(sop->so_owner.data);\n\tsop->so_ops->so_free(sop);\n}\n\nstatic void nfs4_put_stateowner(struct nfs4_stateowner *sop)\n{\n\tstruct nfs4_client *clp = sop->so_client;\n\n\tmight_lock(&clp->cl_lock);\n\n\tif (!atomic_dec_and_lock(&sop->so_count, &clp->cl_lock))\n\t\treturn;\n\tsop->so_ops->so_unhash(sop);\n\tspin_unlock(&clp->cl_lock);\n\tnfs4_free_stateowner(sop);\n}\n\nstatic bool unhash_ol_stateid(struct nfs4_ol_stateid *stp)\n{\n\tstruct nfs4_file *fp = stp->st_stid.sc_file;\n\n\tlockdep_assert_held(&stp->st_stateowner->so_client->cl_lock);\n\n\tif (list_empty(&stp->st_perfile))\n\t\treturn false;\n\n\tspin_lock(&fp->fi_lock);\n\tlist_del_init(&stp->st_perfile);\n\tspin_unlock(&fp->fi_lock);\n\tlist_del(&stp->st_perstateowner);\n\treturn true;\n}\n\nstatic void nfs4_free_ol_stateid(struct nfs4_stid *stid)\n{\n\tstruct nfs4_ol_stateid *stp = openlockstateid(stid);\n\n\tput_clnt_odstate(stp->st_clnt_odstate);\n\trelease_all_access(stp);\n\tif (stp->st_stateowner)\n\t\tnfs4_put_stateowner(stp->st_stateowner);\n\tkmem_cache_free(stateid_slab, stid);\n}\n\nstatic void nfs4_free_lock_stateid(struct nfs4_stid *stid)\n{\n\tstruct nfs4_ol_stateid *stp = openlockstateid(stid);\n\tstruct nfs4_lockowner *lo = lockowner(stp->st_stateowner);\n\tstruct file *file;\n\n\tfile = find_any_file(stp->st_stid.sc_file);\n\tif (file)\n\t\tfilp_close(file, (fl_owner_t)lo);\n\tnfs4_free_ol_stateid(stid);\n}\n\n/*\n * Put the persistent reference to an already unhashed generic stateid, while\n * holding the cl_lock. If it's the last reference, then put it onto the\n * reaplist for later destruction.\n */\nstatic void put_ol_stateid_locked(struct nfs4_ol_stateid *stp,\n\t\t\t\t       struct list_head *reaplist)\n{\n\tstruct nfs4_stid *s = &stp->st_stid;\n\tstruct nfs4_client *clp = s->sc_client;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tWARN_ON_ONCE(!list_empty(&stp->st_locks));\n\n\tif (!atomic_dec_and_test(&s->sc_count)) {\n\t\twake_up_all(&close_wq);\n\t\treturn;\n\t}\n\n\tidr_remove(&clp->cl_stateids, s->sc_stateid.si_opaque.so_id);\n\tlist_add(&stp->st_locks, reaplist);\n}\n\nstatic bool unhash_lock_stateid(struct nfs4_ol_stateid *stp)\n{\n\tlockdep_assert_held(&stp->st_stid.sc_client->cl_lock);\n\n\tlist_del_init(&stp->st_locks);\n\tnfs4_unhash_stid(&stp->st_stid);\n\treturn unhash_ol_stateid(stp);\n}\n\nstatic void release_lock_stateid(struct nfs4_ol_stateid *stp)\n{\n\tstruct nfs4_client *clp = stp->st_stid.sc_client;\n\tbool unhashed;\n\n\tspin_lock(&clp->cl_lock);\n\tunhashed = unhash_lock_stateid(stp);\n\tspin_unlock(&clp->cl_lock);\n\tif (unhashed)\n\t\tnfs4_put_stid(&stp->st_stid);\n}\n\nstatic void unhash_lockowner_locked(struct nfs4_lockowner *lo)\n{\n\tstruct nfs4_client *clp = lo->lo_owner.so_client;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_del_init(&lo->lo_owner.so_strhash);\n}\n\n/*\n * Free a list of generic stateids that were collected earlier after being\n * fully unhashed.\n */\nstatic void\nfree_ol_stateid_reaplist(struct list_head *reaplist)\n{\n\tstruct nfs4_ol_stateid *stp;\n\tstruct nfs4_file *fp;\n\n\tmight_sleep();\n\n\twhile (!list_empty(reaplist)) {\n\t\tstp = list_first_entry(reaplist, struct nfs4_ol_stateid,\n\t\t\t\t       st_locks);\n\t\tlist_del(&stp->st_locks);\n\t\tfp = stp->st_stid.sc_file;\n\t\tstp->st_stid.sc_free(&stp->st_stid);\n\t\tif (fp)\n\t\t\tput_nfs4_file(fp);\n\t}\n}\n\nstatic void release_open_stateid_locks(struct nfs4_ol_stateid *open_stp,\n\t\t\t\t       struct list_head *reaplist)\n{\n\tstruct nfs4_ol_stateid *stp;\n\n\tlockdep_assert_held(&open_stp->st_stid.sc_client->cl_lock);\n\n\twhile (!list_empty(&open_stp->st_locks)) {\n\t\tstp = list_entry(open_stp->st_locks.next,\n\t\t\t\tstruct nfs4_ol_stateid, st_locks);\n\t\tWARN_ON(!unhash_lock_stateid(stp));\n\t\tput_ol_stateid_locked(stp, reaplist);\n\t}\n}\n\nstatic bool unhash_open_stateid(struct nfs4_ol_stateid *stp,\n\t\t\t\tstruct list_head *reaplist)\n{\n\tbool unhashed;\n\n\tlockdep_assert_held(&stp->st_stid.sc_client->cl_lock);\n\n\tunhashed = unhash_ol_stateid(stp);\n\trelease_open_stateid_locks(stp, reaplist);\n\treturn unhashed;\n}\n\nstatic void release_open_stateid(struct nfs4_ol_stateid *stp)\n{\n\tLIST_HEAD(reaplist);\n\n\tspin_lock(&stp->st_stid.sc_client->cl_lock);\n\tif (unhash_open_stateid(stp, &reaplist))\n\t\tput_ol_stateid_locked(stp, &reaplist);\n\tspin_unlock(&stp->st_stid.sc_client->cl_lock);\n\tfree_ol_stateid_reaplist(&reaplist);\n}\n\nstatic void unhash_openowner_locked(struct nfs4_openowner *oo)\n{\n\tstruct nfs4_client *clp = oo->oo_owner.so_client;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_del_init(&oo->oo_owner.so_strhash);\n\tlist_del_init(&oo->oo_perclient);\n}\n\nstatic void release_last_closed_stateid(struct nfs4_openowner *oo)\n{\n\tstruct nfsd_net *nn = net_generic(oo->oo_owner.so_client->net,\n\t\t\t\t\t  nfsd_net_id);\n\tstruct nfs4_ol_stateid *s;\n\n\tspin_lock(&nn->client_lock);\n\ts = oo->oo_last_closed_stid;\n\tif (s) {\n\t\tlist_del_init(&oo->oo_close_lru);\n\t\too->oo_last_closed_stid = NULL;\n\t}\n\tspin_unlock(&nn->client_lock);\n\tif (s)\n\t\tnfs4_put_stid(&s->st_stid);\n}\n\nstatic void release_openowner(struct nfs4_openowner *oo)\n{\n\tstruct nfs4_ol_stateid *stp;\n\tstruct nfs4_client *clp = oo->oo_owner.so_client;\n\tstruct list_head reaplist;\n\n\tINIT_LIST_HEAD(&reaplist);\n\n\tspin_lock(&clp->cl_lock);\n\tunhash_openowner_locked(oo);\n\twhile (!list_empty(&oo->oo_owner.so_stateids)) {\n\t\tstp = list_first_entry(&oo->oo_owner.so_stateids,\n\t\t\t\tstruct nfs4_ol_stateid, st_perstateowner);\n\t\tif (unhash_open_stateid(stp, &reaplist))\n\t\t\tput_ol_stateid_locked(stp, &reaplist);\n\t}\n\tspin_unlock(&clp->cl_lock);\n\tfree_ol_stateid_reaplist(&reaplist);\n\trelease_last_closed_stateid(oo);\n\tnfs4_put_stateowner(&oo->oo_owner);\n}\n\nstatic inline int\nhash_sessionid(struct nfs4_sessionid *sessionid)\n{\n\tstruct nfsd4_sessionid *sid = (struct nfsd4_sessionid *)sessionid;\n\n\treturn sid->sequence % SESSION_HASH_SIZE;\n}\n\n#ifdef CONFIG_SUNRPC_DEBUG\nstatic inline void\ndump_sessionid(const char *fn, struct nfs4_sessionid *sessionid)\n{\n\tu32 *ptr = (u32 *)(&sessionid->data[0]);\n\tdprintk(\"%s: %u:%u:%u:%u\\n\", fn, ptr[0], ptr[1], ptr[2], ptr[3]);\n}\n#else\nstatic inline void\ndump_sessionid(const char *fn, struct nfs4_sessionid *sessionid)\n{\n}\n#endif\n\n/*\n * Bump the seqid on cstate->replay_owner, and clear replay_owner if it\n * won't be used for replay.\n */\nvoid nfsd4_bump_seqid(struct nfsd4_compound_state *cstate, __be32 nfserr)\n{\n\tstruct nfs4_stateowner *so = cstate->replay_owner;\n\n\tif (nfserr == nfserr_replay_me)\n\t\treturn;\n\n\tif (!seqid_mutating_err(ntohl(nfserr))) {\n\t\tnfsd4_cstate_clear_replay(cstate);\n\t\treturn;\n\t}\n\tif (!so)\n\t\treturn;\n\tif (so->so_is_open_owner)\n\t\trelease_last_closed_stateid(openowner(so));\n\tso->so_seqid++;\n\treturn;\n}\n\nstatic void\ngen_sessionid(struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\tstruct nfsd4_sessionid *sid;\n\n\tsid = (struct nfsd4_sessionid *)ses->se_sessionid.data;\n\tsid->clientid = clp->cl_clientid;\n\tsid->sequence = current_sessionid++;\n\tsid->reserved = 0;\n}\n\n/*\n * The protocol defines ca_maxresponssize_cached to include the size of\n * the rpc header, but all we need to cache is the data starting after\n * the end of the initial SEQUENCE operation--the rest we regenerate\n * each time.  Therefore we can advertise a ca_maxresponssize_cached\n * value that is the number of bytes in our cache plus a few additional\n * bytes.  In order to stay on the safe side, and not promise more than\n * we can cache, those additional bytes must be the minimum possible: 24\n * bytes of rpc header (xid through accept state, with AUTH_NULL\n * verifier), 12 for the compound header (with zero-length tag), and 44\n * for the SEQUENCE op response:\n */\n#define NFSD_MIN_HDR_SEQ_SZ  (24 + 12 + 44)\n\nstatic void\nfree_session_slots(struct nfsd4_session *ses)\n{\n\tint i;\n\n\tfor (i = 0; i < ses->se_fchannel.maxreqs; i++)\n\t\tkfree(ses->se_slots[i]);\n}\n\n/*\n * We don't actually need to cache the rpc and session headers, so we\n * can allocate a little less for each slot:\n */\nstatic inline u32 slot_bytes(struct nfsd4_channel_attrs *ca)\n{\n\tu32 size;\n\n\tif (ca->maxresp_cached < NFSD_MIN_HDR_SEQ_SZ)\n\t\tsize = 0;\n\telse\n\t\tsize = ca->maxresp_cached - NFSD_MIN_HDR_SEQ_SZ;\n\treturn size + sizeof(struct nfsd4_slot);\n}\n\n/*\n * XXX: If we run out of reserved DRC memory we could (up to a point)\n * re-negotiate active sessions and reduce their slot usage to make\n * room for new connections. For now we just fail the create session.\n */\nstatic u32 nfsd4_get_drc_mem(struct nfsd4_channel_attrs *ca)\n{\n\tu32 slotsize = slot_bytes(ca);\n\tu32 num = ca->maxreqs;\n\tint avail;\n\n\tspin_lock(&nfsd_drc_lock);\n\tavail = min((unsigned long)NFSD_MAX_MEM_PER_SESSION,\n\t\t    nfsd_drc_max_mem - nfsd_drc_mem_used);\n\tnum = min_t(int, num, avail / slotsize);\n\tnfsd_drc_mem_used += num * slotsize;\n\tspin_unlock(&nfsd_drc_lock);\n\n\treturn num;\n}\n\nstatic void nfsd4_put_drc_mem(struct nfsd4_channel_attrs *ca)\n{\n\tint slotsize = slot_bytes(ca);\n\n\tspin_lock(&nfsd_drc_lock);\n\tnfsd_drc_mem_used -= slotsize * ca->maxreqs;\n\tspin_unlock(&nfsd_drc_lock);\n}\n\nstatic struct nfsd4_session *alloc_session(struct nfsd4_channel_attrs *fattrs,\n\t\t\t\t\t   struct nfsd4_channel_attrs *battrs)\n{\n\tint numslots = fattrs->maxreqs;\n\tint slotsize = slot_bytes(fattrs);\n\tstruct nfsd4_session *new;\n\tint mem, i;\n\n\tBUILD_BUG_ON(NFSD_MAX_SLOTS_PER_SESSION * sizeof(struct nfsd4_slot *)\n\t\t\t+ sizeof(struct nfsd4_session) > PAGE_SIZE);\n\tmem = numslots * sizeof(struct nfsd4_slot *);\n\n\tnew = kzalloc(sizeof(*new) + mem, GFP_KERNEL);\n\tif (!new)\n\t\treturn NULL;\n\t/* allocate each struct nfsd4_slot and data cache in one piece */\n\tfor (i = 0; i < numslots; i++) {\n\t\tnew->se_slots[i] = kzalloc(slotsize, GFP_KERNEL);\n\t\tif (!new->se_slots[i])\n\t\t\tgoto out_free;\n\t}\n\n\tmemcpy(&new->se_fchannel, fattrs, sizeof(struct nfsd4_channel_attrs));\n\tmemcpy(&new->se_bchannel, battrs, sizeof(struct nfsd4_channel_attrs));\n\n\treturn new;\nout_free:\n\twhile (i--)\n\t\tkfree(new->se_slots[i]);\n\tkfree(new);\n\treturn NULL;\n}\n\nstatic void free_conn(struct nfsd4_conn *c)\n{\n\tsvc_xprt_put(c->cn_xprt);\n\tkfree(c);\n}\n\nstatic void nfsd4_conn_lost(struct svc_xpt_user *u)\n{\n\tstruct nfsd4_conn *c = container_of(u, struct nfsd4_conn, cn_xpt_user);\n\tstruct nfs4_client *clp = c->cn_session->se_client;\n\n\tspin_lock(&clp->cl_lock);\n\tif (!list_empty(&c->cn_persession)) {\n\t\tlist_del(&c->cn_persession);\n\t\tfree_conn(c);\n\t}\n\tnfsd4_probe_callback(clp);\n\tspin_unlock(&clp->cl_lock);\n}\n\nstatic struct nfsd4_conn *alloc_conn(struct svc_rqst *rqstp, u32 flags)\n{\n\tstruct nfsd4_conn *conn;\n\n\tconn = kmalloc(sizeof(struct nfsd4_conn), GFP_KERNEL);\n\tif (!conn)\n\t\treturn NULL;\n\tsvc_xprt_get(rqstp->rq_xprt);\n\tconn->cn_xprt = rqstp->rq_xprt;\n\tconn->cn_flags = flags;\n\tINIT_LIST_HEAD(&conn->cn_xpt_user.list);\n\treturn conn;\n}\n\nstatic void __nfsd4_hash_conn(struct nfsd4_conn *conn, struct nfsd4_session *ses)\n{\n\tconn->cn_session = ses;\n\tlist_add(&conn->cn_persession, &ses->se_conns);\n}\n\nstatic void nfsd4_hash_conn(struct nfsd4_conn *conn, struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\n\tspin_lock(&clp->cl_lock);\n\t__nfsd4_hash_conn(conn, ses);\n\tspin_unlock(&clp->cl_lock);\n}\n\nstatic int nfsd4_register_conn(struct nfsd4_conn *conn)\n{\n\tconn->cn_xpt_user.callback = nfsd4_conn_lost;\n\treturn register_xpt_user(conn->cn_xprt, &conn->cn_xpt_user);\n}\n\nstatic void nfsd4_init_conn(struct svc_rqst *rqstp, struct nfsd4_conn *conn, struct nfsd4_session *ses)\n{\n\tint ret;\n\n\tnfsd4_hash_conn(conn, ses);\n\tret = nfsd4_register_conn(conn);\n\tif (ret)\n\t\t/* oops; xprt is already down: */\n\t\tnfsd4_conn_lost(&conn->cn_xpt_user);\n\t/* We may have gained or lost a callback channel: */\n\tnfsd4_probe_callback_sync(ses->se_client);\n}\n\nstatic struct nfsd4_conn *alloc_conn_from_crses(struct svc_rqst *rqstp, struct nfsd4_create_session *cses)\n{\n\tu32 dir = NFS4_CDFC4_FORE;\n\n\tif (cses->flags & SESSION4_BACK_CHAN)\n\t\tdir |= NFS4_CDFC4_BACK;\n\treturn alloc_conn(rqstp, dir);\n}\n\n/* must be called under client_lock */\nstatic void nfsd4_del_conns(struct nfsd4_session *s)\n{\n\tstruct nfs4_client *clp = s->se_client;\n\tstruct nfsd4_conn *c;\n\n\tspin_lock(&clp->cl_lock);\n\twhile (!list_empty(&s->se_conns)) {\n\t\tc = list_first_entry(&s->se_conns, struct nfsd4_conn, cn_persession);\n\t\tlist_del_init(&c->cn_persession);\n\t\tspin_unlock(&clp->cl_lock);\n\n\t\tunregister_xpt_user(c->cn_xprt, &c->cn_xpt_user);\n\t\tfree_conn(c);\n\n\t\tspin_lock(&clp->cl_lock);\n\t}\n\tspin_unlock(&clp->cl_lock);\n}\n\nstatic void __free_session(struct nfsd4_session *ses)\n{\n\tfree_session_slots(ses);\n\tkfree(ses);\n}\n\nstatic void free_session(struct nfsd4_session *ses)\n{\n\tnfsd4_del_conns(ses);\n\tnfsd4_put_drc_mem(&ses->se_fchannel);\n\t__free_session(ses);\n}\n\nstatic void init_session(struct svc_rqst *rqstp, struct nfsd4_session *new, struct nfs4_client *clp, struct nfsd4_create_session *cses)\n{\n\tint idx;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tnew->se_client = clp;\n\tgen_sessionid(new);\n\n\tINIT_LIST_HEAD(&new->se_conns);\n\n\tnew->se_cb_seq_nr = 1;\n\tnew->se_flags = cses->flags;\n\tnew->se_cb_prog = cses->callback_prog;\n\tnew->se_cb_sec = cses->cb_sec;\n\tatomic_set(&new->se_ref, 0);\n\tidx = hash_sessionid(&new->se_sessionid);\n\tlist_add(&new->se_hash, &nn->sessionid_hashtbl[idx]);\n\tspin_lock(&clp->cl_lock);\n\tlist_add(&new->se_perclnt, &clp->cl_sessions);\n\tspin_unlock(&clp->cl_lock);\n\n\t{\n\t\tstruct sockaddr *sa = svc_addr(rqstp);\n\t\t/*\n\t\t * This is a little silly; with sessions there's no real\n\t\t * use for the callback address.  Use the peer address\n\t\t * as a reasonable default for now, but consider fixing\n\t\t * the rpc client not to require an address in the\n\t\t * future:\n\t\t */\n\t\trpc_copy_addr((struct sockaddr *)&clp->cl_cb_conn.cb_addr, sa);\n\t\tclp->cl_cb_conn.cb_addrlen = svc_addr_len(sa);\n\t}\n}\n\n/* caller must hold client_lock */\nstatic struct nfsd4_session *\n__find_in_sessionid_hashtbl(struct nfs4_sessionid *sessionid, struct net *net)\n{\n\tstruct nfsd4_session *elem;\n\tint idx;\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tdump_sessionid(__func__, sessionid);\n\tidx = hash_sessionid(sessionid);\n\t/* Search in the appropriate list */\n\tlist_for_each_entry(elem, &nn->sessionid_hashtbl[idx], se_hash) {\n\t\tif (!memcmp(elem->se_sessionid.data, sessionid->data,\n\t\t\t    NFS4_MAX_SESSIONID_LEN)) {\n\t\t\treturn elem;\n\t\t}\n\t}\n\n\tdprintk(\"%s: session not found\\n\", __func__);\n\treturn NULL;\n}\n\nstatic struct nfsd4_session *\nfind_in_sessionid_hashtbl(struct nfs4_sessionid *sessionid, struct net *net,\n\t\t__be32 *ret)\n{\n\tstruct nfsd4_session *session;\n\t__be32 status = nfserr_badsession;\n\n\tsession = __find_in_sessionid_hashtbl(sessionid, net);\n\tif (!session)\n\t\tgoto out;\n\tstatus = nfsd4_get_session_locked(session);\n\tif (status)\n\t\tsession = NULL;\nout:\n\t*ret = status;\n\treturn session;\n}\n\n/* caller must hold client_lock */\nstatic void\nunhash_session(struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tlist_del(&ses->se_hash);\n\tspin_lock(&ses->se_client->cl_lock);\n\tlist_del(&ses->se_perclnt);\n\tspin_unlock(&ses->se_client->cl_lock);\n}\n\n/* SETCLIENTID and SETCLIENTID_CONFIRM Helper functions */\nstatic int\nSTALE_CLIENTID(clientid_t *clid, struct nfsd_net *nn)\n{\n\t/*\n\t * We're assuming the clid was not given out from a boot\n\t * precisely 2^32 (about 136 years) before this one.  That seems\n\t * a safe assumption:\n\t */\n\tif (clid->cl_boot == (u32)nn->boot_time)\n\t\treturn 0;\n\tdprintk(\"NFSD stale clientid (%08x/%08x) boot_time %08lx\\n\",\n\t\tclid->cl_boot, clid->cl_id, nn->boot_time);\n\treturn 1;\n}\n\n/* \n * XXX Should we use a slab cache ?\n * This type of memory management is somewhat inefficient, but we use it\n * anyway since SETCLIENTID is not a common operation.\n */\nstatic struct nfs4_client *alloc_client(struct xdr_netobj name)\n{\n\tstruct nfs4_client *clp;\n\tint i;\n\n\tclp = kzalloc(sizeof(struct nfs4_client), GFP_KERNEL);\n\tif (clp == NULL)\n\t\treturn NULL;\n\tclp->cl_name.data = kmemdup(name.data, name.len, GFP_KERNEL);\n\tif (clp->cl_name.data == NULL)\n\t\tgoto err_no_name;\n\tclp->cl_ownerstr_hashtbl = kmalloc(sizeof(struct list_head) *\n\t\t\tOWNER_HASH_SIZE, GFP_KERNEL);\n\tif (!clp->cl_ownerstr_hashtbl)\n\t\tgoto err_no_hashtbl;\n\tfor (i = 0; i < OWNER_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&clp->cl_ownerstr_hashtbl[i]);\n\tclp->cl_name.len = name.len;\n\tINIT_LIST_HEAD(&clp->cl_sessions);\n\tidr_init(&clp->cl_stateids);\n\tatomic_set(&clp->cl_refcount, 0);\n\tclp->cl_cb_state = NFSD4_CB_UNKNOWN;\n\tINIT_LIST_HEAD(&clp->cl_idhash);\n\tINIT_LIST_HEAD(&clp->cl_openowners);\n\tINIT_LIST_HEAD(&clp->cl_delegations);\n\tINIT_LIST_HEAD(&clp->cl_lru);\n\tINIT_LIST_HEAD(&clp->cl_revoked);\n#ifdef CONFIG_NFSD_PNFS\n\tINIT_LIST_HEAD(&clp->cl_lo_states);\n#endif\n\tspin_lock_init(&clp->cl_lock);\n\trpc_init_wait_queue(&clp->cl_cb_waitq, \"Backchannel slot table\");\n\treturn clp;\nerr_no_hashtbl:\n\tkfree(clp->cl_name.data);\nerr_no_name:\n\tkfree(clp);\n\treturn NULL;\n}\n\nstatic void\nfree_client(struct nfs4_client *clp)\n{\n\twhile (!list_empty(&clp->cl_sessions)) {\n\t\tstruct nfsd4_session *ses;\n\t\tses = list_entry(clp->cl_sessions.next, struct nfsd4_session,\n\t\t\t\tse_perclnt);\n\t\tlist_del(&ses->se_perclnt);\n\t\tWARN_ON_ONCE(atomic_read(&ses->se_ref));\n\t\tfree_session(ses);\n\t}\n\trpc_destroy_wait_queue(&clp->cl_cb_waitq);\n\tfree_svc_cred(&clp->cl_cred);\n\tkfree(clp->cl_ownerstr_hashtbl);\n\tkfree(clp->cl_name.data);\n\tidr_destroy(&clp->cl_stateids);\n\tkfree(clp);\n}\n\n/* must be called under the client_lock */\nstatic void\nunhash_client_locked(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\tstruct nfsd4_session *ses;\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\t/* Mark the client as expired! */\n\tclp->cl_time = 0;\n\t/* Make it invisible */\n\tif (!list_empty(&clp->cl_idhash)) {\n\t\tlist_del_init(&clp->cl_idhash);\n\t\tif (test_bit(NFSD4_CLIENT_CONFIRMED, &clp->cl_flags))\n\t\t\trb_erase(&clp->cl_namenode, &nn->conf_name_tree);\n\t\telse\n\t\t\trb_erase(&clp->cl_namenode, &nn->unconf_name_tree);\n\t}\n\tlist_del_init(&clp->cl_lru);\n\tspin_lock(&clp->cl_lock);\n\tlist_for_each_entry(ses, &clp->cl_sessions, se_perclnt)\n\t\tlist_del_init(&ses->se_hash);\n\tspin_unlock(&clp->cl_lock);\n}\n\nstatic void\nunhash_client(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tspin_lock(&nn->client_lock);\n\tunhash_client_locked(clp);\n\tspin_unlock(&nn->client_lock);\n}\n\nstatic __be32 mark_client_expired_locked(struct nfs4_client *clp)\n{\n\tif (atomic_read(&clp->cl_refcount))\n\t\treturn nfserr_jukebox;\n\tunhash_client_locked(clp);\n\treturn nfs_ok;\n}\n\nstatic void\n__destroy_client(struct nfs4_client *clp)\n{\n\tstruct nfs4_openowner *oo;\n\tstruct nfs4_delegation *dp;\n\tstruct list_head reaplist;\n\n\tINIT_LIST_HEAD(&reaplist);\n\tspin_lock(&state_lock);\n\twhile (!list_empty(&clp->cl_delegations)) {\n\t\tdp = list_entry(clp->cl_delegations.next, struct nfs4_delegation, dl_perclnt);\n\t\tWARN_ON(!unhash_delegation_locked(dp));\n\t\tlist_add(&dp->dl_recall_lru, &reaplist);\n\t}\n\tspin_unlock(&state_lock);\n\twhile (!list_empty(&reaplist)) {\n\t\tdp = list_entry(reaplist.next, struct nfs4_delegation, dl_recall_lru);\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tput_clnt_odstate(dp->dl_clnt_odstate);\n\t\tnfs4_put_deleg_lease(dp->dl_stid.sc_file);\n\t\tnfs4_put_stid(&dp->dl_stid);\n\t}\n\twhile (!list_empty(&clp->cl_revoked)) {\n\t\tdp = list_entry(clp->cl_revoked.next, struct nfs4_delegation, dl_recall_lru);\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tnfs4_put_stid(&dp->dl_stid);\n\t}\n\twhile (!list_empty(&clp->cl_openowners)) {\n\t\too = list_entry(clp->cl_openowners.next, struct nfs4_openowner, oo_perclient);\n\t\tnfs4_get_stateowner(&oo->oo_owner);\n\t\trelease_openowner(oo);\n\t}\n\tnfsd4_return_all_client_layouts(clp);\n\tnfsd4_shutdown_callback(clp);\n\tif (clp->cl_cb_conn.cb_xprt)\n\t\tsvc_xprt_put(clp->cl_cb_conn.cb_xprt);\n\tfree_client(clp);\n}\n\nstatic void\ndestroy_client(struct nfs4_client *clp)\n{\n\tunhash_client(clp);\n\t__destroy_client(clp);\n}\n\nstatic void expire_client(struct nfs4_client *clp)\n{\n\tunhash_client(clp);\n\tnfsd4_client_record_remove(clp);\n\t__destroy_client(clp);\n}\n\nstatic void copy_verf(struct nfs4_client *target, nfs4_verifier *source)\n{\n\tmemcpy(target->cl_verifier.data, source->data,\n\t\t\tsizeof(target->cl_verifier.data));\n}\n\nstatic void copy_clid(struct nfs4_client *target, struct nfs4_client *source)\n{\n\ttarget->cl_clientid.cl_boot = source->cl_clientid.cl_boot; \n\ttarget->cl_clientid.cl_id = source->cl_clientid.cl_id; \n}\n\nint strdup_if_nonnull(char **target, char *source)\n{\n\tif (source) {\n\t\t*target = kstrdup(source, GFP_KERNEL);\n\t\tif (!*target)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\t*target = NULL;\n\treturn 0;\n}\n\nstatic int copy_cred(struct svc_cred *target, struct svc_cred *source)\n{\n\tint ret;\n\n\tret = strdup_if_nonnull(&target->cr_principal, source->cr_principal);\n\tif (ret)\n\t\treturn ret;\n\tret = strdup_if_nonnull(&target->cr_raw_principal,\n\t\t\t\t\tsource->cr_raw_principal);\n\tif (ret)\n\t\treturn ret;\n\ttarget->cr_flavor = source->cr_flavor;\n\ttarget->cr_uid = source->cr_uid;\n\ttarget->cr_gid = source->cr_gid;\n\ttarget->cr_group_info = source->cr_group_info;\n\tget_group_info(target->cr_group_info);\n\ttarget->cr_gss_mech = source->cr_gss_mech;\n\tif (source->cr_gss_mech)\n\t\tgss_mech_get(source->cr_gss_mech);\n\treturn 0;\n}\n\nstatic int\ncompare_blob(const struct xdr_netobj *o1, const struct xdr_netobj *o2)\n{\n\tif (o1->len < o2->len)\n\t\treturn -1;\n\tif (o1->len > o2->len)\n\t\treturn 1;\n\treturn memcmp(o1->data, o2->data, o1->len);\n}\n\nstatic int same_name(const char *n1, const char *n2)\n{\n\treturn 0 == memcmp(n1, n2, HEXDIR_LEN);\n}\n\nstatic int\nsame_verf(nfs4_verifier *v1, nfs4_verifier *v2)\n{\n\treturn 0 == memcmp(v1->data, v2->data, sizeof(v1->data));\n}\n\nstatic int\nsame_clid(clientid_t *cl1, clientid_t *cl2)\n{\n\treturn (cl1->cl_boot == cl2->cl_boot) && (cl1->cl_id == cl2->cl_id);\n}\n\nstatic bool groups_equal(struct group_info *g1, struct group_info *g2)\n{\n\tint i;\n\n\tif (g1->ngroups != g2->ngroups)\n\t\treturn false;\n\tfor (i=0; i<g1->ngroups; i++)\n\t\tif (!gid_eq(g1->gid[i], g2->gid[i]))\n\t\t\treturn false;\n\treturn true;\n}\n\n/*\n * RFC 3530 language requires clid_inuse be returned when the\n * \"principal\" associated with a requests differs from that previously\n * used.  We use uid, gid's, and gss principal string as our best\n * approximation.  We also don't want to allow non-gss use of a client\n * established using gss: in theory cr_principal should catch that\n * change, but in practice cr_principal can be null even in the gss case\n * since gssd doesn't always pass down a principal string.\n */\nstatic bool is_gss_cred(struct svc_cred *cr)\n{\n\t/* Is cr_flavor one of the gss \"pseudoflavors\"?: */\n\treturn (cr->cr_flavor > RPC_AUTH_MAXFLAVOR);\n}\n\n\nstatic bool\nsame_creds(struct svc_cred *cr1, struct svc_cred *cr2)\n{\n\tif ((is_gss_cred(cr1) != is_gss_cred(cr2))\n\t\t|| (!uid_eq(cr1->cr_uid, cr2->cr_uid))\n\t\t|| (!gid_eq(cr1->cr_gid, cr2->cr_gid))\n\t\t|| !groups_equal(cr1->cr_group_info, cr2->cr_group_info))\n\t\treturn false;\n\tif (cr1->cr_principal == cr2->cr_principal)\n\t\treturn true;\n\tif (!cr1->cr_principal || !cr2->cr_principal)\n\t\treturn false;\n\treturn 0 == strcmp(cr1->cr_principal, cr2->cr_principal);\n}\n\nstatic bool svc_rqst_integrity_protected(struct svc_rqst *rqstp)\n{\n\tstruct svc_cred *cr = &rqstp->rq_cred;\n\tu32 service;\n\n\tif (!cr->cr_gss_mech)\n\t\treturn false;\n\tservice = gss_pseudoflavor_to_service(cr->cr_gss_mech, cr->cr_flavor);\n\treturn service == RPC_GSS_SVC_INTEGRITY ||\n\t       service == RPC_GSS_SVC_PRIVACY;\n}\n\nbool nfsd4_mach_creds_match(struct nfs4_client *cl, struct svc_rqst *rqstp)\n{\n\tstruct svc_cred *cr = &rqstp->rq_cred;\n\n\tif (!cl->cl_mach_cred)\n\t\treturn true;\n\tif (cl->cl_cred.cr_gss_mech != cr->cr_gss_mech)\n\t\treturn false;\n\tif (!svc_rqst_integrity_protected(rqstp))\n\t\treturn false;\n\tif (cl->cl_cred.cr_raw_principal)\n\t\treturn 0 == strcmp(cl->cl_cred.cr_raw_principal,\n\t\t\t\t\t\tcr->cr_raw_principal);\n\tif (!cr->cr_principal)\n\t\treturn false;\n\treturn 0 == strcmp(cl->cl_cred.cr_principal, cr->cr_principal);\n}\n\nstatic void gen_confirm(struct nfs4_client *clp, struct nfsd_net *nn)\n{\n\t__be32 verf[2];\n\n\t/*\n\t * This is opaque to client, so no need to byte-swap. Use\n\t * __force to keep sparse happy\n\t */\n\tverf[0] = (__force __be32)get_seconds();\n\tverf[1] = (__force __be32)nn->clverifier_counter++;\n\tmemcpy(clp->cl_confirm.data, verf, sizeof(clp->cl_confirm.data));\n}\n\nstatic void gen_clid(struct nfs4_client *clp, struct nfsd_net *nn)\n{\n\tclp->cl_clientid.cl_boot = nn->boot_time;\n\tclp->cl_clientid.cl_id = nn->clientid_counter++;\n\tgen_confirm(clp, nn);\n}\n\nstatic struct nfs4_stid *\nfind_stateid_locked(struct nfs4_client *cl, stateid_t *t)\n{\n\tstruct nfs4_stid *ret;\n\n\tret = idr_find(&cl->cl_stateids, t->si_opaque.so_id);\n\tif (!ret || !ret->sc_type)\n\t\treturn NULL;\n\treturn ret;\n}\n\nstatic struct nfs4_stid *\nfind_stateid_by_type(struct nfs4_client *cl, stateid_t *t, char typemask)\n{\n\tstruct nfs4_stid *s;\n\n\tspin_lock(&cl->cl_lock);\n\ts = find_stateid_locked(cl, t);\n\tif (s != NULL) {\n\t\tif (typemask & s->sc_type)\n\t\t\tatomic_inc(&s->sc_count);\n\t\telse\n\t\t\ts = NULL;\n\t}\n\tspin_unlock(&cl->cl_lock);\n\treturn s;\n}\n\nstatic struct nfs4_client *create_client(struct xdr_netobj name,\n\t\tstruct svc_rqst *rqstp, nfs4_verifier *verf)\n{\n\tstruct nfs4_client *clp;\n\tstruct sockaddr *sa = svc_addr(rqstp);\n\tint ret;\n\tstruct net *net = SVC_NET(rqstp);\n\n\tclp = alloc_client(name);\n\tif (clp == NULL)\n\t\treturn NULL;\n\n\tret = copy_cred(&clp->cl_cred, &rqstp->rq_cred);\n\tif (ret) {\n\t\tfree_client(clp);\n\t\treturn NULL;\n\t}\n\tnfsd4_init_cb(&clp->cl_cb_null, clp, NULL, NFSPROC4_CLNT_CB_NULL);\n\tclp->cl_time = get_seconds();\n\tclear_bit(0, &clp->cl_cb_slot_busy);\n\tcopy_verf(clp, verf);\n\trpc_copy_addr((struct sockaddr *) &clp->cl_addr, sa);\n\tclp->cl_cb_session = NULL;\n\tclp->net = net;\n\treturn clp;\n}\n\nstatic void\nadd_clp_to_name_tree(struct nfs4_client *new_clp, struct rb_root *root)\n{\n\tstruct rb_node **new = &(root->rb_node), *parent = NULL;\n\tstruct nfs4_client *clp;\n\n\twhile (*new) {\n\t\tclp = rb_entry(*new, struct nfs4_client, cl_namenode);\n\t\tparent = *new;\n\n\t\tif (compare_blob(&clp->cl_name, &new_clp->cl_name) > 0)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse\n\t\t\tnew = &((*new)->rb_right);\n\t}\n\n\trb_link_node(&new_clp->cl_namenode, parent, new);\n\trb_insert_color(&new_clp->cl_namenode, root);\n}\n\nstatic struct nfs4_client *\nfind_clp_in_name_tree(struct xdr_netobj *name, struct rb_root *root)\n{\n\tint cmp;\n\tstruct rb_node *node = root->rb_node;\n\tstruct nfs4_client *clp;\n\n\twhile (node) {\n\t\tclp = rb_entry(node, struct nfs4_client, cl_namenode);\n\t\tcmp = compare_blob(&clp->cl_name, name);\n\t\tif (cmp > 0)\n\t\t\tnode = node->rb_left;\n\t\telse if (cmp < 0)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn clp;\n\t}\n\treturn NULL;\n}\n\nstatic void\nadd_to_unconfirmed(struct nfs4_client *clp)\n{\n\tunsigned int idhashval;\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tclear_bit(NFSD4_CLIENT_CONFIRMED, &clp->cl_flags);\n\tadd_clp_to_name_tree(clp, &nn->unconf_name_tree);\n\tidhashval = clientid_hashval(clp->cl_clientid.cl_id);\n\tlist_add(&clp->cl_idhash, &nn->unconf_id_hashtbl[idhashval]);\n\trenew_client_locked(clp);\n}\n\nstatic void\nmove_to_confirmed(struct nfs4_client *clp)\n{\n\tunsigned int idhashval = clientid_hashval(clp->cl_clientid.cl_id);\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tdprintk(\"NFSD: move_to_confirm nfs4_client %p\\n\", clp);\n\tlist_move(&clp->cl_idhash, &nn->conf_id_hashtbl[idhashval]);\n\trb_erase(&clp->cl_namenode, &nn->unconf_name_tree);\n\tadd_clp_to_name_tree(clp, &nn->conf_name_tree);\n\tset_bit(NFSD4_CLIENT_CONFIRMED, &clp->cl_flags);\n\trenew_client_locked(clp);\n}\n\nstatic struct nfs4_client *\nfind_client_in_id_table(struct list_head *tbl, clientid_t *clid, bool sessions)\n{\n\tstruct nfs4_client *clp;\n\tunsigned int idhashval = clientid_hashval(clid->cl_id);\n\n\tlist_for_each_entry(clp, &tbl[idhashval], cl_idhash) {\n\t\tif (same_clid(&clp->cl_clientid, clid)) {\n\t\t\tif ((bool)clp->cl_minorversion != sessions)\n\t\t\t\treturn NULL;\n\t\t\trenew_client_locked(clp);\n\t\t\treturn clp;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct nfs4_client *\nfind_confirmed_client(clientid_t *clid, bool sessions, struct nfsd_net *nn)\n{\n\tstruct list_head *tbl = nn->conf_id_hashtbl;\n\n\tlockdep_assert_held(&nn->client_lock);\n\treturn find_client_in_id_table(tbl, clid, sessions);\n}\n\nstatic struct nfs4_client *\nfind_unconfirmed_client(clientid_t *clid, bool sessions, struct nfsd_net *nn)\n{\n\tstruct list_head *tbl = nn->unconf_id_hashtbl;\n\n\tlockdep_assert_held(&nn->client_lock);\n\treturn find_client_in_id_table(tbl, clid, sessions);\n}\n\nstatic bool clp_used_exchangeid(struct nfs4_client *clp)\n{\n\treturn clp->cl_exchange_flags != 0;\n} \n\nstatic struct nfs4_client *\nfind_confirmed_client_by_name(struct xdr_netobj *name, struct nfsd_net *nn)\n{\n\tlockdep_assert_held(&nn->client_lock);\n\treturn find_clp_in_name_tree(name, &nn->conf_name_tree);\n}\n\nstatic struct nfs4_client *\nfind_unconfirmed_client_by_name(struct xdr_netobj *name, struct nfsd_net *nn)\n{\n\tlockdep_assert_held(&nn->client_lock);\n\treturn find_clp_in_name_tree(name, &nn->unconf_name_tree);\n}\n\nstatic void\ngen_callback(struct nfs4_client *clp, struct nfsd4_setclientid *se, struct svc_rqst *rqstp)\n{\n\tstruct nfs4_cb_conn *conn = &clp->cl_cb_conn;\n\tstruct sockaddr\t*sa = svc_addr(rqstp);\n\tu32 scopeid = rpc_get_scope_id(sa);\n\tunsigned short expected_family;\n\n\t/* Currently, we only support tcp and tcp6 for the callback channel */\n\tif (se->se_callback_netid_len == 3 &&\n\t    !memcmp(se->se_callback_netid_val, \"tcp\", 3))\n\t\texpected_family = AF_INET;\n\telse if (se->se_callback_netid_len == 4 &&\n\t\t !memcmp(se->se_callback_netid_val, \"tcp6\", 4))\n\t\texpected_family = AF_INET6;\n\telse\n\t\tgoto out_err;\n\n\tconn->cb_addrlen = rpc_uaddr2sockaddr(clp->net, se->se_callback_addr_val,\n\t\t\t\t\t    se->se_callback_addr_len,\n\t\t\t\t\t    (struct sockaddr *)&conn->cb_addr,\n\t\t\t\t\t    sizeof(conn->cb_addr));\n\n\tif (!conn->cb_addrlen || conn->cb_addr.ss_family != expected_family)\n\t\tgoto out_err;\n\n\tif (conn->cb_addr.ss_family == AF_INET6)\n\t\t((struct sockaddr_in6 *)&conn->cb_addr)->sin6_scope_id = scopeid;\n\n\tconn->cb_prog = se->se_callback_prog;\n\tconn->cb_ident = se->se_callback_ident;\n\tmemcpy(&conn->cb_saddr, &rqstp->rq_daddr, rqstp->rq_daddrlen);\n\treturn;\nout_err:\n\tconn->cb_addr.ss_family = AF_UNSPEC;\n\tconn->cb_addrlen = 0;\n\tdprintk(\"NFSD: this client (clientid %08x/%08x) \"\n\t\t\"will not receive delegations\\n\",\n\t\tclp->cl_clientid.cl_boot, clp->cl_clientid.cl_id);\n\n\treturn;\n}\n\n/*\n * Cache a reply. nfsd4_check_resp_size() has bounded the cache size.\n */\nstatic void\nnfsd4_store_cache_entry(struct nfsd4_compoundres *resp)\n{\n\tstruct xdr_buf *buf = resp->xdr.buf;\n\tstruct nfsd4_slot *slot = resp->cstate.slot;\n\tunsigned int base;\n\n\tdprintk(\"--> %s slot %p\\n\", __func__, slot);\n\n\tslot->sl_opcnt = resp->opcnt;\n\tslot->sl_status = resp->cstate.status;\n\n\tslot->sl_flags |= NFSD4_SLOT_INITIALIZED;\n\tif (nfsd4_not_cached(resp)) {\n\t\tslot->sl_datalen = 0;\n\t\treturn;\n\t}\n\tbase = resp->cstate.data_offset;\n\tslot->sl_datalen = buf->len - base;\n\tif (read_bytes_from_xdr_buf(buf, base, slot->sl_data, slot->sl_datalen))\n\t\tWARN(1, \"%s: sessions DRC could not cache compound\\n\",\n\t\t     __func__);\n\treturn;\n}\n\n/*\n * Encode the replay sequence operation from the slot values.\n * If cachethis is FALSE encode the uncached rep error on the next\n * operation which sets resp->p and increments resp->opcnt for\n * nfs4svc_encode_compoundres.\n *\n */\nstatic __be32\nnfsd4_enc_sequence_replay(struct nfsd4_compoundargs *args,\n\t\t\t  struct nfsd4_compoundres *resp)\n{\n\tstruct nfsd4_op *op;\n\tstruct nfsd4_slot *slot = resp->cstate.slot;\n\n\t/* Encode the replayed sequence operation */\n\top = &args->ops[resp->opcnt - 1];\n\tnfsd4_encode_operation(resp, op);\n\n\t/* Return nfserr_retry_uncached_rep in next operation. */\n\tif (args->opcnt > 1 && !(slot->sl_flags & NFSD4_SLOT_CACHETHIS)) {\n\t\top = &args->ops[resp->opcnt++];\n\t\top->status = nfserr_retry_uncached_rep;\n\t\tnfsd4_encode_operation(resp, op);\n\t}\n\treturn op->status;\n}\n\n/*\n * The sequence operation is not cached because we can use the slot and\n * session values.\n */\nstatic __be32\nnfsd4_replay_cache_entry(struct nfsd4_compoundres *resp,\n\t\t\t struct nfsd4_sequence *seq)\n{\n\tstruct nfsd4_slot *slot = resp->cstate.slot;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\t__be32 status;\n\n\tdprintk(\"--> %s slot %p\\n\", __func__, slot);\n\n\tstatus = nfsd4_enc_sequence_replay(resp->rqstp->rq_argp, resp);\n\tif (status)\n\t\treturn status;\n\n\tp = xdr_reserve_space(xdr, slot->sl_datalen);\n\tif (!p) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn nfserr_serverfault;\n\t}\n\txdr_encode_opaque_fixed(p, slot->sl_data, slot->sl_datalen);\n\txdr_commit_encode(xdr);\n\n\tresp->opcnt = slot->sl_opcnt;\n\treturn slot->sl_status;\n}\n\n/*\n * Set the exchange_id flags returned by the server.\n */\nstatic void\nnfsd4_set_ex_flags(struct nfs4_client *new, struct nfsd4_exchange_id *clid)\n{\n#ifdef CONFIG_NFSD_PNFS\n\tnew->cl_exchange_flags |= EXCHGID4_FLAG_USE_PNFS_MDS;\n#else\n\tnew->cl_exchange_flags |= EXCHGID4_FLAG_USE_NON_PNFS;\n#endif\n\n\t/* Referrals are supported, Migration is not. */\n\tnew->cl_exchange_flags |= EXCHGID4_FLAG_SUPP_MOVED_REFER;\n\n\t/* set the wire flags to return to client. */\n\tclid->flags = new->cl_exchange_flags;\n}\n\nstatic bool client_has_openowners(struct nfs4_client *clp)\n{\n\tstruct nfs4_openowner *oo;\n\n\tlist_for_each_entry(oo, &clp->cl_openowners, oo_perclient) {\n\t\tif (!list_empty(&oo->oo_owner.so_stateids))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic bool client_has_state(struct nfs4_client *clp)\n{\n\treturn client_has_openowners(clp)\n#ifdef CONFIG_NFSD_PNFS\n\t\t|| !list_empty(&clp->cl_lo_states)\n#endif\n\t\t|| !list_empty(&clp->cl_delegations)\n\t\t|| !list_empty(&clp->cl_sessions);\n}\n\n__be32\nnfsd4_exchange_id(struct svc_rqst *rqstp,\n\t\t  struct nfsd4_compound_state *cstate,\n\t\t  struct nfsd4_exchange_id *exid)\n{\n\tstruct nfs4_client *conf, *new;\n\tstruct nfs4_client *unconf = NULL;\n\t__be32 status;\n\tchar\t\t\taddr_str[INET6_ADDRSTRLEN];\n\tnfs4_verifier\t\tverf = exid->verifier;\n\tstruct sockaddr\t\t*sa = svc_addr(rqstp);\n\tbool\tupdate = exid->flags & EXCHGID4_FLAG_UPD_CONFIRMED_REC_A;\n\tstruct nfsd_net\t\t*nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\trpc_ntop(sa, addr_str, sizeof(addr_str));\n\tdprintk(\"%s rqstp=%p exid=%p clname.len=%u clname.data=%p \"\n\t\t\"ip_addr=%s flags %x, spa_how %d\\n\",\n\t\t__func__, rqstp, exid, exid->clname.len, exid->clname.data,\n\t\taddr_str, exid->flags, exid->spa_how);\n\n\tif (exid->flags & ~EXCHGID4_FLAG_MASK_A)\n\t\treturn nfserr_inval;\n\n\tnew = create_client(exid->clname, rqstp, &verf);\n\tif (new == NULL)\n\t\treturn nfserr_jukebox;\n\n\tswitch (exid->spa_how) {\n\tcase SP4_MACH_CRED:\n\t\texid->spo_must_enforce[0] = 0;\n\t\texid->spo_must_enforce[1] = (\n\t\t\t1 << (OP_BIND_CONN_TO_SESSION - 32) |\n\t\t\t1 << (OP_EXCHANGE_ID - 32) |\n\t\t\t1 << (OP_CREATE_SESSION - 32) |\n\t\t\t1 << (OP_DESTROY_SESSION - 32) |\n\t\t\t1 << (OP_DESTROY_CLIENTID - 32));\n\n\t\texid->spo_must_allow[0] &= (1 << (OP_CLOSE) |\n\t\t\t\t\t1 << (OP_OPEN_DOWNGRADE) |\n\t\t\t\t\t1 << (OP_LOCKU) |\n\t\t\t\t\t1 << (OP_DELEGRETURN));\n\n\t\texid->spo_must_allow[1] &= (\n\t\t\t\t\t1 << (OP_TEST_STATEID - 32) |\n\t\t\t\t\t1 << (OP_FREE_STATEID - 32));\n\t\tif (!svc_rqst_integrity_protected(rqstp)) {\n\t\t\tstatus = nfserr_inval;\n\t\t\tgoto out_nolock;\n\t\t}\n\t\t/*\n\t\t * Sometimes userspace doesn't give us a principal.\n\t\t * Which is a bug, really.  Anyway, we can't enforce\n\t\t * MACH_CRED in that case, better to give up now:\n\t\t */\n\t\tif (!new->cl_cred.cr_principal &&\n\t\t\t\t\t!new->cl_cred.cr_raw_principal) {\n\t\t\tstatus = nfserr_serverfault;\n\t\t\tgoto out_nolock;\n\t\t}\n\t\tnew->cl_mach_cred = true;\n\tcase SP4_NONE:\n\t\tbreak;\n\tdefault:\t\t\t\t/* checked by xdr code */\n\t\tWARN_ON_ONCE(1);\n\tcase SP4_SSV:\n\t\tstatus = nfserr_encr_alg_unsupp;\n\t\tgoto out_nolock;\n\t}\n\n\t/* Cases below refer to rfc 5661 section 18.35.4: */\n\tspin_lock(&nn->client_lock);\n\tconf = find_confirmed_client_by_name(&exid->clname, nn);\n\tif (conf) {\n\t\tbool creds_match = same_creds(&conf->cl_cred, &rqstp->rq_cred);\n\t\tbool verfs_match = same_verf(&verf, &conf->cl_verifier);\n\n\t\tif (update) {\n\t\t\tif (!clp_used_exchangeid(conf)) { /* buggy client */\n\t\t\t\tstatus = nfserr_inval;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (!nfsd4_mach_creds_match(conf, rqstp)) {\n\t\t\t\tstatus = nfserr_wrong_cred;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (!creds_match) { /* case 9 */\n\t\t\t\tstatus = nfserr_perm;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (!verfs_match) { /* case 8 */\n\t\t\t\tstatus = nfserr_not_same;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t/* case 6 */\n\t\t\texid->flags |= EXCHGID4_FLAG_CONFIRMED_R;\n\t\t\tgoto out_copy;\n\t\t}\n\t\tif (!creds_match) { /* case 3 */\n\t\t\tif (client_has_state(conf)) {\n\t\t\t\tstatus = nfserr_clid_inuse;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tgoto out_new;\n\t\t}\n\t\tif (verfs_match) { /* case 2 */\n\t\t\tconf->cl_exchange_flags |= EXCHGID4_FLAG_CONFIRMED_R;\n\t\t\tgoto out_copy;\n\t\t}\n\t\t/* case 5, client reboot */\n\t\tconf = NULL;\n\t\tgoto out_new;\n\t}\n\n\tif (update) { /* case 7 */\n\t\tstatus = nfserr_noent;\n\t\tgoto out;\n\t}\n\n\tunconf  = find_unconfirmed_client_by_name(&exid->clname, nn);\n\tif (unconf) /* case 4, possible retry or client restart */\n\t\tunhash_client_locked(unconf);\n\n\t/* case 1 (normal case) */\nout_new:\n\tif (conf) {\n\t\tstatus = mark_client_expired_locked(conf);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\tnew->cl_minorversion = cstate->minorversion;\n\tnew->cl_spo_must_allow.u.words[0] = exid->spo_must_allow[0];\n\tnew->cl_spo_must_allow.u.words[1] = exid->spo_must_allow[1];\n\n\tgen_clid(new, nn);\n\tadd_to_unconfirmed(new);\n\tswap(new, conf);\nout_copy:\n\texid->clientid.cl_boot = conf->cl_clientid.cl_boot;\n\texid->clientid.cl_id = conf->cl_clientid.cl_id;\n\n\texid->seqid = conf->cl_cs_slot.sl_seqid + 1;\n\tnfsd4_set_ex_flags(conf, exid);\n\n\tdprintk(\"nfsd4_exchange_id seqid %d flags %x\\n\",\n\t\tconf->cl_cs_slot.sl_seqid, conf->cl_exchange_flags);\n\tstatus = nfs_ok;\n\nout:\n\tspin_unlock(&nn->client_lock);\nout_nolock:\n\tif (new)\n\t\texpire_client(new);\n\tif (unconf)\n\t\texpire_client(unconf);\n\treturn status;\n}\n\nstatic __be32\ncheck_slot_seqid(u32 seqid, u32 slot_seqid, int slot_inuse)\n{\n\tdprintk(\"%s enter. seqid %d slot_seqid %d\\n\", __func__, seqid,\n\t\tslot_seqid);\n\n\t/* The slot is in use, and no response has been sent. */\n\tif (slot_inuse) {\n\t\tif (seqid == slot_seqid)\n\t\t\treturn nfserr_jukebox;\n\t\telse\n\t\t\treturn nfserr_seq_misordered;\n\t}\n\t/* Note unsigned 32-bit arithmetic handles wraparound: */\n\tif (likely(seqid == slot_seqid + 1))\n\t\treturn nfs_ok;\n\tif (seqid == slot_seqid)\n\t\treturn nfserr_replay_cache;\n\treturn nfserr_seq_misordered;\n}\n\n/*\n * Cache the create session result into the create session single DRC\n * slot cache by saving the xdr structure. sl_seqid has been set.\n * Do this for solo or embedded create session operations.\n */\nstatic void\nnfsd4_cache_create_session(struct nfsd4_create_session *cr_ses,\n\t\t\t   struct nfsd4_clid_slot *slot, __be32 nfserr)\n{\n\tslot->sl_status = nfserr;\n\tmemcpy(&slot->sl_cr_ses, cr_ses, sizeof(*cr_ses));\n}\n\nstatic __be32\nnfsd4_replay_create_session(struct nfsd4_create_session *cr_ses,\n\t\t\t    struct nfsd4_clid_slot *slot)\n{\n\tmemcpy(cr_ses, &slot->sl_cr_ses, sizeof(*cr_ses));\n\treturn slot->sl_status;\n}\n\n#define NFSD_MIN_REQ_HDR_SEQ_SZ\t((\\\n\t\t\t2 * 2 + /* credential,verifier: AUTH_NULL, length 0 */ \\\n\t\t\t1 +\t/* MIN tag is length with zero, only length */ \\\n\t\t\t3 +\t/* version, opcount, opcode */ \\\n\t\t\tXDR_QUADLEN(NFS4_MAX_SESSIONID_LEN) + \\\n\t\t\t\t/* seqid, slotID, slotID, cache */ \\\n\t\t\t4 ) * sizeof(__be32))\n\n#define NFSD_MIN_RESP_HDR_SEQ_SZ ((\\\n\t\t\t2 +\t/* verifier: AUTH_NULL, length 0 */\\\n\t\t\t1 +\t/* status */ \\\n\t\t\t1 +\t/* MIN tag is length with zero, only length */ \\\n\t\t\t3 +\t/* opcount, opcode, opstatus*/ \\\n\t\t\tXDR_QUADLEN(NFS4_MAX_SESSIONID_LEN) + \\\n\t\t\t\t/* seqid, slotID, slotID, slotID, status */ \\\n\t\t\t5 ) * sizeof(__be32))\n\nstatic __be32 check_forechannel_attrs(struct nfsd4_channel_attrs *ca, struct nfsd_net *nn)\n{\n\tu32 maxrpc = nn->nfsd_serv->sv_max_mesg;\n\n\tif (ca->maxreq_sz < NFSD_MIN_REQ_HDR_SEQ_SZ)\n\t\treturn nfserr_toosmall;\n\tif (ca->maxresp_sz < NFSD_MIN_RESP_HDR_SEQ_SZ)\n\t\treturn nfserr_toosmall;\n\tca->headerpadsz = 0;\n\tca->maxreq_sz = min_t(u32, ca->maxreq_sz, maxrpc);\n\tca->maxresp_sz = min_t(u32, ca->maxresp_sz, maxrpc);\n\tca->maxops = min_t(u32, ca->maxops, NFSD_MAX_OPS_PER_COMPOUND);\n\tca->maxresp_cached = min_t(u32, ca->maxresp_cached,\n\t\t\tNFSD_SLOT_CACHE_SIZE + NFSD_MIN_HDR_SEQ_SZ);\n\tca->maxreqs = min_t(u32, ca->maxreqs, NFSD_MAX_SLOTS_PER_SESSION);\n\t/*\n\t * Note decreasing slot size below client's request may make it\n\t * difficult for client to function correctly, whereas\n\t * decreasing the number of slots will (just?) affect\n\t * performance.  When short on memory we therefore prefer to\n\t * decrease number of slots instead of their size.  Clients that\n\t * request larger slots than they need will get poor results:\n\t */\n\tca->maxreqs = nfsd4_get_drc_mem(ca);\n\tif (!ca->maxreqs)\n\t\treturn nfserr_jukebox;\n\n\treturn nfs_ok;\n}\n\n/*\n * Server's NFSv4.1 backchannel support is AUTH_SYS-only for now.\n * These are based on similar macros in linux/sunrpc/msg_prot.h .\n */\n#define RPC_MAX_HEADER_WITH_AUTH_SYS \\\n\t(RPC_CALLHDRSIZE + 2 * (2 + UNX_CALLSLACK))\n\n#define RPC_MAX_REPHEADER_WITH_AUTH_SYS \\\n\t(RPC_REPHDRSIZE + (2 + NUL_REPLYSLACK))\n\n#define NFSD_CB_MAX_REQ_SZ\t((NFS4_enc_cb_recall_sz + \\\n\t\t\t\t RPC_MAX_HEADER_WITH_AUTH_SYS) * sizeof(__be32))\n#define NFSD_CB_MAX_RESP_SZ\t((NFS4_dec_cb_recall_sz + \\\n\t\t\t\t RPC_MAX_REPHEADER_WITH_AUTH_SYS) * \\\n\t\t\t\t sizeof(__be32))\n\nstatic __be32 check_backchannel_attrs(struct nfsd4_channel_attrs *ca)\n{\n\tca->headerpadsz = 0;\n\n\tif (ca->maxreq_sz < NFSD_CB_MAX_REQ_SZ)\n\t\treturn nfserr_toosmall;\n\tif (ca->maxresp_sz < NFSD_CB_MAX_RESP_SZ)\n\t\treturn nfserr_toosmall;\n\tca->maxresp_cached = 0;\n\tif (ca->maxops < 2)\n\t\treturn nfserr_toosmall;\n\n\treturn nfs_ok;\n}\n\nstatic __be32 nfsd4_check_cb_sec(struct nfsd4_cb_sec *cbs)\n{\n\tswitch (cbs->flavor) {\n\tcase RPC_AUTH_NULL:\n\tcase RPC_AUTH_UNIX:\n\t\treturn nfs_ok;\n\tdefault:\n\t\t/*\n\t\t * GSS case: the spec doesn't allow us to return this\n\t\t * error.  But it also doesn't allow us not to support\n\t\t * GSS.\n\t\t * I'd rather this fail hard than return some error the\n\t\t * client might think it can already handle:\n\t\t */\n\t\treturn nfserr_encr_alg_unsupp;\n\t}\n}\n\n__be32\nnfsd4_create_session(struct svc_rqst *rqstp,\n\t\t     struct nfsd4_compound_state *cstate,\n\t\t     struct nfsd4_create_session *cr_ses)\n{\n\tstruct sockaddr *sa = svc_addr(rqstp);\n\tstruct nfs4_client *conf, *unconf;\n\tstruct nfs4_client *old = NULL;\n\tstruct nfsd4_session *new;\n\tstruct nfsd4_conn *conn;\n\tstruct nfsd4_clid_slot *cs_slot = NULL;\n\t__be32 status = 0;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tif (cr_ses->flags & ~SESSION4_FLAG_MASK_A)\n\t\treturn nfserr_inval;\n\tstatus = nfsd4_check_cb_sec(&cr_ses->cb_sec);\n\tif (status)\n\t\treturn status;\n\tstatus = check_forechannel_attrs(&cr_ses->fore_channel, nn);\n\tif (status)\n\t\treturn status;\n\tstatus = check_backchannel_attrs(&cr_ses->back_channel);\n\tif (status)\n\t\tgoto out_release_drc_mem;\n\tstatus = nfserr_jukebox;\n\tnew = alloc_session(&cr_ses->fore_channel, &cr_ses->back_channel);\n\tif (!new)\n\t\tgoto out_release_drc_mem;\n\tconn = alloc_conn_from_crses(rqstp, cr_ses);\n\tif (!conn)\n\t\tgoto out_free_session;\n\n\tspin_lock(&nn->client_lock);\n\tunconf = find_unconfirmed_client(&cr_ses->clientid, true, nn);\n\tconf = find_confirmed_client(&cr_ses->clientid, true, nn);\n\tWARN_ON_ONCE(conf && unconf);\n\n\tif (conf) {\n\t\tstatus = nfserr_wrong_cred;\n\t\tif (!nfsd4_mach_creds_match(conf, rqstp))\n\t\t\tgoto out_free_conn;\n\t\tcs_slot = &conf->cl_cs_slot;\n\t\tstatus = check_slot_seqid(cr_ses->seqid, cs_slot->sl_seqid, 0);\n\t\tif (status) {\n\t\t\tif (status == nfserr_replay_cache)\n\t\t\t\tstatus = nfsd4_replay_create_session(cr_ses, cs_slot);\n\t\t\tgoto out_free_conn;\n\t\t}\n\t} else if (unconf) {\n\t\tif (!same_creds(&unconf->cl_cred, &rqstp->rq_cred) ||\n\t\t    !rpc_cmp_addr(sa, (struct sockaddr *) &unconf->cl_addr)) {\n\t\t\tstatus = nfserr_clid_inuse;\n\t\t\tgoto out_free_conn;\n\t\t}\n\t\tstatus = nfserr_wrong_cred;\n\t\tif (!nfsd4_mach_creds_match(unconf, rqstp))\n\t\t\tgoto out_free_conn;\n\t\tcs_slot = &unconf->cl_cs_slot;\n\t\tstatus = check_slot_seqid(cr_ses->seqid, cs_slot->sl_seqid, 0);\n\t\tif (status) {\n\t\t\t/* an unconfirmed replay returns misordered */\n\t\t\tstatus = nfserr_seq_misordered;\n\t\t\tgoto out_free_conn;\n\t\t}\n\t\told = find_confirmed_client_by_name(&unconf->cl_name, nn);\n\t\tif (old) {\n\t\t\tstatus = mark_client_expired_locked(old);\n\t\t\tif (status) {\n\t\t\t\told = NULL;\n\t\t\t\tgoto out_free_conn;\n\t\t\t}\n\t\t}\n\t\tmove_to_confirmed(unconf);\n\t\tconf = unconf;\n\t} else {\n\t\tstatus = nfserr_stale_clientid;\n\t\tgoto out_free_conn;\n\t}\n\tstatus = nfs_ok;\n\t/* Persistent sessions are not supported */\n\tcr_ses->flags &= ~SESSION4_PERSIST;\n\t/* Upshifting from TCP to RDMA is not supported */\n\tcr_ses->flags &= ~SESSION4_RDMA;\n\n\tinit_session(rqstp, new, conf, cr_ses);\n\tnfsd4_get_session_locked(new);\n\n\tmemcpy(cr_ses->sessionid.data, new->se_sessionid.data,\n\t       NFS4_MAX_SESSIONID_LEN);\n\tcs_slot->sl_seqid++;\n\tcr_ses->seqid = cs_slot->sl_seqid;\n\n\t/* cache solo and embedded create sessions under the client_lock */\n\tnfsd4_cache_create_session(cr_ses, cs_slot, status);\n\tspin_unlock(&nn->client_lock);\n\t/* init connection and backchannel */\n\tnfsd4_init_conn(rqstp, conn, new);\n\tnfsd4_put_session(new);\n\tif (old)\n\t\texpire_client(old);\n\treturn status;\nout_free_conn:\n\tspin_unlock(&nn->client_lock);\n\tfree_conn(conn);\n\tif (old)\n\t\texpire_client(old);\nout_free_session:\n\t__free_session(new);\nout_release_drc_mem:\n\tnfsd4_put_drc_mem(&cr_ses->fore_channel);\n\treturn status;\n}\n\nstatic __be32 nfsd4_map_bcts_dir(u32 *dir)\n{\n\tswitch (*dir) {\n\tcase NFS4_CDFC4_FORE:\n\tcase NFS4_CDFC4_BACK:\n\t\treturn nfs_ok;\n\tcase NFS4_CDFC4_FORE_OR_BOTH:\n\tcase NFS4_CDFC4_BACK_OR_BOTH:\n\t\t*dir = NFS4_CDFC4_BOTH;\n\t\treturn nfs_ok;\n\t};\n\treturn nfserr_inval;\n}\n\n__be32 nfsd4_backchannel_ctl(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_backchannel_ctl *bc)\n{\n\tstruct nfsd4_session *session = cstate->session;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\t__be32 status;\n\n\tstatus = nfsd4_check_cb_sec(&bc->bc_cb_sec);\n\tif (status)\n\t\treturn status;\n\tspin_lock(&nn->client_lock);\n\tsession->se_cb_prog = bc->bc_cb_program;\n\tsession->se_cb_sec = bc->bc_cb_sec;\n\tspin_unlock(&nn->client_lock);\n\n\tnfsd4_probe_callback(session->se_client);\n\n\treturn nfs_ok;\n}\n\n__be32 nfsd4_bind_conn_to_session(struct svc_rqst *rqstp,\n\t\t     struct nfsd4_compound_state *cstate,\n\t\t     struct nfsd4_bind_conn_to_session *bcts)\n{\n\t__be32 status;\n\tstruct nfsd4_conn *conn;\n\tstruct nfsd4_session *session;\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tif (!nfsd4_last_compound_op(rqstp))\n\t\treturn nfserr_not_only_op;\n\tspin_lock(&nn->client_lock);\n\tsession = find_in_sessionid_hashtbl(&bcts->sessionid, net, &status);\n\tspin_unlock(&nn->client_lock);\n\tif (!session)\n\t\tgoto out_no_session;\n\tstatus = nfserr_wrong_cred;\n\tif (!nfsd4_mach_creds_match(session->se_client, rqstp))\n\t\tgoto out;\n\tstatus = nfsd4_map_bcts_dir(&bcts->dir);\n\tif (status)\n\t\tgoto out;\n\tconn = alloc_conn(rqstp, bcts->dir);\n\tstatus = nfserr_jukebox;\n\tif (!conn)\n\t\tgoto out;\n\tnfsd4_init_conn(rqstp, conn, session);\n\tstatus = nfs_ok;\nout:\n\tnfsd4_put_session(session);\nout_no_session:\n\treturn status;\n}\n\nstatic bool nfsd4_compound_in_session(struct nfsd4_session *session, struct nfs4_sessionid *sid)\n{\n\tif (!session)\n\t\treturn 0;\n\treturn !memcmp(sid, &session->se_sessionid, sizeof(*sid));\n}\n\n__be32\nnfsd4_destroy_session(struct svc_rqst *r,\n\t\t      struct nfsd4_compound_state *cstate,\n\t\t      struct nfsd4_destroy_session *sessionid)\n{\n\tstruct nfsd4_session *ses;\n\t__be32 status;\n\tint ref_held_by_me = 0;\n\tstruct net *net = SVC_NET(r);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tstatus = nfserr_not_only_op;\n\tif (nfsd4_compound_in_session(cstate->session, &sessionid->sessionid)) {\n\t\tif (!nfsd4_last_compound_op(r))\n\t\t\tgoto out;\n\t\tref_held_by_me++;\n\t}\n\tdump_sessionid(__func__, &sessionid->sessionid);\n\tspin_lock(&nn->client_lock);\n\tses = find_in_sessionid_hashtbl(&sessionid->sessionid, net, &status);\n\tif (!ses)\n\t\tgoto out_client_lock;\n\tstatus = nfserr_wrong_cred;\n\tif (!nfsd4_mach_creds_match(ses->se_client, r))\n\t\tgoto out_put_session;\n\tstatus = mark_session_dead_locked(ses, 1 + ref_held_by_me);\n\tif (status)\n\t\tgoto out_put_session;\n\tunhash_session(ses);\n\tspin_unlock(&nn->client_lock);\n\n\tnfsd4_probe_callback_sync(ses->se_client);\n\n\tspin_lock(&nn->client_lock);\n\tstatus = nfs_ok;\nout_put_session:\n\tnfsd4_put_session_locked(ses);\nout_client_lock:\n\tspin_unlock(&nn->client_lock);\nout:\n\treturn status;\n}\n\nstatic struct nfsd4_conn *__nfsd4_find_conn(struct svc_xprt *xpt, struct nfsd4_session *s)\n{\n\tstruct nfsd4_conn *c;\n\n\tlist_for_each_entry(c, &s->se_conns, cn_persession) {\n\t\tif (c->cn_xprt == xpt) {\n\t\t\treturn c;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic __be32 nfsd4_sequence_check_conn(struct nfsd4_conn *new, struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\tstruct nfsd4_conn *c;\n\t__be32 status = nfs_ok;\n\tint ret;\n\n\tspin_lock(&clp->cl_lock);\n\tc = __nfsd4_find_conn(new->cn_xprt, ses);\n\tif (c)\n\t\tgoto out_free;\n\tstatus = nfserr_conn_not_bound_to_session;\n\tif (clp->cl_mach_cred)\n\t\tgoto out_free;\n\t__nfsd4_hash_conn(new, ses);\n\tspin_unlock(&clp->cl_lock);\n\tret = nfsd4_register_conn(new);\n\tif (ret)\n\t\t/* oops; xprt is already down: */\n\t\tnfsd4_conn_lost(&new->cn_xpt_user);\n\treturn nfs_ok;\nout_free:\n\tspin_unlock(&clp->cl_lock);\n\tfree_conn(new);\n\treturn status;\n}\n\nstatic bool nfsd4_session_too_many_ops(struct svc_rqst *rqstp, struct nfsd4_session *session)\n{\n\tstruct nfsd4_compoundargs *args = rqstp->rq_argp;\n\n\treturn args->opcnt > session->se_fchannel.maxops;\n}\n\nstatic bool nfsd4_request_too_big(struct svc_rqst *rqstp,\n\t\t\t\t  struct nfsd4_session *session)\n{\n\tstruct xdr_buf *xb = &rqstp->rq_arg;\n\n\treturn xb->len > session->se_fchannel.maxreq_sz;\n}\n\n__be32\nnfsd4_sequence(struct svc_rqst *rqstp,\n\t       struct nfsd4_compound_state *cstate,\n\t       struct nfsd4_sequence *seq)\n{\n\tstruct nfsd4_compoundres *resp = rqstp->rq_resp;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct nfsd4_session *session;\n\tstruct nfs4_client *clp;\n\tstruct nfsd4_slot *slot;\n\tstruct nfsd4_conn *conn;\n\t__be32 status;\n\tint buflen;\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tif (resp->opcnt != 1)\n\t\treturn nfserr_sequence_pos;\n\n\t/*\n\t * Will be either used or freed by nfsd4_sequence_check_conn\n\t * below.\n\t */\n\tconn = alloc_conn(rqstp, NFS4_CDFC4_FORE);\n\tif (!conn)\n\t\treturn nfserr_jukebox;\n\n\tspin_lock(&nn->client_lock);\n\tsession = find_in_sessionid_hashtbl(&seq->sessionid, net, &status);\n\tif (!session)\n\t\tgoto out_no_session;\n\tclp = session->se_client;\n\n\tstatus = nfserr_too_many_ops;\n\tif (nfsd4_session_too_many_ops(rqstp, session))\n\t\tgoto out_put_session;\n\n\tstatus = nfserr_req_too_big;\n\tif (nfsd4_request_too_big(rqstp, session))\n\t\tgoto out_put_session;\n\n\tstatus = nfserr_badslot;\n\tif (seq->slotid >= session->se_fchannel.maxreqs)\n\t\tgoto out_put_session;\n\n\tslot = session->se_slots[seq->slotid];\n\tdprintk(\"%s: slotid %d\\n\", __func__, seq->slotid);\n\n\t/* We do not negotiate the number of slots yet, so set the\n\t * maxslots to the session maxreqs which is used to encode\n\t * sr_highest_slotid and the sr_target_slot id to maxslots */\n\tseq->maxslots = session->se_fchannel.maxreqs;\n\n\tstatus = check_slot_seqid(seq->seqid, slot->sl_seqid,\n\t\t\t\t\tslot->sl_flags & NFSD4_SLOT_INUSE);\n\tif (status == nfserr_replay_cache) {\n\t\tstatus = nfserr_seq_misordered;\n\t\tif (!(slot->sl_flags & NFSD4_SLOT_INITIALIZED))\n\t\t\tgoto out_put_session;\n\t\tcstate->slot = slot;\n\t\tcstate->session = session;\n\t\tcstate->clp = clp;\n\t\t/* Return the cached reply status and set cstate->status\n\t\t * for nfsd4_proc_compound processing */\n\t\tstatus = nfsd4_replay_cache_entry(resp, seq);\n\t\tcstate->status = nfserr_replay_cache;\n\t\tgoto out;\n\t}\n\tif (status)\n\t\tgoto out_put_session;\n\n\tstatus = nfsd4_sequence_check_conn(conn, session);\n\tconn = NULL;\n\tif (status)\n\t\tgoto out_put_session;\n\n\tbuflen = (seq->cachethis) ?\n\t\t\tsession->se_fchannel.maxresp_cached :\n\t\t\tsession->se_fchannel.maxresp_sz;\n\tstatus = (seq->cachethis) ? nfserr_rep_too_big_to_cache :\n\t\t\t\t    nfserr_rep_too_big;\n\tif (xdr_restrict_buflen(xdr, buflen - rqstp->rq_auth_slack))\n\t\tgoto out_put_session;\n\tsvc_reserve(rqstp, buflen);\n\n\tstatus = nfs_ok;\n\t/* Success! bump slot seqid */\n\tslot->sl_seqid = seq->seqid;\n\tslot->sl_flags |= NFSD4_SLOT_INUSE;\n\tif (seq->cachethis)\n\t\tslot->sl_flags |= NFSD4_SLOT_CACHETHIS;\n\telse\n\t\tslot->sl_flags &= ~NFSD4_SLOT_CACHETHIS;\n\n\tcstate->slot = slot;\n\tcstate->session = session;\n\tcstate->clp = clp;\n\nout:\n\tswitch (clp->cl_cb_state) {\n\tcase NFSD4_CB_DOWN:\n\t\tseq->status_flags = SEQ4_STATUS_CB_PATH_DOWN;\n\t\tbreak;\n\tcase NFSD4_CB_FAULT:\n\t\tseq->status_flags = SEQ4_STATUS_BACKCHANNEL_FAULT;\n\t\tbreak;\n\tdefault:\n\t\tseq->status_flags = 0;\n\t}\n\tif (!list_empty(&clp->cl_revoked))\n\t\tseq->status_flags |= SEQ4_STATUS_RECALLABLE_STATE_REVOKED;\nout_no_session:\n\tif (conn)\n\t\tfree_conn(conn);\n\tspin_unlock(&nn->client_lock);\n\treturn status;\nout_put_session:\n\tnfsd4_put_session_locked(session);\n\tgoto out_no_session;\n}\n\nvoid\nnfsd4_sequence_done(struct nfsd4_compoundres *resp)\n{\n\tstruct nfsd4_compound_state *cs = &resp->cstate;\n\n\tif (nfsd4_has_session(cs)) {\n\t\tif (cs->status != nfserr_replay_cache) {\n\t\t\tnfsd4_store_cache_entry(resp);\n\t\t\tcs->slot->sl_flags &= ~NFSD4_SLOT_INUSE;\n\t\t}\n\t\t/* Drop session reference that was taken in nfsd4_sequence() */\n\t\tnfsd4_put_session(cs->session);\n\t} else if (cs->clp)\n\t\tput_client_renew(cs->clp);\n}\n\n__be32\nnfsd4_destroy_clientid(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_destroy_clientid *dc)\n{\n\tstruct nfs4_client *conf, *unconf;\n\tstruct nfs4_client *clp = NULL;\n\t__be32 status = 0;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tspin_lock(&nn->client_lock);\n\tunconf = find_unconfirmed_client(&dc->clientid, true, nn);\n\tconf = find_confirmed_client(&dc->clientid, true, nn);\n\tWARN_ON_ONCE(conf && unconf);\n\n\tif (conf) {\n\t\tif (client_has_state(conf)) {\n\t\t\tstatus = nfserr_clientid_busy;\n\t\t\tgoto out;\n\t\t}\n\t\tstatus = mark_client_expired_locked(conf);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tclp = conf;\n\t} else if (unconf)\n\t\tclp = unconf;\n\telse {\n\t\tstatus = nfserr_stale_clientid;\n\t\tgoto out;\n\t}\n\tif (!nfsd4_mach_creds_match(clp, rqstp)) {\n\t\tclp = NULL;\n\t\tstatus = nfserr_wrong_cred;\n\t\tgoto out;\n\t}\n\tunhash_client_locked(clp);\nout:\n\tspin_unlock(&nn->client_lock);\n\tif (clp)\n\t\texpire_client(clp);\n\treturn status;\n}\n\n__be32\nnfsd4_reclaim_complete(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_reclaim_complete *rc)\n{\n\t__be32 status = 0;\n\n\tif (rc->rca_one_fs) {\n\t\tif (!cstate->current_fh.fh_dentry)\n\t\t\treturn nfserr_nofilehandle;\n\t\t/*\n\t\t * We don't take advantage of the rca_one_fs case.\n\t\t * That's OK, it's optional, we can safely ignore it.\n\t\t */\n\t\treturn nfs_ok;\n\t}\n\n\tstatus = nfserr_complete_already;\n\tif (test_and_set_bit(NFSD4_CLIENT_RECLAIM_COMPLETE,\n\t\t\t     &cstate->session->se_client->cl_flags))\n\t\tgoto out;\n\n\tstatus = nfserr_stale_clientid;\n\tif (is_client_expired(cstate->session->se_client))\n\t\t/*\n\t\t * The following error isn't really legal.\n\t\t * But we only get here if the client just explicitly\n\t\t * destroyed the client.  Surely it no longer cares what\n\t\t * error it gets back on an operation for the dead\n\t\t * client.\n\t\t */\n\t\tgoto out;\n\n\tstatus = nfs_ok;\n\tnfsd4_client_record_create(cstate->session->se_client);\nout:\n\treturn status;\n}\n\n__be32\nnfsd4_setclientid(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t  struct nfsd4_setclientid *setclid)\n{\n\tstruct xdr_netobj \tclname = setclid->se_name;\n\tnfs4_verifier\t\tclverifier = setclid->se_verf;\n\tstruct nfs4_client\t*conf, *new;\n\tstruct nfs4_client\t*unconf = NULL;\n\t__be32 \t\t\tstatus;\n\tstruct nfsd_net\t\t*nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tnew = create_client(clname, rqstp, &clverifier);\n\tif (new == NULL)\n\t\treturn nfserr_jukebox;\n\t/* Cases below refer to rfc 3530 section 14.2.33: */\n\tspin_lock(&nn->client_lock);\n\tconf = find_confirmed_client_by_name(&clname, nn);\n\tif (conf && client_has_state(conf)) {\n\t\t/* case 0: */\n\t\tstatus = nfserr_clid_inuse;\n\t\tif (clp_used_exchangeid(conf))\n\t\t\tgoto out;\n\t\tif (!same_creds(&conf->cl_cred, &rqstp->rq_cred)) {\n\t\t\tchar addr_str[INET6_ADDRSTRLEN];\n\t\t\trpc_ntop((struct sockaddr *) &conf->cl_addr, addr_str,\n\t\t\t\t sizeof(addr_str));\n\t\t\tdprintk(\"NFSD: setclientid: string in use by client \"\n\t\t\t\t\"at %s\\n\", addr_str);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tunconf = find_unconfirmed_client_by_name(&clname, nn);\n\tif (unconf)\n\t\tunhash_client_locked(unconf);\n\tif (conf && same_verf(&conf->cl_verifier, &clverifier)) {\n\t\t/* case 1: probable callback update */\n\t\tcopy_clid(new, conf);\n\t\tgen_confirm(new, nn);\n\t} else /* case 4 (new client) or cases 2, 3 (client reboot): */\n\t\tgen_clid(new, nn);\n\tnew->cl_minorversion = 0;\n\tgen_callback(new, setclid, rqstp);\n\tadd_to_unconfirmed(new);\n\tsetclid->se_clientid.cl_boot = new->cl_clientid.cl_boot;\n\tsetclid->se_clientid.cl_id = new->cl_clientid.cl_id;\n\tmemcpy(setclid->se_confirm.data, new->cl_confirm.data, sizeof(setclid->se_confirm.data));\n\tnew = NULL;\n\tstatus = nfs_ok;\nout:\n\tspin_unlock(&nn->client_lock);\n\tif (new)\n\t\tfree_client(new);\n\tif (unconf)\n\t\texpire_client(unconf);\n\treturn status;\n}\n\n\n__be32\nnfsd4_setclientid_confirm(struct svc_rqst *rqstp,\n\t\t\t struct nfsd4_compound_state *cstate,\n\t\t\t struct nfsd4_setclientid_confirm *setclientid_confirm)\n{\n\tstruct nfs4_client *conf, *unconf;\n\tstruct nfs4_client *old = NULL;\n\tnfs4_verifier confirm = setclientid_confirm->sc_confirm; \n\tclientid_t * clid = &setclientid_confirm->sc_clientid;\n\t__be32 status;\n\tstruct nfsd_net\t*nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tif (STALE_CLIENTID(clid, nn))\n\t\treturn nfserr_stale_clientid;\n\n\tspin_lock(&nn->client_lock);\n\tconf = find_confirmed_client(clid, false, nn);\n\tunconf = find_unconfirmed_client(clid, false, nn);\n\t/*\n\t * We try hard to give out unique clientid's, so if we get an\n\t * attempt to confirm the same clientid with a different cred,\n\t * the client may be buggy; this should never happen.\n\t *\n\t * Nevertheless, RFC 7530 recommends INUSE for this case:\n\t */\n\tstatus = nfserr_clid_inuse;\n\tif (unconf && !same_creds(&unconf->cl_cred, &rqstp->rq_cred))\n\t\tgoto out;\n\tif (conf && !same_creds(&conf->cl_cred, &rqstp->rq_cred))\n\t\tgoto out;\n\t/* cases below refer to rfc 3530 section 14.2.34: */\n\tif (!unconf || !same_verf(&confirm, &unconf->cl_confirm)) {\n\t\tif (conf && same_verf(&confirm, &conf->cl_confirm)) {\n\t\t\t/* case 2: probable retransmit */\n\t\t\tstatus = nfs_ok;\n\t\t} else /* case 4: client hasn't noticed we rebooted yet? */\n\t\t\tstatus = nfserr_stale_clientid;\n\t\tgoto out;\n\t}\n\tstatus = nfs_ok;\n\tif (conf) { /* case 1: callback update */\n\t\told = unconf;\n\t\tunhash_client_locked(old);\n\t\tnfsd4_change_callback(conf, &unconf->cl_cb_conn);\n\t} else { /* case 3: normal case; new or rebooted client */\n\t\told = find_confirmed_client_by_name(&unconf->cl_name, nn);\n\t\tif (old) {\n\t\t\tstatus = nfserr_clid_inuse;\n\t\t\tif (client_has_state(old)\n\t\t\t\t\t&& !same_creds(&unconf->cl_cred,\n\t\t\t\t\t\t\t&old->cl_cred))\n\t\t\t\tgoto out;\n\t\t\tstatus = mark_client_expired_locked(old);\n\t\t\tif (status) {\n\t\t\t\told = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tmove_to_confirmed(unconf);\n\t\tconf = unconf;\n\t}\n\tget_client_locked(conf);\n\tspin_unlock(&nn->client_lock);\n\tnfsd4_probe_callback(conf);\n\tspin_lock(&nn->client_lock);\n\tput_client_renew_locked(conf);\nout:\n\tspin_unlock(&nn->client_lock);\n\tif (old)\n\t\texpire_client(old);\n\treturn status;\n}\n\nstatic struct nfs4_file *nfsd4_alloc_file(void)\n{\n\treturn kmem_cache_alloc(file_slab, GFP_KERNEL);\n}\n\n/* OPEN Share state helper functions */\nstatic void nfsd4_init_file(struct knfsd_fh *fh, unsigned int hashval,\n\t\t\t\tstruct nfs4_file *fp)\n{\n\tlockdep_assert_held(&state_lock);\n\n\tatomic_set(&fp->fi_ref, 1);\n\tspin_lock_init(&fp->fi_lock);\n\tINIT_LIST_HEAD(&fp->fi_stateids);\n\tINIT_LIST_HEAD(&fp->fi_delegations);\n\tINIT_LIST_HEAD(&fp->fi_clnt_odstate);\n\tfh_copy_shallow(&fp->fi_fhandle, fh);\n\tfp->fi_deleg_file = NULL;\n\tfp->fi_had_conflict = false;\n\tfp->fi_share_deny = 0;\n\tmemset(fp->fi_fds, 0, sizeof(fp->fi_fds));\n\tmemset(fp->fi_access, 0, sizeof(fp->fi_access));\n#ifdef CONFIG_NFSD_PNFS\n\tINIT_LIST_HEAD(&fp->fi_lo_states);\n\tatomic_set(&fp->fi_lo_recalls, 0);\n#endif\n\thlist_add_head_rcu(&fp->fi_hash, &file_hashtbl[hashval]);\n}\n\nvoid\nnfsd4_free_slabs(void)\n{\n\tkmem_cache_destroy(odstate_slab);\n\tkmem_cache_destroy(openowner_slab);\n\tkmem_cache_destroy(lockowner_slab);\n\tkmem_cache_destroy(file_slab);\n\tkmem_cache_destroy(stateid_slab);\n\tkmem_cache_destroy(deleg_slab);\n}\n\nint\nnfsd4_init_slabs(void)\n{\n\topenowner_slab = kmem_cache_create(\"nfsd4_openowners\",\n\t\t\tsizeof(struct nfs4_openowner), 0, 0, NULL);\n\tif (openowner_slab == NULL)\n\t\tgoto out;\n\tlockowner_slab = kmem_cache_create(\"nfsd4_lockowners\",\n\t\t\tsizeof(struct nfs4_lockowner), 0, 0, NULL);\n\tif (lockowner_slab == NULL)\n\t\tgoto out_free_openowner_slab;\n\tfile_slab = kmem_cache_create(\"nfsd4_files\",\n\t\t\tsizeof(struct nfs4_file), 0, 0, NULL);\n\tif (file_slab == NULL)\n\t\tgoto out_free_lockowner_slab;\n\tstateid_slab = kmem_cache_create(\"nfsd4_stateids\",\n\t\t\tsizeof(struct nfs4_ol_stateid), 0, 0, NULL);\n\tif (stateid_slab == NULL)\n\t\tgoto out_free_file_slab;\n\tdeleg_slab = kmem_cache_create(\"nfsd4_delegations\",\n\t\t\tsizeof(struct nfs4_delegation), 0, 0, NULL);\n\tif (deleg_slab == NULL)\n\t\tgoto out_free_stateid_slab;\n\todstate_slab = kmem_cache_create(\"nfsd4_odstate\",\n\t\t\tsizeof(struct nfs4_clnt_odstate), 0, 0, NULL);\n\tif (odstate_slab == NULL)\n\t\tgoto out_free_deleg_slab;\n\treturn 0;\n\nout_free_deleg_slab:\n\tkmem_cache_destroy(deleg_slab);\nout_free_stateid_slab:\n\tkmem_cache_destroy(stateid_slab);\nout_free_file_slab:\n\tkmem_cache_destroy(file_slab);\nout_free_lockowner_slab:\n\tkmem_cache_destroy(lockowner_slab);\nout_free_openowner_slab:\n\tkmem_cache_destroy(openowner_slab);\nout:\n\tdprintk(\"nfsd4: out of memory while initializing nfsv4\\n\");\n\treturn -ENOMEM;\n}\n\nstatic void init_nfs4_replay(struct nfs4_replay *rp)\n{\n\trp->rp_status = nfserr_serverfault;\n\trp->rp_buflen = 0;\n\trp->rp_buf = rp->rp_ibuf;\n\tmutex_init(&rp->rp_mutex);\n}\n\nstatic void nfsd4_cstate_assign_replay(struct nfsd4_compound_state *cstate,\n\t\tstruct nfs4_stateowner *so)\n{\n\tif (!nfsd4_has_session(cstate)) {\n\t\tmutex_lock(&so->so_replay.rp_mutex);\n\t\tcstate->replay_owner = nfs4_get_stateowner(so);\n\t}\n}\n\nvoid nfsd4_cstate_clear_replay(struct nfsd4_compound_state *cstate)\n{\n\tstruct nfs4_stateowner *so = cstate->replay_owner;\n\n\tif (so != NULL) {\n\t\tcstate->replay_owner = NULL;\n\t\tmutex_unlock(&so->so_replay.rp_mutex);\n\t\tnfs4_put_stateowner(so);\n\t}\n}\n\nstatic inline void *alloc_stateowner(struct kmem_cache *slab, struct xdr_netobj *owner, struct nfs4_client *clp)\n{\n\tstruct nfs4_stateowner *sop;\n\n\tsop = kmem_cache_alloc(slab, GFP_KERNEL);\n\tif (!sop)\n\t\treturn NULL;\n\n\tsop->so_owner.data = kmemdup(owner->data, owner->len, GFP_KERNEL);\n\tif (!sop->so_owner.data) {\n\t\tkmem_cache_free(slab, sop);\n\t\treturn NULL;\n\t}\n\tsop->so_owner.len = owner->len;\n\n\tINIT_LIST_HEAD(&sop->so_stateids);\n\tsop->so_client = clp;\n\tinit_nfs4_replay(&sop->so_replay);\n\tatomic_set(&sop->so_count, 1);\n\treturn sop;\n}\n\nstatic void hash_openowner(struct nfs4_openowner *oo, struct nfs4_client *clp, unsigned int strhashval)\n{\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_add(&oo->oo_owner.so_strhash,\n\t\t &clp->cl_ownerstr_hashtbl[strhashval]);\n\tlist_add(&oo->oo_perclient, &clp->cl_openowners);\n}\n\nstatic void nfs4_unhash_openowner(struct nfs4_stateowner *so)\n{\n\tunhash_openowner_locked(openowner(so));\n}\n\nstatic void nfs4_free_openowner(struct nfs4_stateowner *so)\n{\n\tstruct nfs4_openowner *oo = openowner(so);\n\n\tkmem_cache_free(openowner_slab, oo);\n}\n\nstatic const struct nfs4_stateowner_operations openowner_ops = {\n\t.so_unhash =\tnfs4_unhash_openowner,\n\t.so_free =\tnfs4_free_openowner,\n};\n\nstatic struct nfs4_ol_stateid *\nnfsd4_find_existing_open(struct nfs4_file *fp, struct nfsd4_open *open)\n{\n\tstruct nfs4_ol_stateid *local, *ret = NULL;\n\tstruct nfs4_openowner *oo = open->op_openowner;\n\n\tlockdep_assert_held(&fp->fi_lock);\n\n\tlist_for_each_entry(local, &fp->fi_stateids, st_perfile) {\n\t\t/* ignore lock owners */\n\t\tif (local->st_stateowner->so_is_open_owner == 0)\n\t\t\tcontinue;\n\t\tif (local->st_stateowner == &oo->oo_owner) {\n\t\t\tret = local;\n\t\t\tatomic_inc(&ret->st_stid.sc_count);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic struct nfs4_openowner *\nalloc_init_open_stateowner(unsigned int strhashval, struct nfsd4_open *open,\n\t\t\t   struct nfsd4_compound_state *cstate)\n{\n\tstruct nfs4_client *clp = cstate->clp;\n\tstruct nfs4_openowner *oo, *ret;\n\n\too = alloc_stateowner(openowner_slab, &open->op_owner, clp);\n\tif (!oo)\n\t\treturn NULL;\n\too->oo_owner.so_ops = &openowner_ops;\n\too->oo_owner.so_is_open_owner = 1;\n\too->oo_owner.so_seqid = open->op_seqid;\n\too->oo_flags = 0;\n\tif (nfsd4_has_session(cstate))\n\t\too->oo_flags |= NFS4_OO_CONFIRMED;\n\too->oo_time = 0;\n\too->oo_last_closed_stid = NULL;\n\tINIT_LIST_HEAD(&oo->oo_close_lru);\n\tspin_lock(&clp->cl_lock);\n\tret = find_openstateowner_str_locked(strhashval, open, clp);\n\tif (ret == NULL) {\n\t\thash_openowner(oo, clp, strhashval);\n\t\tret = oo;\n\t} else\n\t\tnfs4_free_stateowner(&oo->oo_owner);\n\n\tspin_unlock(&clp->cl_lock);\n\treturn ret;\n}\n\nstatic struct nfs4_ol_stateid *\ninit_open_stateid(struct nfs4_file *fp, struct nfsd4_open *open)\n{\n\n\tstruct nfs4_openowner *oo = open->op_openowner;\n\tstruct nfs4_ol_stateid *retstp = NULL;\n\tstruct nfs4_ol_stateid *stp;\n\n\tstp = open->op_stp;\n\t/* We are moving these outside of the spinlocks to avoid the warnings */\n\tmutex_init(&stp->st_mutex);\n\tmutex_lock(&stp->st_mutex);\n\n\tspin_lock(&oo->oo_owner.so_client->cl_lock);\n\tspin_lock(&fp->fi_lock);\n\n\tretstp = nfsd4_find_existing_open(fp, open);\n\tif (retstp)\n\t\tgoto out_unlock;\n\n\topen->op_stp = NULL;\n\tatomic_inc(&stp->st_stid.sc_count);\n\tstp->st_stid.sc_type = NFS4_OPEN_STID;\n\tINIT_LIST_HEAD(&stp->st_locks);\n\tstp->st_stateowner = nfs4_get_stateowner(&oo->oo_owner);\n\tget_nfs4_file(fp);\n\tstp->st_stid.sc_file = fp;\n\tstp->st_access_bmap = 0;\n\tstp->st_deny_bmap = 0;\n\tstp->st_openstp = NULL;\n\tlist_add(&stp->st_perstateowner, &oo->oo_owner.so_stateids);\n\tlist_add(&stp->st_perfile, &fp->fi_stateids);\n\nout_unlock:\n\tspin_unlock(&fp->fi_lock);\n\tspin_unlock(&oo->oo_owner.so_client->cl_lock);\n\tif (retstp) {\n\t\tmutex_lock(&retstp->st_mutex);\n\t\t/* To keep mutex tracking happy */\n\t\tmutex_unlock(&stp->st_mutex);\n\t\tstp = retstp;\n\t}\n\treturn stp;\n}\n\n/*\n * In the 4.0 case we need to keep the owners around a little while to handle\n * CLOSE replay. We still do need to release any file access that is held by\n * them before returning however.\n */\nstatic void\nmove_to_close_lru(struct nfs4_ol_stateid *s, struct net *net)\n{\n\tstruct nfs4_ol_stateid *last;\n\tstruct nfs4_openowner *oo = openowner(s->st_stateowner);\n\tstruct nfsd_net *nn = net_generic(s->st_stid.sc_client->net,\n\t\t\t\t\t\tnfsd_net_id);\n\n\tdprintk(\"NFSD: move_to_close_lru nfs4_openowner %p\\n\", oo);\n\n\t/*\n\t * We know that we hold one reference via nfsd4_close, and another\n\t * \"persistent\" reference for the client. If the refcount is higher\n\t * than 2, then there are still calls in progress that are using this\n\t * stateid. We can't put the sc_file reference until they are finished.\n\t * Wait for the refcount to drop to 2. Since it has been unhashed,\n\t * there should be no danger of the refcount going back up again at\n\t * this point.\n\t */\n\twait_event(close_wq, atomic_read(&s->st_stid.sc_count) == 2);\n\n\trelease_all_access(s);\n\tif (s->st_stid.sc_file) {\n\t\tput_nfs4_file(s->st_stid.sc_file);\n\t\ts->st_stid.sc_file = NULL;\n\t}\n\n\tspin_lock(&nn->client_lock);\n\tlast = oo->oo_last_closed_stid;\n\too->oo_last_closed_stid = s;\n\tlist_move_tail(&oo->oo_close_lru, &nn->close_lru);\n\too->oo_time = get_seconds();\n\tspin_unlock(&nn->client_lock);\n\tif (last)\n\t\tnfs4_put_stid(&last->st_stid);\n}\n\n/* search file_hashtbl[] for file */\nstatic struct nfs4_file *\nfind_file_locked(struct knfsd_fh *fh, unsigned int hashval)\n{\n\tstruct nfs4_file *fp;\n\n\thlist_for_each_entry_rcu(fp, &file_hashtbl[hashval], fi_hash) {\n\t\tif (fh_match(&fp->fi_fhandle, fh)) {\n\t\t\tif (atomic_inc_not_zero(&fp->fi_ref))\n\t\t\t\treturn fp;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstruct nfs4_file *\nfind_file(struct knfsd_fh *fh)\n{\n\tstruct nfs4_file *fp;\n\tunsigned int hashval = file_hashval(fh);\n\n\trcu_read_lock();\n\tfp = find_file_locked(fh, hashval);\n\trcu_read_unlock();\n\treturn fp;\n}\n\nstatic struct nfs4_file *\nfind_or_add_file(struct nfs4_file *new, struct knfsd_fh *fh)\n{\n\tstruct nfs4_file *fp;\n\tunsigned int hashval = file_hashval(fh);\n\n\trcu_read_lock();\n\tfp = find_file_locked(fh, hashval);\n\trcu_read_unlock();\n\tif (fp)\n\t\treturn fp;\n\n\tspin_lock(&state_lock);\n\tfp = find_file_locked(fh, hashval);\n\tif (likely(fp == NULL)) {\n\t\tnfsd4_init_file(fh, hashval, new);\n\t\tfp = new;\n\t}\n\tspin_unlock(&state_lock);\n\n\treturn fp;\n}\n\n/*\n * Called to check deny when READ with all zero stateid or\n * WRITE with all zero or all one stateid\n */\nstatic __be32\nnfs4_share_conflict(struct svc_fh *current_fh, unsigned int deny_type)\n{\n\tstruct nfs4_file *fp;\n\t__be32 ret = nfs_ok;\n\n\tfp = find_file(&current_fh->fh_handle);\n\tif (!fp)\n\t\treturn ret;\n\t/* Check for conflicting share reservations */\n\tspin_lock(&fp->fi_lock);\n\tif (fp->fi_share_deny & deny_type)\n\t\tret = nfserr_locked;\n\tspin_unlock(&fp->fi_lock);\n\tput_nfs4_file(fp);\n\treturn ret;\n}\n\nstatic void nfsd4_cb_recall_prepare(struct nfsd4_callback *cb)\n{\n\tstruct nfs4_delegation *dp = cb_to_delegation(cb);\n\tstruct nfsd_net *nn = net_generic(dp->dl_stid.sc_client->net,\n\t\t\t\t\t  nfsd_net_id);\n\n\tblock_delegations(&dp->dl_stid.sc_file->fi_fhandle);\n\n\t/*\n\t * We can't do this in nfsd_break_deleg_cb because it is\n\t * already holding inode->i_lock.\n\t *\n\t * If the dl_time != 0, then we know that it has already been\n\t * queued for a lease break. Don't queue it again.\n\t */\n\tspin_lock(&state_lock);\n\tif (dp->dl_time == 0) {\n\t\tdp->dl_time = get_seconds();\n\t\tlist_add_tail(&dp->dl_recall_lru, &nn->del_recall_lru);\n\t}\n\tspin_unlock(&state_lock);\n}\n\nstatic int nfsd4_cb_recall_done(struct nfsd4_callback *cb,\n\t\tstruct rpc_task *task)\n{\n\tstruct nfs4_delegation *dp = cb_to_delegation(cb);\n\n\tif (dp->dl_stid.sc_type == NFS4_CLOSED_DELEG_STID)\n\t        return 1;\n\n\tswitch (task->tk_status) {\n\tcase 0:\n\t\treturn 1;\n\tcase -EBADHANDLE:\n\tcase -NFS4ERR_BAD_STATEID:\n\t\t/*\n\t\t * Race: client probably got cb_recall before open reply\n\t\t * granting delegation.\n\t\t */\n\t\tif (dp->dl_retries--) {\n\t\t\trpc_delay(task, 2 * HZ);\n\t\t\treturn 0;\n\t\t}\n\t\t/*FALLTHRU*/\n\tdefault:\n\t\treturn -1;\n\t}\n}\n\nstatic void nfsd4_cb_recall_release(struct nfsd4_callback *cb)\n{\n\tstruct nfs4_delegation *dp = cb_to_delegation(cb);\n\n\tnfs4_put_stid(&dp->dl_stid);\n}\n\nstatic const struct nfsd4_callback_ops nfsd4_cb_recall_ops = {\n\t.prepare\t= nfsd4_cb_recall_prepare,\n\t.done\t\t= nfsd4_cb_recall_done,\n\t.release\t= nfsd4_cb_recall_release,\n};\n\nstatic void nfsd_break_one_deleg(struct nfs4_delegation *dp)\n{\n\t/*\n\t * We're assuming the state code never drops its reference\n\t * without first removing the lease.  Since we're in this lease\n\t * callback (and since the lease code is serialized by the kernel\n\t * lock) we know the server hasn't removed the lease yet, we know\n\t * it's safe to take a reference.\n\t */\n\tatomic_inc(&dp->dl_stid.sc_count);\n\tnfsd4_run_cb(&dp->dl_recall);\n}\n\n/* Called from break_lease() with i_lock held. */\nstatic bool\nnfsd_break_deleg_cb(struct file_lock *fl)\n{\n\tbool ret = false;\n\tstruct nfs4_file *fp = (struct nfs4_file *)fl->fl_owner;\n\tstruct nfs4_delegation *dp;\n\n\tif (!fp) {\n\t\tWARN(1, \"(%p)->fl_owner NULL\\n\", fl);\n\t\treturn ret;\n\t}\n\tif (fp->fi_had_conflict) {\n\t\tWARN(1, \"duplicate break on %p\\n\", fp);\n\t\treturn ret;\n\t}\n\t/*\n\t * We don't want the locks code to timeout the lease for us;\n\t * we'll remove it ourself if a delegation isn't returned\n\t * in time:\n\t */\n\tfl->fl_break_time = 0;\n\n\tspin_lock(&fp->fi_lock);\n\tfp->fi_had_conflict = true;\n\t/*\n\t * If there are no delegations on the list, then return true\n\t * so that the lease code will go ahead and delete it.\n\t */\n\tif (list_empty(&fp->fi_delegations))\n\t\tret = true;\n\telse\n\t\tlist_for_each_entry(dp, &fp->fi_delegations, dl_perfile)\n\t\t\tnfsd_break_one_deleg(dp);\n\tspin_unlock(&fp->fi_lock);\n\treturn ret;\n}\n\nstatic int\nnfsd_change_deleg_cb(struct file_lock *onlist, int arg,\n\t\t     struct list_head *dispose)\n{\n\tif (arg & F_UNLCK)\n\t\treturn lease_modify(onlist, arg, dispose);\n\telse\n\t\treturn -EAGAIN;\n}\n\nstatic const struct lock_manager_operations nfsd_lease_mng_ops = {\n\t.lm_break = nfsd_break_deleg_cb,\n\t.lm_change = nfsd_change_deleg_cb,\n};\n\nstatic __be32 nfsd4_check_seqid(struct nfsd4_compound_state *cstate, struct nfs4_stateowner *so, u32 seqid)\n{\n\tif (nfsd4_has_session(cstate))\n\t\treturn nfs_ok;\n\tif (seqid == so->so_seqid - 1)\n\t\treturn nfserr_replay_me;\n\tif (seqid == so->so_seqid)\n\t\treturn nfs_ok;\n\treturn nfserr_bad_seqid;\n}\n\nstatic __be32 lookup_clientid(clientid_t *clid,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd_net *nn)\n{\n\tstruct nfs4_client *found;\n\n\tif (cstate->clp) {\n\t\tfound = cstate->clp;\n\t\tif (!same_clid(&found->cl_clientid, clid))\n\t\t\treturn nfserr_stale_clientid;\n\t\treturn nfs_ok;\n\t}\n\n\tif (STALE_CLIENTID(clid, nn))\n\t\treturn nfserr_stale_clientid;\n\n\t/*\n\t * For v4.1+ we get the client in the SEQUENCE op. If we don't have one\n\t * cached already then we know this is for is for v4.0 and \"sessions\"\n\t * will be false.\n\t */\n\tWARN_ON_ONCE(cstate->session);\n\tspin_lock(&nn->client_lock);\n\tfound = find_confirmed_client(clid, false, nn);\n\tif (!found) {\n\t\tspin_unlock(&nn->client_lock);\n\t\treturn nfserr_expired;\n\t}\n\tatomic_inc(&found->cl_refcount);\n\tspin_unlock(&nn->client_lock);\n\n\t/* Cache the nfs4_client in cstate! */\n\tcstate->clp = found;\n\treturn nfs_ok;\n}\n\n__be32\nnfsd4_process_open1(struct nfsd4_compound_state *cstate,\n\t\t    struct nfsd4_open *open, struct nfsd_net *nn)\n{\n\tclientid_t *clientid = &open->op_clientid;\n\tstruct nfs4_client *clp = NULL;\n\tunsigned int strhashval;\n\tstruct nfs4_openowner *oo = NULL;\n\t__be32 status;\n\n\tif (STALE_CLIENTID(&open->op_clientid, nn))\n\t\treturn nfserr_stale_clientid;\n\t/*\n\t * In case we need it later, after we've already created the\n\t * file and don't want to risk a further failure:\n\t */\n\topen->op_file = nfsd4_alloc_file();\n\tif (open->op_file == NULL)\n\t\treturn nfserr_jukebox;\n\n\tstatus = lookup_clientid(clientid, cstate, nn);\n\tif (status)\n\t\treturn status;\n\tclp = cstate->clp;\n\n\tstrhashval = ownerstr_hashval(&open->op_owner);\n\too = find_openstateowner_str(strhashval, open, clp);\n\topen->op_openowner = oo;\n\tif (!oo) {\n\t\tgoto new_owner;\n\t}\n\tif (!(oo->oo_flags & NFS4_OO_CONFIRMED)) {\n\t\t/* Replace unconfirmed owners without checking for replay. */\n\t\trelease_openowner(oo);\n\t\topen->op_openowner = NULL;\n\t\tgoto new_owner;\n\t}\n\tstatus = nfsd4_check_seqid(cstate, &oo->oo_owner, open->op_seqid);\n\tif (status)\n\t\treturn status;\n\tgoto alloc_stateid;\nnew_owner:\n\too = alloc_init_open_stateowner(strhashval, open, cstate);\n\tif (oo == NULL)\n\t\treturn nfserr_jukebox;\n\topen->op_openowner = oo;\nalloc_stateid:\n\topen->op_stp = nfs4_alloc_open_stateid(clp);\n\tif (!open->op_stp)\n\t\treturn nfserr_jukebox;\n\n\tif (nfsd4_has_session(cstate) &&\n\t    (cstate->current_fh.fh_export->ex_flags & NFSEXP_PNFS)) {\n\t\topen->op_odstate = alloc_clnt_odstate(clp);\n\t\tif (!open->op_odstate)\n\t\t\treturn nfserr_jukebox;\n\t}\n\n\treturn nfs_ok;\n}\n\nstatic inline __be32\nnfs4_check_delegmode(struct nfs4_delegation *dp, int flags)\n{\n\tif ((flags & WR_STATE) && (dp->dl_type == NFS4_OPEN_DELEGATE_READ))\n\t\treturn nfserr_openmode;\n\telse\n\t\treturn nfs_ok;\n}\n\nstatic int share_access_to_flags(u32 share_access)\n{\n\treturn share_access == NFS4_SHARE_ACCESS_READ ? RD_STATE : WR_STATE;\n}\n\nstatic struct nfs4_delegation *find_deleg_stateid(struct nfs4_client *cl, stateid_t *s)\n{\n\tstruct nfs4_stid *ret;\n\n\tret = find_stateid_by_type(cl, s, NFS4_DELEG_STID);\n\tif (!ret)\n\t\treturn NULL;\n\treturn delegstateid(ret);\n}\n\nstatic bool nfsd4_is_deleg_cur(struct nfsd4_open *open)\n{\n\treturn open->op_claim_type == NFS4_OPEN_CLAIM_DELEGATE_CUR ||\n\t       open->op_claim_type == NFS4_OPEN_CLAIM_DELEG_CUR_FH;\n}\n\nstatic __be32\nnfs4_check_deleg(struct nfs4_client *cl, struct nfsd4_open *open,\n\t\tstruct nfs4_delegation **dp)\n{\n\tint flags;\n\t__be32 status = nfserr_bad_stateid;\n\tstruct nfs4_delegation *deleg;\n\n\tdeleg = find_deleg_stateid(cl, &open->op_delegate_stateid);\n\tif (deleg == NULL)\n\t\tgoto out;\n\tflags = share_access_to_flags(open->op_share_access);\n\tstatus = nfs4_check_delegmode(deleg, flags);\n\tif (status) {\n\t\tnfs4_put_stid(&deleg->dl_stid);\n\t\tgoto out;\n\t}\n\t*dp = deleg;\nout:\n\tif (!nfsd4_is_deleg_cur(open))\n\t\treturn nfs_ok;\n\tif (status)\n\t\treturn status;\n\topen->op_openowner->oo_flags |= NFS4_OO_CONFIRMED;\n\treturn nfs_ok;\n}\n\nstatic inline int nfs4_access_to_access(u32 nfs4_access)\n{\n\tint flags = 0;\n\n\tif (nfs4_access & NFS4_SHARE_ACCESS_READ)\n\t\tflags |= NFSD_MAY_READ;\n\tif (nfs4_access & NFS4_SHARE_ACCESS_WRITE)\n\t\tflags |= NFSD_MAY_WRITE;\n\treturn flags;\n}\n\nstatic inline __be32\nnfsd4_truncate(struct svc_rqst *rqstp, struct svc_fh *fh,\n\t\tstruct nfsd4_open *open)\n{\n\tstruct iattr iattr = {\n\t\t.ia_valid = ATTR_SIZE,\n\t\t.ia_size = 0,\n\t};\n\tif (!open->op_truncate)\n\t\treturn 0;\n\tif (!(open->op_share_access & NFS4_SHARE_ACCESS_WRITE))\n\t\treturn nfserr_inval;\n\treturn nfsd_setattr(rqstp, fh, &iattr, 0, (time_t)0);\n}\n\nstatic __be32 nfs4_get_vfs_file(struct svc_rqst *rqstp, struct nfs4_file *fp,\n\t\tstruct svc_fh *cur_fh, struct nfs4_ol_stateid *stp,\n\t\tstruct nfsd4_open *open)\n{\n\tstruct file *filp = NULL;\n\t__be32 status;\n\tint oflag = nfs4_access_to_omode(open->op_share_access);\n\tint access = nfs4_access_to_access(open->op_share_access);\n\tunsigned char old_access_bmap, old_deny_bmap;\n\n\tspin_lock(&fp->fi_lock);\n\n\t/*\n\t * Are we trying to set a deny mode that would conflict with\n\t * current access?\n\t */\n\tstatus = nfs4_file_check_deny(fp, open->op_share_deny);\n\tif (status != nfs_ok) {\n\t\tspin_unlock(&fp->fi_lock);\n\t\tgoto out;\n\t}\n\n\t/* set access to the file */\n\tstatus = nfs4_file_get_access(fp, open->op_share_access);\n\tif (status != nfs_ok) {\n\t\tspin_unlock(&fp->fi_lock);\n\t\tgoto out;\n\t}\n\n\t/* Set access bits in stateid */\n\told_access_bmap = stp->st_access_bmap;\n\tset_access(open->op_share_access, stp);\n\n\t/* Set new deny mask */\n\told_deny_bmap = stp->st_deny_bmap;\n\tset_deny(open->op_share_deny, stp);\n\tfp->fi_share_deny |= (open->op_share_deny & NFS4_SHARE_DENY_BOTH);\n\n\tif (!fp->fi_fds[oflag]) {\n\t\tspin_unlock(&fp->fi_lock);\n\t\tstatus = nfsd_open(rqstp, cur_fh, S_IFREG, access, &filp);\n\t\tif (status)\n\t\t\tgoto out_put_access;\n\t\tspin_lock(&fp->fi_lock);\n\t\tif (!fp->fi_fds[oflag]) {\n\t\t\tfp->fi_fds[oflag] = filp;\n\t\t\tfilp = NULL;\n\t\t}\n\t}\n\tspin_unlock(&fp->fi_lock);\n\tif (filp)\n\t\tfput(filp);\n\n\tstatus = nfsd4_truncate(rqstp, cur_fh, open);\n\tif (status)\n\t\tgoto out_put_access;\nout:\n\treturn status;\nout_put_access:\n\tstp->st_access_bmap = old_access_bmap;\n\tnfs4_file_put_access(fp, open->op_share_access);\n\treset_union_bmap_deny(bmap_to_share_mode(old_deny_bmap), stp);\n\tgoto out;\n}\n\nstatic __be32\nnfs4_upgrade_open(struct svc_rqst *rqstp, struct nfs4_file *fp, struct svc_fh *cur_fh, struct nfs4_ol_stateid *stp, struct nfsd4_open *open)\n{\n\t__be32 status;\n\tunsigned char old_deny_bmap = stp->st_deny_bmap;\n\n\tif (!test_access(open->op_share_access, stp))\n\t\treturn nfs4_get_vfs_file(rqstp, fp, cur_fh, stp, open);\n\n\t/* test and set deny mode */\n\tspin_lock(&fp->fi_lock);\n\tstatus = nfs4_file_check_deny(fp, open->op_share_deny);\n\tif (status == nfs_ok) {\n\t\tset_deny(open->op_share_deny, stp);\n\t\tfp->fi_share_deny |=\n\t\t\t\t(open->op_share_deny & NFS4_SHARE_DENY_BOTH);\n\t}\n\tspin_unlock(&fp->fi_lock);\n\n\tif (status != nfs_ok)\n\t\treturn status;\n\n\tstatus = nfsd4_truncate(rqstp, cur_fh, open);\n\tif (status != nfs_ok)\n\t\treset_union_bmap_deny(old_deny_bmap, stp);\n\treturn status;\n}\n\n/* Should we give out recallable state?: */\nstatic bool nfsd4_cb_channel_good(struct nfs4_client *clp)\n{\n\tif (clp->cl_cb_state == NFSD4_CB_UP)\n\t\treturn true;\n\t/*\n\t * In the sessions case, since we don't have to establish a\n\t * separate connection for callbacks, we assume it's OK\n\t * until we hear otherwise:\n\t */\n\treturn clp->cl_minorversion && clp->cl_cb_state == NFSD4_CB_UNKNOWN;\n}\n\nstatic struct file_lock *nfs4_alloc_init_lease(struct nfs4_file *fp, int flag)\n{\n\tstruct file_lock *fl;\n\n\tfl = locks_alloc_lock();\n\tif (!fl)\n\t\treturn NULL;\n\tfl->fl_lmops = &nfsd_lease_mng_ops;\n\tfl->fl_flags = FL_DELEG;\n\tfl->fl_type = flag == NFS4_OPEN_DELEGATE_READ? F_RDLCK: F_WRLCK;\n\tfl->fl_end = OFFSET_MAX;\n\tfl->fl_owner = (fl_owner_t)fp;\n\tfl->fl_pid = current->tgid;\n\treturn fl;\n}\n\n/**\n * nfs4_setlease - Obtain a delegation by requesting lease from vfs layer\n * @dp:   a pointer to the nfs4_delegation we're adding.\n *\n * Return:\n *      On success: Return code will be 0 on success.\n *\n *      On error: -EAGAIN if there was an existing delegation.\n *                 nonzero if there is an error in other cases.\n *\n */\n\nstatic int nfs4_setlease(struct nfs4_delegation *dp)\n{\n\tstruct nfs4_file *fp = dp->dl_stid.sc_file;\n\tstruct file_lock *fl;\n\tstruct file *filp;\n\tint status = 0;\n\n\tfl = nfs4_alloc_init_lease(fp, NFS4_OPEN_DELEGATE_READ);\n\tif (!fl)\n\t\treturn -ENOMEM;\n\tfilp = find_readable_file(fp);\n\tif (!filp) {\n\t\t/* We should always have a readable file here */\n\t\tWARN_ON_ONCE(1);\n\t\tlocks_free_lock(fl);\n\t\treturn -EBADF;\n\t}\n\tfl->fl_file = filp;\n\tstatus = vfs_setlease(filp, fl->fl_type, &fl, NULL);\n\tif (fl)\n\t\tlocks_free_lock(fl);\n\tif (status)\n\t\tgoto out_fput;\n\tspin_lock(&state_lock);\n\tspin_lock(&fp->fi_lock);\n\t/* Did the lease get broken before we took the lock? */\n\tstatus = -EAGAIN;\n\tif (fp->fi_had_conflict)\n\t\tgoto out_unlock;\n\t/* Race breaker */\n\tif (fp->fi_deleg_file) {\n\t\tstatus = hash_delegation_locked(dp, fp);\n\t\tgoto out_unlock;\n\t}\n\tfp->fi_deleg_file = filp;\n\tfp->fi_delegees = 0;\n\tstatus = hash_delegation_locked(dp, fp);\n\tspin_unlock(&fp->fi_lock);\n\tspin_unlock(&state_lock);\n\tif (status) {\n\t\t/* Should never happen, this is a new fi_deleg_file  */\n\t\tWARN_ON_ONCE(1);\n\t\tgoto out_fput;\n\t}\n\treturn 0;\nout_unlock:\n\tspin_unlock(&fp->fi_lock);\n\tspin_unlock(&state_lock);\nout_fput:\n\tfput(filp);\n\treturn status;\n}\n\nstatic struct nfs4_delegation *\nnfs4_set_delegation(struct nfs4_client *clp, struct svc_fh *fh,\n\t\t    struct nfs4_file *fp, struct nfs4_clnt_odstate *odstate)\n{\n\tint status;\n\tstruct nfs4_delegation *dp;\n\n\tif (fp->fi_had_conflict)\n\t\treturn ERR_PTR(-EAGAIN);\n\n\tspin_lock(&state_lock);\n\tspin_lock(&fp->fi_lock);\n\tstatus = nfs4_get_existing_delegation(clp, fp);\n\tspin_unlock(&fp->fi_lock);\n\tspin_unlock(&state_lock);\n\n\tif (status)\n\t\treturn ERR_PTR(status);\n\n\tdp = alloc_init_deleg(clp, fh, odstate);\n\tif (!dp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tget_nfs4_file(fp);\n\tspin_lock(&state_lock);\n\tspin_lock(&fp->fi_lock);\n\tdp->dl_stid.sc_file = fp;\n\tif (!fp->fi_deleg_file) {\n\t\tspin_unlock(&fp->fi_lock);\n\t\tspin_unlock(&state_lock);\n\t\tstatus = nfs4_setlease(dp);\n\t\tgoto out;\n\t}\n\tif (fp->fi_had_conflict) {\n\t\tstatus = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\tstatus = hash_delegation_locked(dp, fp);\nout_unlock:\n\tspin_unlock(&fp->fi_lock);\n\tspin_unlock(&state_lock);\nout:\n\tif (status) {\n\t\tput_clnt_odstate(dp->dl_clnt_odstate);\n\t\tnfs4_put_stid(&dp->dl_stid);\n\t\treturn ERR_PTR(status);\n\t}\n\treturn dp;\n}\n\nstatic void nfsd4_open_deleg_none_ext(struct nfsd4_open *open, int status)\n{\n\topen->op_delegate_type = NFS4_OPEN_DELEGATE_NONE_EXT;\n\tif (status == -EAGAIN)\n\t\topen->op_why_no_deleg = WND4_CONTENTION;\n\telse {\n\t\topen->op_why_no_deleg = WND4_RESOURCE;\n\t\tswitch (open->op_deleg_want) {\n\t\tcase NFS4_SHARE_WANT_READ_DELEG:\n\t\tcase NFS4_SHARE_WANT_WRITE_DELEG:\n\t\tcase NFS4_SHARE_WANT_ANY_DELEG:\n\t\t\tbreak;\n\t\tcase NFS4_SHARE_WANT_CANCEL:\n\t\t\topen->op_why_no_deleg = WND4_CANCELLED;\n\t\t\tbreak;\n\t\tcase NFS4_SHARE_WANT_NO_DELEG:\n\t\t\tWARN_ON_ONCE(1);\n\t\t}\n\t}\n}\n\n/*\n * Attempt to hand out a delegation.\n *\n * Note we don't support write delegations, and won't until the vfs has\n * proper support for them.\n */\nstatic void\nnfs4_open_delegation(struct svc_fh *fh, struct nfsd4_open *open,\n\t\t\tstruct nfs4_ol_stateid *stp)\n{\n\tstruct nfs4_delegation *dp;\n\tstruct nfs4_openowner *oo = openowner(stp->st_stateowner);\n\tstruct nfs4_client *clp = stp->st_stid.sc_client;\n\tint cb_up;\n\tint status = 0;\n\n\tcb_up = nfsd4_cb_channel_good(oo->oo_owner.so_client);\n\topen->op_recall = 0;\n\tswitch (open->op_claim_type) {\n\t\tcase NFS4_OPEN_CLAIM_PREVIOUS:\n\t\t\tif (!cb_up)\n\t\t\t\topen->op_recall = 1;\n\t\t\tif (open->op_delegate_type != NFS4_OPEN_DELEGATE_READ)\n\t\t\t\tgoto out_no_deleg;\n\t\t\tbreak;\n\t\tcase NFS4_OPEN_CLAIM_NULL:\n\t\tcase NFS4_OPEN_CLAIM_FH:\n\t\t\t/*\n\t\t\t * Let's not give out any delegations till everyone's\n\t\t\t * had the chance to reclaim theirs, *and* until\n\t\t\t * NLM locks have all been reclaimed:\n\t\t\t */\n\t\t\tif (locks_in_grace(clp->net))\n\t\t\t\tgoto out_no_deleg;\n\t\t\tif (!cb_up || !(oo->oo_flags & NFS4_OO_CONFIRMED))\n\t\t\t\tgoto out_no_deleg;\n\t\t\t/*\n\t\t\t * Also, if the file was opened for write or\n\t\t\t * create, there's a good chance the client's\n\t\t\t * about to write to it, resulting in an\n\t\t\t * immediate recall (since we don't support\n\t\t\t * write delegations):\n\t\t\t */\n\t\t\tif (open->op_share_access & NFS4_SHARE_ACCESS_WRITE)\n\t\t\t\tgoto out_no_deleg;\n\t\t\tif (open->op_create == NFS4_OPEN_CREATE)\n\t\t\t\tgoto out_no_deleg;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto out_no_deleg;\n\t}\n\tdp = nfs4_set_delegation(clp, fh, stp->st_stid.sc_file, stp->st_clnt_odstate);\n\tif (IS_ERR(dp))\n\t\tgoto out_no_deleg;\n\n\tmemcpy(&open->op_delegate_stateid, &dp->dl_stid.sc_stateid, sizeof(dp->dl_stid.sc_stateid));\n\n\tdprintk(\"NFSD: delegation stateid=\" STATEID_FMT \"\\n\",\n\t\tSTATEID_VAL(&dp->dl_stid.sc_stateid));\n\topen->op_delegate_type = NFS4_OPEN_DELEGATE_READ;\n\tnfs4_put_stid(&dp->dl_stid);\n\treturn;\nout_no_deleg:\n\topen->op_delegate_type = NFS4_OPEN_DELEGATE_NONE;\n\tif (open->op_claim_type == NFS4_OPEN_CLAIM_PREVIOUS &&\n\t    open->op_delegate_type != NFS4_OPEN_DELEGATE_NONE) {\n\t\tdprintk(\"NFSD: WARNING: refusing delegation reclaim\\n\");\n\t\topen->op_recall = 1;\n\t}\n\n\t/* 4.1 client asking for a delegation? */\n\tif (open->op_deleg_want)\n\t\tnfsd4_open_deleg_none_ext(open, status);\n\treturn;\n}\n\nstatic void nfsd4_deleg_xgrade_none_ext(struct nfsd4_open *open,\n\t\t\t\t\tstruct nfs4_delegation *dp)\n{\n\tif (open->op_deleg_want == NFS4_SHARE_WANT_READ_DELEG &&\n\t    dp->dl_type == NFS4_OPEN_DELEGATE_WRITE) {\n\t\topen->op_delegate_type = NFS4_OPEN_DELEGATE_NONE_EXT;\n\t\topen->op_why_no_deleg = WND4_NOT_SUPP_DOWNGRADE;\n\t} else if (open->op_deleg_want == NFS4_SHARE_WANT_WRITE_DELEG &&\n\t\t   dp->dl_type == NFS4_OPEN_DELEGATE_WRITE) {\n\t\topen->op_delegate_type = NFS4_OPEN_DELEGATE_NONE_EXT;\n\t\topen->op_why_no_deleg = WND4_NOT_SUPP_UPGRADE;\n\t}\n\t/* Otherwise the client must be confused wanting a delegation\n\t * it already has, therefore we don't return\n\t * NFS4_OPEN_DELEGATE_NONE_EXT and reason.\n\t */\n}\n\n__be32\nnfsd4_process_open2(struct svc_rqst *rqstp, struct svc_fh *current_fh, struct nfsd4_open *open)\n{\n\tstruct nfsd4_compoundres *resp = rqstp->rq_resp;\n\tstruct nfs4_client *cl = open->op_openowner->oo_owner.so_client;\n\tstruct nfs4_file *fp = NULL;\n\tstruct nfs4_ol_stateid *stp = NULL;\n\tstruct nfs4_delegation *dp = NULL;\n\t__be32 status;\n\n\t/*\n\t * Lookup file; if found, lookup stateid and check open request,\n\t * and check for delegations in the process of being recalled.\n\t * If not found, create the nfs4_file struct\n\t */\n\tfp = find_or_add_file(open->op_file, &current_fh->fh_handle);\n\tif (fp != open->op_file) {\n\t\tstatus = nfs4_check_deleg(cl, open, &dp);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tspin_lock(&fp->fi_lock);\n\t\tstp = nfsd4_find_existing_open(fp, open);\n\t\tspin_unlock(&fp->fi_lock);\n\t} else {\n\t\topen->op_file = NULL;\n\t\tstatus = nfserr_bad_stateid;\n\t\tif (nfsd4_is_deleg_cur(open))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * OPEN the file, or upgrade an existing OPEN.\n\t * If truncate fails, the OPEN fails.\n\t */\n\tif (stp) {\n\t\t/* Stateid was found, this is an OPEN upgrade */\n\t\tmutex_lock(&stp->st_mutex);\n\t\tstatus = nfs4_upgrade_open(rqstp, fp, current_fh, stp, open);\n\t\tif (status) {\n\t\t\tmutex_unlock(&stp->st_mutex);\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\t/* stp is returned locked. */\n\t\tstp = init_open_stateid(fp, open);\n\t\t/* See if we lost the race to some other thread */\n\t\tif (stp->st_access_bmap != 0) {\n\t\t\tstatus = nfs4_upgrade_open(rqstp, fp, current_fh,\n\t\t\t\t\t\tstp, open);\n\t\t\tif (status) {\n\t\t\t\tmutex_unlock(&stp->st_mutex);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tgoto upgrade_out;\n\t\t}\n\t\tstatus = nfs4_get_vfs_file(rqstp, fp, current_fh, stp, open);\n\t\tif (status) {\n\t\t\tmutex_unlock(&stp->st_mutex);\n\t\t\trelease_open_stateid(stp);\n\t\t\tgoto out;\n\t\t}\n\n\t\tstp->st_clnt_odstate = find_or_hash_clnt_odstate(fp,\n\t\t\t\t\t\t\topen->op_odstate);\n\t\tif (stp->st_clnt_odstate == open->op_odstate)\n\t\t\topen->op_odstate = NULL;\n\t}\nupgrade_out:\n\tnfs4_inc_and_copy_stateid(&open->op_stateid, &stp->st_stid);\n\tmutex_unlock(&stp->st_mutex);\n\n\tif (nfsd4_has_session(&resp->cstate)) {\n\t\tif (open->op_deleg_want & NFS4_SHARE_WANT_NO_DELEG) {\n\t\t\topen->op_delegate_type = NFS4_OPEN_DELEGATE_NONE_EXT;\n\t\t\topen->op_why_no_deleg = WND4_NOT_WANTED;\n\t\t\tgoto nodeleg;\n\t\t}\n\t}\n\n\t/*\n\t* Attempt to hand out a delegation. No error return, because the\n\t* OPEN succeeds even if we fail.\n\t*/\n\tnfs4_open_delegation(current_fh, open, stp);\nnodeleg:\n\tstatus = nfs_ok;\n\n\tdprintk(\"%s: stateid=\" STATEID_FMT \"\\n\", __func__,\n\t\tSTATEID_VAL(&stp->st_stid.sc_stateid));\nout:\n\t/* 4.1 client trying to upgrade/downgrade delegation? */\n\tif (open->op_delegate_type == NFS4_OPEN_DELEGATE_NONE && dp &&\n\t    open->op_deleg_want)\n\t\tnfsd4_deleg_xgrade_none_ext(open, dp);\n\n\tif (fp)\n\t\tput_nfs4_file(fp);\n\tif (status == 0 && open->op_claim_type == NFS4_OPEN_CLAIM_PREVIOUS)\n\t\topen->op_openowner->oo_flags |= NFS4_OO_CONFIRMED;\n\t/*\n\t* To finish the open response, we just need to set the rflags.\n\t*/\n\topen->op_rflags = NFS4_OPEN_RESULT_LOCKTYPE_POSIX;\n\tif (nfsd4_has_session(&resp->cstate))\n\t\topen->op_rflags |= NFS4_OPEN_RESULT_MAY_NOTIFY_LOCK;\n\telse if (!(open->op_openowner->oo_flags & NFS4_OO_CONFIRMED))\n\t\topen->op_rflags |= NFS4_OPEN_RESULT_CONFIRM;\n\n\tif (dp)\n\t\tnfs4_put_stid(&dp->dl_stid);\n\tif (stp)\n\t\tnfs4_put_stid(&stp->st_stid);\n\n\treturn status;\n}\n\nvoid nfsd4_cleanup_open_state(struct nfsd4_compound_state *cstate,\n\t\t\t      struct nfsd4_open *open)\n{\n\tif (open->op_openowner) {\n\t\tstruct nfs4_stateowner *so = &open->op_openowner->oo_owner;\n\n\t\tnfsd4_cstate_assign_replay(cstate, so);\n\t\tnfs4_put_stateowner(so);\n\t}\n\tif (open->op_file)\n\t\tkmem_cache_free(file_slab, open->op_file);\n\tif (open->op_stp)\n\t\tnfs4_put_stid(&open->op_stp->st_stid);\n\tif (open->op_odstate)\n\t\tkmem_cache_free(odstate_slab, open->op_odstate);\n}\n\n__be32\nnfsd4_renew(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    clientid_t *clid)\n{\n\tstruct nfs4_client *clp;\n\t__be32 status;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tdprintk(\"process_renew(%08x/%08x): starting\\n\", \n\t\t\tclid->cl_boot, clid->cl_id);\n\tstatus = lookup_clientid(clid, cstate, nn);\n\tif (status)\n\t\tgoto out;\n\tclp = cstate->clp;\n\tstatus = nfserr_cb_path_down;\n\tif (!list_empty(&clp->cl_delegations)\n\t\t\t&& clp->cl_cb_state != NFSD4_CB_UP)\n\t\tgoto out;\n\tstatus = nfs_ok;\nout:\n\treturn status;\n}\n\nvoid\nnfsd4_end_grace(struct nfsd_net *nn)\n{\n\t/* do nothing if grace period already ended */\n\tif (nn->grace_ended)\n\t\treturn;\n\n\tdprintk(\"NFSD: end of grace period\\n\");\n\tnn->grace_ended = true;\n\t/*\n\t * If the server goes down again right now, an NFSv4\n\t * client will still be allowed to reclaim after it comes back up,\n\t * even if it hasn't yet had a chance to reclaim state this time.\n\t *\n\t */\n\tnfsd4_record_grace_done(nn);\n\t/*\n\t * At this point, NFSv4 clients can still reclaim.  But if the\n\t * server crashes, any that have not yet reclaimed will be out\n\t * of luck on the next boot.\n\t *\n\t * (NFSv4.1+ clients are considered to have reclaimed once they\n\t * call RECLAIM_COMPLETE.  NFSv4.0 clients are considered to\n\t * have reclaimed after their first OPEN.)\n\t */\n\tlocks_end_grace(&nn->nfsd4_manager);\n\t/*\n\t * At this point, and once lockd and/or any other containers\n\t * exit their grace period, further reclaims will fail and\n\t * regular locking can resume.\n\t */\n}\n\nstatic time_t\nnfs4_laundromat(struct nfsd_net *nn)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfs4_openowner *oo;\n\tstruct nfs4_delegation *dp;\n\tstruct nfs4_ol_stateid *stp;\n\tstruct nfsd4_blocked_lock *nbl;\n\tstruct list_head *pos, *next, reaplist;\n\ttime_t cutoff = get_seconds() - nn->nfsd4_lease;\n\ttime_t t, new_timeo = nn->nfsd4_lease;\n\n\tdprintk(\"NFSD: laundromat service - starting\\n\");\n\tnfsd4_end_grace(nn);\n\tINIT_LIST_HEAD(&reaplist);\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_safe(pos, next, &nn->client_lru) {\n\t\tclp = list_entry(pos, struct nfs4_client, cl_lru);\n\t\tif (time_after((unsigned long)clp->cl_time, (unsigned long)cutoff)) {\n\t\t\tt = clp->cl_time - cutoff;\n\t\t\tnew_timeo = min(new_timeo, t);\n\t\t\tbreak;\n\t\t}\n\t\tif (mark_client_expired_locked(clp)) {\n\t\t\tdprintk(\"NFSD: client in use (clientid %08x)\\n\",\n\t\t\t\tclp->cl_clientid.cl_id);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_add(&clp->cl_lru, &reaplist);\n\t}\n\tspin_unlock(&nn->client_lock);\n\tlist_for_each_safe(pos, next, &reaplist) {\n\t\tclp = list_entry(pos, struct nfs4_client, cl_lru);\n\t\tdprintk(\"NFSD: purging unused client (clientid %08x)\\n\",\n\t\t\tclp->cl_clientid.cl_id);\n\t\tlist_del_init(&clp->cl_lru);\n\t\texpire_client(clp);\n\t}\n\tspin_lock(&state_lock);\n\tlist_for_each_safe(pos, next, &nn->del_recall_lru) {\n\t\tdp = list_entry (pos, struct nfs4_delegation, dl_recall_lru);\n\t\tif (time_after((unsigned long)dp->dl_time, (unsigned long)cutoff)) {\n\t\t\tt = dp->dl_time - cutoff;\n\t\t\tnew_timeo = min(new_timeo, t);\n\t\t\tbreak;\n\t\t}\n\t\tWARN_ON(!unhash_delegation_locked(dp));\n\t\tlist_add(&dp->dl_recall_lru, &reaplist);\n\t}\n\tspin_unlock(&state_lock);\n\twhile (!list_empty(&reaplist)) {\n\t\tdp = list_first_entry(&reaplist, struct nfs4_delegation,\n\t\t\t\t\tdl_recall_lru);\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\trevoke_delegation(dp);\n\t}\n\n\tspin_lock(&nn->client_lock);\n\twhile (!list_empty(&nn->close_lru)) {\n\t\too = list_first_entry(&nn->close_lru, struct nfs4_openowner,\n\t\t\t\t\too_close_lru);\n\t\tif (time_after((unsigned long)oo->oo_time,\n\t\t\t       (unsigned long)cutoff)) {\n\t\t\tt = oo->oo_time - cutoff;\n\t\t\tnew_timeo = min(new_timeo, t);\n\t\t\tbreak;\n\t\t}\n\t\tlist_del_init(&oo->oo_close_lru);\n\t\tstp = oo->oo_last_closed_stid;\n\t\too->oo_last_closed_stid = NULL;\n\t\tspin_unlock(&nn->client_lock);\n\t\tnfs4_put_stid(&stp->st_stid);\n\t\tspin_lock(&nn->client_lock);\n\t}\n\tspin_unlock(&nn->client_lock);\n\n\t/*\n\t * It's possible for a client to try and acquire an already held lock\n\t * that is being held for a long time, and then lose interest in it.\n\t * So, we clean out any un-revisited request after a lease period\n\t * under the assumption that the client is no longer interested.\n\t *\n\t * RFC5661, sec. 9.6 states that the client must not rely on getting\n\t * notifications and must continue to poll for locks, even when the\n\t * server supports them. Thus this shouldn't lead to clients blocking\n\t * indefinitely once the lock does become free.\n\t */\n\tBUG_ON(!list_empty(&reaplist));\n\tspin_lock(&nn->blocked_locks_lock);\n\twhile (!list_empty(&nn->blocked_locks_lru)) {\n\t\tnbl = list_first_entry(&nn->blocked_locks_lru,\n\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_lru);\n\t\tif (time_after((unsigned long)nbl->nbl_time,\n\t\t\t       (unsigned long)cutoff)) {\n\t\t\tt = nbl->nbl_time - cutoff;\n\t\t\tnew_timeo = min(new_timeo, t);\n\t\t\tbreak;\n\t\t}\n\t\tlist_move(&nbl->nbl_lru, &reaplist);\n\t\tlist_del_init(&nbl->nbl_list);\n\t}\n\tspin_unlock(&nn->blocked_locks_lock);\n\n\twhile (!list_empty(&reaplist)) {\n\t\tnbl = list_first_entry(&nn->blocked_locks_lru,\n\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_lru);\n\t\tlist_del_init(&nbl->nbl_lru);\n\t\tposix_unblock_lock(&nbl->nbl_lock);\n\t\tfree_blocked_lock(nbl);\n\t}\n\n\tnew_timeo = max_t(time_t, new_timeo, NFSD_LAUNDROMAT_MINTIMEOUT);\n\treturn new_timeo;\n}\n\nstatic struct workqueue_struct *laundry_wq;\nstatic void laundromat_main(struct work_struct *);\n\nstatic void\nlaundromat_main(struct work_struct *laundry)\n{\n\ttime_t t;\n\tstruct delayed_work *dwork = to_delayed_work(laundry);\n\tstruct nfsd_net *nn = container_of(dwork, struct nfsd_net,\n\t\t\t\t\t   laundromat_work);\n\n\tt = nfs4_laundromat(nn);\n\tdprintk(\"NFSD: laundromat_main - sleeping for %ld seconds\\n\", t);\n\tqueue_delayed_work(laundry_wq, &nn->laundromat_work, t*HZ);\n}\n\nstatic inline __be32 nfs4_check_fh(struct svc_fh *fhp, struct nfs4_stid *stp)\n{\n\tif (!fh_match(&fhp->fh_handle, &stp->sc_file->fi_fhandle))\n\t\treturn nfserr_bad_stateid;\n\treturn nfs_ok;\n}\n\nstatic inline int\naccess_permit_read(struct nfs4_ol_stateid *stp)\n{\n\treturn test_access(NFS4_SHARE_ACCESS_READ, stp) ||\n\t\ttest_access(NFS4_SHARE_ACCESS_BOTH, stp) ||\n\t\ttest_access(NFS4_SHARE_ACCESS_WRITE, stp);\n}\n\nstatic inline int\naccess_permit_write(struct nfs4_ol_stateid *stp)\n{\n\treturn test_access(NFS4_SHARE_ACCESS_WRITE, stp) ||\n\t\ttest_access(NFS4_SHARE_ACCESS_BOTH, stp);\n}\n\nstatic\n__be32 nfs4_check_openmode(struct nfs4_ol_stateid *stp, int flags)\n{\n        __be32 status = nfserr_openmode;\n\n\t/* For lock stateid's, we test the parent open, not the lock: */\n\tif (stp->st_openstp)\n\t\tstp = stp->st_openstp;\n\tif ((flags & WR_STATE) && !access_permit_write(stp))\n                goto out;\n\tif ((flags & RD_STATE) && !access_permit_read(stp))\n                goto out;\n\tstatus = nfs_ok;\nout:\n\treturn status;\n}\n\nstatic inline __be32\ncheck_special_stateids(struct net *net, svc_fh *current_fh, stateid_t *stateid, int flags)\n{\n\tif (ONE_STATEID(stateid) && (flags & RD_STATE))\n\t\treturn nfs_ok;\n\telse if (opens_in_grace(net)) {\n\t\t/* Answer in remaining cases depends on existence of\n\t\t * conflicting state; so we must wait out the grace period. */\n\t\treturn nfserr_grace;\n\t} else if (flags & WR_STATE)\n\t\treturn nfs4_share_conflict(current_fh,\n\t\t\t\tNFS4_SHARE_DENY_WRITE);\n\telse /* (flags & RD_STATE) && ZERO_STATEID(stateid) */\n\t\treturn nfs4_share_conflict(current_fh,\n\t\t\t\tNFS4_SHARE_DENY_READ);\n}\n\n/*\n * Allow READ/WRITE during grace period on recovered state only for files\n * that are not able to provide mandatory locking.\n */\nstatic inline int\ngrace_disallows_io(struct net *net, struct inode *inode)\n{\n\treturn opens_in_grace(net) && mandatory_lock(inode);\n}\n\nstatic __be32 check_stateid_generation(stateid_t *in, stateid_t *ref, bool has_session)\n{\n\t/*\n\t * When sessions are used the stateid generation number is ignored\n\t * when it is zero.\n\t */\n\tif (has_session && in->si_generation == 0)\n\t\treturn nfs_ok;\n\n\tif (in->si_generation == ref->si_generation)\n\t\treturn nfs_ok;\n\n\t/* If the client sends us a stateid from the future, it's buggy: */\n\tif (nfsd4_stateid_generation_after(in, ref))\n\t\treturn nfserr_bad_stateid;\n\t/*\n\t * However, we could see a stateid from the past, even from a\n\t * non-buggy client.  For example, if the client sends a lock\n\t * while some IO is outstanding, the lock may bump si_generation\n\t * while the IO is still in flight.  The client could avoid that\n\t * situation by waiting for responses on all the IO requests,\n\t * but better performance may result in retrying IO that\n\t * receives an old_stateid error if requests are rarely\n\t * reordered in flight:\n\t */\n\treturn nfserr_old_stateid;\n}\n\nstatic __be32 nfsd4_check_openowner_confirmed(struct nfs4_ol_stateid *ols)\n{\n\tif (ols->st_stateowner->so_is_open_owner &&\n\t    !(openowner(ols->st_stateowner)->oo_flags & NFS4_OO_CONFIRMED))\n\t\treturn nfserr_bad_stateid;\n\treturn nfs_ok;\n}\n\nstatic __be32 nfsd4_validate_stateid(struct nfs4_client *cl, stateid_t *stateid)\n{\n\tstruct nfs4_stid *s;\n\t__be32 status = nfserr_bad_stateid;\n\n\tif (ZERO_STATEID(stateid) || ONE_STATEID(stateid))\n\t\treturn status;\n\t/* Client debugging aid. */\n\tif (!same_clid(&stateid->si_opaque.so_clid, &cl->cl_clientid)) {\n\t\tchar addr_str[INET6_ADDRSTRLEN];\n\t\trpc_ntop((struct sockaddr *)&cl->cl_addr, addr_str,\n\t\t\t\t sizeof(addr_str));\n\t\tpr_warn_ratelimited(\"NFSD: client %s testing state ID \"\n\t\t\t\t\t\"with incorrect client ID\\n\", addr_str);\n\t\treturn status;\n\t}\n\tspin_lock(&cl->cl_lock);\n\ts = find_stateid_locked(cl, stateid);\n\tif (!s)\n\t\tgoto out_unlock;\n\tstatus = check_stateid_generation(stateid, &s->sc_stateid, 1);\n\tif (status)\n\t\tgoto out_unlock;\n\tswitch (s->sc_type) {\n\tcase NFS4_DELEG_STID:\n\t\tstatus = nfs_ok;\n\t\tbreak;\n\tcase NFS4_REVOKED_DELEG_STID:\n\t\tstatus = nfserr_deleg_revoked;\n\t\tbreak;\n\tcase NFS4_OPEN_STID:\n\tcase NFS4_LOCK_STID:\n\t\tstatus = nfsd4_check_openowner_confirmed(openlockstateid(s));\n\t\tbreak;\n\tdefault:\n\t\tprintk(\"unknown stateid type %x\\n\", s->sc_type);\n\t\t/* Fallthrough */\n\tcase NFS4_CLOSED_STID:\n\tcase NFS4_CLOSED_DELEG_STID:\n\t\tstatus = nfserr_bad_stateid;\n\t}\nout_unlock:\n\tspin_unlock(&cl->cl_lock);\n\treturn status;\n}\n\n__be32\nnfsd4_lookup_stateid(struct nfsd4_compound_state *cstate,\n\t\t     stateid_t *stateid, unsigned char typemask,\n\t\t     struct nfs4_stid **s, struct nfsd_net *nn)\n{\n\t__be32 status;\n\n\tif (ZERO_STATEID(stateid) || ONE_STATEID(stateid))\n\t\treturn nfserr_bad_stateid;\n\tstatus = lookup_clientid(&stateid->si_opaque.so_clid, cstate, nn);\n\tif (status == nfserr_stale_clientid) {\n\t\tif (cstate->session)\n\t\t\treturn nfserr_bad_stateid;\n\t\treturn nfserr_stale_stateid;\n\t}\n\tif (status)\n\t\treturn status;\n\t*s = find_stateid_by_type(cstate->clp, stateid, typemask);\n\tif (!*s)\n\t\treturn nfserr_bad_stateid;\n\treturn nfs_ok;\n}\n\nstatic struct file *\nnfs4_find_file(struct nfs4_stid *s, int flags)\n{\n\tif (!s)\n\t\treturn NULL;\n\n\tswitch (s->sc_type) {\n\tcase NFS4_DELEG_STID:\n\t\tif (WARN_ON_ONCE(!s->sc_file->fi_deleg_file))\n\t\t\treturn NULL;\n\t\treturn get_file(s->sc_file->fi_deleg_file);\n\tcase NFS4_OPEN_STID:\n\tcase NFS4_LOCK_STID:\n\t\tif (flags & RD_STATE)\n\t\t\treturn find_readable_file(s->sc_file);\n\t\telse\n\t\t\treturn find_writeable_file(s->sc_file);\n\t\tbreak;\n\t}\n\n\treturn NULL;\n}\n\nstatic __be32\nnfs4_check_olstateid(struct svc_fh *fhp, struct nfs4_ol_stateid *ols, int flags)\n{\n\t__be32 status;\n\n\tstatus = nfsd4_check_openowner_confirmed(ols);\n\tif (status)\n\t\treturn status;\n\treturn nfs4_check_openmode(ols, flags);\n}\n\nstatic __be32\nnfs4_check_file(struct svc_rqst *rqstp, struct svc_fh *fhp, struct nfs4_stid *s,\n\t\tstruct file **filpp, bool *tmp_file, int flags)\n{\n\tint acc = (flags & RD_STATE) ? NFSD_MAY_READ : NFSD_MAY_WRITE;\n\tstruct file *file;\n\t__be32 status;\n\n\tfile = nfs4_find_file(s, flags);\n\tif (file) {\n\t\tstatus = nfsd_permission(rqstp, fhp->fh_export, fhp->fh_dentry,\n\t\t\t\tacc | NFSD_MAY_OWNER_OVERRIDE);\n\t\tif (status) {\n\t\t\tfput(file);\n\t\t\treturn status;\n\t\t}\n\n\t\t*filpp = file;\n\t} else {\n\t\tstatus = nfsd_open(rqstp, fhp, S_IFREG, acc, filpp);\n\t\tif (status)\n\t\t\treturn status;\n\n\t\tif (tmp_file)\n\t\t\t*tmp_file = true;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Checks for stateid operations\n */\n__be32\nnfs4_preprocess_stateid_op(struct svc_rqst *rqstp,\n\t\tstruct nfsd4_compound_state *cstate, struct svc_fh *fhp,\n\t\tstateid_t *stateid, int flags, struct file **filpp, bool *tmp_file)\n{\n\tstruct inode *ino = d_inode(fhp->fh_dentry);\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\tstruct nfs4_stid *s = NULL;\n\t__be32 status;\n\n\tif (filpp)\n\t\t*filpp = NULL;\n\tif (tmp_file)\n\t\t*tmp_file = false;\n\n\tif (grace_disallows_io(net, ino))\n\t\treturn nfserr_grace;\n\n\tif (ZERO_STATEID(stateid) || ONE_STATEID(stateid)) {\n\t\tstatus = check_special_stateids(net, fhp, stateid, flags);\n\t\tgoto done;\n\t}\n\n\tstatus = nfsd4_lookup_stateid(cstate, stateid,\n\t\t\t\tNFS4_DELEG_STID|NFS4_OPEN_STID|NFS4_LOCK_STID,\n\t\t\t\t&s, nn);\n\tif (status)\n\t\treturn status;\n\tstatus = check_stateid_generation(stateid, &s->sc_stateid,\n\t\t\tnfsd4_has_session(cstate));\n\tif (status)\n\t\tgoto out;\n\n\tswitch (s->sc_type) {\n\tcase NFS4_DELEG_STID:\n\t\tstatus = nfs4_check_delegmode(delegstateid(s), flags);\n\t\tbreak;\n\tcase NFS4_OPEN_STID:\n\tcase NFS4_LOCK_STID:\n\t\tstatus = nfs4_check_olstateid(fhp, openlockstateid(s), flags);\n\t\tbreak;\n\tdefault:\n\t\tstatus = nfserr_bad_stateid;\n\t\tbreak;\n\t}\n\tif (status)\n\t\tgoto out;\n\tstatus = nfs4_check_fh(fhp, s);\n\ndone:\n\tif (!status && filpp)\n\t\tstatus = nfs4_check_file(rqstp, fhp, s, filpp, tmp_file, flags);\nout:\n\tif (s)\n\t\tnfs4_put_stid(s);\n\treturn status;\n}\n\n/*\n * Test if the stateid is valid\n */\n__be32\nnfsd4_test_stateid(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t   struct nfsd4_test_stateid *test_stateid)\n{\n\tstruct nfsd4_test_stateid_id *stateid;\n\tstruct nfs4_client *cl = cstate->session->se_client;\n\n\tlist_for_each_entry(stateid, &test_stateid->ts_stateid_list, ts_id_list)\n\t\tstateid->ts_id_status =\n\t\t\tnfsd4_validate_stateid(cl, &stateid->ts_id_stateid);\n\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_free_lock_stateid(stateid_t *stateid, struct nfs4_stid *s)\n{\n\tstruct nfs4_ol_stateid *stp = openlockstateid(s);\n\t__be32 ret;\n\n\tmutex_lock(&stp->st_mutex);\n\n\tret = check_stateid_generation(stateid, &s->sc_stateid, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = nfserr_locks_held;\n\tif (check_for_locks(stp->st_stid.sc_file,\n\t\t\t    lockowner(stp->st_stateowner)))\n\t\tgoto out;\n\n\trelease_lock_stateid(stp);\n\tret = nfs_ok;\n\nout:\n\tmutex_unlock(&stp->st_mutex);\n\tnfs4_put_stid(s);\n\treturn ret;\n}\n\n__be32\nnfsd4_free_stateid(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t   struct nfsd4_free_stateid *free_stateid)\n{\n\tstateid_t *stateid = &free_stateid->fr_stateid;\n\tstruct nfs4_stid *s;\n\tstruct nfs4_delegation *dp;\n\tstruct nfs4_client *cl = cstate->session->se_client;\n\t__be32 ret = nfserr_bad_stateid;\n\n\tspin_lock(&cl->cl_lock);\n\ts = find_stateid_locked(cl, stateid);\n\tif (!s)\n\t\tgoto out_unlock;\n\tswitch (s->sc_type) {\n\tcase NFS4_DELEG_STID:\n\t\tret = nfserr_locks_held;\n\t\tbreak;\n\tcase NFS4_OPEN_STID:\n\t\tret = check_stateid_generation(stateid, &s->sc_stateid, 1);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = nfserr_locks_held;\n\t\tbreak;\n\tcase NFS4_LOCK_STID:\n\t\tatomic_inc(&s->sc_count);\n\t\tspin_unlock(&cl->cl_lock);\n\t\tret = nfsd4_free_lock_stateid(stateid, s);\n\t\tgoto out;\n\tcase NFS4_REVOKED_DELEG_STID:\n\t\tdp = delegstateid(s);\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tspin_unlock(&cl->cl_lock);\n\t\tnfs4_put_stid(s);\n\t\tret = nfs_ok;\n\t\tgoto out;\n\t/* Default falls through and returns nfserr_bad_stateid */\n\t}\nout_unlock:\n\tspin_unlock(&cl->cl_lock);\nout:\n\treturn ret;\n}\n\nstatic inline int\nsetlkflg (int type)\n{\n\treturn (type == NFS4_READW_LT || type == NFS4_READ_LT) ?\n\t\tRD_STATE : WR_STATE;\n}\n\nstatic __be32 nfs4_seqid_op_checks(struct nfsd4_compound_state *cstate, stateid_t *stateid, u32 seqid, struct nfs4_ol_stateid *stp)\n{\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\tstruct nfs4_stateowner *sop = stp->st_stateowner;\n\t__be32 status;\n\n\tstatus = nfsd4_check_seqid(cstate, sop, seqid);\n\tif (status)\n\t\treturn status;\n\tif (stp->st_stid.sc_type == NFS4_CLOSED_STID\n\t\t|| stp->st_stid.sc_type == NFS4_REVOKED_DELEG_STID)\n\t\t/*\n\t\t * \"Closed\" stateid's exist *only* to return\n\t\t * nfserr_replay_me from the previous step, and\n\t\t * revoked delegations are kept only for free_stateid.\n\t\t */\n\t\treturn nfserr_bad_stateid;\n\tmutex_lock(&stp->st_mutex);\n\tstatus = check_stateid_generation(stateid, &stp->st_stid.sc_stateid, nfsd4_has_session(cstate));\n\tif (status == nfs_ok)\n\t\tstatus = nfs4_check_fh(current_fh, &stp->st_stid);\n\tif (status != nfs_ok)\n\t\tmutex_unlock(&stp->st_mutex);\n\treturn status;\n}\n\n/* \n * Checks for sequence id mutating operations. \n */\nstatic __be32\nnfs4_preprocess_seqid_op(struct nfsd4_compound_state *cstate, u32 seqid,\n\t\t\t stateid_t *stateid, char typemask,\n\t\t\t struct nfs4_ol_stateid **stpp,\n\t\t\t struct nfsd_net *nn)\n{\n\t__be32 status;\n\tstruct nfs4_stid *s;\n\tstruct nfs4_ol_stateid *stp = NULL;\n\n\tdprintk(\"NFSD: %s: seqid=%d stateid = \" STATEID_FMT \"\\n\", __func__,\n\t\tseqid, STATEID_VAL(stateid));\n\n\t*stpp = NULL;\n\tstatus = nfsd4_lookup_stateid(cstate, stateid, typemask, &s, nn);\n\tif (status)\n\t\treturn status;\n\tstp = openlockstateid(s);\n\tnfsd4_cstate_assign_replay(cstate, stp->st_stateowner);\n\n\tstatus = nfs4_seqid_op_checks(cstate, stateid, seqid, stp);\n\tif (!status)\n\t\t*stpp = stp;\n\telse\n\t\tnfs4_put_stid(&stp->st_stid);\n\treturn status;\n}\n\nstatic __be32 nfs4_preprocess_confirmed_seqid_op(struct nfsd4_compound_state *cstate, u32 seqid,\n\t\t\t\t\t\t stateid_t *stateid, struct nfs4_ol_stateid **stpp, struct nfsd_net *nn)\n{\n\t__be32 status;\n\tstruct nfs4_openowner *oo;\n\tstruct nfs4_ol_stateid *stp;\n\n\tstatus = nfs4_preprocess_seqid_op(cstate, seqid, stateid,\n\t\t\t\t\t\tNFS4_OPEN_STID, &stp, nn);\n\tif (status)\n\t\treturn status;\n\too = openowner(stp->st_stateowner);\n\tif (!(oo->oo_flags & NFS4_OO_CONFIRMED)) {\n\t\tmutex_unlock(&stp->st_mutex);\n\t\tnfs4_put_stid(&stp->st_stid);\n\t\treturn nfserr_bad_stateid;\n\t}\n\t*stpp = stp;\n\treturn nfs_ok;\n}\n\n__be32\nnfsd4_open_confirm(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t   struct nfsd4_open_confirm *oc)\n{\n\t__be32 status;\n\tstruct nfs4_openowner *oo;\n\tstruct nfs4_ol_stateid *stp;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_open_confirm on file %pd\\n\",\n\t\t\tcstate->current_fh.fh_dentry);\n\n\tstatus = fh_verify(rqstp, &cstate->current_fh, S_IFREG, 0);\n\tif (status)\n\t\treturn status;\n\n\tstatus = nfs4_preprocess_seqid_op(cstate,\n\t\t\t\t\toc->oc_seqid, &oc->oc_req_stateid,\n\t\t\t\t\tNFS4_OPEN_STID, &stp, nn);\n\tif (status)\n\t\tgoto out;\n\too = openowner(stp->st_stateowner);\n\tstatus = nfserr_bad_stateid;\n\tif (oo->oo_flags & NFS4_OO_CONFIRMED) {\n\t\tmutex_unlock(&stp->st_mutex);\n\t\tgoto put_stateid;\n\t}\n\too->oo_flags |= NFS4_OO_CONFIRMED;\n\tnfs4_inc_and_copy_stateid(&oc->oc_resp_stateid, &stp->st_stid);\n\tmutex_unlock(&stp->st_mutex);\n\tdprintk(\"NFSD: %s: success, seqid=%d stateid=\" STATEID_FMT \"\\n\",\n\t\t__func__, oc->oc_seqid, STATEID_VAL(&stp->st_stid.sc_stateid));\n\n\tnfsd4_client_record_create(oo->oo_owner.so_client);\n\tstatus = nfs_ok;\nput_stateid:\n\tnfs4_put_stid(&stp->st_stid);\nout:\n\tnfsd4_bump_seqid(cstate, status);\n\treturn status;\n}\n\nstatic inline void nfs4_stateid_downgrade_bit(struct nfs4_ol_stateid *stp, u32 access)\n{\n\tif (!test_access(access, stp))\n\t\treturn;\n\tnfs4_file_put_access(stp->st_stid.sc_file, access);\n\tclear_access(access, stp);\n}\n\nstatic inline void nfs4_stateid_downgrade(struct nfs4_ol_stateid *stp, u32 to_access)\n{\n\tswitch (to_access) {\n\tcase NFS4_SHARE_ACCESS_READ:\n\t\tnfs4_stateid_downgrade_bit(stp, NFS4_SHARE_ACCESS_WRITE);\n\t\tnfs4_stateid_downgrade_bit(stp, NFS4_SHARE_ACCESS_BOTH);\n\t\tbreak;\n\tcase NFS4_SHARE_ACCESS_WRITE:\n\t\tnfs4_stateid_downgrade_bit(stp, NFS4_SHARE_ACCESS_READ);\n\t\tnfs4_stateid_downgrade_bit(stp, NFS4_SHARE_ACCESS_BOTH);\n\t\tbreak;\n\tcase NFS4_SHARE_ACCESS_BOTH:\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n}\n\n__be32\nnfsd4_open_downgrade(struct svc_rqst *rqstp,\n\t\t     struct nfsd4_compound_state *cstate,\n\t\t     struct nfsd4_open_downgrade *od)\n{\n\t__be32 status;\n\tstruct nfs4_ol_stateid *stp;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_open_downgrade on file %pd\\n\", \n\t\t\tcstate->current_fh.fh_dentry);\n\n\t/* We don't yet support WANT bits: */\n\tif (od->od_deleg_want)\n\t\tdprintk(\"NFSD: %s: od_deleg_want=0x%x ignored\\n\", __func__,\n\t\t\tod->od_deleg_want);\n\n\tstatus = nfs4_preprocess_confirmed_seqid_op(cstate, od->od_seqid,\n\t\t\t\t\t&od->od_stateid, &stp, nn);\n\tif (status)\n\t\tgoto out; \n\tstatus = nfserr_inval;\n\tif (!test_access(od->od_share_access, stp)) {\n\t\tdprintk(\"NFSD: access not a subset of current bitmap: 0x%hhx, input access=%08x\\n\",\n\t\t\tstp->st_access_bmap, od->od_share_access);\n\t\tgoto put_stateid;\n\t}\n\tif (!test_deny(od->od_share_deny, stp)) {\n\t\tdprintk(\"NFSD: deny not a subset of current bitmap: 0x%hhx, input deny=%08x\\n\",\n\t\t\tstp->st_deny_bmap, od->od_share_deny);\n\t\tgoto put_stateid;\n\t}\n\tnfs4_stateid_downgrade(stp, od->od_share_access);\n\treset_union_bmap_deny(od->od_share_deny, stp);\n\tnfs4_inc_and_copy_stateid(&od->od_stateid, &stp->st_stid);\n\tstatus = nfs_ok;\nput_stateid:\n\tmutex_unlock(&stp->st_mutex);\n\tnfs4_put_stid(&stp->st_stid);\nout:\n\tnfsd4_bump_seqid(cstate, status);\n\treturn status;\n}\n\nstatic void nfsd4_close_open_stateid(struct nfs4_ol_stateid *s)\n{\n\tstruct nfs4_client *clp = s->st_stid.sc_client;\n\tbool unhashed;\n\tLIST_HEAD(reaplist);\n\n\ts->st_stid.sc_type = NFS4_CLOSED_STID;\n\tspin_lock(&clp->cl_lock);\n\tunhashed = unhash_open_stateid(s, &reaplist);\n\n\tif (clp->cl_minorversion) {\n\t\tif (unhashed)\n\t\t\tput_ol_stateid_locked(s, &reaplist);\n\t\tspin_unlock(&clp->cl_lock);\n\t\tfree_ol_stateid_reaplist(&reaplist);\n\t} else {\n\t\tspin_unlock(&clp->cl_lock);\n\t\tfree_ol_stateid_reaplist(&reaplist);\n\t\tif (unhashed)\n\t\t\tmove_to_close_lru(s, clp->net);\n\t}\n}\n\n/*\n * nfs4_unlock_state() called after encode\n */\n__be32\nnfsd4_close(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct nfsd4_close *close)\n{\n\t__be32 status;\n\tstruct nfs4_ol_stateid *stp;\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_close on file %pd\\n\", \n\t\t\tcstate->current_fh.fh_dentry);\n\n\tstatus = nfs4_preprocess_seqid_op(cstate, close->cl_seqid,\n\t\t\t\t\t&close->cl_stateid,\n\t\t\t\t\tNFS4_OPEN_STID|NFS4_CLOSED_STID,\n\t\t\t\t\t&stp, nn);\n\tnfsd4_bump_seqid(cstate, status);\n\tif (status)\n\t\tgoto out; \n\tnfs4_inc_and_copy_stateid(&close->cl_stateid, &stp->st_stid);\n\tmutex_unlock(&stp->st_mutex);\n\n\tnfsd4_close_open_stateid(stp);\n\n\t/* put reference from nfs4_preprocess_seqid_op */\n\tnfs4_put_stid(&stp->st_stid);\nout:\n\treturn status;\n}\n\n__be32\nnfsd4_delegreturn(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t  struct nfsd4_delegreturn *dr)\n{\n\tstruct nfs4_delegation *dp;\n\tstateid_t *stateid = &dr->dr_stateid;\n\tstruct nfs4_stid *s;\n\t__be32 status;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tif ((status = fh_verify(rqstp, &cstate->current_fh, S_IFREG, 0)))\n\t\treturn status;\n\n\tstatus = nfsd4_lookup_stateid(cstate, stateid, NFS4_DELEG_STID, &s, nn);\n\tif (status)\n\t\tgoto out;\n\tdp = delegstateid(s);\n\tstatus = check_stateid_generation(stateid, &dp->dl_stid.sc_stateid, nfsd4_has_session(cstate));\n\tif (status)\n\t\tgoto put_stateid;\n\n\tdestroy_delegation(dp);\nput_stateid:\n\tnfs4_put_stid(&dp->dl_stid);\nout:\n\treturn status;\n}\n\nstatic inline u64\nend_offset(u64 start, u64 len)\n{\n\tu64 end;\n\n\tend = start + len;\n\treturn end >= start ? end: NFS4_MAX_UINT64;\n}\n\n/* last octet in a range */\nstatic inline u64\nlast_byte_offset(u64 start, u64 len)\n{\n\tu64 end;\n\n\tWARN_ON_ONCE(!len);\n\tend = start + len;\n\treturn end > start ? end - 1: NFS4_MAX_UINT64;\n}\n\n/*\n * TODO: Linux file offsets are _signed_ 64-bit quantities, which means that\n * we can't properly handle lock requests that go beyond the (2^63 - 1)-th\n * byte, because of sign extension problems.  Since NFSv4 calls for 64-bit\n * locking, this prevents us from being completely protocol-compliant.  The\n * real solution to this problem is to start using unsigned file offsets in\n * the VFS, but this is a very deep change!\n */\nstatic inline void\nnfs4_transform_lock_offset(struct file_lock *lock)\n{\n\tif (lock->fl_start < 0)\n\t\tlock->fl_start = OFFSET_MAX;\n\tif (lock->fl_end < 0)\n\t\tlock->fl_end = OFFSET_MAX;\n}\n\nstatic fl_owner_t\nnfsd4_fl_get_owner(fl_owner_t owner)\n{\n\tstruct nfs4_lockowner *lo = (struct nfs4_lockowner *)owner;\n\n\tnfs4_get_stateowner(&lo->lo_owner);\n\treturn owner;\n}\n\nstatic void\nnfsd4_fl_put_owner(fl_owner_t owner)\n{\n\tstruct nfs4_lockowner *lo = (struct nfs4_lockowner *)owner;\n\n\tif (lo)\n\t\tnfs4_put_stateowner(&lo->lo_owner);\n}\n\nstatic void\nnfsd4_lm_notify(struct file_lock *fl)\n{\n\tstruct nfs4_lockowner\t\t*lo = (struct nfs4_lockowner *)fl->fl_owner;\n\tstruct net\t\t\t*net = lo->lo_owner.so_client->net;\n\tstruct nfsd_net\t\t\t*nn = net_generic(net, nfsd_net_id);\n\tstruct nfsd4_blocked_lock\t*nbl = container_of(fl,\n\t\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_lock);\n\tbool queue = false;\n\n\t/* An empty list means that something else is going to be using it */\n\tspin_lock(&nn->blocked_locks_lock);\n\tif (!list_empty(&nbl->nbl_list)) {\n\t\tlist_del_init(&nbl->nbl_list);\n\t\tlist_del_init(&nbl->nbl_lru);\n\t\tqueue = true;\n\t}\n\tspin_unlock(&nn->blocked_locks_lock);\n\n\tif (queue)\n\t\tnfsd4_run_cb(&nbl->nbl_cb);\n}\n\nstatic const struct lock_manager_operations nfsd_posix_mng_ops  = {\n\t.lm_notify = nfsd4_lm_notify,\n\t.lm_get_owner = nfsd4_fl_get_owner,\n\t.lm_put_owner = nfsd4_fl_put_owner,\n};\n\nstatic inline void\nnfs4_set_lock_denied(struct file_lock *fl, struct nfsd4_lock_denied *deny)\n{\n\tstruct nfs4_lockowner *lo;\n\n\tif (fl->fl_lmops == &nfsd_posix_mng_ops) {\n\t\tlo = (struct nfs4_lockowner *) fl->fl_owner;\n\t\tdeny->ld_owner.data = kmemdup(lo->lo_owner.so_owner.data,\n\t\t\t\t\tlo->lo_owner.so_owner.len, GFP_KERNEL);\n\t\tif (!deny->ld_owner.data)\n\t\t\t/* We just don't care that much */\n\t\t\tgoto nevermind;\n\t\tdeny->ld_owner.len = lo->lo_owner.so_owner.len;\n\t\tdeny->ld_clientid = lo->lo_owner.so_client->cl_clientid;\n\t} else {\nnevermind:\n\t\tdeny->ld_owner.len = 0;\n\t\tdeny->ld_owner.data = NULL;\n\t\tdeny->ld_clientid.cl_boot = 0;\n\t\tdeny->ld_clientid.cl_id = 0;\n\t}\n\tdeny->ld_start = fl->fl_start;\n\tdeny->ld_length = NFS4_MAX_UINT64;\n\tif (fl->fl_end != NFS4_MAX_UINT64)\n\t\tdeny->ld_length = fl->fl_end - fl->fl_start + 1;        \n\tdeny->ld_type = NFS4_READ_LT;\n\tif (fl->fl_type != F_RDLCK)\n\t\tdeny->ld_type = NFS4_WRITE_LT;\n}\n\nstatic struct nfs4_lockowner *\nfind_lockowner_str_locked(struct nfs4_client *clp, struct xdr_netobj *owner)\n{\n\tunsigned int strhashval = ownerstr_hashval(owner);\n\tstruct nfs4_stateowner *so;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_for_each_entry(so, &clp->cl_ownerstr_hashtbl[strhashval],\n\t\t\t    so_strhash) {\n\t\tif (so->so_is_open_owner)\n\t\t\tcontinue;\n\t\tif (same_owner_str(so, owner))\n\t\t\treturn lockowner(nfs4_get_stateowner(so));\n\t}\n\treturn NULL;\n}\n\nstatic struct nfs4_lockowner *\nfind_lockowner_str(struct nfs4_client *clp, struct xdr_netobj *owner)\n{\n\tstruct nfs4_lockowner *lo;\n\n\tspin_lock(&clp->cl_lock);\n\tlo = find_lockowner_str_locked(clp, owner);\n\tspin_unlock(&clp->cl_lock);\n\treturn lo;\n}\n\nstatic void nfs4_unhash_lockowner(struct nfs4_stateowner *sop)\n{\n\tunhash_lockowner_locked(lockowner(sop));\n}\n\nstatic void nfs4_free_lockowner(struct nfs4_stateowner *sop)\n{\n\tstruct nfs4_lockowner *lo = lockowner(sop);\n\n\tkmem_cache_free(lockowner_slab, lo);\n}\n\nstatic const struct nfs4_stateowner_operations lockowner_ops = {\n\t.so_unhash =\tnfs4_unhash_lockowner,\n\t.so_free =\tnfs4_free_lockowner,\n};\n\n/*\n * Alloc a lock owner structure.\n * Called in nfsd4_lock - therefore, OPEN and OPEN_CONFIRM (if needed) has \n * occurred. \n *\n * strhashval = ownerstr_hashval\n */\nstatic struct nfs4_lockowner *\nalloc_init_lock_stateowner(unsigned int strhashval, struct nfs4_client *clp,\n\t\t\t   struct nfs4_ol_stateid *open_stp,\n\t\t\t   struct nfsd4_lock *lock)\n{\n\tstruct nfs4_lockowner *lo, *ret;\n\n\tlo = alloc_stateowner(lockowner_slab, &lock->lk_new_owner, clp);\n\tif (!lo)\n\t\treturn NULL;\n\tINIT_LIST_HEAD(&lo->lo_blocked);\n\tINIT_LIST_HEAD(&lo->lo_owner.so_stateids);\n\tlo->lo_owner.so_is_open_owner = 0;\n\tlo->lo_owner.so_seqid = lock->lk_new_lock_seqid;\n\tlo->lo_owner.so_ops = &lockowner_ops;\n\tspin_lock(&clp->cl_lock);\n\tret = find_lockowner_str_locked(clp, &lock->lk_new_owner);\n\tif (ret == NULL) {\n\t\tlist_add(&lo->lo_owner.so_strhash,\n\t\t\t &clp->cl_ownerstr_hashtbl[strhashval]);\n\t\tret = lo;\n\t} else\n\t\tnfs4_free_stateowner(&lo->lo_owner);\n\n\tspin_unlock(&clp->cl_lock);\n\treturn ret;\n}\n\nstatic void\ninit_lock_stateid(struct nfs4_ol_stateid *stp, struct nfs4_lockowner *lo,\n\t\t  struct nfs4_file *fp, struct inode *inode,\n\t\t  struct nfs4_ol_stateid *open_stp)\n{\n\tstruct nfs4_client *clp = lo->lo_owner.so_client;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tatomic_inc(&stp->st_stid.sc_count);\n\tstp->st_stid.sc_type = NFS4_LOCK_STID;\n\tstp->st_stateowner = nfs4_get_stateowner(&lo->lo_owner);\n\tget_nfs4_file(fp);\n\tstp->st_stid.sc_file = fp;\n\tstp->st_access_bmap = 0;\n\tstp->st_deny_bmap = open_stp->st_deny_bmap;\n\tstp->st_openstp = open_stp;\n\tmutex_init(&stp->st_mutex);\n\tlist_add(&stp->st_locks, &open_stp->st_locks);\n\tlist_add(&stp->st_perstateowner, &lo->lo_owner.so_stateids);\n\tspin_lock(&fp->fi_lock);\n\tlist_add(&stp->st_perfile, &fp->fi_stateids);\n\tspin_unlock(&fp->fi_lock);\n}\n\nstatic struct nfs4_ol_stateid *\nfind_lock_stateid(struct nfs4_lockowner *lo, struct nfs4_file *fp)\n{\n\tstruct nfs4_ol_stateid *lst;\n\tstruct nfs4_client *clp = lo->lo_owner.so_client;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_for_each_entry(lst, &lo->lo_owner.so_stateids, st_perstateowner) {\n\t\tif (lst->st_stid.sc_file == fp) {\n\t\t\tatomic_inc(&lst->st_stid.sc_count);\n\t\t\treturn lst;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct nfs4_ol_stateid *\nfind_or_create_lock_stateid(struct nfs4_lockowner *lo, struct nfs4_file *fi,\n\t\t\t    struct inode *inode, struct nfs4_ol_stateid *ost,\n\t\t\t    bool *new)\n{\n\tstruct nfs4_stid *ns = NULL;\n\tstruct nfs4_ol_stateid *lst;\n\tstruct nfs4_openowner *oo = openowner(ost->st_stateowner);\n\tstruct nfs4_client *clp = oo->oo_owner.so_client;\n\n\tspin_lock(&clp->cl_lock);\n\tlst = find_lock_stateid(lo, fi);\n\tif (lst == NULL) {\n\t\tspin_unlock(&clp->cl_lock);\n\t\tns = nfs4_alloc_stid(clp, stateid_slab, nfs4_free_lock_stateid);\n\t\tif (ns == NULL)\n\t\t\treturn NULL;\n\n\t\tspin_lock(&clp->cl_lock);\n\t\tlst = find_lock_stateid(lo, fi);\n\t\tif (likely(!lst)) {\n\t\t\tlst = openlockstateid(ns);\n\t\t\tinit_lock_stateid(lst, lo, fi, inode, ost);\n\t\t\tns = NULL;\n\t\t\t*new = true;\n\t\t}\n\t}\n\tspin_unlock(&clp->cl_lock);\n\tif (ns)\n\t\tnfs4_put_stid(ns);\n\treturn lst;\n}\n\nstatic int\ncheck_lock_length(u64 offset, u64 length)\n{\n\treturn ((length == 0) || ((length != NFS4_MAX_UINT64) &&\n\t\t(length > ~offset)));\n}\n\nstatic void get_lock_access(struct nfs4_ol_stateid *lock_stp, u32 access)\n{\n\tstruct nfs4_file *fp = lock_stp->st_stid.sc_file;\n\n\tlockdep_assert_held(&fp->fi_lock);\n\n\tif (test_access(access, lock_stp))\n\t\treturn;\n\t__nfs4_file_get_access(fp, access);\n\tset_access(access, lock_stp);\n}\n\nstatic __be32\nlookup_or_create_lock_state(struct nfsd4_compound_state *cstate,\n\t\t\t    struct nfs4_ol_stateid *ost,\n\t\t\t    struct nfsd4_lock *lock,\n\t\t\t    struct nfs4_ol_stateid **plst, bool *new)\n{\n\t__be32 status;\n\tstruct nfs4_file *fi = ost->st_stid.sc_file;\n\tstruct nfs4_openowner *oo = openowner(ost->st_stateowner);\n\tstruct nfs4_client *cl = oo->oo_owner.so_client;\n\tstruct inode *inode = d_inode(cstate->current_fh.fh_dentry);\n\tstruct nfs4_lockowner *lo;\n\tstruct nfs4_ol_stateid *lst;\n\tunsigned int strhashval;\n\tbool hashed;\n\n\tlo = find_lockowner_str(cl, &lock->lk_new_owner);\n\tif (!lo) {\n\t\tstrhashval = ownerstr_hashval(&lock->lk_new_owner);\n\t\tlo = alloc_init_lock_stateowner(strhashval, cl, ost, lock);\n\t\tif (lo == NULL)\n\t\t\treturn nfserr_jukebox;\n\t} else {\n\t\t/* with an existing lockowner, seqids must be the same */\n\t\tstatus = nfserr_bad_seqid;\n\t\tif (!cstate->minorversion &&\n\t\t    lock->lk_new_lock_seqid != lo->lo_owner.so_seqid)\n\t\t\tgoto out;\n\t}\n\nretry:\n\tlst = find_or_create_lock_stateid(lo, fi, inode, ost, new);\n\tif (lst == NULL) {\n\t\tstatus = nfserr_jukebox;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&lst->st_mutex);\n\n\t/* See if it's still hashed to avoid race with FREE_STATEID */\n\tspin_lock(&cl->cl_lock);\n\thashed = !list_empty(&lst->st_perfile);\n\tspin_unlock(&cl->cl_lock);\n\n\tif (!hashed) {\n\t\tmutex_unlock(&lst->st_mutex);\n\t\tnfs4_put_stid(&lst->st_stid);\n\t\tgoto retry;\n\t}\n\tstatus = nfs_ok;\n\t*plst = lst;\nout:\n\tnfs4_put_stateowner(&lo->lo_owner);\n\treturn status;\n}\n\n/*\n *  LOCK operation \n */\n__be32\nnfsd4_lock(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t   struct nfsd4_lock *lock)\n{\n\tstruct nfs4_openowner *open_sop = NULL;\n\tstruct nfs4_lockowner *lock_sop = NULL;\n\tstruct nfs4_ol_stateid *lock_stp = NULL;\n\tstruct nfs4_ol_stateid *open_stp = NULL;\n\tstruct nfs4_file *fp;\n\tstruct file *filp = NULL;\n\tstruct nfsd4_blocked_lock *nbl = NULL;\n\tstruct file_lock *file_lock = NULL;\n\tstruct file_lock *conflock = NULL;\n\t__be32 status = 0;\n\tint lkflg;\n\tint err;\n\tbool new = false;\n\tunsigned char fl_type;\n\tunsigned int fl_flags = FL_POSIX;\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_lock: start=%Ld length=%Ld\\n\",\n\t\t(long long) lock->lk_offset,\n\t\t(long long) lock->lk_length);\n\n\tif (check_lock_length(lock->lk_offset, lock->lk_length))\n\t\t return nfserr_inval;\n\n\tif ((status = fh_verify(rqstp, &cstate->current_fh,\n\t\t\t\tS_IFREG, NFSD_MAY_LOCK))) {\n\t\tdprintk(\"NFSD: nfsd4_lock: permission denied!\\n\");\n\t\treturn status;\n\t}\n\n\tif (lock->lk_is_new) {\n\t\tif (nfsd4_has_session(cstate))\n\t\t\t/* See rfc 5661 18.10.3: given clientid is ignored: */\n\t\t\tmemcpy(&lock->lk_new_clientid,\n\t\t\t\t&cstate->session->se_client->cl_clientid,\n\t\t\t\tsizeof(clientid_t));\n\n\t\tstatus = nfserr_stale_clientid;\n\t\tif (STALE_CLIENTID(&lock->lk_new_clientid, nn))\n\t\t\tgoto out;\n\n\t\t/* validate and update open stateid and open seqid */\n\t\tstatus = nfs4_preprocess_confirmed_seqid_op(cstate,\n\t\t\t\t        lock->lk_new_open_seqid,\n\t\t                        &lock->lk_new_open_stateid,\n\t\t\t\t\t&open_stp, nn);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tmutex_unlock(&open_stp->st_mutex);\n\t\topen_sop = openowner(open_stp->st_stateowner);\n\t\tstatus = nfserr_bad_stateid;\n\t\tif (!same_clid(&open_sop->oo_owner.so_client->cl_clientid,\n\t\t\t\t\t\t&lock->lk_new_clientid))\n\t\t\tgoto out;\n\t\tstatus = lookup_or_create_lock_state(cstate, open_stp, lock,\n\t\t\t\t\t\t\t&lock_stp, &new);\n\t} else {\n\t\tstatus = nfs4_preprocess_seqid_op(cstate,\n\t\t\t\t       lock->lk_old_lock_seqid,\n\t\t\t\t       &lock->lk_old_lock_stateid,\n\t\t\t\t       NFS4_LOCK_STID, &lock_stp, nn);\n\t}\n\tif (status)\n\t\tgoto out;\n\tlock_sop = lockowner(lock_stp->st_stateowner);\n\n\tlkflg = setlkflg(lock->lk_type);\n\tstatus = nfs4_check_openmode(lock_stp, lkflg);\n\tif (status)\n\t\tgoto out;\n\n\tstatus = nfserr_grace;\n\tif (locks_in_grace(net) && !lock->lk_reclaim)\n\t\tgoto out;\n\tstatus = nfserr_no_grace;\n\tif (!locks_in_grace(net) && lock->lk_reclaim)\n\t\tgoto out;\n\n\tfp = lock_stp->st_stid.sc_file;\n\tswitch (lock->lk_type) {\n\t\tcase NFS4_READW_LT:\n\t\t\tif (nfsd4_has_session(cstate))\n\t\t\t\tfl_flags |= FL_SLEEP;\n\t\t\t/* Fallthrough */\n\t\tcase NFS4_READ_LT:\n\t\t\tspin_lock(&fp->fi_lock);\n\t\t\tfilp = find_readable_file_locked(fp);\n\t\t\tif (filp)\n\t\t\t\tget_lock_access(lock_stp, NFS4_SHARE_ACCESS_READ);\n\t\t\tspin_unlock(&fp->fi_lock);\n\t\t\tfl_type = F_RDLCK;\n\t\t\tbreak;\n\t\tcase NFS4_WRITEW_LT:\n\t\t\tif (nfsd4_has_session(cstate))\n\t\t\t\tfl_flags |= FL_SLEEP;\n\t\t\t/* Fallthrough */\n\t\tcase NFS4_WRITE_LT:\n\t\t\tspin_lock(&fp->fi_lock);\n\t\t\tfilp = find_writeable_file_locked(fp);\n\t\t\tif (filp)\n\t\t\t\tget_lock_access(lock_stp, NFS4_SHARE_ACCESS_WRITE);\n\t\t\tspin_unlock(&fp->fi_lock);\n\t\t\tfl_type = F_WRLCK;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tstatus = nfserr_inval;\n\t\tgoto out;\n\t}\n\n\tif (!filp) {\n\t\tstatus = nfserr_openmode;\n\t\tgoto out;\n\t}\n\n\tnbl = find_or_allocate_block(lock_sop, &fp->fi_fhandle, nn);\n\tif (!nbl) {\n\t\tdprintk(\"NFSD: %s: unable to allocate block!\\n\", __func__);\n\t\tstatus = nfserr_jukebox;\n\t\tgoto out;\n\t}\n\n\tfile_lock = &nbl->nbl_lock;\n\tfile_lock->fl_type = fl_type;\n\tfile_lock->fl_owner = (fl_owner_t)lockowner(nfs4_get_stateowner(&lock_sop->lo_owner));\n\tfile_lock->fl_pid = current->tgid;\n\tfile_lock->fl_file = filp;\n\tfile_lock->fl_flags = fl_flags;\n\tfile_lock->fl_lmops = &nfsd_posix_mng_ops;\n\tfile_lock->fl_start = lock->lk_offset;\n\tfile_lock->fl_end = last_byte_offset(lock->lk_offset, lock->lk_length);\n\tnfs4_transform_lock_offset(file_lock);\n\n\tconflock = locks_alloc_lock();\n\tif (!conflock) {\n\t\tdprintk(\"NFSD: %s: unable to allocate lock!\\n\", __func__);\n\t\tstatus = nfserr_jukebox;\n\t\tgoto out;\n\t}\n\n\tif (fl_flags & FL_SLEEP) {\n\t\tnbl->nbl_time = jiffies;\n\t\tspin_lock(&nn->blocked_locks_lock);\n\t\tlist_add_tail(&nbl->nbl_list, &lock_sop->lo_blocked);\n\t\tlist_add_tail(&nbl->nbl_lru, &nn->blocked_locks_lru);\n\t\tspin_unlock(&nn->blocked_locks_lock);\n\t}\n\n\terr = vfs_lock_file(filp, F_SETLK, file_lock, conflock);\n\tswitch (err) {\n\tcase 0: /* success! */\n\t\tnfs4_inc_and_copy_stateid(&lock->lk_resp_stateid, &lock_stp->st_stid);\n\t\tstatus = 0;\n\t\tbreak;\n\tcase FILE_LOCK_DEFERRED:\n\t\tnbl = NULL;\n\t\t/* Fallthrough */\n\tcase -EAGAIN:\t\t/* conflock holds conflicting lock */\n\t\tstatus = nfserr_denied;\n\t\tdprintk(\"NFSD: nfsd4_lock: conflicting lock found!\\n\");\n\t\tnfs4_set_lock_denied(conflock, &lock->lk_denied);\n\t\tbreak;\n\tcase -EDEADLK:\n\t\tstatus = nfserr_deadlock;\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"NFSD: nfsd4_lock: vfs_lock_file() failed! status %d\\n\",err);\n\t\tstatus = nfserrno(err);\n\t\tbreak;\n\t}\nout:\n\tif (nbl) {\n\t\t/* dequeue it if we queued it before */\n\t\tif (fl_flags & FL_SLEEP) {\n\t\t\tspin_lock(&nn->blocked_locks_lock);\n\t\t\tlist_del_init(&nbl->nbl_list);\n\t\t\tlist_del_init(&nbl->nbl_lru);\n\t\t\tspin_unlock(&nn->blocked_locks_lock);\n\t\t}\n\t\tfree_blocked_lock(nbl);\n\t}\n\tif (filp)\n\t\tfput(filp);\n\tif (lock_stp) {\n\t\t/* Bump seqid manually if the 4.0 replay owner is openowner */\n\t\tif (cstate->replay_owner &&\n\t\t    cstate->replay_owner != &lock_sop->lo_owner &&\n\t\t    seqid_mutating_err(ntohl(status)))\n\t\t\tlock_sop->lo_owner.so_seqid++;\n\n\t\tmutex_unlock(&lock_stp->st_mutex);\n\n\t\t/*\n\t\t * If this is a new, never-before-used stateid, and we are\n\t\t * returning an error, then just go ahead and release it.\n\t\t */\n\t\tif (status && new)\n\t\t\trelease_lock_stateid(lock_stp);\n\n\t\tnfs4_put_stid(&lock_stp->st_stid);\n\t}\n\tif (open_stp)\n\t\tnfs4_put_stid(&open_stp->st_stid);\n\tnfsd4_bump_seqid(cstate, status);\n\tif (conflock)\n\t\tlocks_free_lock(conflock);\n\treturn status;\n}\n\n/*\n * The NFSv4 spec allows a client to do a LOCKT without holding an OPEN,\n * so we do a temporary open here just to get an open file to pass to\n * vfs_test_lock.  (Arguably perhaps test_lock should be done with an\n * inode operation.)\n */\nstatic __be32 nfsd_test_lock(struct svc_rqst *rqstp, struct svc_fh *fhp, struct file_lock *lock)\n{\n\tstruct file *file;\n\t__be32 err = nfsd_open(rqstp, fhp, S_IFREG, NFSD_MAY_READ, &file);\n\tif (!err) {\n\t\terr = nfserrno(vfs_test_lock(file, lock));\n\t\tfput(file);\n\t}\n\treturn err;\n}\n\n/*\n * LOCKT operation\n */\n__be32\nnfsd4_lockt(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct nfsd4_lockt *lockt)\n{\n\tstruct file_lock *file_lock = NULL;\n\tstruct nfs4_lockowner *lo = NULL;\n\t__be32 status;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tif (locks_in_grace(SVC_NET(rqstp)))\n\t\treturn nfserr_grace;\n\n\tif (check_lock_length(lockt->lt_offset, lockt->lt_length))\n\t\t return nfserr_inval;\n\n\tif (!nfsd4_has_session(cstate)) {\n\t\tstatus = lookup_clientid(&lockt->lt_clientid, cstate, nn);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tif ((status = fh_verify(rqstp, &cstate->current_fh, S_IFREG, 0)))\n\t\tgoto out;\n\n\tfile_lock = locks_alloc_lock();\n\tif (!file_lock) {\n\t\tdprintk(\"NFSD: %s: unable to allocate lock!\\n\", __func__);\n\t\tstatus = nfserr_jukebox;\n\t\tgoto out;\n\t}\n\n\tswitch (lockt->lt_type) {\n\t\tcase NFS4_READ_LT:\n\t\tcase NFS4_READW_LT:\n\t\t\tfile_lock->fl_type = F_RDLCK;\n\t\tbreak;\n\t\tcase NFS4_WRITE_LT:\n\t\tcase NFS4_WRITEW_LT:\n\t\t\tfile_lock->fl_type = F_WRLCK;\n\t\tbreak;\n\t\tdefault:\n\t\t\tdprintk(\"NFSD: nfs4_lockt: bad lock type!\\n\");\n\t\t\tstatus = nfserr_inval;\n\t\tgoto out;\n\t}\n\n\tlo = find_lockowner_str(cstate->clp, &lockt->lt_owner);\n\tif (lo)\n\t\tfile_lock->fl_owner = (fl_owner_t)lo;\n\tfile_lock->fl_pid = current->tgid;\n\tfile_lock->fl_flags = FL_POSIX;\n\n\tfile_lock->fl_start = lockt->lt_offset;\n\tfile_lock->fl_end = last_byte_offset(lockt->lt_offset, lockt->lt_length);\n\n\tnfs4_transform_lock_offset(file_lock);\n\n\tstatus = nfsd_test_lock(rqstp, &cstate->current_fh, file_lock);\n\tif (status)\n\t\tgoto out;\n\n\tif (file_lock->fl_type != F_UNLCK) {\n\t\tstatus = nfserr_denied;\n\t\tnfs4_set_lock_denied(file_lock, &lockt->lt_denied);\n\t}\nout:\n\tif (lo)\n\t\tnfs4_put_stateowner(&lo->lo_owner);\n\tif (file_lock)\n\t\tlocks_free_lock(file_lock);\n\treturn status;\n}\n\n__be32\nnfsd4_locku(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct nfsd4_locku *locku)\n{\n\tstruct nfs4_ol_stateid *stp;\n\tstruct file *filp = NULL;\n\tstruct file_lock *file_lock = NULL;\n\t__be32 status;\n\tint err;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_locku: start=%Ld length=%Ld\\n\",\n\t\t(long long) locku->lu_offset,\n\t\t(long long) locku->lu_length);\n\n\tif (check_lock_length(locku->lu_offset, locku->lu_length))\n\t\t return nfserr_inval;\n\n\tstatus = nfs4_preprocess_seqid_op(cstate, locku->lu_seqid,\n\t\t\t\t\t&locku->lu_stateid, NFS4_LOCK_STID,\n\t\t\t\t\t&stp, nn);\n\tif (status)\n\t\tgoto out;\n\tfilp = find_any_file(stp->st_stid.sc_file);\n\tif (!filp) {\n\t\tstatus = nfserr_lock_range;\n\t\tgoto put_stateid;\n\t}\n\tfile_lock = locks_alloc_lock();\n\tif (!file_lock) {\n\t\tdprintk(\"NFSD: %s: unable to allocate lock!\\n\", __func__);\n\t\tstatus = nfserr_jukebox;\n\t\tgoto fput;\n\t}\n\n\tfile_lock->fl_type = F_UNLCK;\n\tfile_lock->fl_owner = (fl_owner_t)lockowner(nfs4_get_stateowner(stp->st_stateowner));\n\tfile_lock->fl_pid = current->tgid;\n\tfile_lock->fl_file = filp;\n\tfile_lock->fl_flags = FL_POSIX;\n\tfile_lock->fl_lmops = &nfsd_posix_mng_ops;\n\tfile_lock->fl_start = locku->lu_offset;\n\n\tfile_lock->fl_end = last_byte_offset(locku->lu_offset,\n\t\t\t\t\t\tlocku->lu_length);\n\tnfs4_transform_lock_offset(file_lock);\n\n\terr = vfs_lock_file(filp, F_SETLK, file_lock, NULL);\n\tif (err) {\n\t\tdprintk(\"NFSD: nfs4_locku: vfs_lock_file failed!\\n\");\n\t\tgoto out_nfserr;\n\t}\n\tnfs4_inc_and_copy_stateid(&locku->lu_stateid, &stp->st_stid);\nfput:\n\tfput(filp);\nput_stateid:\n\tmutex_unlock(&stp->st_mutex);\n\tnfs4_put_stid(&stp->st_stid);\nout:\n\tnfsd4_bump_seqid(cstate, status);\n\tif (file_lock)\n\t\tlocks_free_lock(file_lock);\n\treturn status;\n\nout_nfserr:\n\tstatus = nfserrno(err);\n\tgoto fput;\n}\n\n/*\n * returns\n * \ttrue:  locks held by lockowner\n * \tfalse: no locks held by lockowner\n */\nstatic bool\ncheck_for_locks(struct nfs4_file *fp, struct nfs4_lockowner *lowner)\n{\n\tstruct file_lock *fl;\n\tint status = false;\n\tstruct file *filp = find_any_file(fp);\n\tstruct inode *inode;\n\tstruct file_lock_context *flctx;\n\n\tif (!filp) {\n\t\t/* Any valid lock stateid should have some sort of access */\n\t\tWARN_ON_ONCE(1);\n\t\treturn status;\n\t}\n\n\tinode = file_inode(filp);\n\tflctx = inode->i_flctx;\n\n\tif (flctx && !list_empty_careful(&flctx->flc_posix)) {\n\t\tspin_lock(&flctx->flc_lock);\n\t\tlist_for_each_entry(fl, &flctx->flc_posix, fl_list) {\n\t\t\tif (fl->fl_owner == (fl_owner_t)lowner) {\n\t\t\t\tstatus = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&flctx->flc_lock);\n\t}\n\tfput(filp);\n\treturn status;\n}\n\n__be32\nnfsd4_release_lockowner(struct svc_rqst *rqstp,\n\t\t\tstruct nfsd4_compound_state *cstate,\n\t\t\tstruct nfsd4_release_lockowner *rlockowner)\n{\n\tclientid_t *clid = &rlockowner->rl_clientid;\n\tstruct nfs4_stateowner *sop;\n\tstruct nfs4_lockowner *lo = NULL;\n\tstruct nfs4_ol_stateid *stp;\n\tstruct xdr_netobj *owner = &rlockowner->rl_owner;\n\tunsigned int hashval = ownerstr_hashval(owner);\n\t__be32 status;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\tstruct nfs4_client *clp;\n\tLIST_HEAD (reaplist);\n\n\tdprintk(\"nfsd4_release_lockowner clientid: (%08x/%08x):\\n\",\n\t\tclid->cl_boot, clid->cl_id);\n\n\tstatus = lookup_clientid(clid, cstate, nn);\n\tif (status)\n\t\treturn status;\n\n\tclp = cstate->clp;\n\t/* Find the matching lock stateowner */\n\tspin_lock(&clp->cl_lock);\n\tlist_for_each_entry(sop, &clp->cl_ownerstr_hashtbl[hashval],\n\t\t\t    so_strhash) {\n\n\t\tif (sop->so_is_open_owner || !same_owner_str(sop, owner))\n\t\t\tcontinue;\n\n\t\t/* see if there are still any locks associated with it */\n\t\tlo = lockowner(sop);\n\t\tlist_for_each_entry(stp, &sop->so_stateids, st_perstateowner) {\n\t\t\tif (check_for_locks(stp->st_stid.sc_file, lo)) {\n\t\t\t\tstatus = nfserr_locks_held;\n\t\t\t\tspin_unlock(&clp->cl_lock);\n\t\t\t\treturn status;\n\t\t\t}\n\t\t}\n\n\t\tnfs4_get_stateowner(sop);\n\t\tbreak;\n\t}\n\tif (!lo) {\n\t\tspin_unlock(&clp->cl_lock);\n\t\treturn status;\n\t}\n\n\tunhash_lockowner_locked(lo);\n\twhile (!list_empty(&lo->lo_owner.so_stateids)) {\n\t\tstp = list_first_entry(&lo->lo_owner.so_stateids,\n\t\t\t\t       struct nfs4_ol_stateid,\n\t\t\t\t       st_perstateowner);\n\t\tWARN_ON(!unhash_lock_stateid(stp));\n\t\tput_ol_stateid_locked(stp, &reaplist);\n\t}\n\tspin_unlock(&clp->cl_lock);\n\tfree_ol_stateid_reaplist(&reaplist);\n\tnfs4_put_stateowner(&lo->lo_owner);\n\n\treturn status;\n}\n\nstatic inline struct nfs4_client_reclaim *\nalloc_reclaim(void)\n{\n\treturn kmalloc(sizeof(struct nfs4_client_reclaim), GFP_KERNEL);\n}\n\nbool\nnfs4_has_reclaimed_state(const char *name, struct nfsd_net *nn)\n{\n\tstruct nfs4_client_reclaim *crp;\n\n\tcrp = nfsd4_find_reclaim_client(name, nn);\n\treturn (crp && crp->cr_clp);\n}\n\n/*\n * failure => all reset bets are off, nfserr_no_grace...\n */\nstruct nfs4_client_reclaim *\nnfs4_client_to_reclaim(const char *name, struct nfsd_net *nn)\n{\n\tunsigned int strhashval;\n\tstruct nfs4_client_reclaim *crp;\n\n\tdprintk(\"NFSD nfs4_client_to_reclaim NAME: %.*s\\n\", HEXDIR_LEN, name);\n\tcrp = alloc_reclaim();\n\tif (crp) {\n\t\tstrhashval = clientstr_hashval(name);\n\t\tINIT_LIST_HEAD(&crp->cr_strhash);\n\t\tlist_add(&crp->cr_strhash, &nn->reclaim_str_hashtbl[strhashval]);\n\t\tmemcpy(crp->cr_recdir, name, HEXDIR_LEN);\n\t\tcrp->cr_clp = NULL;\n\t\tnn->reclaim_str_hashtbl_size++;\n\t}\n\treturn crp;\n}\n\nvoid\nnfs4_remove_reclaim_record(struct nfs4_client_reclaim *crp, struct nfsd_net *nn)\n{\n\tlist_del(&crp->cr_strhash);\n\tkfree(crp);\n\tnn->reclaim_str_hashtbl_size--;\n}\n\nvoid\nnfs4_release_reclaim(struct nfsd_net *nn)\n{\n\tstruct nfs4_client_reclaim *crp = NULL;\n\tint i;\n\n\tfor (i = 0; i < CLIENT_HASH_SIZE; i++) {\n\t\twhile (!list_empty(&nn->reclaim_str_hashtbl[i])) {\n\t\t\tcrp = list_entry(nn->reclaim_str_hashtbl[i].next,\n\t\t\t                struct nfs4_client_reclaim, cr_strhash);\n\t\t\tnfs4_remove_reclaim_record(crp, nn);\n\t\t}\n\t}\n\tWARN_ON_ONCE(nn->reclaim_str_hashtbl_size);\n}\n\n/*\n * called from OPEN, CLAIM_PREVIOUS with a new clientid. */\nstruct nfs4_client_reclaim *\nnfsd4_find_reclaim_client(const char *recdir, struct nfsd_net *nn)\n{\n\tunsigned int strhashval;\n\tstruct nfs4_client_reclaim *crp = NULL;\n\n\tdprintk(\"NFSD: nfs4_find_reclaim_client for recdir %s\\n\", recdir);\n\n\tstrhashval = clientstr_hashval(recdir);\n\tlist_for_each_entry(crp, &nn->reclaim_str_hashtbl[strhashval], cr_strhash) {\n\t\tif (same_name(crp->cr_recdir, recdir)) {\n\t\t\treturn crp;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n/*\n* Called from OPEN. Look for clientid in reclaim list.\n*/\n__be32\nnfs4_check_open_reclaim(clientid_t *clid,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd_net *nn)\n{\n\t__be32 status;\n\n\t/* find clientid in conf_id_hashtbl */\n\tstatus = lookup_clientid(clid, cstate, nn);\n\tif (status)\n\t\treturn nfserr_reclaim_bad;\n\n\tif (test_bit(NFSD4_CLIENT_RECLAIM_COMPLETE, &cstate->clp->cl_flags))\n\t\treturn nfserr_no_grace;\n\n\tif (nfsd4_client_record_check(cstate->clp))\n\t\treturn nfserr_reclaim_bad;\n\n\treturn nfs_ok;\n}\n\n#ifdef CONFIG_NFSD_FAULT_INJECTION\nstatic inline void\nput_client(struct nfs4_client *clp)\n{\n\tatomic_dec(&clp->cl_refcount);\n}\n\nstatic struct nfs4_client *\nnfsd_find_client(struct sockaddr_storage *addr, size_t addr_size)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t  nfsd_net_id);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn NULL;\n\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru) {\n\t\tif (memcmp(&clp->cl_addr, addr, addr_size) == 0)\n\t\t\treturn clp;\n\t}\n\treturn NULL;\n}\n\nu64\nnfsd_inject_print_clients(void)\n{\n\tstruct nfs4_client *clp;\n\tu64 count = 0;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t  nfsd_net_id);\n\tchar buf[INET6_ADDRSTRLEN];\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn 0;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru) {\n\t\trpc_ntop((struct sockaddr *)&clp->cl_addr, buf, sizeof(buf));\n\t\tpr_info(\"NFS Client: %s\\n\", buf);\n\t\t++count;\n\t}\n\tspin_unlock(&nn->client_lock);\n\n\treturn count;\n}\n\nu64\nnfsd_inject_forget_client(struct sockaddr_storage *addr, size_t addr_size)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t  nfsd_net_id);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tclp = nfsd_find_client(addr, addr_size);\n\tif (clp) {\n\t\tif (mark_client_expired_locked(clp) == nfs_ok)\n\t\t\t++count;\n\t\telse\n\t\t\tclp = NULL;\n\t}\n\tspin_unlock(&nn->client_lock);\n\n\tif (clp)\n\t\texpire_client(clp);\n\n\treturn count;\n}\n\nu64\nnfsd_inject_forget_clients(u64 max)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp, *next;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry_safe(clp, next, &nn->client_lru, cl_lru) {\n\t\tif (mark_client_expired_locked(clp) == nfs_ok) {\n\t\t\tlist_add(&clp->cl_lru, &reaplist);\n\t\t\tif (max != 0 && ++count >= max)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&nn->client_lock);\n\n\tlist_for_each_entry_safe(clp, next, &reaplist, cl_lru)\n\t\texpire_client(clp);\n\n\treturn count;\n}\n\nstatic void nfsd_print_count(struct nfs4_client *clp, unsigned int count,\n\t\t\t     const char *type)\n{\n\tchar buf[INET6_ADDRSTRLEN];\n\trpc_ntop((struct sockaddr *)&clp->cl_addr, buf, sizeof(buf));\n\tprintk(KERN_INFO \"NFS Client: %s has %u %s\\n\", buf, count, type);\n}\n\nstatic void\nnfsd_inject_add_lock_to_list(struct nfs4_ol_stateid *lst,\n\t\t\t     struct list_head *collect)\n{\n\tstruct nfs4_client *clp = lst->st_stid.sc_client;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t  nfsd_net_id);\n\n\tif (!collect)\n\t\treturn;\n\n\tlockdep_assert_held(&nn->client_lock);\n\tatomic_inc(&clp->cl_refcount);\n\tlist_add(&lst->st_locks, collect);\n}\n\nstatic u64 nfsd_foreach_client_lock(struct nfs4_client *clp, u64 max,\n\t\t\t\t    struct list_head *collect,\n\t\t\t\t    bool (*func)(struct nfs4_ol_stateid *))\n{\n\tstruct nfs4_openowner *oop;\n\tstruct nfs4_ol_stateid *stp, *st_next;\n\tstruct nfs4_ol_stateid *lst, *lst_next;\n\tu64 count = 0;\n\n\tspin_lock(&clp->cl_lock);\n\tlist_for_each_entry(oop, &clp->cl_openowners, oo_perclient) {\n\t\tlist_for_each_entry_safe(stp, st_next,\n\t\t\t\t&oop->oo_owner.so_stateids, st_perstateowner) {\n\t\t\tlist_for_each_entry_safe(lst, lst_next,\n\t\t\t\t\t&stp->st_locks, st_locks) {\n\t\t\t\tif (func) {\n\t\t\t\t\tif (func(lst))\n\t\t\t\t\t\tnfsd_inject_add_lock_to_list(lst,\n\t\t\t\t\t\t\t\t\tcollect);\n\t\t\t\t}\n\t\t\t\t++count;\n\t\t\t\t/*\n\t\t\t\t * Despite the fact that these functions deal\n\t\t\t\t * with 64-bit integers for \"count\", we must\n\t\t\t\t * ensure that it doesn't blow up the\n\t\t\t\t * clp->cl_refcount. Throw a warning if we\n\t\t\t\t * start to approach INT_MAX here.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(count == (INT_MAX / 2));\n\t\t\t\tif (count == max)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\nout:\n\tspin_unlock(&clp->cl_lock);\n\n\treturn count;\n}\n\nstatic u64\nnfsd_collect_client_locks(struct nfs4_client *clp, struct list_head *collect,\n\t\t\t  u64 max)\n{\n\treturn nfsd_foreach_client_lock(clp, max, collect, unhash_lock_stateid);\n}\n\nstatic u64\nnfsd_print_client_locks(struct nfs4_client *clp)\n{\n\tu64 count = nfsd_foreach_client_lock(clp, 0, NULL, NULL);\n\tnfsd_print_count(clp, count, \"locked files\");\n\treturn count;\n}\n\nu64\nnfsd_inject_print_locks(void)\n{\n\tstruct nfs4_client *clp;\n\tu64 count = 0;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn 0;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru)\n\t\tcount += nfsd_print_client_locks(clp);\n\tspin_unlock(&nn->client_lock);\n\n\treturn count;\n}\n\nstatic void\nnfsd_reap_locks(struct list_head *reaplist)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfs4_ol_stateid *stp, *next;\n\n\tlist_for_each_entry_safe(stp, next, reaplist, st_locks) {\n\t\tlist_del_init(&stp->st_locks);\n\t\tclp = stp->st_stid.sc_client;\n\t\tnfs4_put_stid(&stp->st_stid);\n\t\tput_client(clp);\n\t}\n}\n\nu64\nnfsd_inject_forget_client_locks(struct sockaddr_storage *addr, size_t addr_size)\n{\n\tunsigned int count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tclp = nfsd_find_client(addr, addr_size);\n\tif (clp)\n\t\tcount = nfsd_collect_client_locks(clp, &reaplist, 0);\n\tspin_unlock(&nn->client_lock);\n\tnfsd_reap_locks(&reaplist);\n\treturn count;\n}\n\nu64\nnfsd_inject_forget_locks(u64 max)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru) {\n\t\tcount += nfsd_collect_client_locks(clp, &reaplist, max - count);\n\t\tif (max != 0 && count >= max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&nn->client_lock);\n\tnfsd_reap_locks(&reaplist);\n\treturn count;\n}\n\nstatic u64\nnfsd_foreach_client_openowner(struct nfs4_client *clp, u64 max,\n\t\t\t      struct list_head *collect,\n\t\t\t      void (*func)(struct nfs4_openowner *))\n{\n\tstruct nfs4_openowner *oop, *next;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tu64 count = 0;\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tspin_lock(&clp->cl_lock);\n\tlist_for_each_entry_safe(oop, next, &clp->cl_openowners, oo_perclient) {\n\t\tif (func) {\n\t\t\tfunc(oop);\n\t\t\tif (collect) {\n\t\t\t\tatomic_inc(&clp->cl_refcount);\n\t\t\t\tlist_add(&oop->oo_perclient, collect);\n\t\t\t}\n\t\t}\n\t\t++count;\n\t\t/*\n\t\t * Despite the fact that these functions deal with\n\t\t * 64-bit integers for \"count\", we must ensure that\n\t\t * it doesn't blow up the clp->cl_refcount. Throw a\n\t\t * warning if we start to approach INT_MAX here.\n\t\t */\n\t\tWARN_ON_ONCE(count == (INT_MAX / 2));\n\t\tif (count == max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&clp->cl_lock);\n\n\treturn count;\n}\n\nstatic u64\nnfsd_print_client_openowners(struct nfs4_client *clp)\n{\n\tu64 count = nfsd_foreach_client_openowner(clp, 0, NULL, NULL);\n\n\tnfsd_print_count(clp, count, \"openowners\");\n\treturn count;\n}\n\nstatic u64\nnfsd_collect_client_openowners(struct nfs4_client *clp,\n\t\t\t       struct list_head *collect, u64 max)\n{\n\treturn nfsd_foreach_client_openowner(clp, max, collect,\n\t\t\t\t\t\tunhash_openowner_locked);\n}\n\nu64\nnfsd_inject_print_openowners(void)\n{\n\tstruct nfs4_client *clp;\n\tu64 count = 0;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn 0;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru)\n\t\tcount += nfsd_print_client_openowners(clp);\n\tspin_unlock(&nn->client_lock);\n\n\treturn count;\n}\n\nstatic void\nnfsd_reap_openowners(struct list_head *reaplist)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfs4_openowner *oop, *next;\n\n\tlist_for_each_entry_safe(oop, next, reaplist, oo_perclient) {\n\t\tlist_del_init(&oop->oo_perclient);\n\t\tclp = oop->oo_owner.so_client;\n\t\trelease_openowner(oop);\n\t\tput_client(clp);\n\t}\n}\n\nu64\nnfsd_inject_forget_client_openowners(struct sockaddr_storage *addr,\n\t\t\t\t     size_t addr_size)\n{\n\tunsigned int count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tclp = nfsd_find_client(addr, addr_size);\n\tif (clp)\n\t\tcount = nfsd_collect_client_openowners(clp, &reaplist, 0);\n\tspin_unlock(&nn->client_lock);\n\tnfsd_reap_openowners(&reaplist);\n\treturn count;\n}\n\nu64\nnfsd_inject_forget_openowners(u64 max)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru) {\n\t\tcount += nfsd_collect_client_openowners(clp, &reaplist,\n\t\t\t\t\t\t\tmax - count);\n\t\tif (max != 0 && count >= max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&nn->client_lock);\n\tnfsd_reap_openowners(&reaplist);\n\treturn count;\n}\n\nstatic u64 nfsd_find_all_delegations(struct nfs4_client *clp, u64 max,\n\t\t\t\t     struct list_head *victims)\n{\n\tstruct nfs4_delegation *dp, *next;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tu64 count = 0;\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tspin_lock(&state_lock);\n\tlist_for_each_entry_safe(dp, next, &clp->cl_delegations, dl_perclnt) {\n\t\tif (victims) {\n\t\t\t/*\n\t\t\t * It's not safe to mess with delegations that have a\n\t\t\t * non-zero dl_time. They might have already been broken\n\t\t\t * and could be processed by the laundromat outside of\n\t\t\t * the state_lock. Just leave them be.\n\t\t\t */\n\t\t\tif (dp->dl_time != 0)\n\t\t\t\tcontinue;\n\n\t\t\tatomic_inc(&clp->cl_refcount);\n\t\t\tWARN_ON(!unhash_delegation_locked(dp));\n\t\t\tlist_add(&dp->dl_recall_lru, victims);\n\t\t}\n\t\t++count;\n\t\t/*\n\t\t * Despite the fact that these functions deal with\n\t\t * 64-bit integers for \"count\", we must ensure that\n\t\t * it doesn't blow up the clp->cl_refcount. Throw a\n\t\t * warning if we start to approach INT_MAX here.\n\t\t */\n\t\tWARN_ON_ONCE(count == (INT_MAX / 2));\n\t\tif (count == max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&state_lock);\n\treturn count;\n}\n\nstatic u64\nnfsd_print_client_delegations(struct nfs4_client *clp)\n{\n\tu64 count = nfsd_find_all_delegations(clp, 0, NULL);\n\n\tnfsd_print_count(clp, count, \"delegations\");\n\treturn count;\n}\n\nu64\nnfsd_inject_print_delegations(void)\n{\n\tstruct nfs4_client *clp;\n\tu64 count = 0;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn 0;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru)\n\t\tcount += nfsd_print_client_delegations(clp);\n\tspin_unlock(&nn->client_lock);\n\n\treturn count;\n}\n\nstatic void\nnfsd_forget_delegations(struct list_head *reaplist)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfs4_delegation *dp, *next;\n\n\tlist_for_each_entry_safe(dp, next, reaplist, dl_recall_lru) {\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tclp = dp->dl_stid.sc_client;\n\t\trevoke_delegation(dp);\n\t\tput_client(clp);\n\t}\n}\n\nu64\nnfsd_inject_forget_client_delegations(struct sockaddr_storage *addr,\n\t\t\t\t      size_t addr_size)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tclp = nfsd_find_client(addr, addr_size);\n\tif (clp)\n\t\tcount = nfsd_find_all_delegations(clp, 0, &reaplist);\n\tspin_unlock(&nn->client_lock);\n\n\tnfsd_forget_delegations(&reaplist);\n\treturn count;\n}\n\nu64\nnfsd_inject_forget_delegations(u64 max)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru) {\n\t\tcount += nfsd_find_all_delegations(clp, max - count, &reaplist);\n\t\tif (max != 0 && count >= max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&nn->client_lock);\n\tnfsd_forget_delegations(&reaplist);\n\treturn count;\n}\n\nstatic void\nnfsd_recall_delegations(struct list_head *reaplist)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfs4_delegation *dp, *next;\n\n\tlist_for_each_entry_safe(dp, next, reaplist, dl_recall_lru) {\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tclp = dp->dl_stid.sc_client;\n\t\t/*\n\t\t * We skipped all entries that had a zero dl_time before,\n\t\t * so we can now reset the dl_time back to 0. If a delegation\n\t\t * break comes in now, then it won't make any difference since\n\t\t * we're recalling it either way.\n\t\t */\n\t\tspin_lock(&state_lock);\n\t\tdp->dl_time = 0;\n\t\tspin_unlock(&state_lock);\n\t\tnfsd_break_one_deleg(dp);\n\t\tput_client(clp);\n\t}\n}\n\nu64\nnfsd_inject_recall_client_delegations(struct sockaddr_storage *addr,\n\t\t\t\t      size_t addr_size)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tclp = nfsd_find_client(addr, addr_size);\n\tif (clp)\n\t\tcount = nfsd_find_all_delegations(clp, 0, &reaplist);\n\tspin_unlock(&nn->client_lock);\n\n\tnfsd_recall_delegations(&reaplist);\n\treturn count;\n}\n\nu64\nnfsd_inject_recall_delegations(u64 max)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp, *next;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry_safe(clp, next, &nn->client_lru, cl_lru) {\n\t\tcount += nfsd_find_all_delegations(clp, max - count, &reaplist);\n\t\tif (max != 0 && ++count >= max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&nn->client_lock);\n\tnfsd_recall_delegations(&reaplist);\n\treturn count;\n}\n#endif /* CONFIG_NFSD_FAULT_INJECTION */\n\n/*\n * Since the lifetime of a delegation isn't limited to that of an open, a\n * client may quite reasonably hang on to a delegation as long as it has\n * the inode cached.  This becomes an obvious problem the first time a\n * client's inode cache approaches the size of the server's total memory.\n *\n * For now we avoid this problem by imposing a hard limit on the number\n * of delegations, which varies according to the server's memory size.\n */\nstatic void\nset_max_delegations(void)\n{\n\t/*\n\t * Allow at most 4 delegations per megabyte of RAM.  Quick\n\t * estimates suggest that in the worst case (where every delegation\n\t * is for a different inode), a delegation could take about 1.5K,\n\t * giving a worst case usage of about 6% of memory.\n\t */\n\tmax_delegations = nr_free_buffer_pages() >> (20 - 2 - PAGE_SHIFT);\n}\n\nstatic int nfs4_state_create_net(struct net *net)\n{\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\tint i;\n\n\tnn->conf_id_hashtbl = kmalloc(sizeof(struct list_head) *\n\t\t\tCLIENT_HASH_SIZE, GFP_KERNEL);\n\tif (!nn->conf_id_hashtbl)\n\t\tgoto err;\n\tnn->unconf_id_hashtbl = kmalloc(sizeof(struct list_head) *\n\t\t\tCLIENT_HASH_SIZE, GFP_KERNEL);\n\tif (!nn->unconf_id_hashtbl)\n\t\tgoto err_unconf_id;\n\tnn->sessionid_hashtbl = kmalloc(sizeof(struct list_head) *\n\t\t\tSESSION_HASH_SIZE, GFP_KERNEL);\n\tif (!nn->sessionid_hashtbl)\n\t\tgoto err_sessionid;\n\n\tfor (i = 0; i < CLIENT_HASH_SIZE; i++) {\n\t\tINIT_LIST_HEAD(&nn->conf_id_hashtbl[i]);\n\t\tINIT_LIST_HEAD(&nn->unconf_id_hashtbl[i]);\n\t}\n\tfor (i = 0; i < SESSION_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&nn->sessionid_hashtbl[i]);\n\tnn->conf_name_tree = RB_ROOT;\n\tnn->unconf_name_tree = RB_ROOT;\n\tINIT_LIST_HEAD(&nn->client_lru);\n\tINIT_LIST_HEAD(&nn->close_lru);\n\tINIT_LIST_HEAD(&nn->del_recall_lru);\n\tspin_lock_init(&nn->client_lock);\n\n\tspin_lock_init(&nn->blocked_locks_lock);\n\tINIT_LIST_HEAD(&nn->blocked_locks_lru);\n\n\tINIT_DELAYED_WORK(&nn->laundromat_work, laundromat_main);\n\tget_net(net);\n\n\treturn 0;\n\nerr_sessionid:\n\tkfree(nn->unconf_id_hashtbl);\nerr_unconf_id:\n\tkfree(nn->conf_id_hashtbl);\nerr:\n\treturn -ENOMEM;\n}\n\nstatic void\nnfs4_state_destroy_net(struct net *net)\n{\n\tint i;\n\tstruct nfs4_client *clp = NULL;\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tfor (i = 0; i < CLIENT_HASH_SIZE; i++) {\n\t\twhile (!list_empty(&nn->conf_id_hashtbl[i])) {\n\t\t\tclp = list_entry(nn->conf_id_hashtbl[i].next, struct nfs4_client, cl_idhash);\n\t\t\tdestroy_client(clp);\n\t\t}\n\t}\n\n\tfor (i = 0; i < CLIENT_HASH_SIZE; i++) {\n\t\twhile (!list_empty(&nn->unconf_id_hashtbl[i])) {\n\t\t\tclp = list_entry(nn->unconf_id_hashtbl[i].next, struct nfs4_client, cl_idhash);\n\t\t\tdestroy_client(clp);\n\t\t}\n\t}\n\n\tkfree(nn->sessionid_hashtbl);\n\tkfree(nn->unconf_id_hashtbl);\n\tkfree(nn->conf_id_hashtbl);\n\tput_net(net);\n}\n\nint\nnfs4_state_start_net(struct net *net)\n{\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\tint ret;\n\n\tret = nfs4_state_create_net(net);\n\tif (ret)\n\t\treturn ret;\n\tnn->boot_time = get_seconds();\n\tnn->grace_ended = false;\n\tnn->nfsd4_manager.block_opens = true;\n\tlocks_start_grace(net, &nn->nfsd4_manager);\n\tnfsd4_client_tracking_init(net);\n\tprintk(KERN_INFO \"NFSD: starting %ld-second grace period (net %p)\\n\",\n\t       nn->nfsd4_grace, net);\n\tqueue_delayed_work(laundry_wq, &nn->laundromat_work, nn->nfsd4_grace * HZ);\n\treturn 0;\n}\n\n/* initialization to perform when the nfsd service is started: */\n\nint\nnfs4_state_start(void)\n{\n\tint ret;\n\n\tret = set_callback_cred();\n\tif (ret)\n\t\treturn ret;\n\n\tlaundry_wq = alloc_workqueue(\"%s\", WQ_UNBOUND, 0, \"nfsd4\");\n\tif (laundry_wq == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto out_cleanup_cred;\n\t}\n\tret = nfsd4_create_callback_queue();\n\tif (ret)\n\t\tgoto out_free_laundry;\n\n\tset_max_delegations();\n\treturn 0;\n\nout_free_laundry:\n\tdestroy_workqueue(laundry_wq);\nout_cleanup_cred:\n\tcleanup_callback_cred();\n\treturn ret;\n}\n\nvoid\nnfs4_state_shutdown_net(struct net *net)\n{\n\tstruct nfs4_delegation *dp = NULL;\n\tstruct list_head *pos, *next, reaplist;\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\tstruct nfsd4_blocked_lock *nbl;\n\n\tcancel_delayed_work_sync(&nn->laundromat_work);\n\tlocks_end_grace(&nn->nfsd4_manager);\n\n\tINIT_LIST_HEAD(&reaplist);\n\tspin_lock(&state_lock);\n\tlist_for_each_safe(pos, next, &nn->del_recall_lru) {\n\t\tdp = list_entry (pos, struct nfs4_delegation, dl_recall_lru);\n\t\tWARN_ON(!unhash_delegation_locked(dp));\n\t\tlist_add(&dp->dl_recall_lru, &reaplist);\n\t}\n\tspin_unlock(&state_lock);\n\tlist_for_each_safe(pos, next, &reaplist) {\n\t\tdp = list_entry (pos, struct nfs4_delegation, dl_recall_lru);\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tput_clnt_odstate(dp->dl_clnt_odstate);\n\t\tnfs4_put_deleg_lease(dp->dl_stid.sc_file);\n\t\tnfs4_put_stid(&dp->dl_stid);\n\t}\n\n\tBUG_ON(!list_empty(&reaplist));\n\tspin_lock(&nn->blocked_locks_lock);\n\twhile (!list_empty(&nn->blocked_locks_lru)) {\n\t\tnbl = list_first_entry(&nn->blocked_locks_lru,\n\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_lru);\n\t\tlist_move(&nbl->nbl_lru, &reaplist);\n\t\tlist_del_init(&nbl->nbl_list);\n\t}\n\tspin_unlock(&nn->blocked_locks_lock);\n\n\twhile (!list_empty(&reaplist)) {\n\t\tnbl = list_first_entry(&nn->blocked_locks_lru,\n\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_lru);\n\t\tlist_del_init(&nbl->nbl_lru);\n\t\tposix_unblock_lock(&nbl->nbl_lock);\n\t\tfree_blocked_lock(nbl);\n\t}\n\n\tnfsd4_client_tracking_exit(net);\n\tnfs4_state_destroy_net(net);\n}\n\nvoid\nnfs4_state_shutdown(void)\n{\n\tdestroy_workqueue(laundry_wq);\n\tnfsd4_destroy_callback_queue();\n\tcleanup_callback_cred();\n}\n\nstatic void\nget_stateid(struct nfsd4_compound_state *cstate, stateid_t *stateid)\n{\n\tif (HAS_STATE_ID(cstate, CURRENT_STATE_ID_FLAG) && CURRENT_STATEID(stateid))\n\t\tmemcpy(stateid, &cstate->current_stateid, sizeof(stateid_t));\n}\n\nstatic void\nput_stateid(struct nfsd4_compound_state *cstate, stateid_t *stateid)\n{\n\tif (cstate->minorversion) {\n\t\tmemcpy(&cstate->current_stateid, stateid, sizeof(stateid_t));\n\t\tSET_STATE_ID(cstate, CURRENT_STATE_ID_FLAG);\n\t}\n}\n\nvoid\nclear_current_stateid(struct nfsd4_compound_state *cstate)\n{\n\tCLEAR_STATE_ID(cstate, CURRENT_STATE_ID_FLAG);\n}\n\n/*\n * functions to set current state id\n */\nvoid\nnfsd4_set_opendowngradestateid(struct nfsd4_compound_state *cstate, struct nfsd4_open_downgrade *odp)\n{\n\tput_stateid(cstate, &odp->od_stateid);\n}\n\nvoid\nnfsd4_set_openstateid(struct nfsd4_compound_state *cstate, struct nfsd4_open *open)\n{\n\tput_stateid(cstate, &open->op_stateid);\n}\n\nvoid\nnfsd4_set_closestateid(struct nfsd4_compound_state *cstate, struct nfsd4_close *close)\n{\n\tput_stateid(cstate, &close->cl_stateid);\n}\n\nvoid\nnfsd4_set_lockstateid(struct nfsd4_compound_state *cstate, struct nfsd4_lock *lock)\n{\n\tput_stateid(cstate, &lock->lk_resp_stateid);\n}\n\n/*\n * functions to consume current state id\n */\n\nvoid\nnfsd4_get_opendowngradestateid(struct nfsd4_compound_state *cstate, struct nfsd4_open_downgrade *odp)\n{\n\tget_stateid(cstate, &odp->od_stateid);\n}\n\nvoid\nnfsd4_get_delegreturnstateid(struct nfsd4_compound_state *cstate, struct nfsd4_delegreturn *drp)\n{\n\tget_stateid(cstate, &drp->dr_stateid);\n}\n\nvoid\nnfsd4_get_freestateid(struct nfsd4_compound_state *cstate, struct nfsd4_free_stateid *fsp)\n{\n\tget_stateid(cstate, &fsp->fr_stateid);\n}\n\nvoid\nnfsd4_get_setattrstateid(struct nfsd4_compound_state *cstate, struct nfsd4_setattr *setattr)\n{\n\tget_stateid(cstate, &setattr->sa_stateid);\n}\n\nvoid\nnfsd4_get_closestateid(struct nfsd4_compound_state *cstate, struct nfsd4_close *close)\n{\n\tget_stateid(cstate, &close->cl_stateid);\n}\n\nvoid\nnfsd4_get_lockustateid(struct nfsd4_compound_state *cstate, struct nfsd4_locku *locku)\n{\n\tget_stateid(cstate, &locku->lu_stateid);\n}\n\nvoid\nnfsd4_get_readstateid(struct nfsd4_compound_state *cstate, struct nfsd4_read *read)\n{\n\tget_stateid(cstate, &read->rd_stateid);\n}\n\nvoid\nnfsd4_get_writestateid(struct nfsd4_compound_state *cstate, struct nfsd4_write *write)\n{\n\tget_stateid(cstate, &write->wr_stateid);\n}\n", "/*\n *  Server-side XDR for NFSv4\n *\n *  Copyright (c) 2002 The Regents of the University of Michigan.\n *  All rights reserved.\n *\n *  Kendrick Smith <kmsmith@umich.edu>\n *  Andy Adamson   <andros@umich.edu>\n *\n *  Redistribution and use in source and binary forms, with or without\n *  modification, are permitted provided that the following conditions\n *  are met:\n *\n *  1. Redistributions of source code must retain the above copyright\n *     notice, this list of conditions and the following disclaimer.\n *  2. Redistributions in binary form must reproduce the above copyright\n *     notice, this list of conditions and the following disclaimer in the\n *     documentation and/or other materials provided with the distribution.\n *  3. Neither the name of the University nor the names of its\n *     contributors may be used to endorse or promote products derived\n *     from this software without specific prior written permission.\n *\n *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED\n *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n *  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE\n *  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n *  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n *  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n *  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n *  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n *  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n *  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include <linux/fs_struct.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n#include <linux/namei.h>\n#include <linux/statfs.h>\n#include <linux/utsname.h>\n#include <linux/pagemap.h>\n#include <linux/sunrpc/svcauth_gss.h>\n\n#include \"idmap.h\"\n#include \"acl.h\"\n#include \"xdr4.h\"\n#include \"vfs.h\"\n#include \"state.h\"\n#include \"cache.h\"\n#include \"netns.h\"\n#include \"pnfs.h\"\n\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n#include <linux/security.h>\n#endif\n\n\n#define NFSDDBG_FACILITY\t\tNFSDDBG_XDR\n\nconst u32 nfsd_suppattrs[3][3] = {\n\t{NFSD4_SUPPORTED_ATTRS_WORD0,\n\t NFSD4_SUPPORTED_ATTRS_WORD1,\n\t NFSD4_SUPPORTED_ATTRS_WORD2},\n\n\t{NFSD4_1_SUPPORTED_ATTRS_WORD0,\n\t NFSD4_1_SUPPORTED_ATTRS_WORD1,\n\t NFSD4_1_SUPPORTED_ATTRS_WORD2},\n\n\t{NFSD4_1_SUPPORTED_ATTRS_WORD0,\n\t NFSD4_1_SUPPORTED_ATTRS_WORD1,\n\t NFSD4_2_SUPPORTED_ATTRS_WORD2},\n};\n\n/*\n * As per referral draft, the fsid for a referral MUST be different from the fsid of the containing\n * directory in order to indicate to the client that a filesystem boundary is present\n * We use a fixed fsid for a referral\n */\n#define NFS4_REFERRAL_FSID_MAJOR\t0x8000000ULL\n#define NFS4_REFERRAL_FSID_MINOR\t0x8000000ULL\n\nstatic __be32\ncheck_filename(char *str, int len)\n{\n\tint i;\n\n\tif (len == 0)\n\t\treturn nfserr_inval;\n\tif (isdotent(str, len))\n\t\treturn nfserr_badname;\n\tfor (i = 0; i < len; i++)\n\t\tif (str[i] == '/')\n\t\t\treturn nfserr_badname;\n\treturn 0;\n}\n\n#define DECODE_HEAD\t\t\t\t\\\n\t__be32 *p;\t\t\t\t\\\n\t__be32 status\n#define DECODE_TAIL\t\t\t\t\\\n\tstatus = 0;\t\t\t\t\\\nout:\t\t\t\t\t\t\\\n\treturn status;\t\t\t\t\\\nxdr_error:\t\t\t\t\t\\\n\tdprintk(\"NFSD: xdr error (%s:%d)\\n\",\t\\\n\t\t\t__FILE__, __LINE__);\t\\\n\tstatus = nfserr_bad_xdr;\t\t\\\n\tgoto out\n\n#define READMEM(x,nbytes) do {\t\t\t\\\n\tx = (char *)p;\t\t\t\t\\\n\tp += XDR_QUADLEN(nbytes);\t\t\\\n} while (0)\n#define SAVEMEM(x,nbytes) do {\t\t\t\\\n\tif (!(x = (p==argp->tmp || p == argp->tmpp) ? \\\n \t\tsavemem(argp, p, nbytes) :\t\\\n \t\t(char *)p)) {\t\t\t\\\n\t\tdprintk(\"NFSD: xdr error (%s:%d)\\n\", \\\n\t\t\t\t__FILE__, __LINE__); \\\n\t\tgoto xdr_error;\t\t\t\\\n\t\t}\t\t\t\t\\\n\tp += XDR_QUADLEN(nbytes);\t\t\\\n} while (0)\n#define COPYMEM(x,nbytes) do {\t\t\t\\\n\tmemcpy((x), p, nbytes);\t\t\t\\\n\tp += XDR_QUADLEN(nbytes);\t\t\\\n} while (0)\n\n/* READ_BUF, read_buf(): nbytes must be <= PAGE_SIZE */\n#define READ_BUF(nbytes)  do {\t\t\t\\\n\tif (nbytes <= (u32)((char *)argp->end - (char *)argp->p)) {\t\\\n\t\tp = argp->p;\t\t\t\\\n\t\targp->p += XDR_QUADLEN(nbytes);\t\\\n\t} else if (!(p = read_buf(argp, nbytes))) { \\\n\t\tdprintk(\"NFSD: xdr error (%s:%d)\\n\", \\\n\t\t\t\t__FILE__, __LINE__); \\\n\t\tgoto xdr_error;\t\t\t\\\n\t}\t\t\t\t\t\\\n} while (0)\n\nstatic void next_decode_page(struct nfsd4_compoundargs *argp)\n{\n\targp->p = page_address(argp->pagelist[0]);\n\targp->pagelist++;\n\tif (argp->pagelen < PAGE_SIZE) {\n\t\targp->end = argp->p + (argp->pagelen>>2);\n\t\targp->pagelen = 0;\n\t} else {\n\t\targp->end = argp->p + (PAGE_SIZE>>2);\n\t\targp->pagelen -= PAGE_SIZE;\n\t}\n}\n\nstatic __be32 *read_buf(struct nfsd4_compoundargs *argp, u32 nbytes)\n{\n\t/* We want more bytes than seem to be available.\n\t * Maybe we need a new page, maybe we have just run out\n\t */\n\tunsigned int avail = (char *)argp->end - (char *)argp->p;\n\t__be32 *p;\n\tif (avail + argp->pagelen < nbytes)\n\t\treturn NULL;\n\tif (avail + PAGE_SIZE < nbytes) /* need more than a page !! */\n\t\treturn NULL;\n\t/* ok, we can do it with the current plus the next page */\n\tif (nbytes <= sizeof(argp->tmp))\n\t\tp = argp->tmp;\n\telse {\n\t\tkfree(argp->tmpp);\n\t\tp = argp->tmpp = kmalloc(nbytes, GFP_KERNEL);\n\t\tif (!p)\n\t\t\treturn NULL;\n\t\t\n\t}\n\t/*\n\t * The following memcpy is safe because read_buf is always\n\t * called with nbytes > avail, and the two cases above both\n\t * guarantee p points to at least nbytes bytes.\n\t */\n\tmemcpy(p, argp->p, avail);\n\tnext_decode_page(argp);\n\tmemcpy(((char*)p)+avail, argp->p, (nbytes - avail));\n\targp->p += XDR_QUADLEN(nbytes - avail);\n\treturn p;\n}\n\nstatic int zero_clientid(clientid_t *clid)\n{\n\treturn (clid->cl_boot == 0) && (clid->cl_id == 0);\n}\n\n/**\n * svcxdr_tmpalloc - allocate memory to be freed after compound processing\n * @argp: NFSv4 compound argument structure\n * @p: pointer to be freed (with kfree())\n *\n * Marks @p to be freed when processing the compound operation\n * described in @argp finishes.\n */\nstatic void *\nsvcxdr_tmpalloc(struct nfsd4_compoundargs *argp, u32 len)\n{\n\tstruct svcxdr_tmpbuf *tb;\n\n\ttb = kmalloc(sizeof(*tb) + len, GFP_KERNEL);\n\tif (!tb)\n\t\treturn NULL;\n\ttb->next = argp->to_free;\n\targp->to_free = tb;\n\treturn tb->buf;\n}\n\n/*\n * For xdr strings that need to be passed to other kernel api's\n * as null-terminated strings.\n *\n * Note null-terminating in place usually isn't safe since the\n * buffer might end on a page boundary.\n */\nstatic char *\nsvcxdr_dupstr(struct nfsd4_compoundargs *argp, void *buf, u32 len)\n{\n\tchar *p = svcxdr_tmpalloc(argp, len + 1);\n\n\tif (!p)\n\t\treturn NULL;\n\tmemcpy(p, buf, len);\n\tp[len] = '\\0';\n\treturn p;\n}\n\n/**\n * savemem - duplicate a chunk of memory for later processing\n * @argp: NFSv4 compound argument structure to be freed with\n * @p: pointer to be duplicated\n * @nbytes: length to be duplicated\n *\n * Returns a pointer to a copy of @nbytes bytes of memory at @p\n * that are preserved until processing of the NFSv4 compound\n * operation described by @argp finishes.\n */\nstatic char *savemem(struct nfsd4_compoundargs *argp, __be32 *p, int nbytes)\n{\n\tvoid *ret;\n\n\tret = svcxdr_tmpalloc(argp, nbytes);\n\tif (!ret)\n\t\treturn NULL;\n\tmemcpy(ret, p, nbytes);\n\treturn ret;\n}\n\n/*\n * We require the high 32 bits of 'seconds' to be 0, and\n * we ignore all 32 bits of 'nseconds'.\n */\nstatic __be32\nnfsd4_decode_time(struct nfsd4_compoundargs *argp, struct timespec *tv)\n{\n\tDECODE_HEAD;\n\tu64 sec;\n\n\tREAD_BUF(12);\n\tp = xdr_decode_hyper(p, &sec);\n\ttv->tv_sec = sec;\n\ttv->tv_nsec = be32_to_cpup(p++);\n\tif (tv->tv_nsec >= (u32)1000000000)\n\t\treturn nfserr_inval;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_bitmap(struct nfsd4_compoundargs *argp, u32 *bmval)\n{\n\tu32 bmlen;\n\tDECODE_HEAD;\n\n\tbmval[0] = 0;\n\tbmval[1] = 0;\n\tbmval[2] = 0;\n\n\tREAD_BUF(4);\n\tbmlen = be32_to_cpup(p++);\n\tif (bmlen > 1000)\n\t\tgoto xdr_error;\n\n\tREAD_BUF(bmlen << 2);\n\tif (bmlen > 0)\n\t\tbmval[0] = be32_to_cpup(p++);\n\tif (bmlen > 1)\n\t\tbmval[1] = be32_to_cpup(p++);\n\tif (bmlen > 2)\n\t\tbmval[2] = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_fattr(struct nfsd4_compoundargs *argp, u32 *bmval,\n\t\t   struct iattr *iattr, struct nfs4_acl **acl,\n\t\t   struct xdr_netobj *label, int *umask)\n{\n\tint expected_len, len = 0;\n\tu32 dummy32;\n\tchar *buf;\n\n\tDECODE_HEAD;\n\tiattr->ia_valid = 0;\n\tif ((status = nfsd4_decode_bitmap(argp, bmval)))\n\t\treturn status;\n\n\tif (bmval[0] & ~NFSD_WRITEABLE_ATTRS_WORD0\n\t    || bmval[1] & ~NFSD_WRITEABLE_ATTRS_WORD1\n\t    || bmval[2] & ~NFSD_WRITEABLE_ATTRS_WORD2) {\n\t\tif (nfsd_attrs_supported(argp->minorversion, bmval))\n\t\t\treturn nfserr_inval;\n\t\treturn nfserr_attrnotsupp;\n\t}\n\n\tREAD_BUF(4);\n\texpected_len = be32_to_cpup(p++);\n\n\tif (bmval[0] & FATTR4_WORD0_SIZE) {\n\t\tREAD_BUF(8);\n\t\tlen += 8;\n\t\tp = xdr_decode_hyper(p, &iattr->ia_size);\n\t\tiattr->ia_valid |= ATTR_SIZE;\n\t}\n\tif (bmval[0] & FATTR4_WORD0_ACL) {\n\t\tu32 nace;\n\t\tstruct nfs4_ace *ace;\n\n\t\tREAD_BUF(4); len += 4;\n\t\tnace = be32_to_cpup(p++);\n\n\t\tif (nace > NFS4_ACL_MAX)\n\t\t\treturn nfserr_fbig;\n\n\t\t*acl = svcxdr_tmpalloc(argp, nfs4_acl_bytes(nace));\n\t\tif (*acl == NULL)\n\t\t\treturn nfserr_jukebox;\n\n\t\t(*acl)->naces = nace;\n\t\tfor (ace = (*acl)->aces; ace < (*acl)->aces + nace; ace++) {\n\t\t\tREAD_BUF(16); len += 16;\n\t\t\tace->type = be32_to_cpup(p++);\n\t\t\tace->flag = be32_to_cpup(p++);\n\t\t\tace->access_mask = be32_to_cpup(p++);\n\t\t\tdummy32 = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy32);\n\t\t\tlen += XDR_QUADLEN(dummy32) << 2;\n\t\t\tREADMEM(buf, dummy32);\n\t\t\tace->whotype = nfs4_acl_get_whotype(buf, dummy32);\n\t\t\tstatus = nfs_ok;\n\t\t\tif (ace->whotype != NFS4_ACL_WHO_NAMED)\n\t\t\t\t;\n\t\t\telse if (ace->flag & NFS4_ACE_IDENTIFIER_GROUP)\n\t\t\t\tstatus = nfsd_map_name_to_gid(argp->rqstp,\n\t\t\t\t\t\tbuf, dummy32, &ace->who_gid);\n\t\t\telse\n\t\t\t\tstatus = nfsd_map_name_to_uid(argp->rqstp,\n\t\t\t\t\t\tbuf, dummy32, &ace->who_uid);\n\t\t\tif (status)\n\t\t\t\treturn status;\n\t\t}\n\t} else\n\t\t*acl = NULL;\n\tif (bmval[1] & FATTR4_WORD1_MODE) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tiattr->ia_mode = be32_to_cpup(p++);\n\t\tiattr->ia_mode &= (S_IFMT | S_IALLUGO);\n\t\tiattr->ia_valid |= ATTR_MODE;\n\t}\n\tif (bmval[1] & FATTR4_WORD1_OWNER) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy32);\n\t\tlen += (XDR_QUADLEN(dummy32) << 2);\n\t\tREADMEM(buf, dummy32);\n\t\tif ((status = nfsd_map_name_to_uid(argp->rqstp, buf, dummy32, &iattr->ia_uid)))\n\t\t\treturn status;\n\t\tiattr->ia_valid |= ATTR_UID;\n\t}\n\tif (bmval[1] & FATTR4_WORD1_OWNER_GROUP) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy32);\n\t\tlen += (XDR_QUADLEN(dummy32) << 2);\n\t\tREADMEM(buf, dummy32);\n\t\tif ((status = nfsd_map_name_to_gid(argp->rqstp, buf, dummy32, &iattr->ia_gid)))\n\t\t\treturn status;\n\t\tiattr->ia_valid |= ATTR_GID;\n\t}\n\tif (bmval[1] & FATTR4_WORD1_TIME_ACCESS_SET) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tswitch (dummy32) {\n\t\tcase NFS4_SET_TO_CLIENT_TIME:\n\t\t\tlen += 12;\n\t\t\tstatus = nfsd4_decode_time(argp, &iattr->ia_atime);\n\t\t\tif (status)\n\t\t\t\treturn status;\n\t\t\tiattr->ia_valid |= (ATTR_ATIME | ATTR_ATIME_SET);\n\t\t\tbreak;\n\t\tcase NFS4_SET_TO_SERVER_TIME:\n\t\t\tiattr->ia_valid |= ATTR_ATIME;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto xdr_error;\n\t\t}\n\t}\n\tif (bmval[1] & FATTR4_WORD1_TIME_MODIFY_SET) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tswitch (dummy32) {\n\t\tcase NFS4_SET_TO_CLIENT_TIME:\n\t\t\tlen += 12;\n\t\t\tstatus = nfsd4_decode_time(argp, &iattr->ia_mtime);\n\t\t\tif (status)\n\t\t\t\treturn status;\n\t\t\tiattr->ia_valid |= (ATTR_MTIME | ATTR_MTIME_SET);\n\t\t\tbreak;\n\t\tcase NFS4_SET_TO_SERVER_TIME:\n\t\t\tiattr->ia_valid |= ATTR_MTIME;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto xdr_error;\n\t\t}\n\t}\n\n\tlabel->len = 0;\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n\tif (bmval[2] & FATTR4_WORD2_SECURITY_LABEL) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++); /* lfs: we don't use it */\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++); /* pi: we don't use it either */\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy32);\n\t\tif (dummy32 > NFS4_MAXLABELLEN)\n\t\t\treturn nfserr_badlabel;\n\t\tlen += (XDR_QUADLEN(dummy32) << 2);\n\t\tREADMEM(buf, dummy32);\n\t\tlabel->len = dummy32;\n\t\tlabel->data = svcxdr_dupstr(argp, buf, dummy32);\n\t\tif (!label->data)\n\t\t\treturn nfserr_jukebox;\n\t}\n#endif\n\tif (bmval[2] & FATTR4_WORD2_MODE_UMASK) {\n\t\tif (!umask)\n\t\t\tgoto xdr_error;\n\t\tREAD_BUF(8);\n\t\tlen += 8;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tiattr->ia_mode = dummy32 & (S_IFMT | S_IALLUGO);\n\t\tdummy32 = be32_to_cpup(p++);\n\t\t*umask = dummy32 & S_IRWXUGO;\n\t\tiattr->ia_valid |= ATTR_MODE;\n\t}\n\tif (len != expected_len)\n\t\tgoto xdr_error;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_stateid(struct nfsd4_compoundargs *argp, stateid_t *sid)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(sizeof(stateid_t));\n\tsid->si_generation = be32_to_cpup(p++);\n\tCOPYMEM(&sid->si_opaque, sizeof(stateid_opaque_t));\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_access(struct nfsd4_compoundargs *argp, struct nfsd4_access *access)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\taccess->ac_req_access = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32 nfsd4_decode_cb_sec(struct nfsd4_compoundargs *argp, struct nfsd4_cb_sec *cbs)\n{\n\tDECODE_HEAD;\n\tu32 dummy, uid, gid;\n\tchar *machine_name;\n\tint i;\n\tint nr_secflavs;\n\n\t/* callback_sec_params4 */\n\tREAD_BUF(4);\n\tnr_secflavs = be32_to_cpup(p++);\n\tif (nr_secflavs)\n\t\tcbs->flavor = (u32)(-1);\n\telse\n\t\t/* Is this legal? Be generous, take it to mean AUTH_NONE: */\n\t\tcbs->flavor = 0;\n\tfor (i = 0; i < nr_secflavs; ++i) {\n\t\tREAD_BUF(4);\n\t\tdummy = be32_to_cpup(p++);\n\t\tswitch (dummy) {\n\t\tcase RPC_AUTH_NULL:\n\t\t\t/* Nothing to read */\n\t\t\tif (cbs->flavor == (u32)(-1))\n\t\t\t\tcbs->flavor = RPC_AUTH_NULL;\n\t\t\tbreak;\n\t\tcase RPC_AUTH_UNIX:\n\t\t\tREAD_BUF(8);\n\t\t\t/* stamp */\n\t\t\tdummy = be32_to_cpup(p++);\n\n\t\t\t/* machine name */\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy);\n\t\t\tSAVEMEM(machine_name, dummy);\n\n\t\t\t/* uid, gid */\n\t\t\tREAD_BUF(8);\n\t\t\tuid = be32_to_cpup(p++);\n\t\t\tgid = be32_to_cpup(p++);\n\n\t\t\t/* more gids */\n\t\t\tREAD_BUF(4);\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy * 4);\n\t\t\tif (cbs->flavor == (u32)(-1)) {\n\t\t\t\tkuid_t kuid = make_kuid(&init_user_ns, uid);\n\t\t\t\tkgid_t kgid = make_kgid(&init_user_ns, gid);\n\t\t\t\tif (uid_valid(kuid) && gid_valid(kgid)) {\n\t\t\t\t\tcbs->uid = kuid;\n\t\t\t\t\tcbs->gid = kgid;\n\t\t\t\t\tcbs->flavor = RPC_AUTH_UNIX;\n\t\t\t\t} else {\n\t\t\t\t\tdprintk(\"RPC_AUTH_UNIX with invalid\"\n\t\t\t\t\t\t\"uid or gid ignoring!\\n\");\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase RPC_AUTH_GSS:\n\t\t\tdprintk(\"RPC_AUTH_GSS callback secflavor \"\n\t\t\t\t\"not supported!\\n\");\n\t\t\tREAD_BUF(8);\n\t\t\t/* gcbp_service */\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\t/* gcbp_handle_from_server */\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy);\n\t\t\tp += XDR_QUADLEN(dummy);\n\t\t\t/* gcbp_handle_from_client */\n\t\t\tREAD_BUF(4);\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdprintk(\"Illegal callback secflavor\\n\");\n\t\t\treturn nfserr_inval;\n\t\t}\n\t}\n\tDECODE_TAIL;\n}\n\nstatic __be32 nfsd4_decode_backchannel_ctl(struct nfsd4_compoundargs *argp, struct nfsd4_backchannel_ctl *bc)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tbc->bc_cb_program = be32_to_cpup(p++);\n\tnfsd4_decode_cb_sec(argp, &bc->bc_cb_sec);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32 nfsd4_decode_bind_conn_to_session(struct nfsd4_compoundargs *argp, struct nfsd4_bind_conn_to_session *bcts)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(NFS4_MAX_SESSIONID_LEN + 8);\n\tCOPYMEM(bcts->sessionid.data, NFS4_MAX_SESSIONID_LEN);\n\tbcts->dir = be32_to_cpup(p++);\n\t/* XXX: skipping ctsa_use_conn_in_rdma_mode.  Perhaps Tom Tucker\n\t * could help us figure out we should be using it. */\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_close(struct nfsd4_compoundargs *argp, struct nfsd4_close *close)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tclose->cl_seqid = be32_to_cpup(p++);\n\treturn nfsd4_decode_stateid(argp, &close->cl_stateid);\n\n\tDECODE_TAIL;\n}\n\n\nstatic __be32\nnfsd4_decode_commit(struct nfsd4_compoundargs *argp, struct nfsd4_commit *commit)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(12);\n\tp = xdr_decode_hyper(p, &commit->co_offset);\n\tcommit->co_count = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_create(struct nfsd4_compoundargs *argp, struct nfsd4_create *create)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tcreate->cr_type = be32_to_cpup(p++);\n\tswitch (create->cr_type) {\n\tcase NF4LNK:\n\t\tREAD_BUF(4);\n\t\tcreate->cr_datalen = be32_to_cpup(p++);\n\t\tREAD_BUF(create->cr_datalen);\n\t\tcreate->cr_data = svcxdr_dupstr(argp, p, create->cr_datalen);\n\t\tif (!create->cr_data)\n\t\t\treturn nfserr_jukebox;\n\t\tbreak;\n\tcase NF4BLK:\n\tcase NF4CHR:\n\t\tREAD_BUF(8);\n\t\tcreate->cr_specdata1 = be32_to_cpup(p++);\n\t\tcreate->cr_specdata2 = be32_to_cpup(p++);\n\t\tbreak;\n\tcase NF4SOCK:\n\tcase NF4FIFO:\n\tcase NF4DIR:\n\tdefault:\n\t\tbreak;\n\t}\n\n\tREAD_BUF(4);\n\tcreate->cr_namelen = be32_to_cpup(p++);\n\tREAD_BUF(create->cr_namelen);\n\tSAVEMEM(create->cr_name, create->cr_namelen);\n\tif ((status = check_filename(create->cr_name, create->cr_namelen)))\n\t\treturn status;\n\n\tstatus = nfsd4_decode_fattr(argp, create->cr_bmval, &create->cr_iattr,\n\t\t\t\t    &create->cr_acl, &create->cr_label,\n\t\t\t\t    &current->fs->umask);\n\tif (status)\n\t\tgoto out;\n\n\tDECODE_TAIL;\n}\n\nstatic inline __be32\nnfsd4_decode_delegreturn(struct nfsd4_compoundargs *argp, struct nfsd4_delegreturn *dr)\n{\n\treturn nfsd4_decode_stateid(argp, &dr->dr_stateid);\n}\n\nstatic inline __be32\nnfsd4_decode_getattr(struct nfsd4_compoundargs *argp, struct nfsd4_getattr *getattr)\n{\n\treturn nfsd4_decode_bitmap(argp, getattr->ga_bmval);\n}\n\nstatic __be32\nnfsd4_decode_link(struct nfsd4_compoundargs *argp, struct nfsd4_link *link)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tlink->li_namelen = be32_to_cpup(p++);\n\tREAD_BUF(link->li_namelen);\n\tSAVEMEM(link->li_name, link->li_namelen);\n\tif ((status = check_filename(link->li_name, link->li_namelen)))\n\t\treturn status;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_lock(struct nfsd4_compoundargs *argp, struct nfsd4_lock *lock)\n{\n\tDECODE_HEAD;\n\n\t/*\n\t* type, reclaim(boolean), offset, length, new_lock_owner(boolean)\n\t*/\n\tREAD_BUF(28);\n\tlock->lk_type = be32_to_cpup(p++);\n\tif ((lock->lk_type < NFS4_READ_LT) || (lock->lk_type > NFS4_WRITEW_LT))\n\t\tgoto xdr_error;\n\tlock->lk_reclaim = be32_to_cpup(p++);\n\tp = xdr_decode_hyper(p, &lock->lk_offset);\n\tp = xdr_decode_hyper(p, &lock->lk_length);\n\tlock->lk_is_new = be32_to_cpup(p++);\n\n\tif (lock->lk_is_new) {\n\t\tREAD_BUF(4);\n\t\tlock->lk_new_open_seqid = be32_to_cpup(p++);\n\t\tstatus = nfsd4_decode_stateid(argp, &lock->lk_new_open_stateid);\n\t\tif (status)\n\t\t\treturn status;\n\t\tREAD_BUF(8 + sizeof(clientid_t));\n\t\tlock->lk_new_lock_seqid = be32_to_cpup(p++);\n\t\tCOPYMEM(&lock->lk_new_clientid, sizeof(clientid_t));\n\t\tlock->lk_new_owner.len = be32_to_cpup(p++);\n\t\tREAD_BUF(lock->lk_new_owner.len);\n\t\tREADMEM(lock->lk_new_owner.data, lock->lk_new_owner.len);\n\t} else {\n\t\tstatus = nfsd4_decode_stateid(argp, &lock->lk_old_lock_stateid);\n\t\tif (status)\n\t\t\treturn status;\n\t\tREAD_BUF(4);\n\t\tlock->lk_old_lock_seqid = be32_to_cpup(p++);\n\t}\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_lockt(struct nfsd4_compoundargs *argp, struct nfsd4_lockt *lockt)\n{\n\tDECODE_HEAD;\n\t\t        \n\tREAD_BUF(32);\n\tlockt->lt_type = be32_to_cpup(p++);\n\tif((lockt->lt_type < NFS4_READ_LT) || (lockt->lt_type > NFS4_WRITEW_LT))\n\t\tgoto xdr_error;\n\tp = xdr_decode_hyper(p, &lockt->lt_offset);\n\tp = xdr_decode_hyper(p, &lockt->lt_length);\n\tCOPYMEM(&lockt->lt_clientid, 8);\n\tlockt->lt_owner.len = be32_to_cpup(p++);\n\tREAD_BUF(lockt->lt_owner.len);\n\tREADMEM(lockt->lt_owner.data, lockt->lt_owner.len);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_locku(struct nfsd4_compoundargs *argp, struct nfsd4_locku *locku)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(8);\n\tlocku->lu_type = be32_to_cpup(p++);\n\tif ((locku->lu_type < NFS4_READ_LT) || (locku->lu_type > NFS4_WRITEW_LT))\n\t\tgoto xdr_error;\n\tlocku->lu_seqid = be32_to_cpup(p++);\n\tstatus = nfsd4_decode_stateid(argp, &locku->lu_stateid);\n\tif (status)\n\t\treturn status;\n\tREAD_BUF(16);\n\tp = xdr_decode_hyper(p, &locku->lu_offset);\n\tp = xdr_decode_hyper(p, &locku->lu_length);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_lookup(struct nfsd4_compoundargs *argp, struct nfsd4_lookup *lookup)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tlookup->lo_len = be32_to_cpup(p++);\n\tREAD_BUF(lookup->lo_len);\n\tSAVEMEM(lookup->lo_name, lookup->lo_len);\n\tif ((status = check_filename(lookup->lo_name, lookup->lo_len)))\n\t\treturn status;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32 nfsd4_decode_share_access(struct nfsd4_compoundargs *argp, u32 *share_access, u32 *deleg_want, u32 *deleg_when)\n{\n\t__be32 *p;\n\tu32 w;\n\n\tREAD_BUF(4);\n\tw = be32_to_cpup(p++);\n\t*share_access = w & NFS4_SHARE_ACCESS_MASK;\n\t*deleg_want = w & NFS4_SHARE_WANT_MASK;\n\tif (deleg_when)\n\t\t*deleg_when = w & NFS4_SHARE_WHEN_MASK;\n\n\tswitch (w & NFS4_SHARE_ACCESS_MASK) {\n\tcase NFS4_SHARE_ACCESS_READ:\n\tcase NFS4_SHARE_ACCESS_WRITE:\n\tcase NFS4_SHARE_ACCESS_BOTH:\n\t\tbreak;\n\tdefault:\n\t\treturn nfserr_bad_xdr;\n\t}\n\tw &= ~NFS4_SHARE_ACCESS_MASK;\n\tif (!w)\n\t\treturn nfs_ok;\n\tif (!argp->minorversion)\n\t\treturn nfserr_bad_xdr;\n\tswitch (w & NFS4_SHARE_WANT_MASK) {\n\tcase NFS4_SHARE_WANT_NO_PREFERENCE:\n\tcase NFS4_SHARE_WANT_READ_DELEG:\n\tcase NFS4_SHARE_WANT_WRITE_DELEG:\n\tcase NFS4_SHARE_WANT_ANY_DELEG:\n\tcase NFS4_SHARE_WANT_NO_DELEG:\n\tcase NFS4_SHARE_WANT_CANCEL:\n\t\tbreak;\n\tdefault:\n\t\treturn nfserr_bad_xdr;\n\t}\n\tw &= ~NFS4_SHARE_WANT_MASK;\n\tif (!w)\n\t\treturn nfs_ok;\n\n\tif (!deleg_when)\t/* open_downgrade */\n\t\treturn nfserr_inval;\n\tswitch (w) {\n\tcase NFS4_SHARE_SIGNAL_DELEG_WHEN_RESRC_AVAIL:\n\tcase NFS4_SHARE_PUSH_DELEG_WHEN_UNCONTENDED:\n\tcase (NFS4_SHARE_SIGNAL_DELEG_WHEN_RESRC_AVAIL |\n\t      NFS4_SHARE_PUSH_DELEG_WHEN_UNCONTENDED):\n\t\treturn nfs_ok;\n\t}\nxdr_error:\n\treturn nfserr_bad_xdr;\n}\n\nstatic __be32 nfsd4_decode_share_deny(struct nfsd4_compoundargs *argp, u32 *x)\n{\n\t__be32 *p;\n\n\tREAD_BUF(4);\n\t*x = be32_to_cpup(p++);\n\t/* Note: unlinke access bits, deny bits may be zero. */\n\tif (*x & ~NFS4_SHARE_DENY_BOTH)\n\t\treturn nfserr_bad_xdr;\n\treturn nfs_ok;\nxdr_error:\n\treturn nfserr_bad_xdr;\n}\n\nstatic __be32 nfsd4_decode_opaque(struct nfsd4_compoundargs *argp, struct xdr_netobj *o)\n{\n\t__be32 *p;\n\n\tREAD_BUF(4);\n\to->len = be32_to_cpup(p++);\n\n\tif (o->len == 0 || o->len > NFS4_OPAQUE_LIMIT)\n\t\treturn nfserr_bad_xdr;\n\n\tREAD_BUF(o->len);\n\tSAVEMEM(o->data, o->len);\n\treturn nfs_ok;\nxdr_error:\n\treturn nfserr_bad_xdr;\n}\n\nstatic __be32\nnfsd4_decode_open(struct nfsd4_compoundargs *argp, struct nfsd4_open *open)\n{\n\tDECODE_HEAD;\n\tu32 dummy;\n\n\tmemset(open->op_bmval, 0, sizeof(open->op_bmval));\n\topen->op_iattr.ia_valid = 0;\n\topen->op_openowner = NULL;\n\n\topen->op_xdr_error = 0;\n\t/* seqid, share_access, share_deny, clientid, ownerlen */\n\tREAD_BUF(4);\n\topen->op_seqid = be32_to_cpup(p++);\n\t/* decode, yet ignore deleg_when until supported */\n\tstatus = nfsd4_decode_share_access(argp, &open->op_share_access,\n\t\t\t\t\t   &open->op_deleg_want, &dummy);\n\tif (status)\n\t\tgoto xdr_error;\n\tstatus = nfsd4_decode_share_deny(argp, &open->op_share_deny);\n\tif (status)\n\t\tgoto xdr_error;\n\tREAD_BUF(sizeof(clientid_t));\n\tCOPYMEM(&open->op_clientid, sizeof(clientid_t));\n\tstatus = nfsd4_decode_opaque(argp, &open->op_owner);\n\tif (status)\n\t\tgoto xdr_error;\n\tREAD_BUF(4);\n\topen->op_create = be32_to_cpup(p++);\n\tswitch (open->op_create) {\n\tcase NFS4_OPEN_NOCREATE:\n\t\tbreak;\n\tcase NFS4_OPEN_CREATE:\n\t\tcurrent->fs->umask = 0;\n\t\tREAD_BUF(4);\n\t\topen->op_createmode = be32_to_cpup(p++);\n\t\tswitch (open->op_createmode) {\n\t\tcase NFS4_CREATE_UNCHECKED:\n\t\tcase NFS4_CREATE_GUARDED:\n\t\t\tstatus = nfsd4_decode_fattr(argp, open->op_bmval,\n\t\t\t\t&open->op_iattr, &open->op_acl, &open->op_label,\n\t\t\t\t&current->fs->umask);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase NFS4_CREATE_EXCLUSIVE:\n\t\t\tREAD_BUF(NFS4_VERIFIER_SIZE);\n\t\t\tCOPYMEM(open->op_verf.data, NFS4_VERIFIER_SIZE);\n\t\t\tbreak;\n\t\tcase NFS4_CREATE_EXCLUSIVE4_1:\n\t\t\tif (argp->minorversion < 1)\n\t\t\t\tgoto xdr_error;\n\t\t\tREAD_BUF(NFS4_VERIFIER_SIZE);\n\t\t\tCOPYMEM(open->op_verf.data, NFS4_VERIFIER_SIZE);\n\t\t\tstatus = nfsd4_decode_fattr(argp, open->op_bmval,\n\t\t\t\t&open->op_iattr, &open->op_acl, &open->op_label,\n\t\t\t\t&current->fs->umask);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto xdr_error;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tgoto xdr_error;\n\t}\n\n\t/* open_claim */\n\tREAD_BUF(4);\n\topen->op_claim_type = be32_to_cpup(p++);\n\tswitch (open->op_claim_type) {\n\tcase NFS4_OPEN_CLAIM_NULL:\n\tcase NFS4_OPEN_CLAIM_DELEGATE_PREV:\n\t\tREAD_BUF(4);\n\t\topen->op_fname.len = be32_to_cpup(p++);\n\t\tREAD_BUF(open->op_fname.len);\n\t\tSAVEMEM(open->op_fname.data, open->op_fname.len);\n\t\tif ((status = check_filename(open->op_fname.data, open->op_fname.len)))\n\t\t\treturn status;\n\t\tbreak;\n\tcase NFS4_OPEN_CLAIM_PREVIOUS:\n\t\tREAD_BUF(4);\n\t\topen->op_delegate_type = be32_to_cpup(p++);\n\t\tbreak;\n\tcase NFS4_OPEN_CLAIM_DELEGATE_CUR:\n\t\tstatus = nfsd4_decode_stateid(argp, &open->op_delegate_stateid);\n\t\tif (status)\n\t\t\treturn status;\n\t\tREAD_BUF(4);\n\t\topen->op_fname.len = be32_to_cpup(p++);\n\t\tREAD_BUF(open->op_fname.len);\n\t\tSAVEMEM(open->op_fname.data, open->op_fname.len);\n\t\tif ((status = check_filename(open->op_fname.data, open->op_fname.len)))\n\t\t\treturn status;\n\t\tbreak;\n\tcase NFS4_OPEN_CLAIM_FH:\n\tcase NFS4_OPEN_CLAIM_DELEG_PREV_FH:\n\t\tif (argp->minorversion < 1)\n\t\t\tgoto xdr_error;\n\t\t/* void */\n\t\tbreak;\n\tcase NFS4_OPEN_CLAIM_DELEG_CUR_FH:\n\t\tif (argp->minorversion < 1)\n\t\t\tgoto xdr_error;\n\t\tstatus = nfsd4_decode_stateid(argp, &open->op_delegate_stateid);\n\t\tif (status)\n\t\t\treturn status;\n\t\tbreak;\n\tdefault:\n\t\tgoto xdr_error;\n\t}\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_open_confirm(struct nfsd4_compoundargs *argp, struct nfsd4_open_confirm *open_conf)\n{\n\tDECODE_HEAD;\n\n\tif (argp->minorversion >= 1)\n\t\treturn nfserr_notsupp;\n\n\tstatus = nfsd4_decode_stateid(argp, &open_conf->oc_req_stateid);\n\tif (status)\n\t\treturn status;\n\tREAD_BUF(4);\n\topen_conf->oc_seqid = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_open_downgrade(struct nfsd4_compoundargs *argp, struct nfsd4_open_downgrade *open_down)\n{\n\tDECODE_HEAD;\n\t\t    \n\tstatus = nfsd4_decode_stateid(argp, &open_down->od_stateid);\n\tif (status)\n\t\treturn status;\n\tREAD_BUF(4);\n\topen_down->od_seqid = be32_to_cpup(p++);\n\tstatus = nfsd4_decode_share_access(argp, &open_down->od_share_access,\n\t\t\t\t\t   &open_down->od_deleg_want, NULL);\n\tif (status)\n\t\treturn status;\n\tstatus = nfsd4_decode_share_deny(argp, &open_down->od_share_deny);\n\tif (status)\n\t\treturn status;\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_putfh(struct nfsd4_compoundargs *argp, struct nfsd4_putfh *putfh)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tputfh->pf_fhlen = be32_to_cpup(p++);\n\tif (putfh->pf_fhlen > NFS4_FHSIZE)\n\t\tgoto xdr_error;\n\tREAD_BUF(putfh->pf_fhlen);\n\tSAVEMEM(putfh->pf_fhval, putfh->pf_fhlen);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_putpubfh(struct nfsd4_compoundargs *argp, void *p)\n{\n\tif (argp->minorversion == 0)\n\t\treturn nfs_ok;\n\treturn nfserr_notsupp;\n}\n\nstatic __be32\nnfsd4_decode_read(struct nfsd4_compoundargs *argp, struct nfsd4_read *read)\n{\n\tDECODE_HEAD;\n\n\tstatus = nfsd4_decode_stateid(argp, &read->rd_stateid);\n\tif (status)\n\t\treturn status;\n\tREAD_BUF(12);\n\tp = xdr_decode_hyper(p, &read->rd_offset);\n\tread->rd_length = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_readdir(struct nfsd4_compoundargs *argp, struct nfsd4_readdir *readdir)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(24);\n\tp = xdr_decode_hyper(p, &readdir->rd_cookie);\n\tCOPYMEM(readdir->rd_verf.data, sizeof(readdir->rd_verf.data));\n\treaddir->rd_dircount = be32_to_cpup(p++);\n\treaddir->rd_maxcount = be32_to_cpup(p++);\n\tif ((status = nfsd4_decode_bitmap(argp, readdir->rd_bmval)))\n\t\tgoto out;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_remove(struct nfsd4_compoundargs *argp, struct nfsd4_remove *remove)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tremove->rm_namelen = be32_to_cpup(p++);\n\tREAD_BUF(remove->rm_namelen);\n\tSAVEMEM(remove->rm_name, remove->rm_namelen);\n\tif ((status = check_filename(remove->rm_name, remove->rm_namelen)))\n\t\treturn status;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_rename(struct nfsd4_compoundargs *argp, struct nfsd4_rename *rename)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\trename->rn_snamelen = be32_to_cpup(p++);\n\tREAD_BUF(rename->rn_snamelen);\n\tSAVEMEM(rename->rn_sname, rename->rn_snamelen);\n\tREAD_BUF(4);\n\trename->rn_tnamelen = be32_to_cpup(p++);\n\tREAD_BUF(rename->rn_tnamelen);\n\tSAVEMEM(rename->rn_tname, rename->rn_tnamelen);\n\tif ((status = check_filename(rename->rn_sname, rename->rn_snamelen)))\n\t\treturn status;\n\tif ((status = check_filename(rename->rn_tname, rename->rn_tnamelen)))\n\t\treturn status;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_renew(struct nfsd4_compoundargs *argp, clientid_t *clientid)\n{\n\tDECODE_HEAD;\n\n\tif (argp->minorversion >= 1)\n\t\treturn nfserr_notsupp;\n\n\tREAD_BUF(sizeof(clientid_t));\n\tCOPYMEM(clientid, sizeof(clientid_t));\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_secinfo(struct nfsd4_compoundargs *argp,\n\t\t     struct nfsd4_secinfo *secinfo)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tsecinfo->si_namelen = be32_to_cpup(p++);\n\tREAD_BUF(secinfo->si_namelen);\n\tSAVEMEM(secinfo->si_name, secinfo->si_namelen);\n\tstatus = check_filename(secinfo->si_name, secinfo->si_namelen);\n\tif (status)\n\t\treturn status;\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_secinfo_no_name(struct nfsd4_compoundargs *argp,\n\t\t     struct nfsd4_secinfo_no_name *sin)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tsin->sin_style = be32_to_cpup(p++);\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_setattr(struct nfsd4_compoundargs *argp, struct nfsd4_setattr *setattr)\n{\n\t__be32 status;\n\n\tstatus = nfsd4_decode_stateid(argp, &setattr->sa_stateid);\n\tif (status)\n\t\treturn status;\n\treturn nfsd4_decode_fattr(argp, setattr->sa_bmval, &setattr->sa_iattr,\n\t\t\t\t  &setattr->sa_acl, &setattr->sa_label, NULL);\n}\n\nstatic __be32\nnfsd4_decode_setclientid(struct nfsd4_compoundargs *argp, struct nfsd4_setclientid *setclientid)\n{\n\tDECODE_HEAD;\n\n\tif (argp->minorversion >= 1)\n\t\treturn nfserr_notsupp;\n\n\tREAD_BUF(NFS4_VERIFIER_SIZE);\n\tCOPYMEM(setclientid->se_verf.data, NFS4_VERIFIER_SIZE);\n\n\tstatus = nfsd4_decode_opaque(argp, &setclientid->se_name);\n\tif (status)\n\t\treturn nfserr_bad_xdr;\n\tREAD_BUF(8);\n\tsetclientid->se_callback_prog = be32_to_cpup(p++);\n\tsetclientid->se_callback_netid_len = be32_to_cpup(p++);\n\tREAD_BUF(setclientid->se_callback_netid_len);\n\tSAVEMEM(setclientid->se_callback_netid_val, setclientid->se_callback_netid_len);\n\tREAD_BUF(4);\n\tsetclientid->se_callback_addr_len = be32_to_cpup(p++);\n\n\tREAD_BUF(setclientid->se_callback_addr_len);\n\tSAVEMEM(setclientid->se_callback_addr_val, setclientid->se_callback_addr_len);\n\tREAD_BUF(4);\n\tsetclientid->se_callback_ident = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_setclientid_confirm(struct nfsd4_compoundargs *argp, struct nfsd4_setclientid_confirm *scd_c)\n{\n\tDECODE_HEAD;\n\n\tif (argp->minorversion >= 1)\n\t\treturn nfserr_notsupp;\n\n\tREAD_BUF(8 + NFS4_VERIFIER_SIZE);\n\tCOPYMEM(&scd_c->sc_clientid, 8);\n\tCOPYMEM(&scd_c->sc_confirm, NFS4_VERIFIER_SIZE);\n\n\tDECODE_TAIL;\n}\n\n/* Also used for NVERIFY */\nstatic __be32\nnfsd4_decode_verify(struct nfsd4_compoundargs *argp, struct nfsd4_verify *verify)\n{\n\tDECODE_HEAD;\n\n\tif ((status = nfsd4_decode_bitmap(argp, verify->ve_bmval)))\n\t\tgoto out;\n\n\t/* For convenience's sake, we compare raw xdr'd attributes in\n\t * nfsd4_proc_verify */\n\n\tREAD_BUF(4);\n\tverify->ve_attrlen = be32_to_cpup(p++);\n\tREAD_BUF(verify->ve_attrlen);\n\tSAVEMEM(verify->ve_attrval, verify->ve_attrlen);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_write(struct nfsd4_compoundargs *argp, struct nfsd4_write *write)\n{\n\tint avail;\n\tint len;\n\tDECODE_HEAD;\n\n\tstatus = nfsd4_decode_stateid(argp, &write->wr_stateid);\n\tif (status)\n\t\treturn status;\n\tREAD_BUF(16);\n\tp = xdr_decode_hyper(p, &write->wr_offset);\n\twrite->wr_stable_how = be32_to_cpup(p++);\n\tif (write->wr_stable_how > NFS_FILE_SYNC)\n\t\tgoto xdr_error;\n\twrite->wr_buflen = be32_to_cpup(p++);\n\n\t/* Sorry .. no magic macros for this.. *\n\t * READ_BUF(write->wr_buflen);\n\t * SAVEMEM(write->wr_buf, write->wr_buflen);\n\t */\n\tavail = (char*)argp->end - (char*)argp->p;\n\tif (avail + argp->pagelen < write->wr_buflen) {\n\t\tdprintk(\"NFSD: xdr error (%s:%d)\\n\",\n\t\t\t\t__FILE__, __LINE__);\n\t\tgoto xdr_error;\n\t}\n\twrite->wr_head.iov_base = p;\n\twrite->wr_head.iov_len = avail;\n\twrite->wr_pagelist = argp->pagelist;\n\n\tlen = XDR_QUADLEN(write->wr_buflen) << 2;\n\tif (len >= avail) {\n\t\tint pages;\n\n\t\tlen -= avail;\n\n\t\tpages = len >> PAGE_SHIFT;\n\t\targp->pagelist += pages;\n\t\targp->pagelen -= pages * PAGE_SIZE;\n\t\tlen -= pages * PAGE_SIZE;\n\n\t\targp->p = (__be32 *)page_address(argp->pagelist[0]);\n\t\targp->pagelist++;\n\t\targp->end = argp->p + XDR_QUADLEN(PAGE_SIZE);\n\t}\n\targp->p += XDR_QUADLEN(len);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_release_lockowner(struct nfsd4_compoundargs *argp, struct nfsd4_release_lockowner *rlockowner)\n{\n\tDECODE_HEAD;\n\n\tif (argp->minorversion >= 1)\n\t\treturn nfserr_notsupp;\n\n\tREAD_BUF(12);\n\tCOPYMEM(&rlockowner->rl_clientid, sizeof(clientid_t));\n\trlockowner->rl_owner.len = be32_to_cpup(p++);\n\tREAD_BUF(rlockowner->rl_owner.len);\n\tREADMEM(rlockowner->rl_owner.data, rlockowner->rl_owner.len);\n\n\tif (argp->minorversion && !zero_clientid(&rlockowner->rl_clientid))\n\t\treturn nfserr_inval;\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_exchange_id(struct nfsd4_compoundargs *argp,\n\t\t\t struct nfsd4_exchange_id *exid)\n{\n\tint dummy, tmp;\n\tDECODE_HEAD;\n\n\tREAD_BUF(NFS4_VERIFIER_SIZE);\n\tCOPYMEM(exid->verifier.data, NFS4_VERIFIER_SIZE);\n\n\tstatus = nfsd4_decode_opaque(argp, &exid->clname);\n\tif (status)\n\t\treturn nfserr_bad_xdr;\n\n\tREAD_BUF(4);\n\texid->flags = be32_to_cpup(p++);\n\n\t/* Ignore state_protect4_a */\n\tREAD_BUF(4);\n\texid->spa_how = be32_to_cpup(p++);\n\tswitch (exid->spa_how) {\n\tcase SP4_NONE:\n\t\tbreak;\n\tcase SP4_MACH_CRED:\n\t\t/* spo_must_enforce */\n\t\tstatus = nfsd4_decode_bitmap(argp,\n\t\t\t\t\texid->spo_must_enforce);\n\t\tif (status)\n\t\t\tgoto out;\n\t\t/* spo_must_allow */\n\t\tstatus = nfsd4_decode_bitmap(argp, exid->spo_must_allow);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase SP4_SSV:\n\t\t/* ssp_ops */\n\t\tREAD_BUF(4);\n\t\tdummy = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy * 4);\n\t\tp += dummy;\n\n\t\tREAD_BUF(4);\n\t\tdummy = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy * 4);\n\t\tp += dummy;\n\n\t\t/* ssp_hash_algs<> */\n\t\tREAD_BUF(4);\n\t\ttmp = be32_to_cpup(p++);\n\t\twhile (tmp--) {\n\t\t\tREAD_BUF(4);\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy);\n\t\t\tp += XDR_QUADLEN(dummy);\n\t\t}\n\n\t\t/* ssp_encr_algs<> */\n\t\tREAD_BUF(4);\n\t\ttmp = be32_to_cpup(p++);\n\t\twhile (tmp--) {\n\t\t\tREAD_BUF(4);\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy);\n\t\t\tp += XDR_QUADLEN(dummy);\n\t\t}\n\n\t\t/* ssp_window and ssp_num_gss_handles */\n\t\tREAD_BUF(8);\n\t\tdummy = be32_to_cpup(p++);\n\t\tdummy = be32_to_cpup(p++);\n\t\tbreak;\n\tdefault:\n\t\tgoto xdr_error;\n\t}\n\n\t/* Ignore Implementation ID */\n\tREAD_BUF(4);    /* nfs_impl_id4 array length */\n\tdummy = be32_to_cpup(p++);\n\n\tif (dummy > 1)\n\t\tgoto xdr_error;\n\n\tif (dummy == 1) {\n\t\t/* nii_domain */\n\t\tREAD_BUF(4);\n\t\tdummy = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy);\n\t\tp += XDR_QUADLEN(dummy);\n\n\t\t/* nii_name */\n\t\tREAD_BUF(4);\n\t\tdummy = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy);\n\t\tp += XDR_QUADLEN(dummy);\n\n\t\t/* nii_date */\n\t\tREAD_BUF(12);\n\t\tp += 3;\n\t}\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_create_session(struct nfsd4_compoundargs *argp,\n\t\t\t    struct nfsd4_create_session *sess)\n{\n\tDECODE_HEAD;\n\tu32 dummy;\n\n\tREAD_BUF(16);\n\tCOPYMEM(&sess->clientid, 8);\n\tsess->seqid = be32_to_cpup(p++);\n\tsess->flags = be32_to_cpup(p++);\n\n\t/* Fore channel attrs */\n\tREAD_BUF(28);\n\tdummy = be32_to_cpup(p++); /* headerpadsz is always 0 */\n\tsess->fore_channel.maxreq_sz = be32_to_cpup(p++);\n\tsess->fore_channel.maxresp_sz = be32_to_cpup(p++);\n\tsess->fore_channel.maxresp_cached = be32_to_cpup(p++);\n\tsess->fore_channel.maxops = be32_to_cpup(p++);\n\tsess->fore_channel.maxreqs = be32_to_cpup(p++);\n\tsess->fore_channel.nr_rdma_attrs = be32_to_cpup(p++);\n\tif (sess->fore_channel.nr_rdma_attrs == 1) {\n\t\tREAD_BUF(4);\n\t\tsess->fore_channel.rdma_attrs = be32_to_cpup(p++);\n\t} else if (sess->fore_channel.nr_rdma_attrs > 1) {\n\t\tdprintk(\"Too many fore channel attr bitmaps!\\n\");\n\t\tgoto xdr_error;\n\t}\n\n\t/* Back channel attrs */\n\tREAD_BUF(28);\n\tdummy = be32_to_cpup(p++); /* headerpadsz is always 0 */\n\tsess->back_channel.maxreq_sz = be32_to_cpup(p++);\n\tsess->back_channel.maxresp_sz = be32_to_cpup(p++);\n\tsess->back_channel.maxresp_cached = be32_to_cpup(p++);\n\tsess->back_channel.maxops = be32_to_cpup(p++);\n\tsess->back_channel.maxreqs = be32_to_cpup(p++);\n\tsess->back_channel.nr_rdma_attrs = be32_to_cpup(p++);\n\tif (sess->back_channel.nr_rdma_attrs == 1) {\n\t\tREAD_BUF(4);\n\t\tsess->back_channel.rdma_attrs = be32_to_cpup(p++);\n\t} else if (sess->back_channel.nr_rdma_attrs > 1) {\n\t\tdprintk(\"Too many back channel attr bitmaps!\\n\");\n\t\tgoto xdr_error;\n\t}\n\n\tREAD_BUF(4);\n\tsess->callback_prog = be32_to_cpup(p++);\n\tnfsd4_decode_cb_sec(argp, &sess->cb_sec);\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_destroy_session(struct nfsd4_compoundargs *argp,\n\t\t\t     struct nfsd4_destroy_session *destroy_session)\n{\n\tDECODE_HEAD;\n\tREAD_BUF(NFS4_MAX_SESSIONID_LEN);\n\tCOPYMEM(destroy_session->sessionid.data, NFS4_MAX_SESSIONID_LEN);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_free_stateid(struct nfsd4_compoundargs *argp,\n\t\t\t  struct nfsd4_free_stateid *free_stateid)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(sizeof(stateid_t));\n\tfree_stateid->fr_stateid.si_generation = be32_to_cpup(p++);\n\tCOPYMEM(&free_stateid->fr_stateid.si_opaque, sizeof(stateid_opaque_t));\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_sequence(struct nfsd4_compoundargs *argp,\n\t\t      struct nfsd4_sequence *seq)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(NFS4_MAX_SESSIONID_LEN + 16);\n\tCOPYMEM(seq->sessionid.data, NFS4_MAX_SESSIONID_LEN);\n\tseq->seqid = be32_to_cpup(p++);\n\tseq->slotid = be32_to_cpup(p++);\n\tseq->maxslots = be32_to_cpup(p++);\n\tseq->cachethis = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_test_stateid(struct nfsd4_compoundargs *argp, struct nfsd4_test_stateid *test_stateid)\n{\n\tint i;\n\t__be32 *p, status;\n\tstruct nfsd4_test_stateid_id *stateid;\n\n\tREAD_BUF(4);\n\ttest_stateid->ts_num_ids = ntohl(*p++);\n\n\tINIT_LIST_HEAD(&test_stateid->ts_stateid_list);\n\n\tfor (i = 0; i < test_stateid->ts_num_ids; i++) {\n\t\tstateid = svcxdr_tmpalloc(argp, sizeof(*stateid));\n\t\tif (!stateid) {\n\t\t\tstatus = nfserrno(-ENOMEM);\n\t\t\tgoto out;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&stateid->ts_id_list);\n\t\tlist_add_tail(&stateid->ts_id_list, &test_stateid->ts_stateid_list);\n\n\t\tstatus = nfsd4_decode_stateid(argp, &stateid->ts_id_stateid);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tstatus = 0;\nout:\n\treturn status;\nxdr_error:\n\tdprintk(\"NFSD: xdr error (%s:%d)\\n\", __FILE__, __LINE__);\n\tstatus = nfserr_bad_xdr;\n\tgoto out;\n}\n\nstatic __be32 nfsd4_decode_destroy_clientid(struct nfsd4_compoundargs *argp, struct nfsd4_destroy_clientid *dc)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(8);\n\tCOPYMEM(&dc->clientid, 8);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32 nfsd4_decode_reclaim_complete(struct nfsd4_compoundargs *argp, struct nfsd4_reclaim_complete *rc)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\trc->rca_one_fs = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\n#ifdef CONFIG_NFSD_PNFS\nstatic __be32\nnfsd4_decode_getdeviceinfo(struct nfsd4_compoundargs *argp,\n\t\tstruct nfsd4_getdeviceinfo *gdev)\n{\n\tDECODE_HEAD;\n\tu32 num, i;\n\n\tREAD_BUF(sizeof(struct nfsd4_deviceid) + 3 * 4);\n\tCOPYMEM(&gdev->gd_devid, sizeof(struct nfsd4_deviceid));\n\tgdev->gd_layout_type = be32_to_cpup(p++);\n\tgdev->gd_maxcount = be32_to_cpup(p++);\n\tnum = be32_to_cpup(p++);\n\tif (num) {\n\t\tREAD_BUF(4 * num);\n\t\tgdev->gd_notify_types = be32_to_cpup(p++);\n\t\tfor (i = 1; i < num; i++) {\n\t\t\tif (be32_to_cpup(p++)) {\n\t\t\t\tstatus = nfserr_inval;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_layoutget(struct nfsd4_compoundargs *argp,\n\t\tstruct nfsd4_layoutget *lgp)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(36);\n\tlgp->lg_signal = be32_to_cpup(p++);\n\tlgp->lg_layout_type = be32_to_cpup(p++);\n\tlgp->lg_seg.iomode = be32_to_cpup(p++);\n\tp = xdr_decode_hyper(p, &lgp->lg_seg.offset);\n\tp = xdr_decode_hyper(p, &lgp->lg_seg.length);\n\tp = xdr_decode_hyper(p, &lgp->lg_minlength);\n\n\tstatus = nfsd4_decode_stateid(argp, &lgp->lg_sid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(4);\n\tlgp->lg_maxcount = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_layoutcommit(struct nfsd4_compoundargs *argp,\n\t\tstruct nfsd4_layoutcommit *lcp)\n{\n\tDECODE_HEAD;\n\tu32 timechange;\n\n\tREAD_BUF(20);\n\tp = xdr_decode_hyper(p, &lcp->lc_seg.offset);\n\tp = xdr_decode_hyper(p, &lcp->lc_seg.length);\n\tlcp->lc_reclaim = be32_to_cpup(p++);\n\n\tstatus = nfsd4_decode_stateid(argp, &lcp->lc_sid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(4);\n\tlcp->lc_newoffset = be32_to_cpup(p++);\n\tif (lcp->lc_newoffset) {\n\t\tREAD_BUF(8);\n\t\tp = xdr_decode_hyper(p, &lcp->lc_last_wr);\n\t} else\n\t\tlcp->lc_last_wr = 0;\n\tREAD_BUF(4);\n\ttimechange = be32_to_cpup(p++);\n\tif (timechange) {\n\t\tstatus = nfsd4_decode_time(argp, &lcp->lc_mtime);\n\t\tif (status)\n\t\t\treturn status;\n\t} else {\n\t\tlcp->lc_mtime.tv_nsec = UTIME_NOW;\n\t}\n\tREAD_BUF(8);\n\tlcp->lc_layout_type = be32_to_cpup(p++);\n\n\t/*\n\t * Save the layout update in XDR format and let the layout driver deal\n\t * with it later.\n\t */\n\tlcp->lc_up_len = be32_to_cpup(p++);\n\tif (lcp->lc_up_len > 0) {\n\t\tREAD_BUF(lcp->lc_up_len);\n\t\tREADMEM(lcp->lc_up_layout, lcp->lc_up_len);\n\t}\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_layoutreturn(struct nfsd4_compoundargs *argp,\n\t\tstruct nfsd4_layoutreturn *lrp)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(16);\n\tlrp->lr_reclaim = be32_to_cpup(p++);\n\tlrp->lr_layout_type = be32_to_cpup(p++);\n\tlrp->lr_seg.iomode = be32_to_cpup(p++);\n\tlrp->lr_return_type = be32_to_cpup(p++);\n\tif (lrp->lr_return_type == RETURN_FILE) {\n\t\tREAD_BUF(16);\n\t\tp = xdr_decode_hyper(p, &lrp->lr_seg.offset);\n\t\tp = xdr_decode_hyper(p, &lrp->lr_seg.length);\n\n\t\tstatus = nfsd4_decode_stateid(argp, &lrp->lr_sid);\n\t\tif (status)\n\t\t\treturn status;\n\n\t\tREAD_BUF(4);\n\t\tlrp->lrf_body_len = be32_to_cpup(p++);\n\t\tif (lrp->lrf_body_len > 0) {\n\t\t\tREAD_BUF(lrp->lrf_body_len);\n\t\t\tREADMEM(lrp->lrf_body, lrp->lrf_body_len);\n\t\t}\n\t} else {\n\t\tlrp->lr_seg.offset = 0;\n\t\tlrp->lr_seg.length = NFS4_MAX_UINT64;\n\t}\n\n\tDECODE_TAIL;\n}\n#endif /* CONFIG_NFSD_PNFS */\n\nstatic __be32\nnfsd4_decode_fallocate(struct nfsd4_compoundargs *argp,\n\t\t       struct nfsd4_fallocate *fallocate)\n{\n\tDECODE_HEAD;\n\n\tstatus = nfsd4_decode_stateid(argp, &fallocate->falloc_stateid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(16);\n\tp = xdr_decode_hyper(p, &fallocate->falloc_offset);\n\txdr_decode_hyper(p, &fallocate->falloc_length);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_clone(struct nfsd4_compoundargs *argp, struct nfsd4_clone *clone)\n{\n\tDECODE_HEAD;\n\n\tstatus = nfsd4_decode_stateid(argp, &clone->cl_src_stateid);\n\tif (status)\n\t\treturn status;\n\tstatus = nfsd4_decode_stateid(argp, &clone->cl_dst_stateid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(8 + 8 + 8);\n\tp = xdr_decode_hyper(p, &clone->cl_src_pos);\n\tp = xdr_decode_hyper(p, &clone->cl_dst_pos);\n\tp = xdr_decode_hyper(p, &clone->cl_count);\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_copy(struct nfsd4_compoundargs *argp, struct nfsd4_copy *copy)\n{\n\tDECODE_HEAD;\n\tunsigned int tmp;\n\n\tstatus = nfsd4_decode_stateid(argp, &copy->cp_src_stateid);\n\tif (status)\n\t\treturn status;\n\tstatus = nfsd4_decode_stateid(argp, &copy->cp_dst_stateid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(8 + 8 + 8 + 4 + 4 + 4);\n\tp = xdr_decode_hyper(p, &copy->cp_src_pos);\n\tp = xdr_decode_hyper(p, &copy->cp_dst_pos);\n\tp = xdr_decode_hyper(p, &copy->cp_count);\n\tcopy->cp_consecutive = be32_to_cpup(p++);\n\tcopy->cp_synchronous = be32_to_cpup(p++);\n\ttmp = be32_to_cpup(p); /* Source server list not supported */\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_seek(struct nfsd4_compoundargs *argp, struct nfsd4_seek *seek)\n{\n\tDECODE_HEAD;\n\n\tstatus = nfsd4_decode_stateid(argp, &seek->seek_stateid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(8 + 4);\n\tp = xdr_decode_hyper(p, &seek->seek_offset);\n\tseek->seek_whence = be32_to_cpup(p);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_noop(struct nfsd4_compoundargs *argp, void *p)\n{\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_decode_notsupp(struct nfsd4_compoundargs *argp, void *p)\n{\n\treturn nfserr_notsupp;\n}\n\ntypedef __be32(*nfsd4_dec)(struct nfsd4_compoundargs *argp, void *);\n\nstatic nfsd4_dec nfsd4_dec_ops[] = {\n\t[OP_ACCESS]\t\t= (nfsd4_dec)nfsd4_decode_access,\n\t[OP_CLOSE]\t\t= (nfsd4_dec)nfsd4_decode_close,\n\t[OP_COMMIT]\t\t= (nfsd4_dec)nfsd4_decode_commit,\n\t[OP_CREATE]\t\t= (nfsd4_dec)nfsd4_decode_create,\n\t[OP_DELEGPURGE]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_DELEGRETURN]\t= (nfsd4_dec)nfsd4_decode_delegreturn,\n\t[OP_GETATTR]\t\t= (nfsd4_dec)nfsd4_decode_getattr,\n\t[OP_GETFH]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_LINK]\t\t= (nfsd4_dec)nfsd4_decode_link,\n\t[OP_LOCK]\t\t= (nfsd4_dec)nfsd4_decode_lock,\n\t[OP_LOCKT]\t\t= (nfsd4_dec)nfsd4_decode_lockt,\n\t[OP_LOCKU]\t\t= (nfsd4_dec)nfsd4_decode_locku,\n\t[OP_LOOKUP]\t\t= (nfsd4_dec)nfsd4_decode_lookup,\n\t[OP_LOOKUPP]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_NVERIFY]\t\t= (nfsd4_dec)nfsd4_decode_verify,\n\t[OP_OPEN]\t\t= (nfsd4_dec)nfsd4_decode_open,\n\t[OP_OPENATTR]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_OPEN_CONFIRM]\t= (nfsd4_dec)nfsd4_decode_open_confirm,\n\t[OP_OPEN_DOWNGRADE]\t= (nfsd4_dec)nfsd4_decode_open_downgrade,\n\t[OP_PUTFH]\t\t= (nfsd4_dec)nfsd4_decode_putfh,\n\t[OP_PUTPUBFH]\t\t= (nfsd4_dec)nfsd4_decode_putpubfh,\n\t[OP_PUTROOTFH]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_READ]\t\t= (nfsd4_dec)nfsd4_decode_read,\n\t[OP_READDIR]\t\t= (nfsd4_dec)nfsd4_decode_readdir,\n\t[OP_READLINK]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_REMOVE]\t\t= (nfsd4_dec)nfsd4_decode_remove,\n\t[OP_RENAME]\t\t= (nfsd4_dec)nfsd4_decode_rename,\n\t[OP_RENEW]\t\t= (nfsd4_dec)nfsd4_decode_renew,\n\t[OP_RESTOREFH]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_SAVEFH]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_SECINFO]\t\t= (nfsd4_dec)nfsd4_decode_secinfo,\n\t[OP_SETATTR]\t\t= (nfsd4_dec)nfsd4_decode_setattr,\n\t[OP_SETCLIENTID]\t= (nfsd4_dec)nfsd4_decode_setclientid,\n\t[OP_SETCLIENTID_CONFIRM] = (nfsd4_dec)nfsd4_decode_setclientid_confirm,\n\t[OP_VERIFY]\t\t= (nfsd4_dec)nfsd4_decode_verify,\n\t[OP_WRITE]\t\t= (nfsd4_dec)nfsd4_decode_write,\n\t[OP_RELEASE_LOCKOWNER]\t= (nfsd4_dec)nfsd4_decode_release_lockowner,\n\n\t/* new operations for NFSv4.1 */\n\t[OP_BACKCHANNEL_CTL]\t= (nfsd4_dec)nfsd4_decode_backchannel_ctl,\n\t[OP_BIND_CONN_TO_SESSION]= (nfsd4_dec)nfsd4_decode_bind_conn_to_session,\n\t[OP_EXCHANGE_ID]\t= (nfsd4_dec)nfsd4_decode_exchange_id,\n\t[OP_CREATE_SESSION]\t= (nfsd4_dec)nfsd4_decode_create_session,\n\t[OP_DESTROY_SESSION]\t= (nfsd4_dec)nfsd4_decode_destroy_session,\n\t[OP_FREE_STATEID]\t= (nfsd4_dec)nfsd4_decode_free_stateid,\n\t[OP_GET_DIR_DELEGATION]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n#ifdef CONFIG_NFSD_PNFS\n\t[OP_GETDEVICEINFO]\t= (nfsd4_dec)nfsd4_decode_getdeviceinfo,\n\t[OP_GETDEVICELIST]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTCOMMIT]\t= (nfsd4_dec)nfsd4_decode_layoutcommit,\n\t[OP_LAYOUTGET]\t\t= (nfsd4_dec)nfsd4_decode_layoutget,\n\t[OP_LAYOUTRETURN]\t= (nfsd4_dec)nfsd4_decode_layoutreturn,\n#else\n\t[OP_GETDEVICEINFO]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_GETDEVICELIST]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTCOMMIT]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTGET]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTRETURN]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n#endif\n\t[OP_SECINFO_NO_NAME]\t= (nfsd4_dec)nfsd4_decode_secinfo_no_name,\n\t[OP_SEQUENCE]\t\t= (nfsd4_dec)nfsd4_decode_sequence,\n\t[OP_SET_SSV]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_TEST_STATEID]\t= (nfsd4_dec)nfsd4_decode_test_stateid,\n\t[OP_WANT_DELEGATION]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_DESTROY_CLIENTID]\t= (nfsd4_dec)nfsd4_decode_destroy_clientid,\n\t[OP_RECLAIM_COMPLETE]\t= (nfsd4_dec)nfsd4_decode_reclaim_complete,\n\n\t/* new operations for NFSv4.2 */\n\t[OP_ALLOCATE]\t\t= (nfsd4_dec)nfsd4_decode_fallocate,\n\t[OP_COPY]\t\t= (nfsd4_dec)nfsd4_decode_copy,\n\t[OP_COPY_NOTIFY]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_DEALLOCATE]\t\t= (nfsd4_dec)nfsd4_decode_fallocate,\n\t[OP_IO_ADVISE]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTERROR]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTSTATS]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_OFFLOAD_CANCEL]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_OFFLOAD_STATUS]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_READ_PLUS]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_SEEK]\t\t= (nfsd4_dec)nfsd4_decode_seek,\n\t[OP_WRITE_SAME]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_CLONE]\t\t= (nfsd4_dec)nfsd4_decode_clone,\n};\n\nstatic inline bool\nnfsd4_opnum_in_range(struct nfsd4_compoundargs *argp, struct nfsd4_op *op)\n{\n\tif (op->opnum < FIRST_NFS4_OP)\n\t\treturn false;\n\telse if (argp->minorversion == 0 && op->opnum > LAST_NFS40_OP)\n\t\treturn false;\n\telse if (argp->minorversion == 1 && op->opnum > LAST_NFS41_OP)\n\t\treturn false;\n\telse if (argp->minorversion == 2 && op->opnum > LAST_NFS42_OP)\n\t\treturn false;\n\treturn true;\n}\n\nstatic __be32\nnfsd4_decode_compound(struct nfsd4_compoundargs *argp)\n{\n\tDECODE_HEAD;\n\tstruct nfsd4_op *op;\n\tbool cachethis = false;\n\tint auth_slack= argp->rqstp->rq_auth_slack;\n\tint max_reply = auth_slack + 8; /* opcnt, status */\n\tint readcount = 0;\n\tint readbytes = 0;\n\tint i;\n\n\tREAD_BUF(4);\n\targp->taglen = be32_to_cpup(p++);\n\tREAD_BUF(argp->taglen);\n\tSAVEMEM(argp->tag, argp->taglen);\n\tREAD_BUF(8);\n\targp->minorversion = be32_to_cpup(p++);\n\targp->opcnt = be32_to_cpup(p++);\n\tmax_reply += 4 + (XDR_QUADLEN(argp->taglen) << 2);\n\n\tif (argp->taglen > NFSD4_MAX_TAGLEN)\n\t\tgoto xdr_error;\n\tif (argp->opcnt > 100)\n\t\tgoto xdr_error;\n\n\tif (argp->opcnt > ARRAY_SIZE(argp->iops)) {\n\t\targp->ops = kzalloc(argp->opcnt * sizeof(*argp->ops), GFP_KERNEL);\n\t\tif (!argp->ops) {\n\t\t\targp->ops = argp->iops;\n\t\t\tdprintk(\"nfsd: couldn't allocate room for COMPOUND\\n\");\n\t\t\tgoto xdr_error;\n\t\t}\n\t}\n\n\tif (argp->minorversion > NFSD_SUPPORTED_MINOR_VERSION)\n\t\targp->opcnt = 0;\n\n\tfor (i = 0; i < argp->opcnt; i++) {\n\t\top = &argp->ops[i];\n\t\top->replay = NULL;\n\n\t\tREAD_BUF(4);\n\t\top->opnum = be32_to_cpup(p++);\n\n\t\tif (nfsd4_opnum_in_range(argp, op))\n\t\t\top->status = nfsd4_dec_ops[op->opnum](argp, &op->u);\n\t\telse {\n\t\t\top->opnum = OP_ILLEGAL;\n\t\t\top->status = nfserr_op_illegal;\n\t\t}\n\t\t/*\n\t\t * We'll try to cache the result in the DRC if any one\n\t\t * op in the compound wants to be cached:\n\t\t */\n\t\tcachethis |= nfsd4_cache_this_op(op);\n\n\t\tif (op->opnum == OP_READ) {\n\t\t\treadcount++;\n\t\t\treadbytes += nfsd4_max_reply(argp->rqstp, op);\n\t\t} else\n\t\t\tmax_reply += nfsd4_max_reply(argp->rqstp, op);\n\t\t/*\n\t\t * OP_LOCK and OP_LOCKT may return a conflicting lock.\n\t\t * (Special case because it will just skip encoding this\n\t\t * if it runs out of xdr buffer space, and it is the only\n\t\t * operation that behaves this way.)\n\t\t */\n\t\tif (op->opnum == OP_LOCK || op->opnum == OP_LOCKT)\n\t\t\tmax_reply += NFS4_OPAQUE_LIMIT;\n\n\t\tif (op->status) {\n\t\t\targp->opcnt = i+1;\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Sessions make the DRC unnecessary: */\n\tif (argp->minorversion)\n\t\tcachethis = false;\n\tsvc_reserve(argp->rqstp, max_reply + readbytes);\n\targp->rqstp->rq_cachetype = cachethis ? RC_REPLBUFF : RC_NOCACHE;\n\n\tif (readcount > 1 || max_reply > PAGE_SIZE - auth_slack)\n\t\tclear_bit(RQ_SPLICE_OK, &argp->rqstp->rq_flags);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32 *encode_change(__be32 *p, struct kstat *stat, struct inode *inode,\n\t\t\t     struct svc_export *exp)\n{\n\tif (exp->ex_flags & NFSEXP_V4ROOT) {\n\t\t*p++ = cpu_to_be32(convert_to_wallclock(exp->cd->flush_time));\n\t\t*p++ = 0;\n\t} else if (IS_I_VERSION(inode)) {\n\t\tp = xdr_encode_hyper(p, inode->i_version);\n\t} else {\n\t\t*p++ = cpu_to_be32(stat->ctime.tv_sec);\n\t\t*p++ = cpu_to_be32(stat->ctime.tv_nsec);\n\t}\n\treturn p;\n}\n\nstatic __be32 *encode_cinfo(__be32 *p, struct nfsd4_change_info *c)\n{\n\t*p++ = cpu_to_be32(c->atomic);\n\tif (c->change_supported) {\n\t\tp = xdr_encode_hyper(p, c->before_change);\n\t\tp = xdr_encode_hyper(p, c->after_change);\n\t} else {\n\t\t*p++ = cpu_to_be32(c->before_ctime_sec);\n\t\t*p++ = cpu_to_be32(c->before_ctime_nsec);\n\t\t*p++ = cpu_to_be32(c->after_ctime_sec);\n\t\t*p++ = cpu_to_be32(c->after_ctime_nsec);\n\t}\n\treturn p;\n}\n\n/* Encode as an array of strings the string given with components\n * separated @sep, escaped with esc_enter and esc_exit.\n */\nstatic __be32 nfsd4_encode_components_esc(struct xdr_stream *xdr, char sep,\n\t\t\t\t\t  char *components, char esc_enter,\n\t\t\t\t\t  char esc_exit)\n{\n\t__be32 *p;\n\t__be32 pathlen;\n\tint pathlen_offset;\n\tint strlen, count=0;\n\tchar *str, *end, *next;\n\n\tdprintk(\"nfsd4_encode_components(%s)\\n\", components);\n\n\tpathlen_offset = xdr->buf->len;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tp++; /* We will fill this in with @count later */\n\n\tend = str = components;\n\twhile (*end) {\n\t\tbool found_esc = false;\n\n\t\t/* try to parse as esc_start, ..., esc_end, sep */\n\t\tif (*str == esc_enter) {\n\t\t\tfor (; *end && (*end != esc_exit); end++)\n\t\t\t\t/* find esc_exit or end of string */;\n\t\t\tnext = end + 1;\n\t\t\tif (*end && (!*next || *next == sep)) {\n\t\t\t\tstr++;\n\t\t\t\tfound_esc = true;\n\t\t\t}\n\t\t}\n\n\t\tif (!found_esc)\n\t\t\tfor (; *end && (*end != sep); end++)\n\t\t\t\t/* find sep or end of string */;\n\n\t\tstrlen = end - str;\n\t\tif (strlen) {\n\t\t\tp = xdr_reserve_space(xdr, strlen + 4);\n\t\t\tif (!p)\n\t\t\t\treturn nfserr_resource;\n\t\t\tp = xdr_encode_opaque(p, str, strlen);\n\t\t\tcount++;\n\t\t}\n\t\telse\n\t\t\tend++;\n\t\tif (found_esc)\n\t\t\tend = next;\n\n\t\tstr = end;\n\t}\n\tpathlen = htonl(count);\n\twrite_bytes_to_xdr_buf(xdr->buf, pathlen_offset, &pathlen, 4);\n\treturn 0;\n}\n\n/* Encode as an array of strings the string given with components\n * separated @sep.\n */\nstatic __be32 nfsd4_encode_components(struct xdr_stream *xdr, char sep,\n\t\t\t\t      char *components)\n{\n\treturn nfsd4_encode_components_esc(xdr, sep, components, 0, 0);\n}\n\n/*\n * encode a location element of a fs_locations structure\n */\nstatic __be32 nfsd4_encode_fs_location4(struct xdr_stream *xdr,\n\t\t\t\t\tstruct nfsd4_fs_location *location)\n{\n\t__be32 status;\n\n\tstatus = nfsd4_encode_components_esc(xdr, ':', location->hosts,\n\t\t\t\t\t\t'[', ']');\n\tif (status)\n\t\treturn status;\n\tstatus = nfsd4_encode_components(xdr, '/', location->path);\n\tif (status)\n\t\treturn status;\n\treturn 0;\n}\n\n/*\n * Encode a path in RFC3530 'pathname4' format\n */\nstatic __be32 nfsd4_encode_path(struct xdr_stream *xdr,\n\t\t\t\tconst struct path *root,\n\t\t\t\tconst struct path *path)\n{\n\tstruct path cur = *path;\n\t__be32 *p;\n\tstruct dentry **components = NULL;\n\tunsigned int ncomponents = 0;\n\t__be32 err = nfserr_jukebox;\n\n\tdprintk(\"nfsd4_encode_components(\");\n\n\tpath_get(&cur);\n\t/* First walk the path up to the nfsd root, and store the\n\t * dentries/path components in an array.\n\t */\n\tfor (;;) {\n\t\tif (path_equal(&cur, root))\n\t\t\tbreak;\n\t\tif (cur.dentry == cur.mnt->mnt_root) {\n\t\t\tif (follow_up(&cur))\n\t\t\t\tcontinue;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif ((ncomponents & 15) == 0) {\n\t\t\tstruct dentry **new;\n\t\t\tnew = krealloc(components,\n\t\t\t\t\tsizeof(*new) * (ncomponents + 16),\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!new)\n\t\t\t\tgoto out_free;\n\t\t\tcomponents = new;\n\t\t}\n\t\tcomponents[ncomponents++] = cur.dentry;\n\t\tcur.dentry = dget_parent(cur.dentry);\n\t}\n\terr = nfserr_resource;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\tgoto out_free;\n\t*p++ = cpu_to_be32(ncomponents);\n\n\twhile (ncomponents) {\n\t\tstruct dentry *dentry = components[ncomponents - 1];\n\t\tunsigned int len;\n\n\t\tspin_lock(&dentry->d_lock);\n\t\tlen = dentry->d_name.len;\n\t\tp = xdr_reserve_space(xdr, len + 4);\n\t\tif (!p) {\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tgoto out_free;\n\t\t}\n\t\tp = xdr_encode_opaque(p, dentry->d_name.name, len);\n\t\tdprintk(\"/%pd\", dentry);\n\t\tspin_unlock(&dentry->d_lock);\n\t\tdput(dentry);\n\t\tncomponents--;\n\t}\n\n\terr = 0;\nout_free:\n\tdprintk(\")\\n\");\n\twhile (ncomponents)\n\t\tdput(components[--ncomponents]);\n\tkfree(components);\n\tpath_put(&cur);\n\treturn err;\n}\n\nstatic __be32 nfsd4_encode_fsloc_fsroot(struct xdr_stream *xdr,\n\t\t\tstruct svc_rqst *rqstp, const struct path *path)\n{\n\tstruct svc_export *exp_ps;\n\t__be32 res;\n\n\texp_ps = rqst_find_fsidzero_export(rqstp);\n\tif (IS_ERR(exp_ps))\n\t\treturn nfserrno(PTR_ERR(exp_ps));\n\tres = nfsd4_encode_path(xdr, &exp_ps->ex_path, path);\n\texp_put(exp_ps);\n\treturn res;\n}\n\n/*\n *  encode a fs_locations structure\n */\nstatic __be32 nfsd4_encode_fs_locations(struct xdr_stream *xdr,\n\t\t\tstruct svc_rqst *rqstp, struct svc_export *exp)\n{\n\t__be32 status;\n\tint i;\n\t__be32 *p;\n\tstruct nfsd4_fs_locations *fslocs = &exp->ex_fslocs;\n\n\tstatus = nfsd4_encode_fsloc_fsroot(xdr, rqstp, &exp->ex_path);\n\tif (status)\n\t\treturn status;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(fslocs->locations_count);\n\tfor (i=0; i<fslocs->locations_count; i++) {\n\t\tstatus = nfsd4_encode_fs_location4(xdr, &fslocs->locations[i]);\n\t\tif (status)\n\t\t\treturn status;\n\t}\n\treturn 0;\n}\n\nstatic u32 nfs4_file_type(umode_t mode)\n{\n\tswitch (mode & S_IFMT) {\n\tcase S_IFIFO:\treturn NF4FIFO;\n\tcase S_IFCHR:\treturn NF4CHR;\n\tcase S_IFDIR:\treturn NF4DIR;\n\tcase S_IFBLK:\treturn NF4BLK;\n\tcase S_IFLNK:\treturn NF4LNK;\n\tcase S_IFREG:\treturn NF4REG;\n\tcase S_IFSOCK:\treturn NF4SOCK;\n\tdefault:\treturn NF4BAD;\n\t};\n}\n\nstatic inline __be32\nnfsd4_encode_aclname(struct xdr_stream *xdr, struct svc_rqst *rqstp,\n\t\t     struct nfs4_ace *ace)\n{\n\tif (ace->whotype != NFS4_ACL_WHO_NAMED)\n\t\treturn nfs4_acl_write_who(xdr, ace->whotype);\n\telse if (ace->flag & NFS4_ACE_IDENTIFIER_GROUP)\n\t\treturn nfsd4_encode_group(xdr, rqstp, ace->who_gid);\n\telse\n\t\treturn nfsd4_encode_user(xdr, rqstp, ace->who_uid);\n}\n\nstatic inline __be32\nnfsd4_encode_layout_types(struct xdr_stream *xdr, u32 layout_types)\n{\n\t__be32\t\t*p;\n\tunsigned long\ti = hweight_long(layout_types);\n\n\tp = xdr_reserve_space(xdr, 4 + 4 * i);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t*p++ = cpu_to_be32(i);\n\n\tfor (i = LAYOUT_NFSV4_1_FILES; i < LAYOUT_TYPE_MAX; ++i)\n\t\tif (layout_types & (1 << i))\n\t\t\t*p++ = cpu_to_be32(i);\n\n\treturn 0;\n}\n\n#define WORD0_ABSENT_FS_ATTRS (FATTR4_WORD0_FS_LOCATIONS | FATTR4_WORD0_FSID | \\\n\t\t\t      FATTR4_WORD0_RDATTR_ERROR)\n#define WORD1_ABSENT_FS_ATTRS FATTR4_WORD1_MOUNTED_ON_FILEID\n#define WORD2_ABSENT_FS_ATTRS 0\n\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\nstatic inline __be32\nnfsd4_encode_security_label(struct xdr_stream *xdr, struct svc_rqst *rqstp,\n\t\t\t    void *context, int len)\n{\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, len + 4 + 4 + 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t/*\n\t * For now we use a 0 here to indicate the null translation; in\n\t * the future we may place a call to translation code here.\n\t */\n\t*p++ = cpu_to_be32(0); /* lfs */\n\t*p++ = cpu_to_be32(0); /* pi */\n\tp = xdr_encode_opaque(p, context, len);\n\treturn 0;\n}\n#else\nstatic inline __be32\nnfsd4_encode_security_label(struct xdr_stream *xdr, struct svc_rqst *rqstp,\n\t\t\t    void *context, int len)\n{ return 0; }\n#endif\n\nstatic __be32 fattr_handle_absent_fs(u32 *bmval0, u32 *bmval1, u32 *bmval2, u32 *rdattr_err)\n{\n\t/* As per referral draft:  */\n\tif (*bmval0 & ~WORD0_ABSENT_FS_ATTRS ||\n\t    *bmval1 & ~WORD1_ABSENT_FS_ATTRS) {\n\t\tif (*bmval0 & FATTR4_WORD0_RDATTR_ERROR ||\n\t            *bmval0 & FATTR4_WORD0_FS_LOCATIONS)\n\t\t\t*rdattr_err = NFSERR_MOVED;\n\t\telse\n\t\t\treturn nfserr_moved;\n\t}\n\t*bmval0 &= WORD0_ABSENT_FS_ATTRS;\n\t*bmval1 &= WORD1_ABSENT_FS_ATTRS;\n\t*bmval2 &= WORD2_ABSENT_FS_ATTRS;\n\treturn 0;\n}\n\n\nstatic int get_parent_attributes(struct svc_export *exp, struct kstat *stat)\n{\n\tstruct path path = exp->ex_path;\n\tint err;\n\n\tpath_get(&path);\n\twhile (follow_up(&path)) {\n\t\tif (path.dentry != path.mnt->mnt_root)\n\t\t\tbreak;\n\t}\n\terr = vfs_getattr(&path, stat, STATX_BASIC_STATS, AT_STATX_SYNC_AS_STAT);\n\tpath_put(&path);\n\treturn err;\n}\n\nstatic __be32\nnfsd4_encode_bitmap(struct xdr_stream *xdr, u32 bmval0, u32 bmval1, u32 bmval2)\n{\n\t__be32 *p;\n\n\tif (bmval2) {\n\t\tp = xdr_reserve_space(xdr, 16);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(3);\n\t\t*p++ = cpu_to_be32(bmval0);\n\t\t*p++ = cpu_to_be32(bmval1);\n\t\t*p++ = cpu_to_be32(bmval2);\n\t} else if (bmval1) {\n\t\tp = xdr_reserve_space(xdr, 12);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(2);\n\t\t*p++ = cpu_to_be32(bmval0);\n\t\t*p++ = cpu_to_be32(bmval1);\n\t} else {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t\t*p++ = cpu_to_be32(bmval0);\n\t}\n\n\treturn 0;\nout_resource:\n\treturn nfserr_resource;\n}\n\n/*\n * Note: @fhp can be NULL; in this case, we might have to compose the filehandle\n * ourselves.\n */\nstatic __be32\nnfsd4_encode_fattr(struct xdr_stream *xdr, struct svc_fh *fhp,\n\t\tstruct svc_export *exp,\n\t\tstruct dentry *dentry, u32 *bmval,\n\t\tstruct svc_rqst *rqstp, int ignore_crossmnt)\n{\n\tu32 bmval0 = bmval[0];\n\tu32 bmval1 = bmval[1];\n\tu32 bmval2 = bmval[2];\n\tstruct kstat stat;\n\tstruct svc_fh *tempfh = NULL;\n\tstruct kstatfs statfs;\n\t__be32 *p;\n\tint starting_len = xdr->buf->len;\n\tint attrlen_offset;\n\t__be32 attrlen;\n\tu32 dummy;\n\tu64 dummy64;\n\tu32 rdattr_err = 0;\n\t__be32 status;\n\tint err;\n\tstruct nfs4_acl *acl = NULL;\n\tvoid *context = NULL;\n\tint contextlen;\n\tbool contextsupport = false;\n\tstruct nfsd4_compoundres *resp = rqstp->rq_resp;\n\tu32 minorversion = resp->cstate.minorversion;\n\tstruct path path = {\n\t\t.mnt\t= exp->ex_path.mnt,\n\t\t.dentry\t= dentry,\n\t};\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tBUG_ON(bmval1 & NFSD_WRITEONLY_ATTRS_WORD1);\n\tBUG_ON(!nfsd_attrs_supported(minorversion, bmval));\n\n\tif (exp->ex_fslocs.migrated) {\n\t\tstatus = fattr_handle_absent_fs(&bmval0, &bmval1, &bmval2, &rdattr_err);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\terr = vfs_getattr(&path, &stat, STATX_BASIC_STATS, AT_STATX_SYNC_AS_STAT);\n\tif (err)\n\t\tgoto out_nfserr;\n\tif ((bmval0 & (FATTR4_WORD0_FILES_AVAIL | FATTR4_WORD0_FILES_FREE |\n\t\t\tFATTR4_WORD0_FILES_TOTAL | FATTR4_WORD0_MAXNAME)) ||\n\t    (bmval1 & (FATTR4_WORD1_SPACE_AVAIL | FATTR4_WORD1_SPACE_FREE |\n\t\t       FATTR4_WORD1_SPACE_TOTAL))) {\n\t\terr = vfs_statfs(&path, &statfs);\n\t\tif (err)\n\t\t\tgoto out_nfserr;\n\t}\n\tif ((bmval0 & (FATTR4_WORD0_FILEHANDLE | FATTR4_WORD0_FSID)) && !fhp) {\n\t\ttempfh = kmalloc(sizeof(struct svc_fh), GFP_KERNEL);\n\t\tstatus = nfserr_jukebox;\n\t\tif (!tempfh)\n\t\t\tgoto out;\n\t\tfh_init(tempfh, NFS4_FHSIZE);\n\t\tstatus = fh_compose(tempfh, exp, dentry, NULL);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tfhp = tempfh;\n\t}\n\tif (bmval0 & FATTR4_WORD0_ACL) {\n\t\terr = nfsd4_get_nfs4_acl(rqstp, dentry, &acl);\n\t\tif (err == -EOPNOTSUPP)\n\t\t\tbmval0 &= ~FATTR4_WORD0_ACL;\n\t\telse if (err == -EINVAL) {\n\t\t\tstatus = nfserr_attrnotsupp;\n\t\t\tgoto out;\n\t\t} else if (err != 0)\n\t\t\tgoto out_nfserr;\n\t}\n\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n\tif ((bmval2 & FATTR4_WORD2_SECURITY_LABEL) ||\n\t     bmval0 & FATTR4_WORD0_SUPPORTED_ATTRS) {\n\t\tif (exp->ex_flags & NFSEXP_SECURITY_LABEL)\n\t\t\terr = security_inode_getsecctx(d_inode(dentry),\n\t\t\t\t\t\t&context, &contextlen);\n\t\telse\n\t\t\terr = -EOPNOTSUPP;\n\t\tcontextsupport = (err == 0);\n\t\tif (bmval2 & FATTR4_WORD2_SECURITY_LABEL) {\n\t\t\tif (err == -EOPNOTSUPP)\n\t\t\t\tbmval2 &= ~FATTR4_WORD2_SECURITY_LABEL;\n\t\t\telse if (err)\n\t\t\t\tgoto out_nfserr;\n\t\t}\n\t}\n#endif /* CONFIG_NFSD_V4_SECURITY_LABEL */\n\n\tstatus = nfsd4_encode_bitmap(xdr, bmval0, bmval1, bmval2);\n\tif (status)\n\t\tgoto out;\n\n\tattrlen_offset = xdr->buf->len;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\tgoto out_resource;\n\tp++;                /* to be backfilled later */\n\n\tif (bmval0 & FATTR4_WORD0_SUPPORTED_ATTRS) {\n\t\tu32 supp[3];\n\n\t\tmemcpy(supp, nfsd_suppattrs[minorversion], sizeof(supp));\n\n\t\tif (!IS_POSIXACL(dentry->d_inode))\n\t\t\tsupp[0] &= ~FATTR4_WORD0_ACL;\n\t\tif (!contextsupport)\n\t\t\tsupp[2] &= ~FATTR4_WORD2_SECURITY_LABEL;\n\t\tif (!supp[2]) {\n\t\t\tp = xdr_reserve_space(xdr, 12);\n\t\t\tif (!p)\n\t\t\t\tgoto out_resource;\n\t\t\t*p++ = cpu_to_be32(2);\n\t\t\t*p++ = cpu_to_be32(supp[0]);\n\t\t\t*p++ = cpu_to_be32(supp[1]);\n\t\t} else {\n\t\t\tp = xdr_reserve_space(xdr, 16);\n\t\t\tif (!p)\n\t\t\t\tgoto out_resource;\n\t\t\t*p++ = cpu_to_be32(3);\n\t\t\t*p++ = cpu_to_be32(supp[0]);\n\t\t\t*p++ = cpu_to_be32(supp[1]);\n\t\t\t*p++ = cpu_to_be32(supp[2]);\n\t\t}\n\t}\n\tif (bmval0 & FATTR4_WORD0_TYPE) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tdummy = nfs4_file_type(stat.mode);\n\t\tif (dummy == NF4BAD) {\n\t\t\tstatus = nfserr_serverfault;\n\t\t\tgoto out;\n\t\t}\n\t\t*p++ = cpu_to_be32(dummy);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FH_EXPIRE_TYPE) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tif (exp->ex_flags & NFSEXP_NOSUBTREECHECK)\n\t\t\t*p++ = cpu_to_be32(NFS4_FH_PERSISTENT);\n\t\telse\n\t\t\t*p++ = cpu_to_be32(NFS4_FH_PERSISTENT|\n\t\t\t\t\t\tNFS4_FH_VOL_RENAME);\n\t}\n\tif (bmval0 & FATTR4_WORD0_CHANGE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = encode_change(p, &stat, d_inode(dentry), exp);\n\t}\n\tif (bmval0 & FATTR4_WORD0_SIZE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, stat.size);\n\t}\n\tif (bmval0 & FATTR4_WORD0_LINK_SUPPORT) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_SYMLINK_SUPPORT) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_NAMED_ATTR) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FSID) {\n\t\tp = xdr_reserve_space(xdr, 16);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tif (exp->ex_fslocs.migrated) {\n\t\t\tp = xdr_encode_hyper(p, NFS4_REFERRAL_FSID_MAJOR);\n\t\t\tp = xdr_encode_hyper(p, NFS4_REFERRAL_FSID_MINOR);\n\t\t} else switch(fsid_source(fhp)) {\n\t\tcase FSIDSOURCE_FSID:\n\t\t\tp = xdr_encode_hyper(p, (u64)exp->ex_fsid);\n\t\t\tp = xdr_encode_hyper(p, (u64)0);\n\t\t\tbreak;\n\t\tcase FSIDSOURCE_DEV:\n\t\t\t*p++ = cpu_to_be32(0);\n\t\t\t*p++ = cpu_to_be32(MAJOR(stat.dev));\n\t\t\t*p++ = cpu_to_be32(0);\n\t\t\t*p++ = cpu_to_be32(MINOR(stat.dev));\n\t\t\tbreak;\n\t\tcase FSIDSOURCE_UUID:\n\t\t\tp = xdr_encode_opaque_fixed(p, exp->ex_uuid,\n\t\t\t\t\t\t\t\tEX_UUID_LEN);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (bmval0 & FATTR4_WORD0_UNIQUE_HANDLES) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\tif (bmval0 & FATTR4_WORD0_LEASE_TIME) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(nn->nfsd4_lease);\n\t}\n\tif (bmval0 & FATTR4_WORD0_RDATTR_ERROR) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(rdattr_err);\n\t}\n\tif (bmval0 & FATTR4_WORD0_ACL) {\n\t\tstruct nfs4_ace *ace;\n\n\t\tif (acl == NULL) {\n\t\t\tp = xdr_reserve_space(xdr, 4);\n\t\t\tif (!p)\n\t\t\t\tgoto out_resource;\n\n\t\t\t*p++ = cpu_to_be32(0);\n\t\t\tgoto out_acl;\n\t\t}\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(acl->naces);\n\n\t\tfor (ace = acl->aces; ace < acl->aces + acl->naces; ace++) {\n\t\t\tp = xdr_reserve_space(xdr, 4*3);\n\t\t\tif (!p)\n\t\t\t\tgoto out_resource;\n\t\t\t*p++ = cpu_to_be32(ace->type);\n\t\t\t*p++ = cpu_to_be32(ace->flag);\n\t\t\t*p++ = cpu_to_be32(ace->access_mask &\n\t\t\t\t\t\t\tNFS4_ACE_MASK_ALL);\n\t\t\tstatus = nfsd4_encode_aclname(xdr, rqstp, ace);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout_acl:\n\tif (bmval0 & FATTR4_WORD0_ACLSUPPORT) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(IS_POSIXACL(dentry->d_inode) ?\n\t\t\tACL4_SUPPORT_ALLOW_ACL|ACL4_SUPPORT_DENY_ACL : 0);\n\t}\n\tif (bmval0 & FATTR4_WORD0_CANSETTIME) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_CASE_INSENSITIVE) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\tif (bmval0 & FATTR4_WORD0_CASE_PRESERVING) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_CHOWN_RESTRICTED) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FILEHANDLE) {\n\t\tp = xdr_reserve_space(xdr, fhp->fh_handle.fh_size + 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_opaque(p, &fhp->fh_handle.fh_base,\n\t\t\t\t\tfhp->fh_handle.fh_size);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FILEID) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, stat.ino);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FILES_AVAIL) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (u64) statfs.f_ffree);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FILES_FREE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (u64) statfs.f_ffree);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FILES_TOTAL) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (u64) statfs.f_files);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FS_LOCATIONS) {\n\t\tstatus = nfsd4_encode_fs_locations(xdr, rqstp, exp);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\tif (bmval0 & FATTR4_WORD0_HOMOGENEOUS) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_MAXFILESIZE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, exp->ex_path.mnt->mnt_sb->s_maxbytes);\n\t}\n\tif (bmval0 & FATTR4_WORD0_MAXLINK) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(255);\n\t}\n\tif (bmval0 & FATTR4_WORD0_MAXNAME) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(statfs.f_namelen);\n\t}\n\tif (bmval0 & FATTR4_WORD0_MAXREAD) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (u64) svc_max_payload(rqstp));\n\t}\n\tif (bmval0 & FATTR4_WORD0_MAXWRITE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (u64) svc_max_payload(rqstp));\n\t}\n\tif (bmval1 & FATTR4_WORD1_MODE) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(stat.mode & S_IALLUGO);\n\t}\n\tif (bmval1 & FATTR4_WORD1_NO_TRUNC) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval1 & FATTR4_WORD1_NUMLINKS) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(stat.nlink);\n\t}\n\tif (bmval1 & FATTR4_WORD1_OWNER) {\n\t\tstatus = nfsd4_encode_user(xdr, rqstp, stat.uid);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\tif (bmval1 & FATTR4_WORD1_OWNER_GROUP) {\n\t\tstatus = nfsd4_encode_group(xdr, rqstp, stat.gid);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\tif (bmval1 & FATTR4_WORD1_RAWDEV) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32((u32) MAJOR(stat.rdev));\n\t\t*p++ = cpu_to_be32((u32) MINOR(stat.rdev));\n\t}\n\tif (bmval1 & FATTR4_WORD1_SPACE_AVAIL) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tdummy64 = (u64)statfs.f_bavail * (u64)statfs.f_bsize;\n\t\tp = xdr_encode_hyper(p, dummy64);\n\t}\n\tif (bmval1 & FATTR4_WORD1_SPACE_FREE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tdummy64 = (u64)statfs.f_bfree * (u64)statfs.f_bsize;\n\t\tp = xdr_encode_hyper(p, dummy64);\n\t}\n\tif (bmval1 & FATTR4_WORD1_SPACE_TOTAL) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tdummy64 = (u64)statfs.f_blocks * (u64)statfs.f_bsize;\n\t\tp = xdr_encode_hyper(p, dummy64);\n\t}\n\tif (bmval1 & FATTR4_WORD1_SPACE_USED) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tdummy64 = (u64)stat.blocks << 9;\n\t\tp = xdr_encode_hyper(p, dummy64);\n\t}\n\tif (bmval1 & FATTR4_WORD1_TIME_ACCESS) {\n\t\tp = xdr_reserve_space(xdr, 12);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (s64)stat.atime.tv_sec);\n\t\t*p++ = cpu_to_be32(stat.atime.tv_nsec);\n\t}\n\tif (bmval1 & FATTR4_WORD1_TIME_DELTA) {\n\t\tp = xdr_reserve_space(xdr, 12);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(1);\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\tif (bmval1 & FATTR4_WORD1_TIME_METADATA) {\n\t\tp = xdr_reserve_space(xdr, 12);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (s64)stat.ctime.tv_sec);\n\t\t*p++ = cpu_to_be32(stat.ctime.tv_nsec);\n\t}\n\tif (bmval1 & FATTR4_WORD1_TIME_MODIFY) {\n\t\tp = xdr_reserve_space(xdr, 12);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (s64)stat.mtime.tv_sec);\n\t\t*p++ = cpu_to_be32(stat.mtime.tv_nsec);\n\t}\n\tif (bmval1 & FATTR4_WORD1_MOUNTED_ON_FILEID) {\n\t\tstruct kstat parent_stat;\n\t\tu64 ino = stat.ino;\n\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n                \tgoto out_resource;\n\t\t/*\n\t\t * Get parent's attributes if not ignoring crossmount\n\t\t * and this is the root of a cross-mounted filesystem.\n\t\t */\n\t\tif (ignore_crossmnt == 0 &&\n\t\t    dentry == exp->ex_path.mnt->mnt_root) {\n\t\t\terr = get_parent_attributes(exp, &parent_stat);\n\t\t\tif (err)\n\t\t\t\tgoto out_nfserr;\n\t\t\tino = parent_stat.ino;\n\t\t}\n\t\tp = xdr_encode_hyper(p, ino);\n\t}\n#ifdef CONFIG_NFSD_PNFS\n\tif (bmval1 & FATTR4_WORD1_FS_LAYOUT_TYPES) {\n\t\tstatus = nfsd4_encode_layout_types(xdr, exp->ex_layout_types);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tif (bmval2 & FATTR4_WORD2_LAYOUT_TYPES) {\n\t\tstatus = nfsd4_encode_layout_types(xdr, exp->ex_layout_types);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tif (bmval2 & FATTR4_WORD2_LAYOUT_BLKSIZE) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(stat.blksize);\n\t}\n#endif /* CONFIG_NFSD_PNFS */\n\tif (bmval2 & FATTR4_WORD2_SUPPATTR_EXCLCREAT) {\n\t\tstatus = nfsd4_encode_bitmap(xdr, NFSD_SUPPATTR_EXCLCREAT_WORD0,\n\t\t\t\t\t\t  NFSD_SUPPATTR_EXCLCREAT_WORD1,\n\t\t\t\t\t\t  NFSD_SUPPATTR_EXCLCREAT_WORD2);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tif (bmval2 & FATTR4_WORD2_SECURITY_LABEL) {\n\t\tstatus = nfsd4_encode_security_label(xdr, rqstp, context,\n\t\t\t\t\t\t\t\tcontextlen);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tattrlen = htonl(xdr->buf->len - attrlen_offset - 4);\n\twrite_bytes_to_xdr_buf(xdr->buf, attrlen_offset, &attrlen, 4);\n\tstatus = nfs_ok;\n\nout:\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n\tif (context)\n\t\tsecurity_release_secctx(context, contextlen);\n#endif /* CONFIG_NFSD_V4_SECURITY_LABEL */\n\tkfree(acl);\n\tif (tempfh) {\n\t\tfh_put(tempfh);\n\t\tkfree(tempfh);\n\t}\n\tif (status)\n\t\txdr_truncate_encode(xdr, starting_len);\n\treturn status;\nout_nfserr:\n\tstatus = nfserrno(err);\n\tgoto out;\nout_resource:\n\tstatus = nfserr_resource;\n\tgoto out;\n}\n\nstatic void svcxdr_init_encode_from_buffer(struct xdr_stream *xdr,\n\t\t\t\tstruct xdr_buf *buf, __be32 *p, int bytes)\n{\n\txdr->scratch.iov_len = 0;\n\tmemset(buf, 0, sizeof(struct xdr_buf));\n\tbuf->head[0].iov_base = p;\n\tbuf->head[0].iov_len = 0;\n\tbuf->len = 0;\n\txdr->buf = buf;\n\txdr->iov = buf->head;\n\txdr->p = p;\n\txdr->end = (void *)p + bytes;\n\tbuf->buflen = bytes;\n}\n\n__be32 nfsd4_encode_fattr_to_buf(__be32 **p, int words,\n\t\t\tstruct svc_fh *fhp, struct svc_export *exp,\n\t\t\tstruct dentry *dentry, u32 *bmval,\n\t\t\tstruct svc_rqst *rqstp, int ignore_crossmnt)\n{\n\tstruct xdr_buf dummy;\n\tstruct xdr_stream xdr;\n\t__be32 ret;\n\n\tsvcxdr_init_encode_from_buffer(&xdr, &dummy, *p, words << 2);\n\tret = nfsd4_encode_fattr(&xdr, fhp, exp, dentry, bmval, rqstp,\n\t\t\t\t\t\t\tignore_crossmnt);\n\t*p = xdr.p;\n\treturn ret;\n}\n\nstatic inline int attributes_need_mount(u32 *bmval)\n{\n\tif (bmval[0] & ~(FATTR4_WORD0_RDATTR_ERROR | FATTR4_WORD0_LEASE_TIME))\n\t\treturn 1;\n\tif (bmval[1] & ~FATTR4_WORD1_MOUNTED_ON_FILEID)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic __be32\nnfsd4_encode_dirent_fattr(struct xdr_stream *xdr, struct nfsd4_readdir *cd,\n\t\t\tconst char *name, int namlen)\n{\n\tstruct svc_export *exp = cd->rd_fhp->fh_export;\n\tstruct dentry *dentry;\n\t__be32 nfserr;\n\tint ignore_crossmnt = 0;\n\n\tdentry = lookup_one_len_unlocked(name, cd->rd_fhp->fh_dentry, namlen);\n\tif (IS_ERR(dentry))\n\t\treturn nfserrno(PTR_ERR(dentry));\n\tif (d_really_is_negative(dentry)) {\n\t\t/*\n\t\t * we're not holding the i_mutex here, so there's\n\t\t * a window where this directory entry could have gone\n\t\t * away.\n\t\t */\n\t\tdput(dentry);\n\t\treturn nfserr_noent;\n\t}\n\n\texp_get(exp);\n\t/*\n\t * In the case of a mountpoint, the client may be asking for\n\t * attributes that are only properties of the underlying filesystem\n\t * as opposed to the cross-mounted file system. In such a case,\n\t * we will not follow the cross mount and will fill the attribtutes\n\t * directly from the mountpoint dentry.\n\t */\n\tif (nfsd_mountpoint(dentry, exp)) {\n\t\tint err;\n\n\t\tif (!(exp->ex_flags & NFSEXP_V4ROOT)\n\t\t\t\t&& !attributes_need_mount(cd->rd_bmval)) {\n\t\t\tignore_crossmnt = 1;\n\t\t\tgoto out_encode;\n\t\t}\n\t\t/*\n\t\t * Why the heck aren't we just using nfsd_lookup??\n\t\t * Different \".\"/\"..\" handling?  Something else?\n\t\t * At least, add a comment here to explain....\n\t\t */\n\t\terr = nfsd_cross_mnt(cd->rd_rqstp, &dentry, &exp);\n\t\tif (err) {\n\t\t\tnfserr = nfserrno(err);\n\t\t\tgoto out_put;\n\t\t}\n\t\tnfserr = check_nfsd_access(exp, cd->rd_rqstp);\n\t\tif (nfserr)\n\t\t\tgoto out_put;\n\n\t}\nout_encode:\n\tnfserr = nfsd4_encode_fattr(xdr, NULL, exp, dentry, cd->rd_bmval,\n\t\t\t\t\tcd->rd_rqstp, ignore_crossmnt);\nout_put:\n\tdput(dentry);\n\texp_put(exp);\n\treturn nfserr;\n}\n\nstatic __be32 *\nnfsd4_encode_rdattr_error(struct xdr_stream *xdr, __be32 nfserr)\n{\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, 20);\n\tif (!p)\n\t\treturn NULL;\n\t*p++ = htonl(2);\n\t*p++ = htonl(FATTR4_WORD0_RDATTR_ERROR); /* bmval0 */\n\t*p++ = htonl(0);\t\t\t /* bmval1 */\n\n\t*p++ = htonl(4);     /* attribute length */\n\t*p++ = nfserr;       /* no htonl */\n\treturn p;\n}\n\nstatic int\nnfsd4_encode_dirent(void *ccdv, const char *name, int namlen,\n\t\t    loff_t offset, u64 ino, unsigned int d_type)\n{\n\tstruct readdir_cd *ccd = ccdv;\n\tstruct nfsd4_readdir *cd = container_of(ccd, struct nfsd4_readdir, common);\n\tstruct xdr_stream *xdr = cd->xdr;\n\tint start_offset = xdr->buf->len;\n\tint cookie_offset;\n\tu32 name_and_cookie;\n\tint entry_bytes;\n\t__be32 nfserr = nfserr_toosmall;\n\t__be64 wire_offset;\n\t__be32 *p;\n\n\t/* In nfsv4, \".\" and \"..\" never make it onto the wire.. */\n\tif (name && isdotent(name, namlen)) {\n\t\tcd->common.err = nfs_ok;\n\t\treturn 0;\n\t}\n\n\tif (cd->cookie_offset) {\n\t\twire_offset = cpu_to_be64(offset);\n\t\twrite_bytes_to_xdr_buf(xdr->buf, cd->cookie_offset,\n\t\t\t\t\t\t\t&wire_offset, 8);\n\t}\n\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\tgoto fail;\n\t*p++ = xdr_one;                             /* mark entry present */\n\tcookie_offset = xdr->buf->len;\n\tp = xdr_reserve_space(xdr, 3*4 + namlen);\n\tif (!p)\n\t\tgoto fail;\n\tp = xdr_encode_hyper(p, NFS_OFFSET_MAX);    /* offset of next entry */\n\tp = xdr_encode_array(p, name, namlen);      /* name length & name */\n\n\tnfserr = nfsd4_encode_dirent_fattr(xdr, cd, name, namlen);\n\tswitch (nfserr) {\n\tcase nfs_ok:\n\t\tbreak;\n\tcase nfserr_resource:\n\t\tnfserr = nfserr_toosmall;\n\t\tgoto fail;\n\tcase nfserr_noent:\n\t\txdr_truncate_encode(xdr, start_offset);\n\t\tgoto skip_entry;\n\tdefault:\n\t\t/*\n\t\t * If the client requested the RDATTR_ERROR attribute,\n\t\t * we stuff the error code into this attribute\n\t\t * and continue.  If this attribute was not requested,\n\t\t * then in accordance with the spec, we fail the\n\t\t * entire READDIR operation(!)\n\t\t */\n\t\tif (!(cd->rd_bmval[0] & FATTR4_WORD0_RDATTR_ERROR))\n\t\t\tgoto fail;\n\t\tp = nfsd4_encode_rdattr_error(xdr, nfserr);\n\t\tif (p == NULL) {\n\t\t\tnfserr = nfserr_toosmall;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tnfserr = nfserr_toosmall;\n\tentry_bytes = xdr->buf->len - start_offset;\n\tif (entry_bytes > cd->rd_maxcount)\n\t\tgoto fail;\n\tcd->rd_maxcount -= entry_bytes;\n\t/*\n\t * RFC 3530 14.2.24 describes rd_dircount as only a \"hint\", so\n\t * let's always let through the first entry, at least:\n\t */\n\tif (!cd->rd_dircount)\n\t\tgoto fail;\n\tname_and_cookie = 4 + 4 * XDR_QUADLEN(namlen) + 8;\n\tif (name_and_cookie > cd->rd_dircount && cd->cookie_offset)\n\t\tgoto fail;\n\tcd->rd_dircount -= min(cd->rd_dircount, name_and_cookie);\n\n\tcd->cookie_offset = cookie_offset;\nskip_entry:\n\tcd->common.err = nfs_ok;\n\treturn 0;\nfail:\n\txdr_truncate_encode(xdr, start_offset);\n\tcd->common.err = nfserr;\n\treturn -EINVAL;\n}\n\nstatic __be32\nnfsd4_encode_stateid(struct xdr_stream *xdr, stateid_t *sid)\n{\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, sizeof(stateid_t));\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(sid->si_generation);\n\tp = xdr_encode_opaque_fixed(p, &sid->si_opaque,\n\t\t\t\t\tsizeof(stateid_opaque_t));\n\treturn 0;\n}\n\nstatic __be32\nnfsd4_encode_access(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_access *access)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(access->ac_supported);\n\t\t*p++ = cpu_to_be32(access->ac_resp_access);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32 nfsd4_encode_bind_conn_to_session(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_bind_conn_to_session *bcts)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, NFS4_MAX_SESSIONID_LEN + 8);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = xdr_encode_opaque_fixed(p, bcts->sessionid.data,\n\t\t\t\t\t\tNFS4_MAX_SESSIONID_LEN);\n\t\t*p++ = cpu_to_be32(bcts->dir);\n\t\t/* Upshifting from TCP to RDMA is not supported */\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_close(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_close *close)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (!nfserr)\n\t\tnfserr = nfsd4_encode_stateid(xdr, &close->cl_stateid);\n\n\treturn nfserr;\n}\n\n\nstatic __be32\nnfsd4_encode_commit(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_commit *commit)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, NFS4_VERIFIER_SIZE);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = xdr_encode_opaque_fixed(p, commit->co_verf.data,\n\t\t\t\t\t\tNFS4_VERIFIER_SIZE);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_create(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_create *create)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 20);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tencode_cinfo(p, &create->cr_cinfo);\n\t\tnfserr = nfsd4_encode_bitmap(xdr, create->cr_bmval[0],\n\t\t\t\tcreate->cr_bmval[1], create->cr_bmval[2]);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_getattr(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_getattr *getattr)\n{\n\tstruct svc_fh *fhp = getattr->ga_fhp;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tnfserr = nfsd4_encode_fattr(xdr, fhp, fhp->fh_export, fhp->fh_dentry,\n\t\t\t\t    getattr->ga_bmval,\n\t\t\t\t    resp->rqstp, 0);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_getfh(struct nfsd4_compoundres *resp, __be32 nfserr, struct svc_fh **fhpp)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct svc_fh *fhp = *fhpp;\n\tunsigned int len;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tlen = fhp->fh_handle.fh_size;\n\t\tp = xdr_reserve_space(xdr, len + 4);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = xdr_encode_opaque(p, &fhp->fh_handle.fh_base, len);\n\t}\n\treturn nfserr;\n}\n\n/*\n* Including all fields other than the name, a LOCK4denied structure requires\n*   8(clientid) + 4(namelen) + 8(offset) + 8(length) + 4(type) = 32 bytes.\n*/\nstatic __be32\nnfsd4_encode_lock_denied(struct xdr_stream *xdr, struct nfsd4_lock_denied *ld)\n{\n\tstruct xdr_netobj *conf = &ld->ld_owner;\n\t__be32 *p;\n\nagain:\n\tp = xdr_reserve_space(xdr, 32 + XDR_LEN(conf->len));\n\tif (!p) {\n\t\t/*\n\t\t * Don't fail to return the result just because we can't\n\t\t * return the conflicting open:\n\t\t */\n\t\tif (conf->len) {\n\t\t\tkfree(conf->data);\n\t\t\tconf->len = 0;\n\t\t\tconf->data = NULL;\n\t\t\tgoto again;\n\t\t}\n\t\treturn nfserr_resource;\n\t}\n\tp = xdr_encode_hyper(p, ld->ld_start);\n\tp = xdr_encode_hyper(p, ld->ld_length);\n\t*p++ = cpu_to_be32(ld->ld_type);\n\tif (conf->len) {\n\t\tp = xdr_encode_opaque_fixed(p, &ld->ld_clientid, 8);\n\t\tp = xdr_encode_opaque(p, conf->data, conf->len);\n\t\tkfree(conf->data);\n\t}  else {  /* non - nfsv4 lock in conflict, no clientid nor owner */\n\t\tp = xdr_encode_hyper(p, (u64)0); /* clientid */\n\t\t*p++ = cpu_to_be32(0); /* length of owner name */\n\t}\n\treturn nfserr_denied;\n}\n\nstatic __be32\nnfsd4_encode_lock(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_lock *lock)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (!nfserr)\n\t\tnfserr = nfsd4_encode_stateid(xdr, &lock->lk_resp_stateid);\n\telse if (nfserr == nfserr_denied)\n\t\tnfserr = nfsd4_encode_lock_denied(xdr, &lock->lk_denied);\n\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_lockt(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_lockt *lockt)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (nfserr == nfserr_denied)\n\t\tnfsd4_encode_lock_denied(xdr, &lockt->lt_denied);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_locku(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_locku *locku)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (!nfserr)\n\t\tnfserr = nfsd4_encode_stateid(xdr, &locku->lu_stateid);\n\n\treturn nfserr;\n}\n\n\nstatic __be32\nnfsd4_encode_link(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_link *link)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 20);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = encode_cinfo(p, &link->li_cinfo);\n\t}\n\treturn nfserr;\n}\n\n\nstatic __be32\nnfsd4_encode_open(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_open *open)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfsd4_encode_stateid(xdr, &open->op_stateid);\n\tif (nfserr)\n\t\tgoto out;\n\tp = xdr_reserve_space(xdr, 24);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tp = encode_cinfo(p, &open->op_cinfo);\n\t*p++ = cpu_to_be32(open->op_rflags);\n\n\tnfserr = nfsd4_encode_bitmap(xdr, open->op_bmval[0], open->op_bmval[1],\n\t\t\t\t\topen->op_bmval[2]);\n\tif (nfserr)\n\t\tgoto out;\n\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t*p++ = cpu_to_be32(open->op_delegate_type);\n\tswitch (open->op_delegate_type) {\n\tcase NFS4_OPEN_DELEGATE_NONE:\n\t\tbreak;\n\tcase NFS4_OPEN_DELEGATE_READ:\n\t\tnfserr = nfsd4_encode_stateid(xdr, &open->op_delegate_stateid);\n\t\tif (nfserr)\n\t\t\treturn nfserr;\n\t\tp = xdr_reserve_space(xdr, 20);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(open->op_recall);\n\n\t\t/*\n\t\t * TODO: ACE's in delegations\n\t\t */\n\t\t*p++ = cpu_to_be32(NFS4_ACE_ACCESS_ALLOWED_ACE_TYPE);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);   /* XXX: is NULL principal ok? */\n\t\tbreak;\n\tcase NFS4_OPEN_DELEGATE_WRITE:\n\t\tnfserr = nfsd4_encode_stateid(xdr, &open->op_delegate_stateid);\n\t\tif (nfserr)\n\t\t\treturn nfserr;\n\t\tp = xdr_reserve_space(xdr, 32);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(0);\n\n\t\t/*\n\t\t * TODO: space_limit's in delegations\n\t\t */\n\t\t*p++ = cpu_to_be32(NFS4_LIMIT_SIZE);\n\t\t*p++ = cpu_to_be32(~(u32)0);\n\t\t*p++ = cpu_to_be32(~(u32)0);\n\n\t\t/*\n\t\t * TODO: ACE's in delegations\n\t\t */\n\t\t*p++ = cpu_to_be32(NFS4_ACE_ACCESS_ALLOWED_ACE_TYPE);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);   /* XXX: is NULL principal ok? */\n\t\tbreak;\n\tcase NFS4_OPEN_DELEGATE_NONE_EXT: /* 4.1 */\n\t\tswitch (open->op_why_no_deleg) {\n\t\tcase WND4_CONTENTION:\n\t\tcase WND4_RESOURCE:\n\t\t\tp = xdr_reserve_space(xdr, 8);\n\t\t\tif (!p)\n\t\t\t\treturn nfserr_resource;\n\t\t\t*p++ = cpu_to_be32(open->op_why_no_deleg);\n\t\t\t/* deleg signaling not supported yet: */\n\t\t\t*p++ = cpu_to_be32(0);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tp = xdr_reserve_space(xdr, 4);\n\t\t\tif (!p)\n\t\t\t\treturn nfserr_resource;\n\t\t\t*p++ = cpu_to_be32(open->op_why_no_deleg);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\t/* XXX save filehandle here */\nout:\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_open_confirm(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_open_confirm *oc)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (!nfserr)\n\t\tnfserr = nfsd4_encode_stateid(xdr, &oc->oc_resp_stateid);\n\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_open_downgrade(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_open_downgrade *od)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (!nfserr)\n\t\tnfserr = nfsd4_encode_stateid(xdr, &od->od_stateid);\n\n\treturn nfserr;\n}\n\nstatic __be32 nfsd4_encode_splice_read(\n\t\t\t\tstruct nfsd4_compoundres *resp,\n\t\t\t\tstruct nfsd4_read *read,\n\t\t\t\tstruct file *file, unsigned long maxcount)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct xdr_buf *buf = xdr->buf;\n\tu32 eof;\n\tlong len;\n\tint space_left;\n\t__be32 nfserr;\n\t__be32 *p = xdr->p - 2;\n\n\t/* Make sure there will be room for padding if needed */\n\tif (xdr->end - xdr->p < 1)\n\t\treturn nfserr_resource;\n\n\tlen = maxcount;\n\tnfserr = nfsd_splice_read(read->rd_rqstp, file,\n\t\t\t\t  read->rd_offset, &maxcount);\n\tif (nfserr) {\n\t\t/*\n\t\t * nfsd_splice_actor may have already messed with the\n\t\t * page length; reset it so as not to confuse\n\t\t * xdr_truncate_encode:\n\t\t */\n\t\tbuf->page_len = 0;\n\t\treturn nfserr;\n\t}\n\n\teof = nfsd_eof_on_read(len, maxcount, read->rd_offset,\n\t\t\t\td_inode(read->rd_fhp->fh_dentry)->i_size);\n\n\t*(p++) = htonl(eof);\n\t*(p++) = htonl(maxcount);\n\n\tbuf->page_len = maxcount;\n\tbuf->len += maxcount;\n\txdr->page_ptr += (buf->page_base + maxcount + PAGE_SIZE - 1)\n\t\t\t\t\t\t\t/ PAGE_SIZE;\n\n\t/* Use rest of head for padding and remaining ops: */\n\tbuf->tail[0].iov_base = xdr->p;\n\tbuf->tail[0].iov_len = 0;\n\txdr->iov = buf->tail;\n\tif (maxcount&3) {\n\t\tint pad = 4 - (maxcount&3);\n\n\t\t*(xdr->p++) = 0;\n\n\t\tbuf->tail[0].iov_base += maxcount&3;\n\t\tbuf->tail[0].iov_len = pad;\n\t\tbuf->len += pad;\n\t}\n\n\tspace_left = min_t(int, (void *)xdr->end - (void *)xdr->p,\n\t\t\t\tbuf->buflen - buf->len);\n\tbuf->buflen = buf->len + space_left;\n\txdr->end = (__be32 *)((void *)xdr->end + space_left);\n\n\treturn 0;\n}\n\nstatic __be32 nfsd4_encode_readv(struct nfsd4_compoundres *resp,\n\t\t\t\t struct nfsd4_read *read,\n\t\t\t\t struct file *file, unsigned long maxcount)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tu32 eof;\n\tint v;\n\tint starting_len = xdr->buf->len - 8;\n\tlong len;\n\tint thislen;\n\t__be32 nfserr;\n\t__be32 tmp;\n\t__be32 *p;\n\tu32 zzz = 0;\n\tint pad;\n\n\tlen = maxcount;\n\tv = 0;\n\n\tthislen = min_t(long, len, ((void *)xdr->end - (void *)xdr->p));\n\tp = xdr_reserve_space(xdr, (thislen+3)&~3);\n\tWARN_ON_ONCE(!p);\n\tresp->rqstp->rq_vec[v].iov_base = p;\n\tresp->rqstp->rq_vec[v].iov_len = thislen;\n\tv++;\n\tlen -= thislen;\n\n\twhile (len) {\n\t\tthislen = min_t(long, len, PAGE_SIZE);\n\t\tp = xdr_reserve_space(xdr, (thislen+3)&~3);\n\t\tWARN_ON_ONCE(!p);\n\t\tresp->rqstp->rq_vec[v].iov_base = p;\n\t\tresp->rqstp->rq_vec[v].iov_len = thislen;\n\t\tv++;\n\t\tlen -= thislen;\n\t}\n\tread->rd_vlen = v;\n\n\tlen = maxcount;\n\tnfserr = nfsd_readv(file, read->rd_offset, resp->rqstp->rq_vec,\n\t\t\tread->rd_vlen, &maxcount);\n\tif (nfserr)\n\t\treturn nfserr;\n\txdr_truncate_encode(xdr, starting_len + 8 + ((maxcount+3)&~3));\n\n\teof = nfsd_eof_on_read(len, maxcount, read->rd_offset,\n\t\t\t\td_inode(read->rd_fhp->fh_dentry)->i_size);\n\n\ttmp = htonl(eof);\n\twrite_bytes_to_xdr_buf(xdr->buf, starting_len    , &tmp, 4);\n\ttmp = htonl(maxcount);\n\twrite_bytes_to_xdr_buf(xdr->buf, starting_len + 4, &tmp, 4);\n\n\tpad = (maxcount&3) ? 4 - (maxcount&3) : 0;\n\twrite_bytes_to_xdr_buf(xdr->buf, starting_len + 8 + maxcount,\n\t\t\t\t\t\t\t\t&zzz, pad);\n\treturn 0;\n\n}\n\nstatic __be32\nnfsd4_encode_read(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t  struct nfsd4_read *read)\n{\n\tunsigned long maxcount;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct file *file = read->rd_filp;\n\tint starting_len = xdr->buf->len;\n\tstruct raparms *ra = NULL;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\tgoto out;\n\n\tp = xdr_reserve_space(xdr, 8); /* eof flag and byte count */\n\tif (!p) {\n\t\tWARN_ON_ONCE(test_bit(RQ_SPLICE_OK, &resp->rqstp->rq_flags));\n\t\tnfserr = nfserr_resource;\n\t\tgoto out;\n\t}\n\tif (resp->xdr.buf->page_len &&\n\t    test_bit(RQ_SPLICE_OK, &resp->rqstp->rq_flags)) {\n\t\tWARN_ON_ONCE(1);\n\t\tnfserr = nfserr_resource;\n\t\tgoto out;\n\t}\n\txdr_commit_encode(xdr);\n\n\tmaxcount = svc_max_payload(resp->rqstp);\n\tmaxcount = min_t(unsigned long, maxcount,\n\t\t\t (xdr->buf->buflen - xdr->buf->len));\n\tmaxcount = min_t(unsigned long, maxcount, read->rd_length);\n\n\tif (read->rd_tmp_file)\n\t\tra = nfsd_init_raparms(file);\n\n\tif (file->f_op->splice_read &&\n\t    test_bit(RQ_SPLICE_OK, &resp->rqstp->rq_flags))\n\t\tnfserr = nfsd4_encode_splice_read(resp, read, file, maxcount);\n\telse\n\t\tnfserr = nfsd4_encode_readv(resp, read, file, maxcount);\n\n\tif (ra)\n\t\tnfsd_put_raparams(file, ra);\n\n\tif (nfserr)\n\t\txdr_truncate_encode(xdr, starting_len);\n\nout:\n\tif (file)\n\t\tfput(file);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_readlink(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_readlink *readlink)\n{\n\tint maxcount;\n\t__be32 wire_count;\n\tint zero = 0;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tint length_offset = xdr->buf->len;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tmaxcount = PAGE_SIZE;\n\n\tp = xdr_reserve_space(xdr, maxcount);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t/*\n\t * XXX: By default, vfs_readlink() will truncate symlinks if they\n\t * would overflow the buffer.  Is this kosher in NFSv4?  If not, one\n\t * easy fix is: if vfs_readlink() precisely fills the buffer, assume\n\t * that truncation occurred, and return NFS4ERR_RESOURCE.\n\t */\n\tnfserr = nfsd_readlink(readlink->rl_rqstp, readlink->rl_fhp,\n\t\t\t\t\t\t(char *)p, &maxcount);\n\tif (nfserr == nfserr_isdir)\n\t\tnfserr = nfserr_inval;\n\tif (nfserr) {\n\t\txdr_truncate_encode(xdr, length_offset);\n\t\treturn nfserr;\n\t}\n\n\twire_count = htonl(maxcount);\n\twrite_bytes_to_xdr_buf(xdr->buf, length_offset, &wire_count, 4);\n\txdr_truncate_encode(xdr, length_offset + 4 + ALIGN(maxcount, 4));\n\tif (maxcount & 3)\n\t\twrite_bytes_to_xdr_buf(xdr->buf, length_offset + 4 + maxcount,\n\t\t\t\t\t\t&zero, 4 - (maxcount&3));\n\treturn 0;\n}\n\nstatic __be32\nnfsd4_encode_readdir(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_readdir *readdir)\n{\n\tint maxcount;\n\tint bytes_left;\n\tloff_t offset;\n\t__be64 wire_offset;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tint starting_len = xdr->buf->len;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, NFS4_VERIFIER_SIZE);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t/* XXX: Following NFSv3, we ignore the READDIR verifier for now. */\n\t*p++ = cpu_to_be32(0);\n\t*p++ = cpu_to_be32(0);\n\tresp->xdr.buf->head[0].iov_len = ((char *)resp->xdr.p)\n\t\t\t\t- (char *)resp->xdr.buf->head[0].iov_base;\n\n\t/*\n\t * Number of bytes left for directory entries allowing for the\n\t * final 8 bytes of the readdir and a following failed op:\n\t */\n\tbytes_left = xdr->buf->buflen - xdr->buf->len\n\t\t\t- COMPOUND_ERR_SLACK_SPACE - 8;\n\tif (bytes_left < 0) {\n\t\tnfserr = nfserr_resource;\n\t\tgoto err_no_verf;\n\t}\n\tmaxcount = min_t(u32, readdir->rd_maxcount, INT_MAX);\n\t/*\n\t * Note the rfc defines rd_maxcount as the size of the\n\t * READDIR4resok structure, which includes the verifier above\n\t * and the 8 bytes encoded at the end of this function:\n\t */\n\tif (maxcount < 16) {\n\t\tnfserr = nfserr_toosmall;\n\t\tgoto err_no_verf;\n\t}\n\tmaxcount = min_t(int, maxcount-16, bytes_left);\n\n\t/* RFC 3530 14.2.24 allows us to ignore dircount when it's 0: */\n\tif (!readdir->rd_dircount)\n\t\treaddir->rd_dircount = INT_MAX;\n\n\treaddir->xdr = xdr;\n\treaddir->rd_maxcount = maxcount;\n\treaddir->common.err = 0;\n\treaddir->cookie_offset = 0;\n\n\toffset = readdir->rd_cookie;\n\tnfserr = nfsd_readdir(readdir->rd_rqstp, readdir->rd_fhp,\n\t\t\t      &offset,\n\t\t\t      &readdir->common, nfsd4_encode_dirent);\n\tif (nfserr == nfs_ok &&\n\t    readdir->common.err == nfserr_toosmall &&\n\t    xdr->buf->len == starting_len + 8) {\n\t\t/* nothing encoded; which limit did we hit?: */\n\t\tif (maxcount - 16 < bytes_left)\n\t\t\t/* It was the fault of rd_maxcount: */\n\t\t\tnfserr = nfserr_toosmall;\n\t\telse\n\t\t\t/* We ran out of buffer space: */\n\t\t\tnfserr = nfserr_resource;\n\t}\n\tif (nfserr)\n\t\tgoto err_no_verf;\n\n\tif (readdir->cookie_offset) {\n\t\twire_offset = cpu_to_be64(offset);\n\t\twrite_bytes_to_xdr_buf(xdr->buf, readdir->cookie_offset,\n\t\t\t\t\t\t\t&wire_offset, 8);\n\t}\n\n\tp = xdr_reserve_space(xdr, 8);\n\tif (!p) {\n\t\tWARN_ON_ONCE(1);\n\t\tgoto err_no_verf;\n\t}\n\t*p++ = 0;\t/* no more entries */\n\t*p++ = htonl(readdir->common.err == nfserr_eof);\n\n\treturn 0;\nerr_no_verf:\n\txdr_truncate_encode(xdr, starting_len);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_remove(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_remove *remove)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 20);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = encode_cinfo(p, &remove->rm_cinfo);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_rename(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_rename *rename)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 40);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = encode_cinfo(p, &rename->rn_sinfo);\n\t\tp = encode_cinfo(p, &rename->rn_tinfo);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_do_encode_secinfo(struct xdr_stream *xdr,\n\t\t\t __be32 nfserr, struct svc_export *exp)\n{\n\tu32 i, nflavs, supported;\n\tstruct exp_flavor_info *flavs;\n\tstruct exp_flavor_info def_flavs[2];\n\t__be32 *p, *flavorsp;\n\tstatic bool report = true;\n\n\tif (nfserr)\n\t\tgoto out;\n\tnfserr = nfserr_resource;\n\tif (exp->ex_nflavors) {\n\t\tflavs = exp->ex_flavors;\n\t\tnflavs = exp->ex_nflavors;\n\t} else { /* Handling of some defaults in absence of real secinfo: */\n\t\tflavs = def_flavs;\n\t\tif (exp->ex_client->flavour->flavour == RPC_AUTH_UNIX) {\n\t\t\tnflavs = 2;\n\t\t\tflavs[0].pseudoflavor = RPC_AUTH_UNIX;\n\t\t\tflavs[1].pseudoflavor = RPC_AUTH_NULL;\n\t\t} else if (exp->ex_client->flavour->flavour == RPC_AUTH_GSS) {\n\t\t\tnflavs = 1;\n\t\t\tflavs[0].pseudoflavor\n\t\t\t\t\t= svcauth_gss_flavor(exp->ex_client);\n\t\t} else {\n\t\t\tnflavs = 1;\n\t\t\tflavs[0].pseudoflavor\n\t\t\t\t\t= exp->ex_client->flavour->flavour;\n\t\t}\n\t}\n\n\tsupported = 0;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\tgoto out;\n\tflavorsp = p++;\t\t/* to be backfilled later */\n\n\tfor (i = 0; i < nflavs; i++) {\n\t\trpc_authflavor_t pf = flavs[i].pseudoflavor;\n\t\tstruct rpcsec_gss_info info;\n\n\t\tif (rpcauth_get_gssinfo(pf, &info) == 0) {\n\t\t\tsupported++;\n\t\t\tp = xdr_reserve_space(xdr, 4 + 4 +\n\t\t\t\t\t      XDR_LEN(info.oid.len) + 4 + 4);\n\t\t\tif (!p)\n\t\t\t\tgoto out;\n\t\t\t*p++ = cpu_to_be32(RPC_AUTH_GSS);\n\t\t\tp = xdr_encode_opaque(p,  info.oid.data, info.oid.len);\n\t\t\t*p++ = cpu_to_be32(info.qop);\n\t\t\t*p++ = cpu_to_be32(info.service);\n\t\t} else if (pf < RPC_AUTH_MAXFLAVOR) {\n\t\t\tsupported++;\n\t\t\tp = xdr_reserve_space(xdr, 4);\n\t\t\tif (!p)\n\t\t\t\tgoto out;\n\t\t\t*p++ = cpu_to_be32(pf);\n\t\t} else {\n\t\t\tif (report)\n\t\t\t\tpr_warn(\"NFS: SECINFO: security flavor %u \"\n\t\t\t\t\t\"is not supported\\n\", pf);\n\t\t}\n\t}\n\n\tif (nflavs != supported)\n\t\treport = false;\n\t*flavorsp = htonl(supported);\n\tnfserr = 0;\nout:\n\tif (exp)\n\t\texp_put(exp);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_secinfo(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t     struct nfsd4_secinfo *secinfo)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\treturn nfsd4_do_encode_secinfo(xdr, nfserr, secinfo->si_exp);\n}\n\nstatic __be32\nnfsd4_encode_secinfo_no_name(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t     struct nfsd4_secinfo_no_name *secinfo)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\treturn nfsd4_do_encode_secinfo(xdr, nfserr, secinfo->sin_exp);\n}\n\n/*\n * The SETATTR encode routine is special -- it always encodes a bitmap,\n * regardless of the error status.\n */\nstatic __be32\nnfsd4_encode_setattr(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_setattr *setattr)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, 16);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tif (nfserr) {\n\t\t*p++ = cpu_to_be32(3);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\telse {\n\t\t*p++ = cpu_to_be32(3);\n\t\t*p++ = cpu_to_be32(setattr->sa_bmval[0]);\n\t\t*p++ = cpu_to_be32(setattr->sa_bmval[1]);\n\t\t*p++ = cpu_to_be32(setattr->sa_bmval[2]);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_setclientid(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_setclientid *scd)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 8 + NFS4_VERIFIER_SIZE);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = xdr_encode_opaque_fixed(p, &scd->se_clientid, 8);\n\t\tp = xdr_encode_opaque_fixed(p, &scd->se_confirm,\n\t\t\t\t\t\tNFS4_VERIFIER_SIZE);\n\t}\n\telse if (nfserr == nfserr_clid_inuse) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_write(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_write *write)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 16);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(write->wr_bytes_written);\n\t\t*p++ = cpu_to_be32(write->wr_how_written);\n\t\tp = xdr_encode_opaque_fixed(p, write->wr_verifier.data,\n\t\t\t\t\t\t\tNFS4_VERIFIER_SIZE);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_exchange_id(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t\t struct nfsd4_exchange_id *exid)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\tchar *major_id;\n\tchar *server_scope;\n\tint major_id_sz;\n\tint server_scope_sz;\n\tint status = 0;\n\tuint64_t minor_id = 0;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tmajor_id = utsname()->nodename;\n\tmajor_id_sz = strlen(major_id);\n\tserver_scope = utsname()->nodename;\n\tserver_scope_sz = strlen(server_scope);\n\n\tp = xdr_reserve_space(xdr,\n\t\t8 /* eir_clientid */ +\n\t\t4 /* eir_sequenceid */ +\n\t\t4 /* eir_flags */ +\n\t\t4 /* spr_how */);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\tp = xdr_encode_opaque_fixed(p, &exid->clientid, 8);\n\t*p++ = cpu_to_be32(exid->seqid);\n\t*p++ = cpu_to_be32(exid->flags);\n\n\t*p++ = cpu_to_be32(exid->spa_how);\n\n\tswitch (exid->spa_how) {\n\tcase SP4_NONE:\n\t\tbreak;\n\tcase SP4_MACH_CRED:\n\t\t/* spo_must_enforce bitmap: */\n\t\tstatus = nfsd4_encode_bitmap(xdr,\n\t\t\t\t\texid->spo_must_enforce[0],\n\t\t\t\t\texid->spo_must_enforce[1],\n\t\t\t\t\texid->spo_must_enforce[2]);\n\t\tif (status)\n\t\t\tgoto out;\n\t\t/* spo_must_allow bitmap: */\n\t\tstatus = nfsd4_encode_bitmap(xdr,\n\t\t\t\t\texid->spo_must_allow[0],\n\t\t\t\t\texid->spo_must_allow[1],\n\t\t\t\t\texid->spo_must_allow[2]);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\n\tp = xdr_reserve_space(xdr,\n\t\t8 /* so_minor_id */ +\n\t\t4 /* so_major_id.len */ +\n\t\t(XDR_QUADLEN(major_id_sz) * 4) +\n\t\t4 /* eir_server_scope.len */ +\n\t\t(XDR_QUADLEN(server_scope_sz) * 4) +\n\t\t4 /* eir_server_impl_id.count (0) */);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t/* The server_owner struct */\n\tp = xdr_encode_hyper(p, minor_id);      /* Minor id */\n\t/* major id */\n\tp = xdr_encode_opaque(p, major_id, major_id_sz);\n\n\t/* Server scope */\n\tp = xdr_encode_opaque(p, server_scope, server_scope_sz);\n\n\t/* Implementation id */\n\t*p++ = cpu_to_be32(0);\t/* zero length nfs_impl_id4 array */\n\treturn 0;\nout:\n\treturn status;\n}\n\nstatic __be32\nnfsd4_encode_create_session(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t\t    struct nfsd4_create_session *sess)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, 24);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tp = xdr_encode_opaque_fixed(p, sess->sessionid.data,\n\t\t\t\t\tNFS4_MAX_SESSIONID_LEN);\n\t*p++ = cpu_to_be32(sess->seqid);\n\t*p++ = cpu_to_be32(sess->flags);\n\n\tp = xdr_reserve_space(xdr, 28);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(0); /* headerpadsz */\n\t*p++ = cpu_to_be32(sess->fore_channel.maxreq_sz);\n\t*p++ = cpu_to_be32(sess->fore_channel.maxresp_sz);\n\t*p++ = cpu_to_be32(sess->fore_channel.maxresp_cached);\n\t*p++ = cpu_to_be32(sess->fore_channel.maxops);\n\t*p++ = cpu_to_be32(sess->fore_channel.maxreqs);\n\t*p++ = cpu_to_be32(sess->fore_channel.nr_rdma_attrs);\n\n\tif (sess->fore_channel.nr_rdma_attrs) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(sess->fore_channel.rdma_attrs);\n\t}\n\n\tp = xdr_reserve_space(xdr, 28);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(0); /* headerpadsz */\n\t*p++ = cpu_to_be32(sess->back_channel.maxreq_sz);\n\t*p++ = cpu_to_be32(sess->back_channel.maxresp_sz);\n\t*p++ = cpu_to_be32(sess->back_channel.maxresp_cached);\n\t*p++ = cpu_to_be32(sess->back_channel.maxops);\n\t*p++ = cpu_to_be32(sess->back_channel.maxreqs);\n\t*p++ = cpu_to_be32(sess->back_channel.nr_rdma_attrs);\n\n\tif (sess->back_channel.nr_rdma_attrs) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(sess->back_channel.rdma_attrs);\n\t}\n\treturn 0;\n}\n\nstatic __be32\nnfsd4_encode_sequence(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t      struct nfsd4_sequence *seq)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, NFS4_MAX_SESSIONID_LEN + 20);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tp = xdr_encode_opaque_fixed(p, seq->sessionid.data,\n\t\t\t\t\tNFS4_MAX_SESSIONID_LEN);\n\t*p++ = cpu_to_be32(seq->seqid);\n\t*p++ = cpu_to_be32(seq->slotid);\n\t/* Note slotid's are numbered from zero: */\n\t*p++ = cpu_to_be32(seq->maxslots - 1); /* sr_highest_slotid */\n\t*p++ = cpu_to_be32(seq->maxslots - 1); /* sr_target_highest_slotid */\n\t*p++ = cpu_to_be32(seq->status_flags);\n\n\tresp->cstate.data_offset = xdr->buf->len; /* DRC cache data pointer */\n\treturn 0;\n}\n\nstatic __be32\nnfsd4_encode_test_stateid(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t\t  struct nfsd4_test_stateid *test_stateid)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct nfsd4_test_stateid_id *stateid, *next;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, 4 + (4 * test_stateid->ts_num_ids));\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = htonl(test_stateid->ts_num_ids);\n\n\tlist_for_each_entry_safe(stateid, next, &test_stateid->ts_stateid_list, ts_id_list) {\n\t\t*p++ = stateid->ts_id_status;\n\t}\n\n\treturn nfserr;\n}\n\n#ifdef CONFIG_NFSD_PNFS\nstatic __be32\nnfsd4_encode_getdeviceinfo(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\tstruct nfsd4_getdeviceinfo *gdev)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tconst struct nfsd4_layout_ops *ops =\n\t\tnfsd4_layout_ops[gdev->gd_layout_type];\n\tu32 starting_len = xdr->buf->len, needed_len;\n\t__be32 *p;\n\n\tdprintk(\"%s: err %d\\n\", __func__, be32_to_cpu(nfserr));\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfserr_resource;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\tgoto out;\n\n\t*p++ = cpu_to_be32(gdev->gd_layout_type);\n\n\t/* If maxcount is 0 then just update notifications */\n\tif (gdev->gd_maxcount != 0) {\n\t\tnfserr = ops->encode_getdeviceinfo(xdr, gdev);\n\t\tif (nfserr) {\n\t\t\t/*\n\t\t\t * We don't bother to burden the layout drivers with\n\t\t\t * enforcing gd_maxcount, just tell the client to\n\t\t\t * come back with a bigger buffer if it's not enough.\n\t\t\t */\n\t\t\tif (xdr->buf->len + 4 > gdev->gd_maxcount)\n\t\t\t\tgoto toosmall;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnfserr = nfserr_resource;\n\tif (gdev->gd_notify_types) {\n\t\tp = xdr_reserve_space(xdr, 4 + 4);\n\t\tif (!p)\n\t\t\tgoto out;\n\t\t*p++ = cpu_to_be32(1);\t\t\t/* bitmap length */\n\t\t*p++ = cpu_to_be32(gdev->gd_notify_types);\n\t} else {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out;\n\t\t*p++ = 0;\n\t}\n\n\tnfserr = 0;\nout:\n\tkfree(gdev->gd_device);\n\tdprintk(\"%s: done: %d\\n\", __func__, be32_to_cpu(nfserr));\n\treturn nfserr;\n\ntoosmall:\n\tdprintk(\"%s: maxcount too small\\n\", __func__);\n\tneeded_len = xdr->buf->len + 4 /* notifications */;\n\txdr_truncate_encode(xdr, starting_len);\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p) {\n\t\tnfserr = nfserr_resource;\n\t} else {\n\t\t*p++ = cpu_to_be32(needed_len);\n\t\tnfserr = nfserr_toosmall;\n\t}\n\tgoto out;\n}\n\nstatic __be32\nnfsd4_encode_layoutget(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\tstruct nfsd4_layoutget *lgp)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tconst struct nfsd4_layout_ops *ops =\n\t\tnfsd4_layout_ops[lgp->lg_layout_type];\n\t__be32 *p;\n\n\tdprintk(\"%s: err %d\\n\", __func__, nfserr);\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfserr_resource;\n\tp = xdr_reserve_space(xdr, 36 + sizeof(stateid_opaque_t));\n\tif (!p)\n\t\tgoto out;\n\n\t*p++ = cpu_to_be32(1);\t/* we always set return-on-close */\n\t*p++ = cpu_to_be32(lgp->lg_sid.si_generation);\n\tp = xdr_encode_opaque_fixed(p, &lgp->lg_sid.si_opaque,\n\t\t\t\t    sizeof(stateid_opaque_t));\n\n\t*p++ = cpu_to_be32(1);\t/* we always return a single layout */\n\tp = xdr_encode_hyper(p, lgp->lg_seg.offset);\n\tp = xdr_encode_hyper(p, lgp->lg_seg.length);\n\t*p++ = cpu_to_be32(lgp->lg_seg.iomode);\n\t*p++ = cpu_to_be32(lgp->lg_layout_type);\n\n\tnfserr = ops->encode_layoutget(xdr, lgp);\nout:\n\tkfree(lgp->lg_content);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_layoutcommit(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t\t  struct nfsd4_layoutcommit *lcp)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(lcp->lc_size_chg);\n\tif (lcp->lc_size_chg) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = xdr_encode_hyper(p, lcp->lc_newsize);\n\t}\n\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_encode_layoutreturn(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\tstruct nfsd4_layoutreturn *lrp)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(lrp->lrs_present);\n\tif (lrp->lrs_present)\n\t\treturn nfsd4_encode_stateid(xdr, &lrp->lr_sid);\n\treturn nfs_ok;\n}\n#endif /* CONFIG_NFSD_PNFS */\n\nstatic __be32\nnfsd42_encode_write_res(struct nfsd4_compoundres *resp, struct nfsd42_write_res *write)\n{\n\t__be32 *p;\n\n\tp = xdr_reserve_space(&resp->xdr, 4 + 8 + 4 + NFS4_VERIFIER_SIZE);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t*p++ = cpu_to_be32(0);\n\tp = xdr_encode_hyper(p, write->wr_bytes_written);\n\t*p++ = cpu_to_be32(write->wr_stable_how);\n\tp = xdr_encode_opaque_fixed(p, write->wr_verifier.data,\n\t\t\t\t    NFS4_VERIFIER_SIZE);\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_encode_copy(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t  struct nfsd4_copy *copy)\n{\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tnfserr = nfsd42_encode_write_res(resp, &copy->cp_res);\n\t\tif (nfserr)\n\t\t\treturn nfserr;\n\n\t\tp = xdr_reserve_space(&resp->xdr, 4 + 4);\n\t\t*p++ = cpu_to_be32(copy->cp_consecutive);\n\t\t*p++ = cpu_to_be32(copy->cp_synchronous);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_seek(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t  struct nfsd4_seek *seek)\n{\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(&resp->xdr, 4 + 8);\n\t*p++ = cpu_to_be32(seek->seek_eof);\n\tp = xdr_encode_hyper(p, seek->seek_pos);\n\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_noop(struct nfsd4_compoundres *resp, __be32 nfserr, void *p)\n{\n\treturn nfserr;\n}\n\ntypedef __be32(* nfsd4_enc)(struct nfsd4_compoundres *, __be32, void *);\n\n/*\n * Note: nfsd4_enc_ops vector is shared for v4.0 and v4.1\n * since we don't need to filter out obsolete ops as this is\n * done in the decoding phase.\n */\nstatic nfsd4_enc nfsd4_enc_ops[] = {\n\t[OP_ACCESS]\t\t= (nfsd4_enc)nfsd4_encode_access,\n\t[OP_CLOSE]\t\t= (nfsd4_enc)nfsd4_encode_close,\n\t[OP_COMMIT]\t\t= (nfsd4_enc)nfsd4_encode_commit,\n\t[OP_CREATE]\t\t= (nfsd4_enc)nfsd4_encode_create,\n\t[OP_DELEGPURGE]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_DELEGRETURN]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_GETATTR]\t\t= (nfsd4_enc)nfsd4_encode_getattr,\n\t[OP_GETFH]\t\t= (nfsd4_enc)nfsd4_encode_getfh,\n\t[OP_LINK]\t\t= (nfsd4_enc)nfsd4_encode_link,\n\t[OP_LOCK]\t\t= (nfsd4_enc)nfsd4_encode_lock,\n\t[OP_LOCKT]\t\t= (nfsd4_enc)nfsd4_encode_lockt,\n\t[OP_LOCKU]\t\t= (nfsd4_enc)nfsd4_encode_locku,\n\t[OP_LOOKUP]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LOOKUPP]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_NVERIFY]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_OPEN]\t\t= (nfsd4_enc)nfsd4_encode_open,\n\t[OP_OPENATTR]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_OPEN_CONFIRM]\t= (nfsd4_enc)nfsd4_encode_open_confirm,\n\t[OP_OPEN_DOWNGRADE]\t= (nfsd4_enc)nfsd4_encode_open_downgrade,\n\t[OP_PUTFH]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_PUTPUBFH]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_PUTROOTFH]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_READ]\t\t= (nfsd4_enc)nfsd4_encode_read,\n\t[OP_READDIR]\t\t= (nfsd4_enc)nfsd4_encode_readdir,\n\t[OP_READLINK]\t\t= (nfsd4_enc)nfsd4_encode_readlink,\n\t[OP_REMOVE]\t\t= (nfsd4_enc)nfsd4_encode_remove,\n\t[OP_RENAME]\t\t= (nfsd4_enc)nfsd4_encode_rename,\n\t[OP_RENEW]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_RESTOREFH]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_SAVEFH]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_SECINFO]\t\t= (nfsd4_enc)nfsd4_encode_secinfo,\n\t[OP_SETATTR]\t\t= (nfsd4_enc)nfsd4_encode_setattr,\n\t[OP_SETCLIENTID]\t= (nfsd4_enc)nfsd4_encode_setclientid,\n\t[OP_SETCLIENTID_CONFIRM] = (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_VERIFY]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_WRITE]\t\t= (nfsd4_enc)nfsd4_encode_write,\n\t[OP_RELEASE_LOCKOWNER]\t= (nfsd4_enc)nfsd4_encode_noop,\n\n\t/* NFSv4.1 operations */\n\t[OP_BACKCHANNEL_CTL]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_BIND_CONN_TO_SESSION] = (nfsd4_enc)nfsd4_encode_bind_conn_to_session,\n\t[OP_EXCHANGE_ID]\t= (nfsd4_enc)nfsd4_encode_exchange_id,\n\t[OP_CREATE_SESSION]\t= (nfsd4_enc)nfsd4_encode_create_session,\n\t[OP_DESTROY_SESSION]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_FREE_STATEID]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_GET_DIR_DELEGATION]\t= (nfsd4_enc)nfsd4_encode_noop,\n#ifdef CONFIG_NFSD_PNFS\n\t[OP_GETDEVICEINFO]\t= (nfsd4_enc)nfsd4_encode_getdeviceinfo,\n\t[OP_GETDEVICELIST]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTCOMMIT]\t= (nfsd4_enc)nfsd4_encode_layoutcommit,\n\t[OP_LAYOUTGET]\t\t= (nfsd4_enc)nfsd4_encode_layoutget,\n\t[OP_LAYOUTRETURN]\t= (nfsd4_enc)nfsd4_encode_layoutreturn,\n#else\n\t[OP_GETDEVICEINFO]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_GETDEVICELIST]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTCOMMIT]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTGET]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTRETURN]\t= (nfsd4_enc)nfsd4_encode_noop,\n#endif\n\t[OP_SECINFO_NO_NAME]\t= (nfsd4_enc)nfsd4_encode_secinfo_no_name,\n\t[OP_SEQUENCE]\t\t= (nfsd4_enc)nfsd4_encode_sequence,\n\t[OP_SET_SSV]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_TEST_STATEID]\t= (nfsd4_enc)nfsd4_encode_test_stateid,\n\t[OP_WANT_DELEGATION]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_DESTROY_CLIENTID]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_RECLAIM_COMPLETE]\t= (nfsd4_enc)nfsd4_encode_noop,\n\n\t/* NFSv4.2 operations */\n\t[OP_ALLOCATE]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_COPY]\t\t= (nfsd4_enc)nfsd4_encode_copy,\n\t[OP_COPY_NOTIFY]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_DEALLOCATE]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_IO_ADVISE]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTERROR]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTSTATS]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_OFFLOAD_CANCEL]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_OFFLOAD_STATUS]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_READ_PLUS]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_SEEK]\t\t= (nfsd4_enc)nfsd4_encode_seek,\n\t[OP_WRITE_SAME]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_CLONE]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n};\n\n/*\n * Calculate whether we still have space to encode repsize bytes.\n * There are two considerations:\n *     - For NFS versions >=4.1, the size of the reply must stay within\n *       session limits\n *     - For all NFS versions, we must stay within limited preallocated\n *       buffer space.\n *\n * This is called before the operation is processed, so can only provide\n * an upper estimate.  For some nonidempotent operations (such as\n * getattr), it's not necessarily a problem if that estimate is wrong,\n * as we can fail it after processing without significant side effects.\n */\n__be32 nfsd4_check_resp_size(struct nfsd4_compoundres *resp, u32 respsize)\n{\n\tstruct xdr_buf *buf = &resp->rqstp->rq_res;\n\tstruct nfsd4_slot *slot = resp->cstate.slot;\n\n\tif (buf->len + respsize <= buf->buflen)\n\t\treturn nfs_ok;\n\tif (!nfsd4_has_session(&resp->cstate))\n\t\treturn nfserr_resource;\n\tif (slot->sl_flags & NFSD4_SLOT_CACHETHIS) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn nfserr_rep_too_big_to_cache;\n\t}\n\treturn nfserr_rep_too_big;\n}\n\nvoid\nnfsd4_encode_operation(struct nfsd4_compoundres *resp, struct nfsd4_op *op)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct nfs4_stateowner *so = resp->cstate.replay_owner;\n\tstruct svc_rqst *rqstp = resp->rqstp;\n\tint post_err_offset;\n\tnfsd4_enc encoder;\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, 8);\n\tif (!p) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\t*p++ = cpu_to_be32(op->opnum);\n\tpost_err_offset = xdr->buf->len;\n\n\tif (op->opnum == OP_ILLEGAL)\n\t\tgoto status;\n\tBUG_ON(op->opnum < 0 || op->opnum >= ARRAY_SIZE(nfsd4_enc_ops) ||\n\t       !nfsd4_enc_ops[op->opnum]);\n\tencoder = nfsd4_enc_ops[op->opnum];\n\top->status = encoder(resp, op->status, &op->u);\n\txdr_commit_encode(xdr);\n\n\t/* nfsd4_check_resp_size guarantees enough room for error status */\n\tif (!op->status) {\n\t\tint space_needed = 0;\n\t\tif (!nfsd4_last_compound_op(rqstp))\n\t\t\tspace_needed = COMPOUND_ERR_SLACK_SPACE;\n\t\top->status = nfsd4_check_resp_size(resp, space_needed);\n\t}\n\tif (op->status == nfserr_resource && nfsd4_has_session(&resp->cstate)) {\n\t\tstruct nfsd4_slot *slot = resp->cstate.slot;\n\n\t\tif (slot->sl_flags & NFSD4_SLOT_CACHETHIS)\n\t\t\top->status = nfserr_rep_too_big_to_cache;\n\t\telse\n\t\t\top->status = nfserr_rep_too_big;\n\t}\n\tif (op->status == nfserr_resource ||\n\t    op->status == nfserr_rep_too_big ||\n\t    op->status == nfserr_rep_too_big_to_cache) {\n\t\t/*\n\t\t * The operation may have already been encoded or\n\t\t * partially encoded.  No op returns anything additional\n\t\t * in the case of one of these three errors, so we can\n\t\t * just truncate back to after the status.  But it's a\n\t\t * bug if we had to do this on a non-idempotent op:\n\t\t */\n\t\twarn_on_nonidempotent_op(op);\n\t\txdr_truncate_encode(xdr, post_err_offset);\n\t}\n\tif (so) {\n\t\tint len = xdr->buf->len - post_err_offset;\n\n\t\tso->so_replay.rp_status = op->status;\n\t\tso->so_replay.rp_buflen = len;\n\t\tread_bytes_from_xdr_buf(xdr->buf, post_err_offset,\n\t\t\t\t\t\tso->so_replay.rp_buf, len);\n\t}\nstatus:\n\t/* Note that op->status is already in network byte order: */\n\twrite_bytes_to_xdr_buf(xdr->buf, post_err_offset - 4, &op->status, 4);\n}\n\n/* \n * Encode the reply stored in the stateowner reply cache \n * \n * XDR note: do not encode rp->rp_buflen: the buffer contains the\n * previously sent already encoded operation.\n */\nvoid\nnfsd4_encode_replay(struct xdr_stream *xdr, struct nfsd4_op *op)\n{\n\t__be32 *p;\n\tstruct nfs4_replay *rp = op->replay;\n\n\tBUG_ON(!rp);\n\n\tp = xdr_reserve_space(xdr, 8 + rp->rp_buflen);\n\tif (!p) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\t*p++ = cpu_to_be32(op->opnum);\n\t*p++ = rp->rp_status;  /* already xdr'ed */\n\n\tp = xdr_encode_opaque_fixed(p, rp->rp_buf, rp->rp_buflen);\n}\n\nint\nnfs4svc_encode_voidres(struct svc_rqst *rqstp, __be32 *p, void *dummy)\n{\n        return xdr_ressize_check(rqstp, p);\n}\n\nint nfsd4_release_compoundargs(void *rq, __be32 *p, void *resp)\n{\n\tstruct svc_rqst *rqstp = rq;\n\tstruct nfsd4_compoundargs *args = rqstp->rq_argp;\n\n\tif (args->ops != args->iops) {\n\t\tkfree(args->ops);\n\t\targs->ops = args->iops;\n\t}\n\tkfree(args->tmpp);\n\targs->tmpp = NULL;\n\twhile (args->to_free) {\n\t\tstruct svcxdr_tmpbuf *tb = args->to_free;\n\t\targs->to_free = tb->next;\n\t\tkfree(tb);\n\t}\n\treturn 1;\n}\n\nint\nnfs4svc_decode_compoundargs(struct svc_rqst *rqstp, __be32 *p, struct nfsd4_compoundargs *args)\n{\n\tif (rqstp->rq_arg.head[0].iov_len % 4) {\n\t\t/* client is nuts */\n\t\tdprintk(\"%s: compound not properly padded! (peeraddr=%pISc xid=0x%x)\",\n\t\t\t__func__, svc_addr(rqstp), be32_to_cpu(rqstp->rq_xid));\n\t\treturn 0;\n\t}\n\targs->p = p;\n\targs->end = rqstp->rq_arg.head[0].iov_base + rqstp->rq_arg.head[0].iov_len;\n\targs->pagelist = rqstp->rq_arg.pages;\n\targs->pagelen = rqstp->rq_arg.page_len;\n\targs->tmpp = NULL;\n\targs->to_free = NULL;\n\targs->ops = args->iops;\n\targs->rqstp = rqstp;\n\n\treturn !nfsd4_decode_compound(args);\n}\n\nint\nnfs4svc_encode_compoundres(struct svc_rqst *rqstp, __be32 *p, struct nfsd4_compoundres *resp)\n{\n\t/*\n\t * All that remains is to write the tag and operation count...\n\t */\n\tstruct xdr_buf *buf = resp->xdr.buf;\n\n\tWARN_ON_ONCE(buf->len != buf->head[0].iov_len + buf->page_len +\n\t\t\t\t buf->tail[0].iov_len);\n\n\trqstp->rq_next_page = resp->xdr.page_ptr + 1;\n\n\tp = resp->tagp;\n\t*p++ = htonl(resp->taglen);\n\tmemcpy(p, resp->tag, resp->taglen);\n\tp += XDR_QUADLEN(resp->taglen);\n\t*p++ = htonl(resp->opcnt);\n\n\tnfsd4_sequence_done(resp);\n\treturn 1;\n}\n\n/*\n * Local variables:\n *  c-basic-offset: 8\n * End:\n */\n", "/*\n * XDR support for nfsd\n *\n * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>\n */\n\n#include \"vfs.h\"\n#include \"xdr.h\"\n#include \"auth.h\"\n\n#define NFSDDBG_FACILITY\t\tNFSDDBG_XDR\n\n/*\n * Mapping of S_IF* types to NFS file types\n */\nstatic u32\tnfs_ftypes[] = {\n\tNFNON,  NFCHR,  NFCHR, NFBAD,\n\tNFDIR,  NFBAD,  NFBLK, NFBAD,\n\tNFREG,  NFBAD,  NFLNK, NFBAD,\n\tNFSOCK, NFBAD,  NFLNK, NFBAD,\n};\n\n\n/*\n * XDR functions for basic NFS types\n */\nstatic __be32 *\ndecode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\tfh_init(fhp, NFS_FHSIZE);\n\tmemcpy(&fhp->fh_handle.fh_base, p, NFS_FHSIZE);\n\tfhp->fh_handle.fh_size = NFS_FHSIZE;\n\n\t/* FIXME: Look up export pointer here and verify\n\t * Sun Secure RPC if requested */\n\treturn p + (NFS_FHSIZE >> 2);\n}\n\n/* Helper function for NFSv2 ACL code */\n__be32 *nfs2svc_decode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\treturn decode_fh(p, fhp);\n}\n\nstatic __be32 *\nencode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\tmemcpy(p, &fhp->fh_handle.fh_base, NFS_FHSIZE);\n\treturn p + (NFS_FHSIZE>> 2);\n}\n\n/*\n * Decode a file name and make sure that the path contains\n * no slashes or null bytes.\n */\nstatic __be32 *\ndecode_filename(__be32 *p, char **namp, unsigned int *lenp)\n{\n\tchar\t\t*name;\n\tunsigned int\ti;\n\n\tif ((p = xdr_decode_string_inplace(p, namp, lenp, NFS_MAXNAMLEN)) != NULL) {\n\t\tfor (i = 0, name = *namp; i < *lenp; i++, name++) {\n\t\t\tif (*name == '\\0' || *name == '/')\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn p;\n}\n\nstatic __be32 *\ndecode_pathname(__be32 *p, char **namp, unsigned int *lenp)\n{\n\tchar\t\t*name;\n\tunsigned int\ti;\n\n\tif ((p = xdr_decode_string_inplace(p, namp, lenp, NFS_MAXPATHLEN)) != NULL) {\n\t\tfor (i = 0, name = *namp; i < *lenp; i++, name++) {\n\t\t\tif (*name == '\\0')\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn p;\n}\n\nstatic __be32 *\ndecode_sattr(__be32 *p, struct iattr *iap)\n{\n\tu32\ttmp, tmp1;\n\n\tiap->ia_valid = 0;\n\n\t/* Sun client bug compatibility check: some sun clients seem to\n\t * put 0xffff in the mode field when they mean 0xffffffff.\n\t * Quoting the 4.4BSD nfs server code: Nah nah nah nah na nah.\n\t */\n\tif ((tmp = ntohl(*p++)) != (u32)-1 && tmp != 0xffff) {\n\t\tiap->ia_valid |= ATTR_MODE;\n\t\tiap->ia_mode = tmp;\n\t}\n\tif ((tmp = ntohl(*p++)) != (u32)-1) {\n\t\tiap->ia_uid = make_kuid(&init_user_ns, tmp);\n\t\tif (uid_valid(iap->ia_uid))\n\t\t\tiap->ia_valid |= ATTR_UID;\n\t}\n\tif ((tmp = ntohl(*p++)) != (u32)-1) {\n\t\tiap->ia_gid = make_kgid(&init_user_ns, tmp);\n\t\tif (gid_valid(iap->ia_gid))\n\t\t\tiap->ia_valid |= ATTR_GID;\n\t}\n\tif ((tmp = ntohl(*p++)) != (u32)-1) {\n\t\tiap->ia_valid |= ATTR_SIZE;\n\t\tiap->ia_size = tmp;\n\t}\n\ttmp  = ntohl(*p++); tmp1 = ntohl(*p++);\n\tif (tmp != (u32)-1 && tmp1 != (u32)-1) {\n\t\tiap->ia_valid |= ATTR_ATIME | ATTR_ATIME_SET;\n\t\tiap->ia_atime.tv_sec = tmp;\n\t\tiap->ia_atime.tv_nsec = tmp1 * 1000; \n\t}\n\ttmp  = ntohl(*p++); tmp1 = ntohl(*p++);\n\tif (tmp != (u32)-1 && tmp1 != (u32)-1) {\n\t\tiap->ia_valid |= ATTR_MTIME | ATTR_MTIME_SET;\n\t\tiap->ia_mtime.tv_sec = tmp;\n\t\tiap->ia_mtime.tv_nsec = tmp1 * 1000; \n\t\t/*\n\t\t * Passing the invalid value useconds=1000000 for mtime\n\t\t * is a Sun convention for \"set both mtime and atime to\n\t\t * current server time\".  It's needed to make permissions\n\t\t * checks for the \"touch\" program across v2 mounts to\n\t\t * Solaris and Irix boxes work correctly. See description of\n\t\t * sattr in section 6.1 of \"NFS Illustrated\" by\n\t\t * Brent Callaghan, Addison-Wesley, ISBN 0-201-32750-5\n\t\t */\n\t\tif (tmp1 == 1000000)\n\t\t\tiap->ia_valid &= ~(ATTR_ATIME_SET|ATTR_MTIME_SET);\n\t}\n\treturn p;\n}\n\nstatic __be32 *\nencode_fattr(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp,\n\t     struct kstat *stat)\n{\n\tstruct dentry\t*dentry = fhp->fh_dentry;\n\tint type;\n\tstruct timespec time;\n\tu32 f;\n\n\ttype = (stat->mode & S_IFMT);\n\n\t*p++ = htonl(nfs_ftypes[type >> 12]);\n\t*p++ = htonl((u32) stat->mode);\n\t*p++ = htonl((u32) stat->nlink);\n\t*p++ = htonl((u32) from_kuid(&init_user_ns, stat->uid));\n\t*p++ = htonl((u32) from_kgid(&init_user_ns, stat->gid));\n\n\tif (S_ISLNK(type) && stat->size > NFS_MAXPATHLEN) {\n\t\t*p++ = htonl(NFS_MAXPATHLEN);\n\t} else {\n\t\t*p++ = htonl((u32) stat->size);\n\t}\n\t*p++ = htonl((u32) stat->blksize);\n\tif (S_ISCHR(type) || S_ISBLK(type))\n\t\t*p++ = htonl(new_encode_dev(stat->rdev));\n\telse\n\t\t*p++ = htonl(0xffffffff);\n\t*p++ = htonl((u32) stat->blocks);\n\tswitch (fsid_source(fhp)) {\n\tdefault:\n\tcase FSIDSOURCE_DEV:\n\t\t*p++ = htonl(new_encode_dev(stat->dev));\n\t\tbreak;\n\tcase FSIDSOURCE_FSID:\n\t\t*p++ = htonl((u32) fhp->fh_export->ex_fsid);\n\t\tbreak;\n\tcase FSIDSOURCE_UUID:\n\t\tf = ((u32*)fhp->fh_export->ex_uuid)[0];\n\t\tf ^= ((u32*)fhp->fh_export->ex_uuid)[1];\n\t\tf ^= ((u32*)fhp->fh_export->ex_uuid)[2];\n\t\tf ^= ((u32*)fhp->fh_export->ex_uuid)[3];\n\t\t*p++ = htonl(f);\n\t\tbreak;\n\t}\n\t*p++ = htonl((u32) stat->ino);\n\t*p++ = htonl((u32) stat->atime.tv_sec);\n\t*p++ = htonl(stat->atime.tv_nsec ? stat->atime.tv_nsec / 1000 : 0);\n\tlease_get_mtime(d_inode(dentry), &time); \n\t*p++ = htonl((u32) time.tv_sec);\n\t*p++ = htonl(time.tv_nsec ? time.tv_nsec / 1000 : 0); \n\t*p++ = htonl((u32) stat->ctime.tv_sec);\n\t*p++ = htonl(stat->ctime.tv_nsec ? stat->ctime.tv_nsec / 1000 : 0);\n\n\treturn p;\n}\n\n/* Helper function for NFSv2 ACL code */\n__be32 *nfs2svc_encode_fattr(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp, struct kstat *stat)\n{\n\treturn encode_fattr(rqstp, p, fhp, stat);\n}\n\n/*\n * XDR decode functions\n */\nint\nnfssvc_decode_void(struct svc_rqst *rqstp, __be32 *p, void *dummy)\n{\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_fhandle(struct svc_rqst *rqstp, __be32 *p, struct nfsd_fhandle *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_sattrargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_sattrargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = decode_sattr(p, &args->attrs);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_diropargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_diropargs *args)\n{\n\tif (!(p = decode_fh(p, &args->fh))\n\t || !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_readargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_readargs *args)\n{\n\tunsigned int len;\n\tint v;\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\n\targs->offset    = ntohl(*p++);\n\tlen = args->count     = ntohl(*p++);\n\tp++; /* totalcount - unused */\n\n\tlen = min_t(unsigned int, len, NFSSVC_MAXBLKSIZE_V2);\n\n\t/* set up somewhere to store response.\n\t * We take pages, put them on reslist and include in iovec\n\t */\n\tv=0;\n\twhile (len > 0) {\n\t\tstruct page *p = *(rqstp->rq_next_page++);\n\n\t\trqstp->rq_vec[v].iov_base = page_address(p);\n\t\trqstp->rq_vec[v].iov_len = min_t(unsigned int, len, PAGE_SIZE);\n\t\tlen -= rqstp->rq_vec[v].iov_len;\n\t\tv++;\n\t}\n\targs->vlen = v;\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_writeargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_writeargs *args)\n{\n\tunsigned int len, hdr, dlen;\n\tstruct kvec *head = rqstp->rq_arg.head;\n\tint v;\n\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\n\tp++;\t\t\t\t/* beginoffset */\n\targs->offset = ntohl(*p++);\t/* offset */\n\tp++;\t\t\t\t/* totalcount */\n\tlen = args->len = ntohl(*p++);\n\t/*\n\t * The protocol specifies a maximum of 8192 bytes.\n\t */\n\tif (len > NFSSVC_MAXBLKSIZE_V2)\n\t\treturn 0;\n\n\t/*\n\t * Check to make sure that we got the right number of\n\t * bytes.\n\t */\n\thdr = (void*)p - head->iov_base;\n\tif (hdr > head->iov_len)\n\t\treturn 0;\n\tdlen = head->iov_len + rqstp->rq_arg.page_len - hdr;\n\n\t/*\n\t * Round the length of the data which was specified up to\n\t * the next multiple of XDR units and then compare that\n\t * against the length which was actually received.\n\t * Note that when RPCSEC/GSS (for example) is used, the\n\t * data buffer can be padded so dlen might be larger\n\t * than required.  It must never be smaller.\n\t */\n\tif (dlen < XDR_QUADLEN(len)*4)\n\t\treturn 0;\n\n\trqstp->rq_vec[0].iov_base = (void*)p;\n\trqstp->rq_vec[0].iov_len = head->iov_len - hdr;\n\tv = 0;\n\twhile (len > rqstp->rq_vec[v].iov_len) {\n\t\tlen -= rqstp->rq_vec[v].iov_len;\n\t\tv++;\n\t\trqstp->rq_vec[v].iov_base = page_address(rqstp->rq_pages[v]);\n\t\trqstp->rq_vec[v].iov_len = PAGE_SIZE;\n\t}\n\trqstp->rq_vec[v].iov_len = len;\n\targs->vlen = v + 1;\n\treturn 1;\n}\n\nint\nnfssvc_decode_createargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_createargs *args)\n{\n\tif (   !(p = decode_fh(p, &args->fh))\n\t    || !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\tp = decode_sattr(p, &args->attrs);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_renameargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_renameargs *args)\n{\n\tif (!(p = decode_fh(p, &args->ffh))\n\t || !(p = decode_filename(p, &args->fname, &args->flen))\n\t || !(p = decode_fh(p, &args->tfh))\n\t || !(p = decode_filename(p, &args->tname, &args->tlen)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_readlinkargs(struct svc_rqst *rqstp, __be32 *p, struct nfsd_readlinkargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\targs->buffer = page_address(*(rqstp->rq_next_page++));\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_linkargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_linkargs *args)\n{\n\tif (!(p = decode_fh(p, &args->ffh))\n\t || !(p = decode_fh(p, &args->tfh))\n\t || !(p = decode_filename(p, &args->tname, &args->tlen)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_symlinkargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_symlinkargs *args)\n{\n\tif (   !(p = decode_fh(p, &args->ffh))\n\t    || !(p = decode_filename(p, &args->fname, &args->flen))\n\t    || !(p = decode_pathname(p, &args->tname, &args->tlen)))\n\t\treturn 0;\n\tp = decode_sattr(p, &args->attrs);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_readdirargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_readdirargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\targs->cookie = ntohl(*p++);\n\targs->count  = ntohl(*p++);\n\targs->count  = min_t(u32, args->count, PAGE_SIZE);\n\targs->buffer = page_address(*(rqstp->rq_next_page++));\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\n/*\n * XDR encode functions\n */\nint\nnfssvc_encode_void(struct svc_rqst *rqstp, __be32 *p, void *dummy)\n{\n\treturn xdr_ressize_check(rqstp, p);\n}\n\nint\nnfssvc_encode_attrstat(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_attrstat *resp)\n{\n\tp = encode_fattr(rqstp, p, &resp->fh, &resp->stat);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\nint\nnfssvc_encode_diropres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_diropres *resp)\n{\n\tp = encode_fh(p, &resp->fh);\n\tp = encode_fattr(rqstp, p, &resp->fh, &resp->stat);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\nint\nnfssvc_encode_readlinkres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_readlinkres *resp)\n{\n\t*p++ = htonl(resp->len);\n\txdr_ressize_check(rqstp, p);\n\trqstp->rq_res.page_len = resp->len;\n\tif (resp->len & 3) {\n\t\t/* need to pad the tail */\n\t\trqstp->rq_res.tail[0].iov_base = p;\n\t\t*p = 0;\n\t\trqstp->rq_res.tail[0].iov_len = 4 - (resp->len&3);\n\t}\n\treturn 1;\n}\n\nint\nnfssvc_encode_readres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_readres *resp)\n{\n\tp = encode_fattr(rqstp, p, &resp->fh, &resp->stat);\n\t*p++ = htonl(resp->count);\n\txdr_ressize_check(rqstp, p);\n\n\t/* now update rqstp->rq_res to reflect data as well */\n\trqstp->rq_res.page_len = resp->count;\n\tif (resp->count & 3) {\n\t\t/* need to pad the tail */\n\t\trqstp->rq_res.tail[0].iov_base = p;\n\t\t*p = 0;\n\t\trqstp->rq_res.tail[0].iov_len = 4 - (resp->count&3);\n\t}\n\treturn 1;\n}\n\nint\nnfssvc_encode_readdirres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_readdirres *resp)\n{\n\txdr_ressize_check(rqstp, p);\n\tp = resp->buffer;\n\t*p++ = 0;\t\t\t/* no more entries */\n\t*p++ = htonl((resp->common.err == nfserr_eof));\n\trqstp->rq_res.page_len = (((unsigned long)p-1) & ~PAGE_MASK)+1;\n\n\treturn 1;\n}\n\nint\nnfssvc_encode_statfsres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_statfsres *resp)\n{\n\tstruct kstatfs\t*stat = &resp->stats;\n\n\t*p++ = htonl(NFSSVC_MAXBLKSIZE_V2);\t/* max transfer size */\n\t*p++ = htonl(stat->f_bsize);\n\t*p++ = htonl(stat->f_blocks);\n\t*p++ = htonl(stat->f_bfree);\n\t*p++ = htonl(stat->f_bavail);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\nint\nnfssvc_encode_entry(void *ccdv, const char *name,\n\t\t    int namlen, loff_t offset, u64 ino, unsigned int d_type)\n{\n\tstruct readdir_cd *ccd = ccdv;\n\tstruct nfsd_readdirres *cd = container_of(ccd, struct nfsd_readdirres, common);\n\t__be32\t*p = cd->buffer;\n\tint\tbuflen, slen;\n\n\t/*\n\tdprintk(\"nfsd: entry(%.*s off %ld ino %ld)\\n\",\n\t\t\tnamlen, name, offset, ino);\n\t */\n\n\tif (offset > ~((u32) 0)) {\n\t\tcd->common.err = nfserr_fbig;\n\t\treturn -EINVAL;\n\t}\n\tif (cd->offset)\n\t\t*cd->offset = htonl(offset);\n\n\t/* truncate filename */\n\tnamlen = min(namlen, NFS2_MAXNAMLEN);\n\tslen = XDR_QUADLEN(namlen);\n\n\tif ((buflen = cd->buflen - slen - 4) < 0) {\n\t\tcd->common.err = nfserr_toosmall;\n\t\treturn -EINVAL;\n\t}\n\tif (ino > ~((u32) 0)) {\n\t\tcd->common.err = nfserr_fbig;\n\t\treturn -EINVAL;\n\t}\n\t*p++ = xdr_one;\t\t\t\t/* mark entry present */\n\t*p++ = htonl((u32) ino);\t\t/* file id */\n\tp    = xdr_encode_array(p, name, namlen);/* name length & name */\n\tcd->offset = p;\t\t\t/* remember pointer */\n\t*p++ = htonl(~0U);\t\t/* offset of next entry */\n\n\tcd->buflen = buflen;\n\tcd->buffer = p;\n\tcd->common.err = nfs_ok;\n\treturn 0;\n}\n\n/*\n * XDR release functions\n */\nint\nnfssvc_release_fhandle(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_fhandle *resp)\n{\n\tfh_put(&resp->fh);\n\treturn 1;\n}\n", "/*\n * File operations used by nfsd. Some of these have been ripped from\n * other parts of the kernel because they weren't exported, others\n * are partial duplicates with added or changed functionality.\n *\n * Note that several functions dget() the dentry upon which they want\n * to act, most notably those that create directory entries. Response\n * dentry's are dput()'d if necessary in the release callback.\n * So if you notice code paths that apparently fail to dput() the\n * dentry, don't worry--they have been taken care of.\n *\n * Copyright (C) 1995-1999 Olaf Kirch <okir@monad.swb.de>\n * Zerocpy NFS support (C) 2002 Hirokazu Takahashi <taka@valinux.co.jp>\n */\n\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/splice.h>\n#include <linux/falloc.h>\n#include <linux/fcntl.h>\n#include <linux/namei.h>\n#include <linux/delay.h>\n#include <linux/fsnotify.h>\n#include <linux/posix_acl_xattr.h>\n#include <linux/xattr.h>\n#include <linux/jhash.h>\n#include <linux/ima.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/exportfs.h>\n#include <linux/writeback.h>\n#include <linux/security.h>\n\n#ifdef CONFIG_NFSD_V3\n#include \"xdr3.h\"\n#endif /* CONFIG_NFSD_V3 */\n\n#ifdef CONFIG_NFSD_V4\n#include \"../internal.h\"\n#include \"acl.h\"\n#include \"idmap.h\"\n#endif /* CONFIG_NFSD_V4 */\n\n#include \"nfsd.h\"\n#include \"vfs.h\"\n#include \"trace.h\"\n\n#define NFSDDBG_FACILITY\t\tNFSDDBG_FILEOP\n\n\n/*\n * This is a cache of readahead params that help us choose the proper\n * readahead strategy. Initially, we set all readahead parameters to 0\n * and let the VFS handle things.\n * If you increase the number of cached files very much, you'll need to\n * add a hash table here.\n */\nstruct raparms {\n\tstruct raparms\t\t*p_next;\n\tunsigned int\t\tp_count;\n\tino_t\t\t\tp_ino;\n\tdev_t\t\t\tp_dev;\n\tint\t\t\tp_set;\n\tstruct file_ra_state\tp_ra;\n\tunsigned int\t\tp_hindex;\n};\n\nstruct raparm_hbucket {\n\tstruct raparms\t\t*pb_head;\n\tspinlock_t\t\tpb_lock;\n} ____cacheline_aligned_in_smp;\n\n#define RAPARM_HASH_BITS\t4\n#define RAPARM_HASH_SIZE\t(1<<RAPARM_HASH_BITS)\n#define RAPARM_HASH_MASK\t(RAPARM_HASH_SIZE-1)\nstatic struct raparm_hbucket\traparm_hash[RAPARM_HASH_SIZE];\n\n/* \n * Called from nfsd_lookup and encode_dirent. Check if we have crossed \n * a mount point.\n * Returns -EAGAIN or -ETIMEDOUT leaving *dpp and *expp unchanged,\n *  or nfs_ok having possibly changed *dpp and *expp\n */\nint\nnfsd_cross_mnt(struct svc_rqst *rqstp, struct dentry **dpp, \n\t\t        struct svc_export **expp)\n{\n\tstruct svc_export *exp = *expp, *exp2 = NULL;\n\tstruct dentry *dentry = *dpp;\n\tstruct path path = {.mnt = mntget(exp->ex_path.mnt),\n\t\t\t    .dentry = dget(dentry)};\n\tint err = 0;\n\n\terr = follow_down(&path);\n\tif (err < 0)\n\t\tgoto out;\n\n\texp2 = rqst_exp_get_by_name(rqstp, &path);\n\tif (IS_ERR(exp2)) {\n\t\terr = PTR_ERR(exp2);\n\t\t/*\n\t\t * We normally allow NFS clients to continue\n\t\t * \"underneath\" a mountpoint that is not exported.\n\t\t * The exception is V4ROOT, where no traversal is ever\n\t\t * allowed without an explicit export of the new\n\t\t * directory.\n\t\t */\n\t\tif (err == -ENOENT && !(exp->ex_flags & NFSEXP_V4ROOT))\n\t\t\terr = 0;\n\t\tpath_put(&path);\n\t\tgoto out;\n\t}\n\tif (nfsd_v4client(rqstp) ||\n\t\t(exp->ex_flags & NFSEXP_CROSSMOUNT) || EX_NOHIDE(exp2)) {\n\t\t/* successfully crossed mount point */\n\t\t/*\n\t\t * This is subtle: path.dentry is *not* on path.mnt\n\t\t * at this point.  The only reason we are safe is that\n\t\t * original mnt is pinned down by exp, so we should\n\t\t * put path *before* putting exp\n\t\t */\n\t\t*dpp = path.dentry;\n\t\tpath.dentry = dentry;\n\t\t*expp = exp2;\n\t\texp2 = exp;\n\t}\n\tpath_put(&path);\n\texp_put(exp2);\nout:\n\treturn err;\n}\n\nstatic void follow_to_parent(struct path *path)\n{\n\tstruct dentry *dp;\n\n\twhile (path->dentry == path->mnt->mnt_root && follow_up(path))\n\t\t;\n\tdp = dget_parent(path->dentry);\n\tdput(path->dentry);\n\tpath->dentry = dp;\n}\n\nstatic int nfsd_lookup_parent(struct svc_rqst *rqstp, struct dentry *dparent, struct svc_export **exp, struct dentry **dentryp)\n{\n\tstruct svc_export *exp2;\n\tstruct path path = {.mnt = mntget((*exp)->ex_path.mnt),\n\t\t\t    .dentry = dget(dparent)};\n\n\tfollow_to_parent(&path);\n\n\texp2 = rqst_exp_parent(rqstp, &path);\n\tif (PTR_ERR(exp2) == -ENOENT) {\n\t\t*dentryp = dget(dparent);\n\t} else if (IS_ERR(exp2)) {\n\t\tpath_put(&path);\n\t\treturn PTR_ERR(exp2);\n\t} else {\n\t\t*dentryp = dget(path.dentry);\n\t\texp_put(*exp);\n\t\t*exp = exp2;\n\t}\n\tpath_put(&path);\n\treturn 0;\n}\n\n/*\n * For nfsd purposes, we treat V4ROOT exports as though there was an\n * export at *every* directory.\n */\nint nfsd_mountpoint(struct dentry *dentry, struct svc_export *exp)\n{\n\tif (d_mountpoint(dentry))\n\t\treturn 1;\n\tif (nfsd4_is_junction(dentry))\n\t\treturn 1;\n\tif (!(exp->ex_flags & NFSEXP_V4ROOT))\n\t\treturn 0;\n\treturn d_inode(dentry) != NULL;\n}\n\n__be32\nnfsd_lookup_dentry(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\t   const char *name, unsigned int len,\n\t\t   struct svc_export **exp_ret, struct dentry **dentry_ret)\n{\n\tstruct svc_export\t*exp;\n\tstruct dentry\t\t*dparent;\n\tstruct dentry\t\t*dentry;\n\tint\t\t\thost_err;\n\n\tdprintk(\"nfsd: nfsd_lookup(fh %s, %.*s)\\n\", SVCFH_fmt(fhp), len,name);\n\n\tdparent = fhp->fh_dentry;\n\texp = exp_get(fhp->fh_export);\n\n\t/* Lookup the name, but don't follow links */\n\tif (isdotent(name, len)) {\n\t\tif (len==1)\n\t\t\tdentry = dget(dparent);\n\t\telse if (dparent != exp->ex_path.dentry)\n\t\t\tdentry = dget_parent(dparent);\n\t\telse if (!EX_NOHIDE(exp) && !nfsd_v4client(rqstp))\n\t\t\tdentry = dget(dparent); /* .. == . just like at / */\n\t\telse {\n\t\t\t/* checking mountpoint crossing is very different when stepping up */\n\t\t\thost_err = nfsd_lookup_parent(rqstp, dparent, &exp, &dentry);\n\t\t\tif (host_err)\n\t\t\t\tgoto out_nfserr;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * In the nfsd4_open() case, this may be held across\n\t\t * subsequent open and delegation acquisition which may\n\t\t * need to take the child's i_mutex:\n\t\t */\n\t\tfh_lock_nested(fhp, I_MUTEX_PARENT);\n\t\tdentry = lookup_one_len(name, dparent, len);\n\t\thost_err = PTR_ERR(dentry);\n\t\tif (IS_ERR(dentry))\n\t\t\tgoto out_nfserr;\n\t\tif (nfsd_mountpoint(dentry, exp)) {\n\t\t\t/*\n\t\t\t * We don't need the i_mutex after all.  It's\n\t\t\t * still possible we could open this (regular\n\t\t\t * files can be mountpoints too), but the\n\t\t\t * i_mutex is just there to prevent renames of\n\t\t\t * something that we might be about to delegate,\n\t\t\t * and a mountpoint won't be renamed:\n\t\t\t */\n\t\t\tfh_unlock(fhp);\n\t\t\tif ((host_err = nfsd_cross_mnt(rqstp, &dentry, &exp))) {\n\t\t\t\tdput(dentry);\n\t\t\t\tgoto out_nfserr;\n\t\t\t}\n\t\t}\n\t}\n\t*dentry_ret = dentry;\n\t*exp_ret = exp;\n\treturn 0;\n\nout_nfserr:\n\texp_put(exp);\n\treturn nfserrno(host_err);\n}\n\n/*\n * Look up one component of a pathname.\n * N.B. After this call _both_ fhp and resfh need an fh_put\n *\n * If the lookup would cross a mountpoint, and the mounted filesystem\n * is exported to the client with NFSEXP_NOHIDE, then the lookup is\n * accepted as it stands and the mounted directory is\n * returned. Otherwise the covered directory is returned.\n * NOTE: this mountpoint crossing is not supported properly by all\n *   clients and is explicitly disallowed for NFSv3\n *      NeilBrown <neilb@cse.unsw.edu.au>\n */\n__be32\nnfsd_lookup(struct svc_rqst *rqstp, struct svc_fh *fhp, const char *name,\n\t\t\t\tunsigned int len, struct svc_fh *resfh)\n{\n\tstruct svc_export\t*exp;\n\tstruct dentry\t\t*dentry;\n\t__be32 err;\n\n\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_EXEC);\n\tif (err)\n\t\treturn err;\n\terr = nfsd_lookup_dentry(rqstp, fhp, name, len, &exp, &dentry);\n\tif (err)\n\t\treturn err;\n\terr = check_nfsd_access(exp, rqstp);\n\tif (err)\n\t\tgoto out;\n\t/*\n\t * Note: we compose the file handle now, but as the\n\t * dentry may be negative, it may need to be updated.\n\t */\n\terr = fh_compose(resfh, exp, dentry, fhp);\n\tif (!err && d_really_is_negative(dentry))\n\t\terr = nfserr_noent;\nout:\n\tdput(dentry);\n\texp_put(exp);\n\treturn err;\n}\n\n/*\n * Commit metadata changes to stable storage.\n */\nstatic int\ncommit_metadata(struct svc_fh *fhp)\n{\n\tstruct inode *inode = d_inode(fhp->fh_dentry);\n\tconst struct export_operations *export_ops = inode->i_sb->s_export_op;\n\n\tif (!EX_ISSYNC(fhp->fh_export))\n\t\treturn 0;\n\n\tif (export_ops->commit_metadata)\n\t\treturn export_ops->commit_metadata(inode);\n\treturn sync_inode_metadata(inode, 1);\n}\n\n/*\n * Go over the attributes and take care of the small differences between\n * NFS semantics and what Linux expects.\n */\nstatic void\nnfsd_sanitize_attrs(struct inode *inode, struct iattr *iap)\n{\n\t/* sanitize the mode change */\n\tif (iap->ia_valid & ATTR_MODE) {\n\t\tiap->ia_mode &= S_IALLUGO;\n\t\tiap->ia_mode |= (inode->i_mode & ~S_IALLUGO);\n\t}\n\n\t/* Revoke setuid/setgid on chown */\n\tif (!S_ISDIR(inode->i_mode) &&\n\t    ((iap->ia_valid & ATTR_UID) || (iap->ia_valid & ATTR_GID))) {\n\t\tiap->ia_valid |= ATTR_KILL_PRIV;\n\t\tif (iap->ia_valid & ATTR_MODE) {\n\t\t\t/* we're setting mode too, just clear the s*id bits */\n\t\t\tiap->ia_mode &= ~S_ISUID;\n\t\t\tif (iap->ia_mode & S_IXGRP)\n\t\t\t\tiap->ia_mode &= ~S_ISGID;\n\t\t} else {\n\t\t\t/* set ATTR_KILL_* bits and let VFS handle it */\n\t\t\tiap->ia_valid |= (ATTR_KILL_SUID | ATTR_KILL_SGID);\n\t\t}\n\t}\n}\n\nstatic __be32\nnfsd_get_write_access(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tstruct iattr *iap)\n{\n\tstruct inode *inode = d_inode(fhp->fh_dentry);\n\tint host_err;\n\n\tif (iap->ia_size < inode->i_size) {\n\t\t__be32 err;\n\n\t\terr = nfsd_permission(rqstp, fhp->fh_export, fhp->fh_dentry,\n\t\t\t\tNFSD_MAY_TRUNC | NFSD_MAY_OWNER_OVERRIDE);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\thost_err = get_write_access(inode);\n\tif (host_err)\n\t\tgoto out_nfserrno;\n\n\thost_err = locks_verify_truncate(inode, NULL, iap->ia_size);\n\tif (host_err)\n\t\tgoto out_put_write_access;\n\treturn 0;\n\nout_put_write_access:\n\tput_write_access(inode);\nout_nfserrno:\n\treturn nfserrno(host_err);\n}\n\n/*\n * Set various file attributes.  After this call fhp needs an fh_put.\n */\n__be32\nnfsd_setattr(struct svc_rqst *rqstp, struct svc_fh *fhp, struct iattr *iap,\n\t     int check_guard, time_t guardtime)\n{\n\tstruct dentry\t*dentry;\n\tstruct inode\t*inode;\n\tint\t\taccmode = NFSD_MAY_SATTR;\n\tumode_t\t\tftype = 0;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\tbool\t\tget_write_count;\n\tbool\t\tsize_change = (iap->ia_valid & ATTR_SIZE);\n\n\tif (iap->ia_valid & (ATTR_ATIME | ATTR_MTIME | ATTR_SIZE))\n\t\taccmode |= NFSD_MAY_WRITE|NFSD_MAY_OWNER_OVERRIDE;\n\tif (iap->ia_valid & ATTR_SIZE)\n\t\tftype = S_IFREG;\n\n\t/* Callers that do fh_verify should do the fh_want_write: */\n\tget_write_count = !fhp->fh_dentry;\n\n\t/* Get inode */\n\terr = fh_verify(rqstp, fhp, ftype, accmode);\n\tif (err)\n\t\treturn err;\n\tif (get_write_count) {\n\t\thost_err = fh_want_write(fhp);\n\t\tif (host_err)\n\t\t\tgoto out;\n\t}\n\n\tdentry = fhp->fh_dentry;\n\tinode = d_inode(dentry);\n\n\t/* Ignore any mode updates on symlinks */\n\tif (S_ISLNK(inode->i_mode))\n\t\tiap->ia_valid &= ~ATTR_MODE;\n\n\tif (!iap->ia_valid)\n\t\treturn 0;\n\n\tnfsd_sanitize_attrs(inode, iap);\n\n\tif (check_guard && guardtime != inode->i_ctime.tv_sec)\n\t\treturn nfserr_notsync;\n\n\t/*\n\t * The size case is special, it changes the file in addition to the\n\t * attributes, and file systems don't expect it to be mixed with\n\t * \"random\" attribute changes.  We thus split out the size change\n\t * into a separate call to ->setattr, and do the rest as a separate\n\t * setattr call.\n\t */\n\tif (size_change) {\n\t\terr = nfsd_get_write_access(rqstp, fhp, iap);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tfh_lock(fhp);\n\tif (size_change) {\n\t\t/*\n\t\t * RFC5661, Section 18.30.4:\n\t\t *   Changing the size of a file with SETATTR indirectly\n\t\t *   changes the time_modify and change attributes.\n\t\t *\n\t\t * (and similar for the older RFCs)\n\t\t */\n\t\tstruct iattr size_attr = {\n\t\t\t.ia_valid\t= ATTR_SIZE | ATTR_CTIME | ATTR_MTIME,\n\t\t\t.ia_size\t= iap->ia_size,\n\t\t};\n\n\t\thost_err = notify_change(dentry, &size_attr, NULL);\n\t\tif (host_err)\n\t\t\tgoto out_unlock;\n\t\tiap->ia_valid &= ~ATTR_SIZE;\n\n\t\t/*\n\t\t * Avoid the additional setattr call below if the only other\n\t\t * attribute that the client sends is the mtime, as we update\n\t\t * it as part of the size change above.\n\t\t */\n\t\tif ((iap->ia_valid & ~ATTR_MTIME) == 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\tiap->ia_valid |= ATTR_CTIME;\n\thost_err = notify_change(dentry, iap, NULL);\n\nout_unlock:\n\tfh_unlock(fhp);\n\tif (size_change)\n\t\tput_write_access(inode);\nout:\n\tif (!host_err)\n\t\thost_err = commit_metadata(fhp);\n\treturn nfserrno(host_err);\n}\n\n#if defined(CONFIG_NFSD_V4)\n/*\n * NFS junction information is stored in an extended attribute.\n */\n#define NFSD_JUNCTION_XATTR_NAME\tXATTR_TRUSTED_PREFIX \"junction.nfs\"\n\n/**\n * nfsd4_is_junction - Test if an object could be an NFS junction\n *\n * @dentry: object to test\n *\n * Returns 1 if \"dentry\" appears to contain NFS junction information.\n * Otherwise 0 is returned.\n */\nint nfsd4_is_junction(struct dentry *dentry)\n{\n\tstruct inode *inode = d_inode(dentry);\n\n\tif (inode == NULL)\n\t\treturn 0;\n\tif (inode->i_mode & S_IXUGO)\n\t\treturn 0;\n\tif (!(inode->i_mode & S_ISVTX))\n\t\treturn 0;\n\tif (vfs_getxattr(dentry, NFSD_JUNCTION_XATTR_NAME, NULL, 0) <= 0)\n\t\treturn 0;\n\treturn 1;\n}\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n__be32 nfsd4_set_nfs4_label(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tstruct xdr_netobj *label)\n{\n\t__be32 error;\n\tint host_error;\n\tstruct dentry *dentry;\n\n\terror = fh_verify(rqstp, fhp, 0 /* S_IFREG */, NFSD_MAY_SATTR);\n\tif (error)\n\t\treturn error;\n\n\tdentry = fhp->fh_dentry;\n\n\tinode_lock(d_inode(dentry));\n\thost_error = security_inode_setsecctx(dentry, label->data, label->len);\n\tinode_unlock(d_inode(dentry));\n\treturn nfserrno(host_error);\n}\n#else\n__be32 nfsd4_set_nfs4_label(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tstruct xdr_netobj *label)\n{\n\treturn nfserr_notsupp;\n}\n#endif\n\n__be32 nfsd4_clone_file_range(struct file *src, u64 src_pos, struct file *dst,\n\t\tu64 dst_pos, u64 count)\n{\n\treturn nfserrno(do_clone_file_range(src, src_pos, dst, dst_pos, count));\n}\n\nssize_t nfsd_copy_file_range(struct file *src, u64 src_pos, struct file *dst,\n\t\t\t     u64 dst_pos, u64 count)\n{\n\n\t/*\n\t * Limit copy to 4MB to prevent indefinitely blocking an nfsd\n\t * thread and client rpc slot.  The choice of 4MB is somewhat\n\t * arbitrary.  We might instead base this on r/wsize, or make it\n\t * tunable, or use a time instead of a byte limit, or implement\n\t * asynchronous copy.  In theory a client could also recognize a\n\t * limit like this and pipeline multiple COPY requests.\n\t */\n\tcount = min_t(u64, count, 1 << 22);\n\treturn vfs_copy_file_range(src, src_pos, dst, dst_pos, count, 0);\n}\n\n__be32 nfsd4_vfs_fallocate(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\t\t   struct file *file, loff_t offset, loff_t len,\n\t\t\t   int flags)\n{\n\tint error;\n\n\tif (!S_ISREG(file_inode(file)->i_mode))\n\t\treturn nfserr_inval;\n\n\terror = vfs_fallocate(file, flags, offset, len);\n\tif (!error)\n\t\terror = commit_metadata(fhp);\n\n\treturn nfserrno(error);\n}\n#endif /* defined(CONFIG_NFSD_V4) */\n\n#ifdef CONFIG_NFSD_V3\n/*\n * Check server access rights to a file system object\n */\nstruct accessmap {\n\tu32\t\taccess;\n\tint\t\thow;\n};\nstatic struct accessmap\tnfs3_regaccess[] = {\n    {\tNFS3_ACCESS_READ,\tNFSD_MAY_READ\t\t\t},\n    {\tNFS3_ACCESS_EXECUTE,\tNFSD_MAY_EXEC\t\t\t},\n    {\tNFS3_ACCESS_MODIFY,\tNFSD_MAY_WRITE|NFSD_MAY_TRUNC\t},\n    {\tNFS3_ACCESS_EXTEND,\tNFSD_MAY_WRITE\t\t\t},\n\n    {\t0,\t\t\t0\t\t\t\t}\n};\n\nstatic struct accessmap\tnfs3_diraccess[] = {\n    {\tNFS3_ACCESS_READ,\tNFSD_MAY_READ\t\t\t},\n    {\tNFS3_ACCESS_LOOKUP,\tNFSD_MAY_EXEC\t\t\t},\n    {\tNFS3_ACCESS_MODIFY,\tNFSD_MAY_EXEC|NFSD_MAY_WRITE|NFSD_MAY_TRUNC},\n    {\tNFS3_ACCESS_EXTEND,\tNFSD_MAY_EXEC|NFSD_MAY_WRITE\t},\n    {\tNFS3_ACCESS_DELETE,\tNFSD_MAY_REMOVE\t\t\t},\n\n    {\t0,\t\t\t0\t\t\t\t}\n};\n\nstatic struct accessmap\tnfs3_anyaccess[] = {\n\t/* Some clients - Solaris 2.6 at least, make an access call\n\t * to the server to check for access for things like /dev/null\n\t * (which really, the server doesn't care about).  So\n\t * We provide simple access checking for them, looking\n\t * mainly at mode bits, and we make sure to ignore read-only\n\t * filesystem checks\n\t */\n    {\tNFS3_ACCESS_READ,\tNFSD_MAY_READ\t\t\t},\n    {\tNFS3_ACCESS_EXECUTE,\tNFSD_MAY_EXEC\t\t\t},\n    {\tNFS3_ACCESS_MODIFY,\tNFSD_MAY_WRITE|NFSD_MAY_LOCAL_ACCESS\t},\n    {\tNFS3_ACCESS_EXTEND,\tNFSD_MAY_WRITE|NFSD_MAY_LOCAL_ACCESS\t},\n\n    {\t0,\t\t\t0\t\t\t\t}\n};\n\n__be32\nnfsd_access(struct svc_rqst *rqstp, struct svc_fh *fhp, u32 *access, u32 *supported)\n{\n\tstruct accessmap\t*map;\n\tstruct svc_export\t*export;\n\tstruct dentry\t\t*dentry;\n\tu32\t\t\tquery, result = 0, sresult = 0;\n\t__be32\t\t\terror;\n\n\terror = fh_verify(rqstp, fhp, 0, NFSD_MAY_NOP);\n\tif (error)\n\t\tgoto out;\n\n\texport = fhp->fh_export;\n\tdentry = fhp->fh_dentry;\n\n\tif (d_is_reg(dentry))\n\t\tmap = nfs3_regaccess;\n\telse if (d_is_dir(dentry))\n\t\tmap = nfs3_diraccess;\n\telse\n\t\tmap = nfs3_anyaccess;\n\n\n\tquery = *access;\n\tfor  (; map->access; map++) {\n\t\tif (map->access & query) {\n\t\t\t__be32 err2;\n\n\t\t\tsresult |= map->access;\n\n\t\t\terr2 = nfsd_permission(rqstp, export, dentry, map->how);\n\t\t\tswitch (err2) {\n\t\t\tcase nfs_ok:\n\t\t\t\tresult |= map->access;\n\t\t\t\tbreak;\n\t\t\t\t\n\t\t\t/* the following error codes just mean the access was not allowed,\n\t\t\t * rather than an error occurred */\n\t\t\tcase nfserr_rofs:\n\t\t\tcase nfserr_acces:\n\t\t\tcase nfserr_perm:\n\t\t\t\t/* simply don't \"or\" in the access bit. */\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terror = err2;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\t*access = result;\n\tif (supported)\n\t\t*supported = sresult;\n\n out:\n\treturn error;\n}\n#endif /* CONFIG_NFSD_V3 */\n\nstatic int nfsd_open_break_lease(struct inode *inode, int access)\n{\n\tunsigned int mode;\n\n\tif (access & NFSD_MAY_NOT_BREAK_LEASE)\n\t\treturn 0;\n\tmode = (access & NFSD_MAY_WRITE) ? O_WRONLY : O_RDONLY;\n\treturn break_lease(inode, mode | O_NONBLOCK);\n}\n\n/*\n * Open an existing file or directory.\n * The may_flags argument indicates the type of open (read/write/lock)\n * and additional flags.\n * N.B. After this call fhp needs an fh_put\n */\n__be32\nnfsd_open(struct svc_rqst *rqstp, struct svc_fh *fhp, umode_t type,\n\t\t\tint may_flags, struct file **filp)\n{\n\tstruct path\tpath;\n\tstruct inode\t*inode;\n\tstruct file\t*file;\n\tint\t\tflags = O_RDONLY|O_LARGEFILE;\n\t__be32\t\terr;\n\tint\t\thost_err = 0;\n\n\tvalidate_process_creds();\n\n\t/*\n\t * If we get here, then the client has already done an \"open\",\n\t * and (hopefully) checked permission - so allow OWNER_OVERRIDE\n\t * in case a chmod has now revoked permission.\n\t *\n\t * Arguably we should also allow the owner override for\n\t * directories, but we never have and it doesn't seem to have\n\t * caused anyone a problem.  If we were to change this, note\n\t * also that our filldir callbacks would need a variant of\n\t * lookup_one_len that doesn't check permissions.\n\t */\n\tif (type == S_IFREG)\n\t\tmay_flags |= NFSD_MAY_OWNER_OVERRIDE;\n\terr = fh_verify(rqstp, fhp, type, may_flags);\n\tif (err)\n\t\tgoto out;\n\n\tpath.mnt = fhp->fh_export->ex_path.mnt;\n\tpath.dentry = fhp->fh_dentry;\n\tinode = d_inode(path.dentry);\n\n\t/* Disallow write access to files with the append-only bit set\n\t * or any access when mandatory locking enabled\n\t */\n\terr = nfserr_perm;\n\tif (IS_APPEND(inode) && (may_flags & NFSD_MAY_WRITE))\n\t\tgoto out;\n\t/*\n\t * We must ignore files (but only files) which might have mandatory\n\t * locks on them because there is no way to know if the accesser has\n\t * the lock.\n\t */\n\tif (S_ISREG((inode)->i_mode) && mandatory_lock(inode))\n\t\tgoto out;\n\n\tif (!inode->i_fop)\n\t\tgoto out;\n\n\thost_err = nfsd_open_break_lease(inode, may_flags);\n\tif (host_err) /* NOMEM or WOULDBLOCK */\n\t\tgoto out_nfserr;\n\n\tif (may_flags & NFSD_MAY_WRITE) {\n\t\tif (may_flags & NFSD_MAY_READ)\n\t\t\tflags = O_RDWR|O_LARGEFILE;\n\t\telse\n\t\t\tflags = O_WRONLY|O_LARGEFILE;\n\t}\n\n\tfile = dentry_open(&path, flags, current_cred());\n\tif (IS_ERR(file)) {\n\t\thost_err = PTR_ERR(file);\n\t\tgoto out_nfserr;\n\t}\n\n\thost_err = ima_file_check(file, may_flags, 0);\n\tif (host_err) {\n\t\tfput(file);\n\t\tgoto out_nfserr;\n\t}\n\n\tif (may_flags & NFSD_MAY_64BIT_COOKIE)\n\t\tfile->f_mode |= FMODE_64BITHASH;\n\telse\n\t\tfile->f_mode |= FMODE_32BITHASH;\n\n\t*filp = file;\nout_nfserr:\n\terr = nfserrno(host_err);\nout:\n\tvalidate_process_creds();\n\treturn err;\n}\n\nstruct raparms *\nnfsd_init_raparms(struct file *file)\n{\n\tstruct inode *inode = file_inode(file);\n\tdev_t dev = inode->i_sb->s_dev;\n\tino_t ino = inode->i_ino;\n\tstruct raparms\t*ra, **rap, **frap = NULL;\n\tint depth = 0;\n\tunsigned int hash;\n\tstruct raparm_hbucket *rab;\n\n\thash = jhash_2words(dev, ino, 0xfeedbeef) & RAPARM_HASH_MASK;\n\trab = &raparm_hash[hash];\n\n\tspin_lock(&rab->pb_lock);\n\tfor (rap = &rab->pb_head; (ra = *rap); rap = &ra->p_next) {\n\t\tif (ra->p_ino == ino && ra->p_dev == dev)\n\t\t\tgoto found;\n\t\tdepth++;\n\t\tif (ra->p_count == 0)\n\t\t\tfrap = rap;\n\t}\n\tdepth = nfsdstats.ra_size;\n\tif (!frap) {\t\n\t\tspin_unlock(&rab->pb_lock);\n\t\treturn NULL;\n\t}\n\trap = frap;\n\tra = *frap;\n\tra->p_dev = dev;\n\tra->p_ino = ino;\n\tra->p_set = 0;\n\tra->p_hindex = hash;\nfound:\n\tif (rap != &rab->pb_head) {\n\t\t*rap = ra->p_next;\n\t\tra->p_next   = rab->pb_head;\n\t\trab->pb_head = ra;\n\t}\n\tra->p_count++;\n\tnfsdstats.ra_depth[depth*10/nfsdstats.ra_size]++;\n\tspin_unlock(&rab->pb_lock);\n\n\tif (ra->p_set)\n\t\tfile->f_ra = ra->p_ra;\n\treturn ra;\n}\n\nvoid nfsd_put_raparams(struct file *file, struct raparms *ra)\n{\n\tstruct raparm_hbucket *rab = &raparm_hash[ra->p_hindex];\n\n\tspin_lock(&rab->pb_lock);\n\tra->p_ra = file->f_ra;\n\tra->p_set = 1;\n\tra->p_count--;\n\tspin_unlock(&rab->pb_lock);\n}\n\n/*\n * Grab and keep cached pages associated with a file in the svc_rqst\n * so that they can be passed to the network sendmsg/sendpage routines\n * directly. They will be released after the sending has completed.\n */\nstatic int\nnfsd_splice_actor(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t  struct splice_desc *sd)\n{\n\tstruct svc_rqst *rqstp = sd->u.data;\n\tstruct page **pp = rqstp->rq_next_page;\n\tstruct page *page = buf->page;\n\tsize_t size;\n\n\tsize = sd->len;\n\n\tif (rqstp->rq_res.page_len == 0) {\n\t\tget_page(page);\n\t\tput_page(*rqstp->rq_next_page);\n\t\t*(rqstp->rq_next_page++) = page;\n\t\trqstp->rq_res.page_base = buf->offset;\n\t\trqstp->rq_res.page_len = size;\n\t} else if (page != pp[-1]) {\n\t\tget_page(page);\n\t\tif (*rqstp->rq_next_page)\n\t\t\tput_page(*rqstp->rq_next_page);\n\t\t*(rqstp->rq_next_page++) = page;\n\t\trqstp->rq_res.page_len += size;\n\t} else\n\t\trqstp->rq_res.page_len += size;\n\n\treturn size;\n}\n\nstatic int nfsd_direct_splice_actor(struct pipe_inode_info *pipe,\n\t\t\t\t    struct splice_desc *sd)\n{\n\treturn __splice_from_pipe(pipe, sd, nfsd_splice_actor);\n}\n\nstatic __be32\nnfsd_finish_read(struct file *file, unsigned long *count, int host_err)\n{\n\tif (host_err >= 0) {\n\t\tnfsdstats.io_read += host_err;\n\t\t*count = host_err;\n\t\tfsnotify_access(file);\n\t\treturn 0;\n\t} else \n\t\treturn nfserrno(host_err);\n}\n\n__be32 nfsd_splice_read(struct svc_rqst *rqstp,\n\t\t     struct file *file, loff_t offset, unsigned long *count)\n{\n\tstruct splice_desc sd = {\n\t\t.len\t\t= 0,\n\t\t.total_len\t= *count,\n\t\t.pos\t\t= offset,\n\t\t.u.data\t\t= rqstp,\n\t};\n\tint host_err;\n\n\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\thost_err = splice_direct_to_actor(file, &sd, nfsd_direct_splice_actor);\n\treturn nfsd_finish_read(file, count, host_err);\n}\n\n__be32 nfsd_readv(struct file *file, loff_t offset, struct kvec *vec, int vlen,\n\t\tunsigned long *count)\n{\n\tmm_segment_t oldfs;\n\tint host_err;\n\n\toldfs = get_fs();\n\tset_fs(KERNEL_DS);\n\thost_err = vfs_readv(file, (struct iovec __user *)vec, vlen, &offset, 0);\n\tset_fs(oldfs);\n\treturn nfsd_finish_read(file, count, host_err);\n}\n\nstatic __be32\nnfsd_vfs_read(struct svc_rqst *rqstp, struct file *file,\n\t      loff_t offset, struct kvec *vec, int vlen, unsigned long *count)\n{\n\tif (file->f_op->splice_read && test_bit(RQ_SPLICE_OK, &rqstp->rq_flags))\n\t\treturn nfsd_splice_read(rqstp, file, offset, count);\n\telse\n\t\treturn nfsd_readv(file, offset, vec, vlen, count);\n}\n\n/*\n * Gathered writes: If another process is currently writing to the file,\n * there's a high chance this is another nfsd (triggered by a bulk write\n * from a client's biod). Rather than syncing the file with each write\n * request, we sleep for 10 msec.\n *\n * I don't know if this roughly approximates C. Juszak's idea of\n * gathered writes, but it's a nice and simple solution (IMHO), and it\n * seems to work:-)\n *\n * Note: we do this only in the NFSv2 case, since v3 and higher have a\n * better tool (separate unstable writes and commits) for solving this\n * problem.\n */\nstatic int wait_for_concurrent_writes(struct file *file)\n{\n\tstruct inode *inode = file_inode(file);\n\tstatic ino_t last_ino;\n\tstatic dev_t last_dev;\n\tint err = 0;\n\n\tif (atomic_read(&inode->i_writecount) > 1\n\t    || (last_ino == inode->i_ino && last_dev == inode->i_sb->s_dev)) {\n\t\tdprintk(\"nfsd: write defer %d\\n\", task_pid_nr(current));\n\t\tmsleep(10);\n\t\tdprintk(\"nfsd: write resume %d\\n\", task_pid_nr(current));\n\t}\n\n\tif (inode->i_state & I_DIRTY) {\n\t\tdprintk(\"nfsd: write sync %d\\n\", task_pid_nr(current));\n\t\terr = vfs_fsync(file, 0);\n\t}\n\tlast_ino = inode->i_ino;\n\tlast_dev = inode->i_sb->s_dev;\n\treturn err;\n}\n\n__be32\nnfsd_vfs_write(struct svc_rqst *rqstp, struct svc_fh *fhp, struct file *file,\n\t\t\t\tloff_t offset, struct kvec *vec, int vlen,\n\t\t\t\tunsigned long *cnt, int stable)\n{\n\tstruct svc_export\t*exp;\n\tmm_segment_t\t\toldfs;\n\t__be32\t\t\terr = 0;\n\tint\t\t\thost_err;\n\tint\t\t\tuse_wgather;\n\tloff_t\t\t\tpos = offset;\n\tunsigned int\t\tpflags = current->flags;\n\tint\t\t\tflags = 0;\n\n\tif (test_bit(RQ_LOCAL, &rqstp->rq_flags))\n\t\t/*\n\t\t * We want less throttling in balance_dirty_pages()\n\t\t * and shrink_inactive_list() so that nfs to\n\t\t * localhost doesn't cause nfsd to lock up due to all\n\t\t * the client's dirty pages or its congested queue.\n\t\t */\n\t\tcurrent->flags |= PF_LESS_THROTTLE;\n\n\texp = fhp->fh_export;\n\tuse_wgather = (rqstp->rq_vers == 2) && EX_WGATHER(exp);\n\n\tif (!EX_ISSYNC(exp))\n\t\tstable = NFS_UNSTABLE;\n\n\tif (stable && !use_wgather)\n\t\tflags |= RWF_SYNC;\n\n\t/* Write the data. */\n\toldfs = get_fs(); set_fs(KERNEL_DS);\n\thost_err = vfs_writev(file, (struct iovec __user *)vec, vlen, &pos, flags);\n\tset_fs(oldfs);\n\tif (host_err < 0)\n\t\tgoto out_nfserr;\n\t*cnt = host_err;\n\tnfsdstats.io_write += host_err;\n\tfsnotify_modify(file);\n\n\tif (stable && use_wgather)\n\t\thost_err = wait_for_concurrent_writes(file);\n\nout_nfserr:\n\tdprintk(\"nfsd: write complete host_err=%d\\n\", host_err);\n\tif (host_err >= 0)\n\t\terr = 0;\n\telse\n\t\terr = nfserrno(host_err);\n\tif (test_bit(RQ_LOCAL, &rqstp->rq_flags))\n\t\tcurrent_restore_flags(pflags, PF_LESS_THROTTLE);\n\treturn err;\n}\n\n/*\n * Read data from a file. count must contain the requested read count\n * on entry. On return, *count contains the number of bytes actually read.\n * N.B. After this call fhp needs an fh_put\n */\n__be32 nfsd_read(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\tloff_t offset, struct kvec *vec, int vlen, unsigned long *count)\n{\n\tstruct file *file;\n\tstruct raparms\t*ra;\n\t__be32 err;\n\n\ttrace_read_start(rqstp, fhp, offset, vlen);\n\terr = nfsd_open(rqstp, fhp, S_IFREG, NFSD_MAY_READ, &file);\n\tif (err)\n\t\treturn err;\n\n\tra = nfsd_init_raparms(file);\n\n\ttrace_read_opened(rqstp, fhp, offset, vlen);\n\terr = nfsd_vfs_read(rqstp, file, offset, vec, vlen, count);\n\ttrace_read_io_done(rqstp, fhp, offset, vlen);\n\n\tif (ra)\n\t\tnfsd_put_raparams(file, ra);\n\tfput(file);\n\n\ttrace_read_done(rqstp, fhp, offset, vlen);\n\n\treturn err;\n}\n\n/*\n * Write data to a file.\n * The stable flag requests synchronous writes.\n * N.B. After this call fhp needs an fh_put\n */\n__be32\nnfsd_write(struct svc_rqst *rqstp, struct svc_fh *fhp, loff_t offset,\n\t   struct kvec *vec, int vlen, unsigned long *cnt, int stable)\n{\n\tstruct file *file = NULL;\n\t__be32 err = 0;\n\n\ttrace_write_start(rqstp, fhp, offset, vlen);\n\n\terr = nfsd_open(rqstp, fhp, S_IFREG, NFSD_MAY_WRITE, &file);\n\tif (err)\n\t\tgoto out;\n\n\ttrace_write_opened(rqstp, fhp, offset, vlen);\n\terr = nfsd_vfs_write(rqstp, fhp, file, offset, vec, vlen, cnt, stable);\n\ttrace_write_io_done(rqstp, fhp, offset, vlen);\n\tfput(file);\nout:\n\ttrace_write_done(rqstp, fhp, offset, vlen);\n\treturn err;\n}\n\n#ifdef CONFIG_NFSD_V3\n/*\n * Commit all pending writes to stable storage.\n *\n * Note: we only guarantee that data that lies within the range specified\n * by the 'offset' and 'count' parameters will be synced.\n *\n * Unfortunately we cannot lock the file to make sure we return full WCC\n * data to the client, as locking happens lower down in the filesystem.\n */\n__be32\nnfsd_commit(struct svc_rqst *rqstp, struct svc_fh *fhp,\n               loff_t offset, unsigned long count)\n{\n\tstruct file\t*file;\n\tloff_t\t\tend = LLONG_MAX;\n\t__be32\t\terr = nfserr_inval;\n\n\tif (offset < 0)\n\t\tgoto out;\n\tif (count != 0) {\n\t\tend = offset + (loff_t)count - 1;\n\t\tif (end < offset)\n\t\t\tgoto out;\n\t}\n\n\terr = nfsd_open(rqstp, fhp, S_IFREG,\n\t\t\tNFSD_MAY_WRITE|NFSD_MAY_NOT_BREAK_LEASE, &file);\n\tif (err)\n\t\tgoto out;\n\tif (EX_ISSYNC(fhp->fh_export)) {\n\t\tint err2 = vfs_fsync_range(file, offset, end, 0);\n\n\t\tif (err2 != -EINVAL)\n\t\t\terr = nfserrno(err2);\n\t\telse\n\t\t\terr = nfserr_notsupp;\n\t}\n\n\tfput(file);\nout:\n\treturn err;\n}\n#endif /* CONFIG_NFSD_V3 */\n\nstatic __be32\nnfsd_create_setattr(struct svc_rqst *rqstp, struct svc_fh *resfhp,\n\t\t\tstruct iattr *iap)\n{\n\t/*\n\t * Mode has already been set earlier in create:\n\t */\n\tiap->ia_valid &= ~ATTR_MODE;\n\t/*\n\t * Setting uid/gid works only for root.  Irix appears to\n\t * send along the gid on create when it tries to implement\n\t * setgid directories via NFS:\n\t */\n\tif (!uid_eq(current_fsuid(), GLOBAL_ROOT_UID))\n\t\tiap->ia_valid &= ~(ATTR_UID|ATTR_GID);\n\tif (iap->ia_valid)\n\t\treturn nfsd_setattr(rqstp, resfhp, iap, 0, (time_t)0);\n\t/* Callers expect file metadata to be committed here */\n\treturn nfserrno(commit_metadata(resfhp));\n}\n\n/* HPUX client sometimes creates a file in mode 000, and sets size to 0.\n * setting size to 0 may fail for some specific file systems by the permission\n * checking which requires WRITE permission but the mode is 000.\n * we ignore the resizing(to 0) on the just new created file, since the size is\n * 0 after file created.\n *\n * call this only after vfs_create() is called.\n * */\nstatic void\nnfsd_check_ignore_resizing(struct iattr *iap)\n{\n\tif ((iap->ia_valid & ATTR_SIZE) && (iap->ia_size == 0))\n\t\tiap->ia_valid &= ~ATTR_SIZE;\n}\n\n/* The parent directory should already be locked: */\n__be32\nnfsd_create_locked(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tchar *fname, int flen, struct iattr *iap,\n\t\tint type, dev_t rdev, struct svc_fh *resfhp)\n{\n\tstruct dentry\t*dentry, *dchild;\n\tstruct inode\t*dirp;\n\t__be32\t\terr;\n\t__be32\t\terr2;\n\tint\t\thost_err;\n\n\tdentry = fhp->fh_dentry;\n\tdirp = d_inode(dentry);\n\n\tdchild = dget(resfhp->fh_dentry);\n\tif (!fhp->fh_locked) {\n\t\tWARN_ONCE(1, \"nfsd_create: parent %pd2 not locked!\\n\",\n\t\t\t\tdentry);\n\t\terr = nfserr_io;\n\t\tgoto out;\n\t}\n\n\terr = nfsd_permission(rqstp, fhp->fh_export, dentry, NFSD_MAY_CREATE);\n\tif (err)\n\t\tgoto out;\n\n\tif (!(iap->ia_valid & ATTR_MODE))\n\t\tiap->ia_mode = 0;\n\tiap->ia_mode = (iap->ia_mode & S_IALLUGO) | type;\n\n\terr = 0;\n\thost_err = 0;\n\tswitch (type) {\n\tcase S_IFREG:\n\t\thost_err = vfs_create(dirp, dchild, iap->ia_mode, true);\n\t\tif (!host_err)\n\t\t\tnfsd_check_ignore_resizing(iap);\n\t\tbreak;\n\tcase S_IFDIR:\n\t\thost_err = vfs_mkdir(dirp, dchild, iap->ia_mode);\n\t\tbreak;\n\tcase S_IFCHR:\n\tcase S_IFBLK:\n\tcase S_IFIFO:\n\tcase S_IFSOCK:\n\t\thost_err = vfs_mknod(dirp, dchild, iap->ia_mode, rdev);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_WARNING \"nfsd: bad file type %o in nfsd_create\\n\",\n\t\t       type);\n\t\thost_err = -EINVAL;\n\t}\n\tif (host_err < 0)\n\t\tgoto out_nfserr;\n\n\terr = nfsd_create_setattr(rqstp, resfhp, iap);\n\n\t/*\n\t * nfsd_create_setattr already committed the child.  Transactional\n\t * filesystems had a chance to commit changes for both parent and\n\t * child simultaneously making the following commit_metadata a\n\t * noop.\n\t */\n\terr2 = nfserrno(commit_metadata(fhp));\n\tif (err2)\n\t\terr = err2;\n\t/*\n\t * Update the file handle to get the new inode info.\n\t */\n\tif (!err)\n\t\terr = fh_update(resfhp);\nout:\n\tdput(dchild);\n\treturn err;\n\nout_nfserr:\n\terr = nfserrno(host_err);\n\tgoto out;\n}\n\n/*\n * Create a filesystem object (regular, directory, special).\n * Note that the parent directory is left locked.\n *\n * N.B. Every call to nfsd_create needs an fh_put for _both_ fhp and resfhp\n */\n__be32\nnfsd_create(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tchar *fname, int flen, struct iattr *iap,\n\t\tint type, dev_t rdev, struct svc_fh *resfhp)\n{\n\tstruct dentry\t*dentry, *dchild = NULL;\n\tstruct inode\t*dirp;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\n\tif (isdotent(fname, flen))\n\t\treturn nfserr_exist;\n\n\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_NOP);\n\tif (err)\n\t\treturn err;\n\n\tdentry = fhp->fh_dentry;\n\tdirp = d_inode(dentry);\n\n\thost_err = fh_want_write(fhp);\n\tif (host_err)\n\t\treturn nfserrno(host_err);\n\n\tfh_lock_nested(fhp, I_MUTEX_PARENT);\n\tdchild = lookup_one_len(fname, dentry, flen);\n\thost_err = PTR_ERR(dchild);\n\tif (IS_ERR(dchild))\n\t\treturn nfserrno(host_err);\n\terr = fh_compose(resfhp, fhp->fh_export, dchild, fhp);\n\t/*\n\t * We unconditionally drop our ref to dchild as fh_compose will have\n\t * already grabbed its own ref for it.\n\t */\n\tdput(dchild);\n\tif (err)\n\t\treturn err;\n\treturn nfsd_create_locked(rqstp, fhp, fname, flen, iap, type,\n\t\t\t\t\trdev, resfhp);\n}\n\n#ifdef CONFIG_NFSD_V3\n\n/*\n * NFSv3 and NFSv4 version of nfsd_create\n */\n__be32\ndo_nfsd_create(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tchar *fname, int flen, struct iattr *iap,\n\t\tstruct svc_fh *resfhp, int createmode, u32 *verifier,\n\t        bool *truncp, bool *created)\n{\n\tstruct dentry\t*dentry, *dchild = NULL;\n\tstruct inode\t*dirp;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\t__u32\t\tv_mtime=0, v_atime=0;\n\n\terr = nfserr_perm;\n\tif (!flen)\n\t\tgoto out;\n\terr = nfserr_exist;\n\tif (isdotent(fname, flen))\n\t\tgoto out;\n\tif (!(iap->ia_valid & ATTR_MODE))\n\t\tiap->ia_mode = 0;\n\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_EXEC);\n\tif (err)\n\t\tgoto out;\n\n\tdentry = fhp->fh_dentry;\n\tdirp = d_inode(dentry);\n\n\thost_err = fh_want_write(fhp);\n\tif (host_err)\n\t\tgoto out_nfserr;\n\n\tfh_lock_nested(fhp, I_MUTEX_PARENT);\n\n\t/*\n\t * Compose the response file handle.\n\t */\n\tdchild = lookup_one_len(fname, dentry, flen);\n\thost_err = PTR_ERR(dchild);\n\tif (IS_ERR(dchild))\n\t\tgoto out_nfserr;\n\n\t/* If file doesn't exist, check for permissions to create one */\n\tif (d_really_is_negative(dchild)) {\n\t\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_CREATE);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = fh_compose(resfhp, fhp->fh_export, dchild, fhp);\n\tif (err)\n\t\tgoto out;\n\n\tif (nfsd_create_is_exclusive(createmode)) {\n\t\t/* solaris7 gets confused (bugid 4218508) if these have\n\t\t * the high bit set, so just clear the high bits. If this is\n\t\t * ever changed to use different attrs for storing the\n\t\t * verifier, then do_open_lookup() will also need to be fixed\n\t\t * accordingly.\n\t\t */\n\t\tv_mtime = verifier[0]&0x7fffffff;\n\t\tv_atime = verifier[1]&0x7fffffff;\n\t}\n\t\n\tif (d_really_is_positive(dchild)) {\n\t\terr = 0;\n\n\t\tswitch (createmode) {\n\t\tcase NFS3_CREATE_UNCHECKED:\n\t\t\tif (! d_is_reg(dchild))\n\t\t\t\tgoto out;\n\t\t\telse if (truncp) {\n\t\t\t\t/* in nfsv4, we need to treat this case a little\n\t\t\t\t * differently.  we don't want to truncate the\n\t\t\t\t * file now; this would be wrong if the OPEN\n\t\t\t\t * fails for some other reason.  furthermore,\n\t\t\t\t * if the size is nonzero, we should ignore it\n\t\t\t\t * according to spec!\n\t\t\t\t */\n\t\t\t\t*truncp = (iap->ia_valid & ATTR_SIZE) && !iap->ia_size;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tiap->ia_valid &= ATTR_SIZE;\n\t\t\t\tgoto set_attr;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFS3_CREATE_EXCLUSIVE:\n\t\t\tif (   d_inode(dchild)->i_mtime.tv_sec == v_mtime\n\t\t\t    && d_inode(dchild)->i_atime.tv_sec == v_atime\n\t\t\t    && d_inode(dchild)->i_size  == 0 ) {\n\t\t\t\tif (created)\n\t\t\t\t\t*created = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tcase NFS4_CREATE_EXCLUSIVE4_1:\n\t\t\tif (   d_inode(dchild)->i_mtime.tv_sec == v_mtime\n\t\t\t    && d_inode(dchild)->i_atime.tv_sec == v_atime\n\t\t\t    && d_inode(dchild)->i_size  == 0 ) {\n\t\t\t\tif (created)\n\t\t\t\t\t*created = 1;\n\t\t\t\tgoto set_attr;\n\t\t\t}\n\t\t\t /* fallthru */\n\t\tcase NFS3_CREATE_GUARDED:\n\t\t\terr = nfserr_exist;\n\t\t}\n\t\tfh_drop_write(fhp);\n\t\tgoto out;\n\t}\n\n\thost_err = vfs_create(dirp, dchild, iap->ia_mode, true);\n\tif (host_err < 0) {\n\t\tfh_drop_write(fhp);\n\t\tgoto out_nfserr;\n\t}\n\tif (created)\n\t\t*created = 1;\n\n\tnfsd_check_ignore_resizing(iap);\n\n\tif (nfsd_create_is_exclusive(createmode)) {\n\t\t/* Cram the verifier into atime/mtime */\n\t\tiap->ia_valid = ATTR_MTIME|ATTR_ATIME\n\t\t\t| ATTR_MTIME_SET|ATTR_ATIME_SET;\n\t\t/* XXX someone who knows this better please fix it for nsec */ \n\t\tiap->ia_mtime.tv_sec = v_mtime;\n\t\tiap->ia_atime.tv_sec = v_atime;\n\t\tiap->ia_mtime.tv_nsec = 0;\n\t\tiap->ia_atime.tv_nsec = 0;\n\t}\n\n set_attr:\n\terr = nfsd_create_setattr(rqstp, resfhp, iap);\n\n\t/*\n\t * nfsd_create_setattr already committed the child\n\t * (and possibly also the parent).\n\t */\n\tif (!err)\n\t\terr = nfserrno(commit_metadata(fhp));\n\n\t/*\n\t * Update the filehandle to get the new inode info.\n\t */\n\tif (!err)\n\t\terr = fh_update(resfhp);\n\n out:\n\tfh_unlock(fhp);\n\tif (dchild && !IS_ERR(dchild))\n\t\tdput(dchild);\n\tfh_drop_write(fhp);\n \treturn err;\n \n out_nfserr:\n\terr = nfserrno(host_err);\n\tgoto out;\n}\n#endif /* CONFIG_NFSD_V3 */\n\n/*\n * Read a symlink. On entry, *lenp must contain the maximum path length that\n * fits into the buffer. On return, it contains the true length.\n * N.B. After this call fhp needs an fh_put\n */\n__be32\nnfsd_readlink(struct svc_rqst *rqstp, struct svc_fh *fhp, char *buf, int *lenp)\n{\n\tmm_segment_t\toldfs;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\tstruct path path;\n\n\terr = fh_verify(rqstp, fhp, S_IFLNK, NFSD_MAY_NOP);\n\tif (err)\n\t\tgoto out;\n\n\tpath.mnt = fhp->fh_export->ex_path.mnt;\n\tpath.dentry = fhp->fh_dentry;\n\n\terr = nfserr_inval;\n\tif (!d_is_symlink(path.dentry))\n\t\tgoto out;\n\n\ttouch_atime(&path);\n\t/* N.B. Why does this call need a get_fs()??\n\t * Remove the set_fs and watch the fireworks:-) --okir\n\t */\n\n\toldfs = get_fs(); set_fs(KERNEL_DS);\n\thost_err = vfs_readlink(path.dentry, (char __user *)buf, *lenp);\n\tset_fs(oldfs);\n\n\tif (host_err < 0)\n\t\tgoto out_nfserr;\n\t*lenp = host_err;\n\terr = 0;\nout:\n\treturn err;\n\nout_nfserr:\n\terr = nfserrno(host_err);\n\tgoto out;\n}\n\n/*\n * Create a symlink and look up its inode\n * N.B. After this call _both_ fhp and resfhp need an fh_put\n */\n__be32\nnfsd_symlink(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\t\t\tchar *fname, int flen,\n\t\t\t\tchar *path,\n\t\t\t\tstruct svc_fh *resfhp)\n{\n\tstruct dentry\t*dentry, *dnew;\n\t__be32\t\terr, cerr;\n\tint\t\thost_err;\n\n\terr = nfserr_noent;\n\tif (!flen || path[0] == '\\0')\n\t\tgoto out;\n\terr = nfserr_exist;\n\tif (isdotent(fname, flen))\n\t\tgoto out;\n\n\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_CREATE);\n\tif (err)\n\t\tgoto out;\n\n\thost_err = fh_want_write(fhp);\n\tif (host_err)\n\t\tgoto out_nfserr;\n\n\tfh_lock(fhp);\n\tdentry = fhp->fh_dentry;\n\tdnew = lookup_one_len(fname, dentry, flen);\n\thost_err = PTR_ERR(dnew);\n\tif (IS_ERR(dnew))\n\t\tgoto out_nfserr;\n\n\thost_err = vfs_symlink(d_inode(dentry), dnew, path);\n\terr = nfserrno(host_err);\n\tif (!err)\n\t\terr = nfserrno(commit_metadata(fhp));\n\tfh_unlock(fhp);\n\n\tfh_drop_write(fhp);\n\n\tcerr = fh_compose(resfhp, fhp->fh_export, dnew, fhp);\n\tdput(dnew);\n\tif (err==0) err = cerr;\nout:\n\treturn err;\n\nout_nfserr:\n\terr = nfserrno(host_err);\n\tgoto out;\n}\n\n/*\n * Create a hardlink\n * N.B. After this call _both_ ffhp and tfhp need an fh_put\n */\n__be32\nnfsd_link(struct svc_rqst *rqstp, struct svc_fh *ffhp,\n\t\t\t\tchar *name, int len, struct svc_fh *tfhp)\n{\n\tstruct dentry\t*ddir, *dnew, *dold;\n\tstruct inode\t*dirp;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\n\terr = fh_verify(rqstp, ffhp, S_IFDIR, NFSD_MAY_CREATE);\n\tif (err)\n\t\tgoto out;\n\terr = fh_verify(rqstp, tfhp, 0, NFSD_MAY_NOP);\n\tif (err)\n\t\tgoto out;\n\terr = nfserr_isdir;\n\tif (d_is_dir(tfhp->fh_dentry))\n\t\tgoto out;\n\terr = nfserr_perm;\n\tif (!len)\n\t\tgoto out;\n\terr = nfserr_exist;\n\tif (isdotent(name, len))\n\t\tgoto out;\n\n\thost_err = fh_want_write(tfhp);\n\tif (host_err) {\n\t\terr = nfserrno(host_err);\n\t\tgoto out;\n\t}\n\n\tfh_lock_nested(ffhp, I_MUTEX_PARENT);\n\tddir = ffhp->fh_dentry;\n\tdirp = d_inode(ddir);\n\n\tdnew = lookup_one_len(name, ddir, len);\n\thost_err = PTR_ERR(dnew);\n\tif (IS_ERR(dnew))\n\t\tgoto out_nfserr;\n\n\tdold = tfhp->fh_dentry;\n\n\terr = nfserr_noent;\n\tif (d_really_is_negative(dold))\n\t\tgoto out_dput;\n\thost_err = vfs_link(dold, dirp, dnew, NULL);\n\tif (!host_err) {\n\t\terr = nfserrno(commit_metadata(ffhp));\n\t\tif (!err)\n\t\t\terr = nfserrno(commit_metadata(tfhp));\n\t} else {\n\t\tif (host_err == -EXDEV && rqstp->rq_vers == 2)\n\t\t\terr = nfserr_acces;\n\t\telse\n\t\t\terr = nfserrno(host_err);\n\t}\nout_dput:\n\tdput(dnew);\nout_unlock:\n\tfh_unlock(ffhp);\n\tfh_drop_write(tfhp);\nout:\n\treturn err;\n\nout_nfserr:\n\terr = nfserrno(host_err);\n\tgoto out_unlock;\n}\n\n/*\n * Rename a file\n * N.B. After this call _both_ ffhp and tfhp need an fh_put\n */\n__be32\nnfsd_rename(struct svc_rqst *rqstp, struct svc_fh *ffhp, char *fname, int flen,\n\t\t\t    struct svc_fh *tfhp, char *tname, int tlen)\n{\n\tstruct dentry\t*fdentry, *tdentry, *odentry, *ndentry, *trap;\n\tstruct inode\t*fdir, *tdir;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\n\terr = fh_verify(rqstp, ffhp, S_IFDIR, NFSD_MAY_REMOVE);\n\tif (err)\n\t\tgoto out;\n\terr = fh_verify(rqstp, tfhp, S_IFDIR, NFSD_MAY_CREATE);\n\tif (err)\n\t\tgoto out;\n\n\tfdentry = ffhp->fh_dentry;\n\tfdir = d_inode(fdentry);\n\n\ttdentry = tfhp->fh_dentry;\n\ttdir = d_inode(tdentry);\n\n\terr = nfserr_perm;\n\tif (!flen || isdotent(fname, flen) || !tlen || isdotent(tname, tlen))\n\t\tgoto out;\n\n\thost_err = fh_want_write(ffhp);\n\tif (host_err) {\n\t\terr = nfserrno(host_err);\n\t\tgoto out;\n\t}\n\n\t/* cannot use fh_lock as we need deadlock protective ordering\n\t * so do it by hand */\n\ttrap = lock_rename(tdentry, fdentry);\n\tffhp->fh_locked = tfhp->fh_locked = true;\n\tfill_pre_wcc(ffhp);\n\tfill_pre_wcc(tfhp);\n\n\todentry = lookup_one_len(fname, fdentry, flen);\n\thost_err = PTR_ERR(odentry);\n\tif (IS_ERR(odentry))\n\t\tgoto out_nfserr;\n\n\thost_err = -ENOENT;\n\tif (d_really_is_negative(odentry))\n\t\tgoto out_dput_old;\n\thost_err = -EINVAL;\n\tif (odentry == trap)\n\t\tgoto out_dput_old;\n\n\tndentry = lookup_one_len(tname, tdentry, tlen);\n\thost_err = PTR_ERR(ndentry);\n\tif (IS_ERR(ndentry))\n\t\tgoto out_dput_old;\n\thost_err = -ENOTEMPTY;\n\tif (ndentry == trap)\n\t\tgoto out_dput_new;\n\n\thost_err = -EXDEV;\n\tif (ffhp->fh_export->ex_path.mnt != tfhp->fh_export->ex_path.mnt)\n\t\tgoto out_dput_new;\n\tif (ffhp->fh_export->ex_path.dentry != tfhp->fh_export->ex_path.dentry)\n\t\tgoto out_dput_new;\n\n\thost_err = vfs_rename(fdir, odentry, tdir, ndentry, NULL, 0);\n\tif (!host_err) {\n\t\thost_err = commit_metadata(tfhp);\n\t\tif (!host_err)\n\t\t\thost_err = commit_metadata(ffhp);\n\t}\n out_dput_new:\n\tdput(ndentry);\n out_dput_old:\n\tdput(odentry);\n out_nfserr:\n\terr = nfserrno(host_err);\n\t/*\n\t * We cannot rely on fh_unlock on the two filehandles,\n\t * as that would do the wrong thing if the two directories\n\t * were the same, so again we do it by hand.\n\t */\n\tfill_post_wcc(ffhp);\n\tfill_post_wcc(tfhp);\n\tunlock_rename(tdentry, fdentry);\n\tffhp->fh_locked = tfhp->fh_locked = false;\n\tfh_drop_write(ffhp);\n\nout:\n\treturn err;\n}\n\n/*\n * Unlink a file or directory\n * N.B. After this call fhp needs an fh_put\n */\n__be32\nnfsd_unlink(struct svc_rqst *rqstp, struct svc_fh *fhp, int type,\n\t\t\t\tchar *fname, int flen)\n{\n\tstruct dentry\t*dentry, *rdentry;\n\tstruct inode\t*dirp;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\n\terr = nfserr_acces;\n\tif (!flen || isdotent(fname, flen))\n\t\tgoto out;\n\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_REMOVE);\n\tif (err)\n\t\tgoto out;\n\n\thost_err = fh_want_write(fhp);\n\tif (host_err)\n\t\tgoto out_nfserr;\n\n\tfh_lock_nested(fhp, I_MUTEX_PARENT);\n\tdentry = fhp->fh_dentry;\n\tdirp = d_inode(dentry);\n\n\trdentry = lookup_one_len(fname, dentry, flen);\n\thost_err = PTR_ERR(rdentry);\n\tif (IS_ERR(rdentry))\n\t\tgoto out_nfserr;\n\n\tif (d_really_is_negative(rdentry)) {\n\t\tdput(rdentry);\n\t\terr = nfserr_noent;\n\t\tgoto out;\n\t}\n\n\tif (!type)\n\t\ttype = d_inode(rdentry)->i_mode & S_IFMT;\n\n\tif (type != S_IFDIR)\n\t\thost_err = vfs_unlink(dirp, rdentry, NULL);\n\telse\n\t\thost_err = vfs_rmdir(dirp, rdentry);\n\tif (!host_err)\n\t\thost_err = commit_metadata(fhp);\n\tdput(rdentry);\n\nout_nfserr:\n\terr = nfserrno(host_err);\nout:\n\treturn err;\n}\n\n/*\n * We do this buffering because we must not call back into the file\n * system's ->lookup() method from the filldir callback. That may well\n * deadlock a number of file systems.\n *\n * This is based heavily on the implementation of same in XFS.\n */\nstruct buffered_dirent {\n\tu64\t\tino;\n\tloff_t\t\toffset;\n\tint\t\tnamlen;\n\tunsigned int\td_type;\n\tchar\t\tname[];\n};\n\nstruct readdir_data {\n\tstruct dir_context ctx;\n\tchar\t\t*dirent;\n\tsize_t\t\tused;\n\tint\t\tfull;\n};\n\nstatic int nfsd_buffered_filldir(struct dir_context *ctx, const char *name,\n\t\t\t\t int namlen, loff_t offset, u64 ino,\n\t\t\t\t unsigned int d_type)\n{\n\tstruct readdir_data *buf =\n\t\tcontainer_of(ctx, struct readdir_data, ctx);\n\tstruct buffered_dirent *de = (void *)(buf->dirent + buf->used);\n\tunsigned int reclen;\n\n\treclen = ALIGN(sizeof(struct buffered_dirent) + namlen, sizeof(u64));\n\tif (buf->used + reclen > PAGE_SIZE) {\n\t\tbuf->full = 1;\n\t\treturn -EINVAL;\n\t}\n\n\tde->namlen = namlen;\n\tde->offset = offset;\n\tde->ino = ino;\n\tde->d_type = d_type;\n\tmemcpy(de->name, name, namlen);\n\tbuf->used += reclen;\n\n\treturn 0;\n}\n\nstatic __be32 nfsd_buffered_readdir(struct file *file, nfsd_filldir_t func,\n\t\t\t\t    struct readdir_cd *cdp, loff_t *offsetp)\n{\n\tstruct buffered_dirent *de;\n\tint host_err;\n\tint size;\n\tloff_t offset;\n\tstruct readdir_data buf = {\n\t\t.ctx.actor = nfsd_buffered_filldir,\n\t\t.dirent = (void *)__get_free_page(GFP_KERNEL)\n\t};\n\n\tif (!buf.dirent)\n\t\treturn nfserrno(-ENOMEM);\n\n\toffset = *offsetp;\n\n\twhile (1) {\n\t\tunsigned int reclen;\n\n\t\tcdp->err = nfserr_eof; /* will be cleared on successful read */\n\t\tbuf.used = 0;\n\t\tbuf.full = 0;\n\n\t\thost_err = iterate_dir(file, &buf.ctx);\n\t\tif (buf.full)\n\t\t\thost_err = 0;\n\n\t\tif (host_err < 0)\n\t\t\tbreak;\n\n\t\tsize = buf.used;\n\n\t\tif (!size)\n\t\t\tbreak;\n\n\t\tde = (struct buffered_dirent *)buf.dirent;\n\t\twhile (size > 0) {\n\t\t\toffset = de->offset;\n\n\t\t\tif (func(cdp, de->name, de->namlen, de->offset,\n\t\t\t\t de->ino, de->d_type))\n\t\t\t\tbreak;\n\n\t\t\tif (cdp->err != nfs_ok)\n\t\t\t\tbreak;\n\n\t\t\treclen = ALIGN(sizeof(*de) + de->namlen,\n\t\t\t\t       sizeof(u64));\n\t\t\tsize -= reclen;\n\t\t\tde = (struct buffered_dirent *)((char *)de + reclen);\n\t\t}\n\t\tif (size > 0) /* We bailed out early */\n\t\t\tbreak;\n\n\t\toffset = vfs_llseek(file, 0, SEEK_CUR);\n\t}\n\n\tfree_page((unsigned long)(buf.dirent));\n\n\tif (host_err)\n\t\treturn nfserrno(host_err);\n\n\t*offsetp = offset;\n\treturn cdp->err;\n}\n\n/*\n * Read entries from a directory.\n * The  NFSv3/4 verifier we ignore for now.\n */\n__be32\nnfsd_readdir(struct svc_rqst *rqstp, struct svc_fh *fhp, loff_t *offsetp, \n\t     struct readdir_cd *cdp, nfsd_filldir_t func)\n{\n\t__be32\t\terr;\n\tstruct file\t*file;\n\tloff_t\t\toffset = *offsetp;\n\tint             may_flags = NFSD_MAY_READ;\n\n\t/* NFSv2 only supports 32 bit cookies */\n\tif (rqstp->rq_vers > 2)\n\t\tmay_flags |= NFSD_MAY_64BIT_COOKIE;\n\n\terr = nfsd_open(rqstp, fhp, S_IFDIR, may_flags, &file);\n\tif (err)\n\t\tgoto out;\n\n\toffset = vfs_llseek(file, offset, SEEK_SET);\n\tif (offset < 0) {\n\t\terr = nfserrno((int)offset);\n\t\tgoto out_close;\n\t}\n\n\terr = nfsd_buffered_readdir(file, func, cdp, offsetp);\n\n\tif (err == nfserr_eof || err == nfserr_toosmall)\n\t\terr = nfs_ok; /* can still be found in ->err */\nout_close:\n\tfput(file);\nout:\n\treturn err;\n}\n\n/*\n * Get file system stats\n * N.B. After this call fhp needs an fh_put\n */\n__be32\nnfsd_statfs(struct svc_rqst *rqstp, struct svc_fh *fhp, struct kstatfs *stat, int access)\n{\n\t__be32 err;\n\n\terr = fh_verify(rqstp, fhp, 0, NFSD_MAY_NOP | access);\n\tif (!err) {\n\t\tstruct path path = {\n\t\t\t.mnt\t= fhp->fh_export->ex_path.mnt,\n\t\t\t.dentry\t= fhp->fh_dentry,\n\t\t};\n\t\tif (vfs_statfs(&path, stat))\n\t\t\terr = nfserr_io;\n\t}\n\treturn err;\n}\n\nstatic int exp_rdonly(struct svc_rqst *rqstp, struct svc_export *exp)\n{\n\treturn nfsexp_flags(rqstp, exp) & NFSEXP_READONLY;\n}\n\n/*\n * Check for a user's access permissions to this inode.\n */\n__be32\nnfsd_permission(struct svc_rqst *rqstp, struct svc_export *exp,\n\t\t\t\t\tstruct dentry *dentry, int acc)\n{\n\tstruct inode\t*inode = d_inode(dentry);\n\tint\t\terr;\n\n\tif ((acc & NFSD_MAY_MASK) == NFSD_MAY_NOP)\n\t\treturn 0;\n#if 0\n\tdprintk(\"nfsd: permission 0x%x%s%s%s%s%s%s%s mode 0%o%s%s%s\\n\",\n\t\tacc,\n\t\t(acc & NFSD_MAY_READ)?\t\" read\"  : \"\",\n\t\t(acc & NFSD_MAY_WRITE)?\t\" write\" : \"\",\n\t\t(acc & NFSD_MAY_EXEC)?\t\" exec\"  : \"\",\n\t\t(acc & NFSD_MAY_SATTR)?\t\" sattr\" : \"\",\n\t\t(acc & NFSD_MAY_TRUNC)?\t\" trunc\" : \"\",\n\t\t(acc & NFSD_MAY_LOCK)?\t\" lock\"  : \"\",\n\t\t(acc & NFSD_MAY_OWNER_OVERRIDE)? \" owneroverride\" : \"\",\n\t\tinode->i_mode,\n\t\tIS_IMMUTABLE(inode)?\t\" immut\" : \"\",\n\t\tIS_APPEND(inode)?\t\" append\" : \"\",\n\t\t__mnt_is_readonly(exp->ex_path.mnt)?\t\" ro\" : \"\");\n\tdprintk(\"      owner %d/%d user %d/%d\\n\",\n\t\tinode->i_uid, inode->i_gid, current_fsuid(), current_fsgid());\n#endif\n\n\t/* Normally we reject any write/sattr etc access on a read-only file\n\t * system.  But if it is IRIX doing check on write-access for a \n\t * device special file, we ignore rofs.\n\t */\n\tif (!(acc & NFSD_MAY_LOCAL_ACCESS))\n\t\tif (acc & (NFSD_MAY_WRITE | NFSD_MAY_SATTR | NFSD_MAY_TRUNC)) {\n\t\t\tif (exp_rdonly(rqstp, exp) ||\n\t\t\t    __mnt_is_readonly(exp->ex_path.mnt))\n\t\t\t\treturn nfserr_rofs;\n\t\t\tif (/* (acc & NFSD_MAY_WRITE) && */ IS_IMMUTABLE(inode))\n\t\t\t\treturn nfserr_perm;\n\t\t}\n\tif ((acc & NFSD_MAY_TRUNC) && IS_APPEND(inode))\n\t\treturn nfserr_perm;\n\n\tif (acc & NFSD_MAY_LOCK) {\n\t\t/* If we cannot rely on authentication in NLM requests,\n\t\t * just allow locks, otherwise require read permission, or\n\t\t * ownership\n\t\t */\n\t\tif (exp->ex_flags & NFSEXP_NOAUTHNLM)\n\t\t\treturn 0;\n\t\telse\n\t\t\tacc = NFSD_MAY_READ | NFSD_MAY_OWNER_OVERRIDE;\n\t}\n\t/*\n\t * The file owner always gets access permission for accesses that\n\t * would normally be checked at open time. This is to make\n\t * file access work even when the client has done a fchmod(fd, 0).\n\t *\n\t * However, `cp foo bar' should fail nevertheless when bar is\n\t * readonly. A sensible way to do this might be to reject all\n\t * attempts to truncate a read-only file, because a creat() call\n\t * always implies file truncation.\n\t * ... but this isn't really fair.  A process may reasonably call\n\t * ftruncate on an open file descriptor on a file with perm 000.\n\t * We must trust the client to do permission checking - using \"ACCESS\"\n\t * with NFSv3.\n\t */\n\tif ((acc & NFSD_MAY_OWNER_OVERRIDE) &&\n\t    uid_eq(inode->i_uid, current_fsuid()))\n\t\treturn 0;\n\n\t/* This assumes  NFSD_MAY_{READ,WRITE,EXEC} == MAY_{READ,WRITE,EXEC} */\n\terr = inode_permission(inode, acc & (MAY_READ|MAY_WRITE|MAY_EXEC));\n\n\t/* Allow read access to binaries even when mode 111 */\n\tif (err == -EACCES && S_ISREG(inode->i_mode) &&\n\t     (acc == (NFSD_MAY_READ | NFSD_MAY_OWNER_OVERRIDE) ||\n\t      acc == (NFSD_MAY_READ | NFSD_MAY_READ_IF_EXEC)))\n\t\terr = inode_permission(inode, MAY_EXEC);\n\n\treturn err? nfserrno(err) : 0;\n}\n\nvoid\nnfsd_racache_shutdown(void)\n{\n\tstruct raparms *raparm, *last_raparm;\n\tunsigned int i;\n\n\tdprintk(\"nfsd: freeing readahead buffers.\\n\");\n\n\tfor (i = 0; i < RAPARM_HASH_SIZE; i++) {\n\t\traparm = raparm_hash[i].pb_head;\n\t\twhile(raparm) {\n\t\t\tlast_raparm = raparm;\n\t\t\traparm = raparm->p_next;\n\t\t\tkfree(last_raparm);\n\t\t}\n\t\traparm_hash[i].pb_head = NULL;\n\t}\n}\n/*\n * Initialize readahead param cache\n */\nint\nnfsd_racache_init(int cache_size)\n{\n\tint\ti;\n\tint\tj = 0;\n\tint\tnperbucket;\n\tstruct raparms **raparm = NULL;\n\n\n\tif (raparm_hash[0].pb_head)\n\t\treturn 0;\n\tnperbucket = DIV_ROUND_UP(cache_size, RAPARM_HASH_SIZE);\n\tnperbucket = max(2, nperbucket);\n\tcache_size = nperbucket * RAPARM_HASH_SIZE;\n\n\tdprintk(\"nfsd: allocating %d readahead buffers.\\n\", cache_size);\n\n\tfor (i = 0; i < RAPARM_HASH_SIZE; i++) {\n\t\tspin_lock_init(&raparm_hash[i].pb_lock);\n\n\t\traparm = &raparm_hash[i].pb_head;\n\t\tfor (j = 0; j < nperbucket; j++) {\n\t\t\t*raparm = kzalloc(sizeof(struct raparms), GFP_KERNEL);\n\t\t\tif (!*raparm)\n\t\t\t\tgoto out_nomem;\n\t\t\traparm = &(*raparm)->p_next;\n\t\t}\n\t\t*raparm = NULL;\n\t}\n\n\tnfsdstats.ra_size = cache_size;\n\treturn 0;\n\nout_nomem:\n\tdprintk(\"nfsd: kmalloc failed, freeing readahead buffers\\n\");\n\tnfsd_racache_shutdown();\n\treturn -ENOMEM;\n}\n", "/*\n * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n#ifndef _LINUX_SUNRPC_RPC_RDMA_H\n#define _LINUX_SUNRPC_RPC_RDMA_H\n\n#include <linux/types.h>\n#include <linux/bitops.h>\n\n#define RPCRDMA_VERSION\t\t1\n#define rpcrdma_version\t\tcpu_to_be32(RPCRDMA_VERSION)\n\nenum {\n\tRPCRDMA_V1_DEF_INLINE_SIZE\t= 1024,\n};\n\nstruct rpcrdma_segment {\n\t__be32 rs_handle;\t/* Registered memory handle */\n\t__be32 rs_length;\t/* Length of the chunk in bytes */\n\t__be64 rs_offset;\t/* Chunk virtual address or offset */\n};\n\n/*\n * read chunk(s), encoded as a linked list.\n */\nstruct rpcrdma_read_chunk {\n\t__be32 rc_discrim;\t/* 1 indicates presence */\n\t__be32 rc_position;\t/* Position in XDR stream */\n\tstruct rpcrdma_segment rc_target;\n};\n\n/*\n * write chunk, and reply chunk.\n */\nstruct rpcrdma_write_chunk {\n\tstruct rpcrdma_segment wc_target;\n};\n\n/*\n * write chunk(s), encoded as a counted array.\n */\nstruct rpcrdma_write_array {\n\t__be32 wc_discrim;\t/* 1 indicates presence */\n\t__be32 wc_nchunks;\t/* Array count */\n\tstruct rpcrdma_write_chunk wc_array[0];\n};\n\nstruct rpcrdma_msg {\n\t__be32 rm_xid;\t/* Mirrors the RPC header xid */\n\t__be32 rm_vers;\t/* Version of this protocol */\n\t__be32 rm_credit;\t/* Buffers requested/granted */\n\t__be32 rm_type;\t/* Type of message (enum rpcrdma_proc) */\n\tunion {\n\n\t\tstruct {\t\t\t/* no chunks */\n\t\t\t__be32 rm_empty[3];\t/* 3 empty chunk lists */\n\t\t} rm_nochunks;\n\n\t\tstruct {\t\t\t/* no chunks and padded */\n\t\t\t__be32 rm_align;\t/* Padding alignment */\n\t\t\t__be32 rm_thresh;\t/* Padding threshold */\n\t\t\t__be32 rm_pempty[3];\t/* 3 empty chunk lists */\n\t\t} rm_padded;\n\n\t\tstruct {\n\t\t\t__be32 rm_err;\n\t\t\t__be32 rm_vers_low;\n\t\t\t__be32 rm_vers_high;\n\t\t} rm_error;\n\n\t\t__be32 rm_chunks[0];\t/* read, write and reply chunks */\n\n\t} rm_body;\n};\n\n/*\n * XDR sizes, in quads\n */\nenum {\n\trpcrdma_fixed_maxsz\t= 4,\n\trpcrdma_segment_maxsz\t= 4,\n\trpcrdma_readchunk_maxsz\t= 2 + rpcrdma_segment_maxsz,\n};\n\n/*\n * Smallest RPC/RDMA header: rm_xid through rm_type, then rm_nochunks\n */\n#define RPCRDMA_HDRLEN_MIN\t(sizeof(__be32) * 7)\n#define RPCRDMA_HDRLEN_ERR\t(sizeof(__be32) * 5)\n\nenum rpcrdma_errcode {\n\tERR_VERS = 1,\n\tERR_CHUNK = 2\n};\n\nenum rpcrdma_proc {\n\tRDMA_MSG = 0,\t\t/* An RPC call or reply msg */\n\tRDMA_NOMSG = 1,\t\t/* An RPC call or reply msg - separate body */\n\tRDMA_MSGP = 2,\t\t/* An RPC call or reply msg with padding */\n\tRDMA_DONE = 3,\t\t/* Client signals reply completion */\n\tRDMA_ERROR = 4\t\t/* An RPC RDMA encoding error */\n};\n\n#define rdma_msg\tcpu_to_be32(RDMA_MSG)\n#define rdma_nomsg\tcpu_to_be32(RDMA_NOMSG)\n#define rdma_msgp\tcpu_to_be32(RDMA_MSGP)\n#define rdma_done\tcpu_to_be32(RDMA_DONE)\n#define rdma_error\tcpu_to_be32(RDMA_ERROR)\n\n/*\n * Private extension to RPC-over-RDMA Version One.\n * Message passed during RDMA-CM connection set-up.\n *\n * Add new fields at the end, and don't permute existing\n * fields.\n */\nstruct rpcrdma_connect_private {\n\t__be32\t\t\tcp_magic;\n\tu8\t\t\tcp_version;\n\tu8\t\t\tcp_flags;\n\tu8\t\t\tcp_send_size;\n\tu8\t\t\tcp_recv_size;\n} __packed;\n\n#define rpcrdma_cmp_magic\t__cpu_to_be32(0xf6ab0e18)\n\nenum {\n\tRPCRDMA_CMP_VERSION\t\t= 1,\n\tRPCRDMA_CMP_F_SND_W_INV_OK\t= BIT(0),\n};\n\nstatic inline u8\nrpcrdma_encode_buffer_size(unsigned int size)\n{\n\treturn (size >> 10) - 1;\n}\n\nstatic inline unsigned int\nrpcrdma_decode_buffer_size(u8 val)\n{\n\treturn ((unsigned int)val + 1) << 10;\n}\n\n#endif\t\t\t\t/* _LINUX_SUNRPC_RPC_RDMA_H */\n", "/*\n * linux/include/linux/sunrpc/svc.h\n *\n * RPC server declarations.\n *\n * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>\n */\n\n\n#ifndef SUNRPC_SVC_H\n#define SUNRPC_SVC_H\n\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <linux/sunrpc/types.h>\n#include <linux/sunrpc/xdr.h>\n#include <linux/sunrpc/auth.h>\n#include <linux/sunrpc/svcauth.h>\n#include <linux/wait.h>\n#include <linux/mm.h>\n\n/* statistics for svc_pool structures */\nstruct svc_pool_stats {\n\tatomic_long_t\tpackets;\n\tunsigned long\tsockets_queued;\n\tatomic_long_t\tthreads_woken;\n\tatomic_long_t\tthreads_timedout;\n};\n\n/*\n *\n * RPC service thread pool.\n *\n * Pool of threads and temporary sockets.  Generally there is only\n * a single one of these per RPC service, but on NUMA machines those\n * services that can benefit from it (i.e. nfs but not lockd) will\n * have one pool per NUMA node.  This optimisation reduces cross-\n * node traffic on multi-node NUMA NFS servers.\n */\nstruct svc_pool {\n\tunsigned int\t\tsp_id;\t    \t/* pool id; also node id on NUMA */\n\tspinlock_t\t\tsp_lock;\t/* protects all fields */\n\tstruct list_head\tsp_sockets;\t/* pending sockets */\n\tunsigned int\t\tsp_nrthreads;\t/* # of threads in pool */\n\tstruct list_head\tsp_all_threads;\t/* all server threads */\n\tstruct svc_pool_stats\tsp_stats;\t/* statistics on pool operation */\n#define\tSP_TASK_PENDING\t\t(0)\t\t/* still work to do even if no\n\t\t\t\t\t\t * xprt is queued. */\n\tunsigned long\t\tsp_flags;\n} ____cacheline_aligned_in_smp;\n\nstruct svc_serv;\n\nstruct svc_serv_ops {\n\t/* Callback to use when last thread exits. */\n\tvoid\t\t(*svo_shutdown)(struct svc_serv *, struct net *);\n\n\t/* function for service threads to run */\n\tint\t\t(*svo_function)(void *);\n\n\t/* queue up a transport for servicing */\n\tvoid\t\t(*svo_enqueue_xprt)(struct svc_xprt *);\n\n\t/* set up thread (or whatever) execution context */\n\tint\t\t(*svo_setup)(struct svc_serv *, struct svc_pool *, int);\n\n\t/* optional module to count when adding threads (pooled svcs only) */\n\tstruct module\t*svo_module;\n};\n\n/*\n * RPC service.\n *\n * An RPC service is a ``daemon,'' possibly multithreaded, which\n * receives and processes incoming RPC messages.\n * It has one or more transport sockets associated with it, and maintains\n * a list of idle threads waiting for input.\n *\n * We currently do not support more than one RPC program per daemon.\n */\nstruct svc_serv {\n\tstruct svc_program *\tsv_program;\t/* RPC program */\n\tstruct svc_stat *\tsv_stats;\t/* RPC statistics */\n\tspinlock_t\t\tsv_lock;\n\tunsigned int\t\tsv_nrthreads;\t/* # of server threads */\n\tunsigned int\t\tsv_maxconn;\t/* max connections allowed or\n\t\t\t\t\t\t * '0' causing max to be based\n\t\t\t\t\t\t * on number of threads. */\n\n\tunsigned int\t\tsv_max_payload;\t/* datagram payload size */\n\tunsigned int\t\tsv_max_mesg;\t/* max_payload + 1 page for overheads */\n\tunsigned int\t\tsv_xdrsize;\t/* XDR buffer size */\n\tstruct list_head\tsv_permsocks;\t/* all permanent sockets */\n\tstruct list_head\tsv_tempsocks;\t/* all temporary sockets */\n\tint\t\t\tsv_tmpcnt;\t/* count of temporary sockets */\n\tstruct timer_list\tsv_temptimer;\t/* timer for aging temporary sockets */\n\n\tchar *\t\t\tsv_name;\t/* service name */\n\n\tunsigned int\t\tsv_nrpools;\t/* number of thread pools */\n\tstruct svc_pool *\tsv_pools;\t/* array of thread pools */\n\tstruct svc_serv_ops\t*sv_ops;\t/* server operations */\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\n\tstruct list_head\tsv_cb_list;\t/* queue for callback requests\n\t\t\t\t\t\t * that arrive over the same\n\t\t\t\t\t\t * connection */\n\tspinlock_t\t\tsv_cb_lock;\t/* protects the svc_cb_list */\n\twait_queue_head_t\tsv_cb_waitq;\t/* sleep here if there are no\n\t\t\t\t\t\t * entries in the svc_cb_list */\n\tstruct svc_xprt\t\t*sv_bc_xprt;\t/* callback on fore channel */\n#endif /* CONFIG_SUNRPC_BACKCHANNEL */\n};\n\n/*\n * We use sv_nrthreads as a reference count.  svc_destroy() drops\n * this refcount, so we need to bump it up around operations that\n * change the number of threads.  Horrible, but there it is.\n * Should be called with the \"service mutex\" held.\n */\nstatic inline void svc_get(struct svc_serv *serv)\n{\n\tserv->sv_nrthreads++;\n}\n\n/*\n * Maximum payload size supported by a kernel RPC server.\n * This is use to determine the max number of pages nfsd is\n * willing to return in a single READ operation.\n *\n * These happen to all be powers of 2, which is not strictly\n * necessary but helps enforce the real limitation, which is\n * that they should be multiples of PAGE_SIZE.\n *\n * For UDP transports, a block plus NFS,RPC, and UDP headers\n * has to fit into the IP datagram limit of 64K.  The largest\n * feasible number for all known page sizes is probably 48K,\n * but we choose 32K here.  This is the same as the historical\n * Linux limit; someone who cares more about NFS/UDP performance\n * can test a larger number.\n *\n * For TCP transports we have more freedom.  A size of 1MB is\n * chosen to match the client limit.  Other OSes are known to\n * have larger limits, but those numbers are probably beyond\n * the point of diminishing returns.\n */\n#define RPCSVC_MAXPAYLOAD\t(1*1024*1024u)\n#define RPCSVC_MAXPAYLOAD_TCP\tRPCSVC_MAXPAYLOAD\n#define RPCSVC_MAXPAYLOAD_UDP\t(32*1024u)\n\nextern u32 svc_max_payload(const struct svc_rqst *rqstp);\n\n/*\n * RPC Requsts and replies are stored in one or more pages.\n * We maintain an array of pages for each server thread.\n * Requests are copied into these pages as they arrive.  Remaining\n * pages are available to write the reply into.\n *\n * Pages are sent using ->sendpage so each server thread needs to\n * allocate more to replace those used in sending.  To help keep track\n * of these pages we have a receive list where all pages initialy live,\n * and a send list where pages are moved to when there are to be part\n * of a reply.\n *\n * We use xdr_buf for holding responses as it fits well with NFS\n * read responses (that have a header, and some data pages, and possibly\n * a tail) and means we can share some client side routines.\n *\n * The xdr_buf.head kvec always points to the first page in the rq_*pages\n * list.  The xdr_buf.pages pointer points to the second page on that\n * list.  xdr_buf.tail points to the end of the first page.\n * This assumes that the non-page part of an rpc reply will fit\n * in a page - NFSd ensures this.  lockd also has no trouble.\n *\n * Each request/reply pair can have at most one \"payload\", plus two pages,\n * one for the request, and one for the reply.\n * We using ->sendfile to return read data, we might need one extra page\n * if the request is not page-aligned.  So add another '1'.\n */\n#define RPCSVC_MAXPAGES\t\t((RPCSVC_MAXPAYLOAD+PAGE_SIZE-1)/PAGE_SIZE \\\n\t\t\t\t+ 2 + 1)\n\nstatic inline u32 svc_getnl(struct kvec *iov)\n{\n\t__be32 val, *vp;\n\tvp = iov->iov_base;\n\tval = *vp++;\n\tiov->iov_base = (void*)vp;\n\tiov->iov_len -= sizeof(__be32);\n\treturn ntohl(val);\n}\n\nstatic inline void svc_putnl(struct kvec *iov, u32 val)\n{\n\t__be32 *vp = iov->iov_base + iov->iov_len;\n\t*vp = htonl(val);\n\tiov->iov_len += sizeof(__be32);\n}\n\nstatic inline __be32 svc_getu32(struct kvec *iov)\n{\n\t__be32 val, *vp;\n\tvp = iov->iov_base;\n\tval = *vp++;\n\tiov->iov_base = (void*)vp;\n\tiov->iov_len -= sizeof(__be32);\n\treturn val;\n}\n\nstatic inline void svc_ungetu32(struct kvec *iov)\n{\n\t__be32 *vp = (__be32 *)iov->iov_base;\n\tiov->iov_base = (void *)(vp - 1);\n\tiov->iov_len += sizeof(*vp);\n}\n\nstatic inline void svc_putu32(struct kvec *iov, __be32 val)\n{\n\t__be32 *vp = iov->iov_base + iov->iov_len;\n\t*vp = val;\n\tiov->iov_len += sizeof(__be32);\n}\n\n/*\n * The context of a single thread, including the request currently being\n * processed.\n */\nstruct svc_rqst {\n\tstruct list_head\trq_all;\t\t/* all threads list */\n\tstruct rcu_head\t\trq_rcu_head;\t/* for RCU deferred kfree */\n\tstruct svc_xprt *\trq_xprt;\t/* transport ptr */\n\n\tstruct sockaddr_storage\trq_addr;\t/* peer address */\n\tsize_t\t\t\trq_addrlen;\n\tstruct sockaddr_storage\trq_daddr;\t/* dest addr of request\n\t\t\t\t\t\t *  - reply from here */\n\tsize_t\t\t\trq_daddrlen;\n\n\tstruct svc_serv *\trq_server;\t/* RPC service definition */\n\tstruct svc_pool *\trq_pool;\t/* thread pool */\n\tstruct svc_procedure *\trq_procinfo;\t/* procedure info */\n\tstruct auth_ops *\trq_authop;\t/* authentication flavour */\n\tstruct svc_cred\t\trq_cred;\t/* auth info */\n\tvoid *\t\t\trq_xprt_ctxt;\t/* transport specific context ptr */\n\tstruct svc_deferred_req*rq_deferred;\t/* deferred request we are replaying */\n\n\tsize_t\t\t\trq_xprt_hlen;\t/* xprt header len */\n\tstruct xdr_buf\t\trq_arg;\n\tstruct xdr_buf\t\trq_res;\n\tstruct page *\t\trq_pages[RPCSVC_MAXPAGES];\n\tstruct page *\t\t*rq_respages;\t/* points into rq_pages */\n\tstruct page *\t\t*rq_next_page; /* next reply page to use */\n\tstruct page *\t\t*rq_page_end;  /* one past the last page */\n\n\tstruct kvec\t\trq_vec[RPCSVC_MAXPAGES]; /* generally useful.. */\n\n\t__be32\t\t\trq_xid;\t\t/* transmission id */\n\tu32\t\t\trq_prog;\t/* program number */\n\tu32\t\t\trq_vers;\t/* program version */\n\tu32\t\t\trq_proc;\t/* procedure number */\n\tu32\t\t\trq_prot;\t/* IP protocol */\n\tint\t\t\trq_cachetype;\t/* catering to nfsd */\n#define\tRQ_SECURE\t(0)\t\t\t/* secure port */\n#define\tRQ_LOCAL\t(1)\t\t\t/* local request */\n#define\tRQ_USEDEFERRAL\t(2)\t\t\t/* use deferral */\n#define\tRQ_DROPME\t(3)\t\t\t/* drop current reply */\n#define\tRQ_SPLICE_OK\t(4)\t\t\t/* turned off in gss privacy\n\t\t\t\t\t\t * to prevent encrypting page\n\t\t\t\t\t\t * cache pages */\n#define\tRQ_VICTIM\t(5)\t\t\t/* about to be shut down */\n#define\tRQ_BUSY\t\t(6)\t\t\t/* request is busy */\n#define\tRQ_DATA\t\t(7)\t\t\t/* request has data */\n\tunsigned long\t\trq_flags;\t/* flags field */\n\n\tvoid *\t\t\trq_argp;\t/* decoded arguments */\n\tvoid *\t\t\trq_resp;\t/* xdr'd results */\n\tvoid *\t\t\trq_auth_data;\t/* flavor-specific data */\n\tint\t\t\trq_auth_slack;\t/* extra space xdr code\n\t\t\t\t\t\t * should leave in head\n\t\t\t\t\t\t * for krb5i, krb5p.\n\t\t\t\t\t\t */\n\tint\t\t\trq_reserved;\t/* space on socket outq\n\t\t\t\t\t\t * reserved for this request\n\t\t\t\t\t\t */\n\n\tstruct cache_req\trq_chandle;\t/* handle passed to caches for \n\t\t\t\t\t\t * request delaying \n\t\t\t\t\t\t */\n\t/* Catering to nfsd */\n\tstruct auth_domain *\trq_client;\t/* RPC peer info */\n\tstruct auth_domain *\trq_gssclient;\t/* \"gss/\"-style peer info */\n\tstruct svc_cacherep *\trq_cacherep;\t/* cache info */\n\tstruct task_struct\t*rq_task;\t/* service thread */\n\tspinlock_t\t\trq_lock;\t/* per-request lock */\n};\n\n#define SVC_NET(svc_rqst)\t(svc_rqst->rq_xprt->xpt_net)\n\n/*\n * Rigorous type checking on sockaddr type conversions\n */\nstatic inline struct sockaddr_in *svc_addr_in(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr_in *) &rqst->rq_addr;\n}\n\nstatic inline struct sockaddr_in6 *svc_addr_in6(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr_in6 *) &rqst->rq_addr;\n}\n\nstatic inline struct sockaddr *svc_addr(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr *) &rqst->rq_addr;\n}\n\nstatic inline struct sockaddr_in *svc_daddr_in(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr_in *) &rqst->rq_daddr;\n}\n\nstatic inline struct sockaddr_in6 *svc_daddr_in6(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr_in6 *) &rqst->rq_daddr;\n}\n\nstatic inline struct sockaddr *svc_daddr(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr *) &rqst->rq_daddr;\n}\n\n/*\n * Check buffer bounds after decoding arguments\n */\nstatic inline int\nxdr_argsize_check(struct svc_rqst *rqstp, __be32 *p)\n{\n\tchar *cp = (char *)p;\n\tstruct kvec *vec = &rqstp->rq_arg.head[0];\n\treturn cp >= (char*)vec->iov_base\n\t\t&& cp <= (char*)vec->iov_base + vec->iov_len;\n}\n\nstatic inline int\nxdr_ressize_check(struct svc_rqst *rqstp, __be32 *p)\n{\n\tstruct kvec *vec = &rqstp->rq_res.head[0];\n\tchar *cp = (char*)p;\n\n\tvec->iov_len = cp - (char*)vec->iov_base;\n\n\treturn vec->iov_len <= PAGE_SIZE;\n}\n\nstatic inline void svc_free_res_pages(struct svc_rqst *rqstp)\n{\n\twhile (rqstp->rq_next_page != rqstp->rq_respages) {\n\t\tstruct page **pp = --rqstp->rq_next_page;\n\t\tif (*pp) {\n\t\t\tput_page(*pp);\n\t\t\t*pp = NULL;\n\t\t}\n\t}\n}\n\nstruct svc_deferred_req {\n\tu32\t\t\tprot;\t/* protocol (UDP or TCP) */\n\tstruct svc_xprt\t\t*xprt;\n\tstruct sockaddr_storage\taddr;\t/* where reply must go */\n\tsize_t\t\t\taddrlen;\n\tstruct sockaddr_storage\tdaddr;\t/* where reply must come from */\n\tsize_t\t\t\tdaddrlen;\n\tstruct cache_deferred_req handle;\n\tsize_t\t\t\txprt_hlen;\n\tint\t\t\targslen;\n\t__be32\t\t\targs[0];\n};\n\n/*\n * List of RPC programs on the same transport endpoint\n */\nstruct svc_program {\n\tstruct svc_program *\tpg_next;\t/* other programs (same xprt) */\n\tu32\t\t\tpg_prog;\t/* program number */\n\tunsigned int\t\tpg_lovers;\t/* lowest version */\n\tunsigned int\t\tpg_hivers;\t/* highest version */\n\tunsigned int\t\tpg_nvers;\t/* number of versions */\n\tstruct svc_version **\tpg_vers;\t/* version array */\n\tchar *\t\t\tpg_name;\t/* service name */\n\tchar *\t\t\tpg_class;\t/* class name: services sharing authentication */\n\tstruct svc_stat *\tpg_stats;\t/* rpc statistics */\n\tint\t\t\t(*pg_authenticate)(struct svc_rqst *);\n};\n\n/*\n * RPC program version\n */\nstruct svc_version {\n\tu32\t\t\tvs_vers;\t/* version number */\n\tu32\t\t\tvs_nproc;\t/* number of procedures */\n\tstruct svc_procedure *\tvs_proc;\t/* per-procedure info */\n\tu32\t\t\tvs_xdrsize;\t/* xdrsize needed for this version */\n\n\t/* Don't register with rpcbind */\n\tbool\t\t\tvs_hidden;\n\n\t/* Don't care if the rpcbind registration fails */\n\tbool\t\t\tvs_rpcb_optnl;\n\n\t/* Need xprt with congestion control */\n\tbool\t\t\tvs_need_cong_ctrl;\n\n\t/* Override dispatch function (e.g. when caching replies).\n\t * A return value of 0 means drop the request. \n\t * vs_dispatch == NULL means use default dispatcher.\n\t */\n\tint\t\t\t(*vs_dispatch)(struct svc_rqst *, __be32 *);\n};\n\n/*\n * RPC procedure info\n */\ntypedef __be32\t(*svc_procfunc)(struct svc_rqst *, void *argp, void *resp);\nstruct svc_procedure {\n\tsvc_procfunc\t\tpc_func;\t/* process the request */\n\tkxdrproc_t\t\tpc_decode;\t/* XDR decode args */\n\tkxdrproc_t\t\tpc_encode;\t/* XDR encode result */\n\tkxdrproc_t\t\tpc_release;\t/* XDR free result */\n\tunsigned int\t\tpc_argsize;\t/* argument struct size */\n\tunsigned int\t\tpc_ressize;\t/* result struct size */\n\tunsigned int\t\tpc_count;\t/* call count */\n\tunsigned int\t\tpc_cachetype;\t/* cache info (NFS) */\n\tunsigned int\t\tpc_xdrressize;\t/* maximum size of XDR reply */\n};\n\n/*\n * Mode for mapping cpus to pools.\n */\nenum {\n\tSVC_POOL_AUTO = -1,\t/* choose one of the others */\n\tSVC_POOL_GLOBAL,\t/* no mapping, just a single global pool\n\t\t\t\t * (legacy & UP mode) */\n\tSVC_POOL_PERCPU,\t/* one pool per cpu */\n\tSVC_POOL_PERNODE\t/* one pool per numa node */\n};\n\nstruct svc_pool_map {\n\tint count;\t\t\t/* How many svc_servs use us */\n\tint mode;\t\t\t/* Note: int not enum to avoid\n\t\t\t\t\t * warnings about \"enumeration value\n\t\t\t\t\t * not handled in switch\" */\n\tunsigned int npools;\n\tunsigned int *pool_to;\t\t/* maps pool id to cpu or node */\n\tunsigned int *to_pool;\t\t/* maps cpu or node to pool id */\n};\n\nextern struct svc_pool_map svc_pool_map;\n\n/*\n * Function prototypes.\n */\nint svc_rpcb_setup(struct svc_serv *serv, struct net *net);\nvoid svc_rpcb_cleanup(struct svc_serv *serv, struct net *net);\nint svc_bind(struct svc_serv *serv, struct net *net);\nstruct svc_serv *svc_create(struct svc_program *, unsigned int,\n\t\t\t    struct svc_serv_ops *);\nstruct svc_rqst *svc_rqst_alloc(struct svc_serv *serv,\n\t\t\t\t\tstruct svc_pool *pool, int node);\nstruct svc_rqst *svc_prepare_thread(struct svc_serv *serv,\n\t\t\t\t\tstruct svc_pool *pool, int node);\nvoid\t\t   svc_rqst_free(struct svc_rqst *);\nvoid\t\t   svc_exit_thread(struct svc_rqst *);\nunsigned int\t   svc_pool_map_get(void);\nvoid\t\t   svc_pool_map_put(void);\nstruct svc_serv *  svc_create_pooled(struct svc_program *, unsigned int,\n\t\t\tstruct svc_serv_ops *);\nint\t\t   svc_set_num_threads(struct svc_serv *, struct svc_pool *, int);\nint\t\t   svc_pool_stats_open(struct svc_serv *serv, struct file *file);\nvoid\t\t   svc_destroy(struct svc_serv *);\nvoid\t\t   svc_shutdown_net(struct svc_serv *, struct net *);\nint\t\t   svc_process(struct svc_rqst *);\nint\t\t   bc_svc_process(struct svc_serv *, struct rpc_rqst *,\n\t\t\tstruct svc_rqst *);\nint\t\t   svc_register(const struct svc_serv *, struct net *, const int,\n\t\t\t\tconst unsigned short, const unsigned short);\n\nvoid\t\t   svc_wake_up(struct svc_serv *);\nvoid\t\t   svc_reserve(struct svc_rqst *rqstp, int space);\nstruct svc_pool *  svc_pool_for_cpu(struct svc_serv *serv, int cpu);\nchar *\t\t   svc_print_addr(struct svc_rqst *, char *, size_t);\n\n#define\tRPC_MAX_ADDRBUFLEN\t(63U)\n\n/*\n * When we want to reduce the size of the reserved space in the response\n * buffer, we need to take into account the size of any checksum data that\n * may be at the end of the packet. This is difficult to determine exactly\n * for all cases without actually generating the checksum, so we just use a\n * static value.\n */\nstatic inline void svc_reserve_auth(struct svc_rqst *rqstp, int space)\n{\n\tsvc_reserve(rqstp, space + rqstp->rq_auth_slack);\n}\n\n#endif /* SUNRPC_SVC_H */\n", "/*\n * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n#ifndef SVC_RDMA_H\n#define SVC_RDMA_H\n#include <linux/sunrpc/xdr.h>\n#include <linux/sunrpc/svcsock.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n#define SVCRDMA_DEBUG\n\n/* RPC/RDMA parameters and stats */\nextern unsigned int svcrdma_ord;\nextern unsigned int svcrdma_max_requests;\nextern unsigned int svcrdma_max_bc_requests;\nextern unsigned int svcrdma_max_req_size;\n\nextern atomic_t rdma_stat_recv;\nextern atomic_t rdma_stat_read;\nextern atomic_t rdma_stat_write;\nextern atomic_t rdma_stat_sq_starve;\nextern atomic_t rdma_stat_rq_starve;\nextern atomic_t rdma_stat_rq_poll;\nextern atomic_t rdma_stat_rq_prod;\nextern atomic_t rdma_stat_sq_poll;\nextern atomic_t rdma_stat_sq_prod;\n\n/*\n * Contexts are built when an RDMA request is created and are a\n * record of the resources that can be recovered when the request\n * completes.\n */\nstruct svc_rdma_op_ctxt {\n\tstruct list_head list;\n\tstruct svc_rdma_op_ctxt *read_hdr;\n\tstruct svc_rdma_fastreg_mr *frmr;\n\tint hdr_count;\n\tstruct xdr_buf arg;\n\tstruct ib_cqe cqe;\n\tstruct ib_cqe reg_cqe;\n\tstruct ib_cqe inv_cqe;\n\tu32 byte_len;\n\tu32 position;\n\tstruct svcxprt_rdma *xprt;\n\tunsigned long flags;\n\tenum dma_data_direction direction;\n\tint count;\n\tunsigned int mapped_sges;\n\tstruct ib_sge sge[RPCSVC_MAXPAGES];\n\tstruct page *pages[RPCSVC_MAXPAGES];\n};\n\n/*\n * NFS_ requests are mapped on the client side by the chunk lists in\n * the RPCRDMA header. During the fetching of the RPC from the client\n * and the writing of the reply to the client, the memory in the\n * client and the memory in the server must be mapped as contiguous\n * vaddr/len for access by the hardware. These data strucures keep\n * these mappings.\n *\n * For an RDMA_WRITE, the 'sge' maps the RPC REPLY. For RDMA_READ, the\n * 'sge' in the svc_rdma_req_map maps the server side RPC reply and the\n * 'ch' field maps the read-list of the RPCRDMA header to the 'sge'\n * mapping of the reply.\n */\nstruct svc_rdma_chunk_sge {\n\tint start;\t\t/* sge no for this chunk */\n\tint count;\t\t/* sge count for this chunk */\n};\nstruct svc_rdma_fastreg_mr {\n\tstruct ib_mr *mr;\n\tstruct scatterlist *sg;\n\tint sg_nents;\n\tunsigned long access_flags;\n\tenum dma_data_direction direction;\n\tstruct list_head frmr_list;\n};\nstruct svc_rdma_req_map {\n\tstruct list_head free;\n\tunsigned long count;\n\tunion {\n\t\tstruct kvec sge[RPCSVC_MAXPAGES];\n\t\tstruct svc_rdma_chunk_sge ch[RPCSVC_MAXPAGES];\n\t\tunsigned long lkey[RPCSVC_MAXPAGES];\n\t};\n};\n#define RDMACTXT_F_LAST_CTXT\t2\n\n#define\tSVCRDMA_DEVCAP_FAST_REG\t\t1\t/* fast mr registration */\n#define\tSVCRDMA_DEVCAP_READ_W_INV\t2\t/* read w/ invalidate */\n\nstruct svcxprt_rdma {\n\tstruct svc_xprt      sc_xprt;\t\t/* SVC transport structure */\n\tstruct rdma_cm_id    *sc_cm_id;\t\t/* RDMA connection id */\n\tstruct list_head     sc_accept_q;\t/* Conn. waiting accept */\n\tint\t\t     sc_ord;\t\t/* RDMA read limit */\n\tint                  sc_max_sge;\n\tint                  sc_max_sge_rd;\t/* max sge for read target */\n\tbool\t\t     sc_snd_w_inv;\t/* OK to use Send With Invalidate */\n\n\tatomic_t             sc_sq_avail;\t/* SQEs ready to be consumed */\n\tunsigned int\t     sc_sq_depth;\t/* Depth of SQ */\n\tunsigned int\t     sc_rq_depth;\t/* Depth of RQ */\n\t__be32\t\t     sc_fc_credits;\t/* Forward credits */\n\tu32\t\t     sc_max_requests;\t/* Max requests */\n\tu32\t\t     sc_max_bc_requests;/* Backward credits */\n\tint                  sc_max_req_size;\t/* Size of each RQ WR buf */\n\n\tstruct ib_pd         *sc_pd;\n\n\tspinlock_t\t     sc_ctxt_lock;\n\tstruct list_head     sc_ctxts;\n\tint\t\t     sc_ctxt_used;\n\tspinlock_t\t     sc_map_lock;\n\tstruct list_head     sc_maps;\n\n\tstruct list_head     sc_rq_dto_q;\n\tspinlock_t\t     sc_rq_dto_lock;\n\tstruct ib_qp         *sc_qp;\n\tstruct ib_cq         *sc_rq_cq;\n\tstruct ib_cq         *sc_sq_cq;\n\tint\t\t     (*sc_reader)(struct svcxprt_rdma *,\n\t\t\t\t\t  struct svc_rqst *,\n\t\t\t\t\t  struct svc_rdma_op_ctxt *,\n\t\t\t\t\t  int *, u32 *, u32, u32, u64, bool);\n\tu32\t\t     sc_dev_caps;\t/* distilled device caps */\n\tunsigned int\t     sc_frmr_pg_list_len;\n\tstruct list_head     sc_frmr_q;\n\tspinlock_t\t     sc_frmr_q_lock;\n\n\tspinlock_t\t     sc_lock;\t\t/* transport lock */\n\n\twait_queue_head_t    sc_send_wait;\t/* SQ exhaustion waitlist */\n\tunsigned long\t     sc_flags;\n\tstruct list_head     sc_read_complete_q;\n\tstruct work_struct   sc_work;\n};\n/* sc_flags */\n#define RDMAXPRT_CONN_PENDING\t3\n\n#define RPCRDMA_LISTEN_BACKLOG  10\n/* The default ORD value is based on two outstanding full-size writes with a\n * page size of 4k, or 32k * 2 ops / 4k = 16 outstanding RDMA_READ.  */\n#define RPCRDMA_ORD             (64/4)\n#define RPCRDMA_SQ_DEPTH_MULT   8\n#define RPCRDMA_MAX_REQUESTS    32\n#define RPCRDMA_MAX_REQ_SIZE    4096\n\n/* Typical ULP usage of BC requests is NFSv4.1 backchannel. Our\n * current NFSv4.1 implementation supports one backchannel slot.\n */\n#define RPCRDMA_MAX_BC_REQUESTS\t2\n\n#define RPCSVC_MAXPAYLOAD_RDMA\tRPCSVC_MAXPAYLOAD\n\n/* Track DMA maps for this transport and context */\nstatic inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,\n\t\t\t\t\t   struct svc_rdma_op_ctxt *ctxt)\n{\n\tctxt->mapped_sges++;\n}\n\n/* svc_rdma_backchannel.c */\nextern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,\n\t\t\t\t    struct rpcrdma_msg *rmsgp,\n\t\t\t\t    struct xdr_buf *rcvbuf);\n\n/* svc_rdma_marshal.c */\nextern int svc_rdma_xdr_decode_req(struct xdr_buf *);\nextern int svc_rdma_xdr_encode_error(struct svcxprt_rdma *,\n\t\t\t\t     struct rpcrdma_msg *,\n\t\t\t\t     enum rpcrdma_errcode, __be32 *);\nextern void svc_rdma_xdr_encode_write_list(struct rpcrdma_msg *, int);\nextern void svc_rdma_xdr_encode_reply_array(struct rpcrdma_write_array *, int);\nextern void svc_rdma_xdr_encode_array_chunk(struct rpcrdma_write_array *, int,\n\t\t\t\t\t    __be32, __be64, u32);\nextern unsigned int svc_rdma_xdr_get_reply_hdr_len(__be32 *rdma_resp);\n\n/* svc_rdma_recvfrom.c */\nextern int svc_rdma_recvfrom(struct svc_rqst *);\nextern int rdma_read_chunk_lcl(struct svcxprt_rdma *, struct svc_rqst *,\n\t\t\t       struct svc_rdma_op_ctxt *, int *, u32 *,\n\t\t\t       u32, u32, u64, bool);\nextern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,\n\t\t\t\tstruct svc_rdma_op_ctxt *, int *, u32 *,\n\t\t\t\tu32, u32, u64, bool);\n\n/* svc_rdma_sendto.c */\nextern int svc_rdma_map_xdr(struct svcxprt_rdma *, struct xdr_buf *,\n\t\t\t    struct svc_rdma_req_map *, bool);\nextern int svc_rdma_sendto(struct svc_rqst *);\nextern void svc_rdma_send_error(struct svcxprt_rdma *, struct rpcrdma_msg *,\n\t\t\t\tint);\n\n/* svc_rdma_transport.c */\nextern void svc_rdma_wc_send(struct ib_cq *, struct ib_wc *);\nextern void svc_rdma_wc_write(struct ib_cq *, struct ib_wc *);\nextern void svc_rdma_wc_reg(struct ib_cq *, struct ib_wc *);\nextern void svc_rdma_wc_read(struct ib_cq *, struct ib_wc *);\nextern void svc_rdma_wc_inv(struct ib_cq *, struct ib_wc *);\nextern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);\nextern int svc_rdma_post_recv(struct svcxprt_rdma *, gfp_t);\nextern int svc_rdma_repost_recv(struct svcxprt_rdma *, gfp_t);\nextern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);\nextern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);\nextern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);\nextern void svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt);\nextern struct svc_rdma_req_map *svc_rdma_get_req_map(struct svcxprt_rdma *);\nextern void svc_rdma_put_req_map(struct svcxprt_rdma *,\n\t\t\t\t struct svc_rdma_req_map *);\nextern struct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *);\nextern void svc_rdma_put_frmr(struct svcxprt_rdma *,\n\t\t\t      struct svc_rdma_fastreg_mr *);\nextern void svc_sq_reap(struct svcxprt_rdma *);\nextern void svc_rq_reap(struct svcxprt_rdma *);\nextern void svc_rdma_prep_reply_hdr(struct svc_rqst *);\n\nextern struct svc_xprt_class svc_rdma_class;\n#ifdef CONFIG_SUNRPC_BACKCHANNEL\nextern struct svc_xprt_class svc_rdma_bc_class;\n#endif\n\n/* svc_rdma.c */\nextern struct workqueue_struct *svc_rdma_wq;\nextern int svc_rdma_init(void);\nextern void svc_rdma_cleanup(void);\n\n#endif\n", "/*\n * Upcall description for nfsdcld communication\n *\n * Copyright (c) 2012 Red Hat, Inc.\n * Author(s): Jeff Layton <jlayton@redhat.com>\n *\n *  This program is free software; you can redistribute it and/or modify\n *  it under the terms of the GNU General Public License as published by\n *  the Free Software Foundation; either version 2 of the License, or\n *  (at your option) any later version.\n *\n *  This program is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *  GNU General Public License for more details.\n *\n *  You should have received a copy of the GNU General Public License\n *  along with this program; if not, write to the Free Software\n *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n */\n\n#ifndef _NFSD_CLD_H\n#define _NFSD_CLD_H\n\n/* latest upcall version available */\n#define CLD_UPCALL_VERSION 1\n\n/* defined by RFC3530 */\n#define NFS4_OPAQUE_LIMIT 1024\n\nenum cld_command {\n\tCld_Create,\t\t/* create a record for this cm_id */\n\tCld_Remove,\t\t/* remove record of this cm_id */\n\tCld_Check,\t\t/* is this cm_id allowed? */\n\tCld_GraceDone,\t\t/* grace period is complete */\n};\n\n/* representation of long-form NFSv4 client ID */\nstruct cld_name {\n\tuint16_t\tcn_len;\t\t\t\t/* length of cm_id */\n\tunsigned char\tcn_id[NFS4_OPAQUE_LIMIT];\t/* client-provided */\n} __attribute__((packed));\n\n/* message struct for communication with userspace */\nstruct cld_msg {\n\tuint8_t\t\tcm_vers;\t\t/* upcall version */\n\tuint8_t\t\tcm_cmd;\t\t\t/* upcall command */\n\tint16_t\t\tcm_status;\t\t/* return code */\n\tuint32_t\tcm_xid;\t\t\t/* transaction id */\n\tunion {\n\t\tint64_t\t\tcm_gracetime;\t/* grace period start time */\n\t\tstruct cld_name\tcm_name;\n\t} __attribute__((packed)) cm_u;\n} __attribute__((packed));\n\n#endif /* !_NFSD_CLD_H */\n", "config SUNRPC\n\ttristate\n\tdepends on MULTIUSER\n\nconfig SUNRPC_GSS\n\ttristate\n\tselect OID_REGISTRY\n\tdepends on MULTIUSER\n\nconfig SUNRPC_BACKCHANNEL\n\tbool\n\tdepends on SUNRPC\n\nconfig SUNRPC_SWAP\n\tbool\n\tdepends on SUNRPC\n\nconfig RPCSEC_GSS_KRB5\n\ttristate \"Secure RPC: Kerberos V mechanism\"\n\tdepends on SUNRPC && CRYPTO\n\tdepends on CRYPTO_MD5 && CRYPTO_DES && CRYPTO_CBC && CRYPTO_CTS\n\tdepends on CRYPTO_ECB && CRYPTO_HMAC && CRYPTO_SHA1 && CRYPTO_AES\n\tdepends on CRYPTO_ARC4\n\tdefault y\n\tselect SUNRPC_GSS\n\thelp\n\t  Choose Y here to enable Secure RPC using the Kerberos version 5\n\t  GSS-API mechanism (RFC 1964).\n\n\t  Secure RPC calls with Kerberos require an auxiliary user-space\n\t  daemon which may be found in the Linux nfs-utils package\n\t  available from http://linux-nfs.org/.  In addition, user-space\n\t  Kerberos support should be installed.\n\n\t  If unsure, say Y.\n\nconfig SUNRPC_DEBUG\n\tbool \"RPC: Enable dprintk debugging\"\n\tdepends on SUNRPC && SYSCTL\n\tselect DEBUG_FS\n\thelp\n\t  This option enables a sysctl-based debugging interface\n\t  that is be used by the 'rpcdebug' utility to turn on or off\n\t  logging of different aspects of the kernel RPC activity.\n\n\t  Disabling this option will make your kernel slightly smaller,\n\t  but makes troubleshooting NFS issues significantly harder.\n\n\t  If unsure, say Y.\n\nconfig SUNRPC_XPRT_RDMA\n\ttristate \"RPC-over-RDMA transport\"\n\tdepends on SUNRPC && INFINIBAND && INFINIBAND_ADDR_TRANS\n\tdefault SUNRPC && INFINIBAND\n\thelp\n\t  This option allows the NFS client and server to use RDMA\n\t  transports (InfiniBand, iWARP, or RoCE).\n\n\t  To compile this support as a module, choose M. The module\n\t  will be called rpcrdma.ko.\n\n\t  If unsure, or you know there is no RDMA capability on your\n\t  hardware platform, say N.\n", "/*\n * linux/net/sunrpc/svc.c\n *\n * High-level RPC service routines\n *\n * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>\n *\n * Multiple threads pools and NUMAisation\n * Copyright (c) 2006 Silicon Graphics, Inc.\n * by Greg Banks <gnb@melbourne.sgi.com>\n */\n\n#include <linux/linkage.h>\n#include <linux/sched/signal.h>\n#include <linux/errno.h>\n#include <linux/net.h>\n#include <linux/in.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/kthread.h>\n#include <linux/slab.h>\n\n#include <linux/sunrpc/types.h>\n#include <linux/sunrpc/xdr.h>\n#include <linux/sunrpc/stats.h>\n#include <linux/sunrpc/svcsock.h>\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/bc_xprt.h>\n\n#include <trace/events/sunrpc.h>\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCDSP\n\nstatic void svc_unregister(const struct svc_serv *serv, struct net *net);\n\n#define svc_serv_is_pooled(serv)    ((serv)->sv_ops->svo_function)\n\n#define SVC_POOL_DEFAULT\tSVC_POOL_GLOBAL\n\n/*\n * Structure for mapping cpus to pools and vice versa.\n * Setup once during sunrpc initialisation.\n */\nstruct svc_pool_map svc_pool_map = {\n\t.mode = SVC_POOL_DEFAULT\n};\nEXPORT_SYMBOL_GPL(svc_pool_map);\n\nstatic DEFINE_MUTEX(svc_pool_map_mutex);/* protects svc_pool_map.count only */\n\nstatic int\nparam_set_pool_mode(const char *val, struct kernel_param *kp)\n{\n\tint *ip = (int *)kp->arg;\n\tstruct svc_pool_map *m = &svc_pool_map;\n\tint err;\n\n\tmutex_lock(&svc_pool_map_mutex);\n\n\terr = -EBUSY;\n\tif (m->count)\n\t\tgoto out;\n\n\terr = 0;\n\tif (!strncmp(val, \"auto\", 4))\n\t\t*ip = SVC_POOL_AUTO;\n\telse if (!strncmp(val, \"global\", 6))\n\t\t*ip = SVC_POOL_GLOBAL;\n\telse if (!strncmp(val, \"percpu\", 6))\n\t\t*ip = SVC_POOL_PERCPU;\n\telse if (!strncmp(val, \"pernode\", 7))\n\t\t*ip = SVC_POOL_PERNODE;\n\telse\n\t\terr = -EINVAL;\n\nout:\n\tmutex_unlock(&svc_pool_map_mutex);\n\treturn err;\n}\n\nstatic int\nparam_get_pool_mode(char *buf, struct kernel_param *kp)\n{\n\tint *ip = (int *)kp->arg;\n\n\tswitch (*ip)\n\t{\n\tcase SVC_POOL_AUTO:\n\t\treturn strlcpy(buf, \"auto\", 20);\n\tcase SVC_POOL_GLOBAL:\n\t\treturn strlcpy(buf, \"global\", 20);\n\tcase SVC_POOL_PERCPU:\n\t\treturn strlcpy(buf, \"percpu\", 20);\n\tcase SVC_POOL_PERNODE:\n\t\treturn strlcpy(buf, \"pernode\", 20);\n\tdefault:\n\t\treturn sprintf(buf, \"%d\", *ip);\n\t}\n}\n\nmodule_param_call(pool_mode, param_set_pool_mode, param_get_pool_mode,\n\t\t &svc_pool_map.mode, 0644);\n\n/*\n * Detect best pool mapping mode heuristically,\n * according to the machine's topology.\n */\nstatic int\nsvc_pool_map_choose_mode(void)\n{\n\tunsigned int node;\n\n\tif (nr_online_nodes > 1) {\n\t\t/*\n\t\t * Actually have multiple NUMA nodes,\n\t\t * so split pools on NUMA node boundaries\n\t\t */\n\t\treturn SVC_POOL_PERNODE;\n\t}\n\n\tnode = first_online_node;\n\tif (nr_cpus_node(node) > 2) {\n\t\t/*\n\t\t * Non-trivial SMP, or CONFIG_NUMA on\n\t\t * non-NUMA hardware, e.g. with a generic\n\t\t * x86_64 kernel on Xeons.  In this case we\n\t\t * want to divide the pools on cpu boundaries.\n\t\t */\n\t\treturn SVC_POOL_PERCPU;\n\t}\n\n\t/* default: one global pool */\n\treturn SVC_POOL_GLOBAL;\n}\n\n/*\n * Allocate the to_pool[] and pool_to[] arrays.\n * Returns 0 on success or an errno.\n */\nstatic int\nsvc_pool_map_alloc_arrays(struct svc_pool_map *m, unsigned int maxpools)\n{\n\tm->to_pool = kcalloc(maxpools, sizeof(unsigned int), GFP_KERNEL);\n\tif (!m->to_pool)\n\t\tgoto fail;\n\tm->pool_to = kcalloc(maxpools, sizeof(unsigned int), GFP_KERNEL);\n\tif (!m->pool_to)\n\t\tgoto fail_free;\n\n\treturn 0;\n\nfail_free:\n\tkfree(m->to_pool);\n\tm->to_pool = NULL;\nfail:\n\treturn -ENOMEM;\n}\n\n/*\n * Initialise the pool map for SVC_POOL_PERCPU mode.\n * Returns number of pools or <0 on error.\n */\nstatic int\nsvc_pool_map_init_percpu(struct svc_pool_map *m)\n{\n\tunsigned int maxpools = nr_cpu_ids;\n\tunsigned int pidx = 0;\n\tunsigned int cpu;\n\tint err;\n\n\terr = svc_pool_map_alloc_arrays(m, maxpools);\n\tif (err)\n\t\treturn err;\n\n\tfor_each_online_cpu(cpu) {\n\t\tBUG_ON(pidx >= maxpools);\n\t\tm->to_pool[cpu] = pidx;\n\t\tm->pool_to[pidx] = cpu;\n\t\tpidx++;\n\t}\n\t/* cpus brought online later all get mapped to pool0, sorry */\n\n\treturn pidx;\n};\n\n\n/*\n * Initialise the pool map for SVC_POOL_PERNODE mode.\n * Returns number of pools or <0 on error.\n */\nstatic int\nsvc_pool_map_init_pernode(struct svc_pool_map *m)\n{\n\tunsigned int maxpools = nr_node_ids;\n\tunsigned int pidx = 0;\n\tunsigned int node;\n\tint err;\n\n\terr = svc_pool_map_alloc_arrays(m, maxpools);\n\tif (err)\n\t\treturn err;\n\n\tfor_each_node_with_cpus(node) {\n\t\t/* some architectures (e.g. SN2) have cpuless nodes */\n\t\tBUG_ON(pidx > maxpools);\n\t\tm->to_pool[node] = pidx;\n\t\tm->pool_to[pidx] = node;\n\t\tpidx++;\n\t}\n\t/* nodes brought online later all get mapped to pool0, sorry */\n\n\treturn pidx;\n}\n\n\n/*\n * Add a reference to the global map of cpus to pools (and\n * vice versa).  Initialise the map if we're the first user.\n * Returns the number of pools.\n */\nunsigned int\nsvc_pool_map_get(void)\n{\n\tstruct svc_pool_map *m = &svc_pool_map;\n\tint npools = -1;\n\n\tmutex_lock(&svc_pool_map_mutex);\n\n\tif (m->count++) {\n\t\tmutex_unlock(&svc_pool_map_mutex);\n\t\treturn m->npools;\n\t}\n\n\tif (m->mode == SVC_POOL_AUTO)\n\t\tm->mode = svc_pool_map_choose_mode();\n\n\tswitch (m->mode) {\n\tcase SVC_POOL_PERCPU:\n\t\tnpools = svc_pool_map_init_percpu(m);\n\t\tbreak;\n\tcase SVC_POOL_PERNODE:\n\t\tnpools = svc_pool_map_init_pernode(m);\n\t\tbreak;\n\t}\n\n\tif (npools < 0) {\n\t\t/* default, or memory allocation failure */\n\t\tnpools = 1;\n\t\tm->mode = SVC_POOL_GLOBAL;\n\t}\n\tm->npools = npools;\n\n\tmutex_unlock(&svc_pool_map_mutex);\n\treturn m->npools;\n}\nEXPORT_SYMBOL_GPL(svc_pool_map_get);\n\n/*\n * Drop a reference to the global map of cpus to pools.\n * When the last reference is dropped, the map data is\n * freed; this allows the sysadmin to change the pool\n * mode using the pool_mode module option without\n * rebooting or re-loading sunrpc.ko.\n */\nvoid\nsvc_pool_map_put(void)\n{\n\tstruct svc_pool_map *m = &svc_pool_map;\n\n\tmutex_lock(&svc_pool_map_mutex);\n\n\tif (!--m->count) {\n\t\tkfree(m->to_pool);\n\t\tm->to_pool = NULL;\n\t\tkfree(m->pool_to);\n\t\tm->pool_to = NULL;\n\t\tm->npools = 0;\n\t}\n\n\tmutex_unlock(&svc_pool_map_mutex);\n}\nEXPORT_SYMBOL_GPL(svc_pool_map_put);\n\nstatic int svc_pool_map_get_node(unsigned int pidx)\n{\n\tconst struct svc_pool_map *m = &svc_pool_map;\n\n\tif (m->count) {\n\t\tif (m->mode == SVC_POOL_PERCPU)\n\t\t\treturn cpu_to_node(m->pool_to[pidx]);\n\t\tif (m->mode == SVC_POOL_PERNODE)\n\t\t\treturn m->pool_to[pidx];\n\t}\n\treturn NUMA_NO_NODE;\n}\n/*\n * Set the given thread's cpus_allowed mask so that it\n * will only run on cpus in the given pool.\n */\nstatic inline void\nsvc_pool_map_set_cpumask(struct task_struct *task, unsigned int pidx)\n{\n\tstruct svc_pool_map *m = &svc_pool_map;\n\tunsigned int node = m->pool_to[pidx];\n\n\t/*\n\t * The caller checks for sv_nrpools > 1, which\n\t * implies that we've been initialized.\n\t */\n\tWARN_ON_ONCE(m->count == 0);\n\tif (m->count == 0)\n\t\treturn;\n\n\tswitch (m->mode) {\n\tcase SVC_POOL_PERCPU:\n\t{\n\t\tset_cpus_allowed_ptr(task, cpumask_of(node));\n\t\tbreak;\n\t}\n\tcase SVC_POOL_PERNODE:\n\t{\n\t\tset_cpus_allowed_ptr(task, cpumask_of_node(node));\n\t\tbreak;\n\t}\n\t}\n}\n\n/*\n * Use the mapping mode to choose a pool for a given CPU.\n * Used when enqueueing an incoming RPC.  Always returns\n * a non-NULL pool pointer.\n */\nstruct svc_pool *\nsvc_pool_for_cpu(struct svc_serv *serv, int cpu)\n{\n\tstruct svc_pool_map *m = &svc_pool_map;\n\tunsigned int pidx = 0;\n\n\t/*\n\t * An uninitialised map happens in a pure client when\n\t * lockd is brought up, so silently treat it the\n\t * same as SVC_POOL_GLOBAL.\n\t */\n\tif (svc_serv_is_pooled(serv)) {\n\t\tswitch (m->mode) {\n\t\tcase SVC_POOL_PERCPU:\n\t\t\tpidx = m->to_pool[cpu];\n\t\t\tbreak;\n\t\tcase SVC_POOL_PERNODE:\n\t\t\tpidx = m->to_pool[cpu_to_node(cpu)];\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn &serv->sv_pools[pidx % serv->sv_nrpools];\n}\n\nint svc_rpcb_setup(struct svc_serv *serv, struct net *net)\n{\n\tint err;\n\n\terr = rpcb_create_local(net);\n\tif (err)\n\t\treturn err;\n\n\t/* Remove any stale portmap registrations */\n\tsvc_unregister(serv, net);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(svc_rpcb_setup);\n\nvoid svc_rpcb_cleanup(struct svc_serv *serv, struct net *net)\n{\n\tsvc_unregister(serv, net);\n\trpcb_put_local(net);\n}\nEXPORT_SYMBOL_GPL(svc_rpcb_cleanup);\n\nstatic int svc_uses_rpcbind(struct svc_serv *serv)\n{\n\tstruct svc_program\t*progp;\n\tunsigned int\t\ti;\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next) {\n\t\tfor (i = 0; i < progp->pg_nvers; i++) {\n\t\t\tif (progp->pg_vers[i] == NULL)\n\t\t\t\tcontinue;\n\t\t\tif (!progp->pg_vers[i]->vs_hidden)\n\t\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint svc_bind(struct svc_serv *serv, struct net *net)\n{\n\tif (!svc_uses_rpcbind(serv))\n\t\treturn 0;\n\treturn svc_rpcb_setup(serv, net);\n}\nEXPORT_SYMBOL_GPL(svc_bind);\n\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\nstatic void\n__svc_init_bc(struct svc_serv *serv)\n{\n\tINIT_LIST_HEAD(&serv->sv_cb_list);\n\tspin_lock_init(&serv->sv_cb_lock);\n\tinit_waitqueue_head(&serv->sv_cb_waitq);\n}\n#else\nstatic void\n__svc_init_bc(struct svc_serv *serv)\n{\n}\n#endif\n\n/*\n * Create an RPC service\n */\nstatic struct svc_serv *\n__svc_create(struct svc_program *prog, unsigned int bufsize, int npools,\n\t     struct svc_serv_ops *ops)\n{\n\tstruct svc_serv\t*serv;\n\tunsigned int vers;\n\tunsigned int xdrsize;\n\tunsigned int i;\n\n\tif (!(serv = kzalloc(sizeof(*serv), GFP_KERNEL)))\n\t\treturn NULL;\n\tserv->sv_name      = prog->pg_name;\n\tserv->sv_program   = prog;\n\tserv->sv_nrthreads = 1;\n\tserv->sv_stats     = prog->pg_stats;\n\tif (bufsize > RPCSVC_MAXPAYLOAD)\n\t\tbufsize = RPCSVC_MAXPAYLOAD;\n\tserv->sv_max_payload = bufsize? bufsize : 4096;\n\tserv->sv_max_mesg  = roundup(serv->sv_max_payload + PAGE_SIZE, PAGE_SIZE);\n\tserv->sv_ops = ops;\n\txdrsize = 0;\n\twhile (prog) {\n\t\tprog->pg_lovers = prog->pg_nvers-1;\n\t\tfor (vers=0; vers<prog->pg_nvers ; vers++)\n\t\t\tif (prog->pg_vers[vers]) {\n\t\t\t\tprog->pg_hivers = vers;\n\t\t\t\tif (prog->pg_lovers > vers)\n\t\t\t\t\tprog->pg_lovers = vers;\n\t\t\t\tif (prog->pg_vers[vers]->vs_xdrsize > xdrsize)\n\t\t\t\t\txdrsize = prog->pg_vers[vers]->vs_xdrsize;\n\t\t\t}\n\t\tprog = prog->pg_next;\n\t}\n\tserv->sv_xdrsize   = xdrsize;\n\tINIT_LIST_HEAD(&serv->sv_tempsocks);\n\tINIT_LIST_HEAD(&serv->sv_permsocks);\n\tinit_timer(&serv->sv_temptimer);\n\tspin_lock_init(&serv->sv_lock);\n\n\t__svc_init_bc(serv);\n\n\tserv->sv_nrpools = npools;\n\tserv->sv_pools =\n\t\tkcalloc(serv->sv_nrpools, sizeof(struct svc_pool),\n\t\t\tGFP_KERNEL);\n\tif (!serv->sv_pools) {\n\t\tkfree(serv);\n\t\treturn NULL;\n\t}\n\n\tfor (i = 0; i < serv->sv_nrpools; i++) {\n\t\tstruct svc_pool *pool = &serv->sv_pools[i];\n\n\t\tdprintk(\"svc: initialising pool %u for %s\\n\",\n\t\t\t\ti, serv->sv_name);\n\n\t\tpool->sp_id = i;\n\t\tINIT_LIST_HEAD(&pool->sp_sockets);\n\t\tINIT_LIST_HEAD(&pool->sp_all_threads);\n\t\tspin_lock_init(&pool->sp_lock);\n\t}\n\n\treturn serv;\n}\n\nstruct svc_serv *\nsvc_create(struct svc_program *prog, unsigned int bufsize,\n\t   struct svc_serv_ops *ops)\n{\n\treturn __svc_create(prog, bufsize, /*npools*/1, ops);\n}\nEXPORT_SYMBOL_GPL(svc_create);\n\nstruct svc_serv *\nsvc_create_pooled(struct svc_program *prog, unsigned int bufsize,\n\t\t  struct svc_serv_ops *ops)\n{\n\tstruct svc_serv *serv;\n\tunsigned int npools = svc_pool_map_get();\n\n\tserv = __svc_create(prog, bufsize, npools, ops);\n\tif (!serv)\n\t\tgoto out_err;\n\treturn serv;\nout_err:\n\tsvc_pool_map_put();\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(svc_create_pooled);\n\nvoid svc_shutdown_net(struct svc_serv *serv, struct net *net)\n{\n\tsvc_close_net(serv, net);\n\n\tif (serv->sv_ops->svo_shutdown)\n\t\tserv->sv_ops->svo_shutdown(serv, net);\n}\nEXPORT_SYMBOL_GPL(svc_shutdown_net);\n\n/*\n * Destroy an RPC service. Should be called with appropriate locking to\n * protect the sv_nrthreads, sv_permsocks and sv_tempsocks.\n */\nvoid\nsvc_destroy(struct svc_serv *serv)\n{\n\tdprintk(\"svc: svc_destroy(%s, %d)\\n\",\n\t\t\t\tserv->sv_program->pg_name,\n\t\t\t\tserv->sv_nrthreads);\n\n\tif (serv->sv_nrthreads) {\n\t\tif (--(serv->sv_nrthreads) != 0) {\n\t\t\tsvc_sock_update_bufs(serv);\n\t\t\treturn;\n\t\t}\n\t} else\n\t\tprintk(\"svc_destroy: no threads for serv=%p!\\n\", serv);\n\n\tdel_timer_sync(&serv->sv_temptimer);\n\n\t/*\n\t * The last user is gone and thus all sockets have to be destroyed to\n\t * the point. Check this.\n\t */\n\tBUG_ON(!list_empty(&serv->sv_permsocks));\n\tBUG_ON(!list_empty(&serv->sv_tempsocks));\n\n\tcache_clean_deferred(serv);\n\n\tif (svc_serv_is_pooled(serv))\n\t\tsvc_pool_map_put();\n\n\tkfree(serv->sv_pools);\n\tkfree(serv);\n}\nEXPORT_SYMBOL_GPL(svc_destroy);\n\n/*\n * Allocate an RPC server's buffer space.\n * We allocate pages and place them in rq_argpages.\n */\nstatic int\nsvc_init_buffer(struct svc_rqst *rqstp, unsigned int size, int node)\n{\n\tunsigned int pages, arghi;\n\n\t/* bc_xprt uses fore channel allocated buffers */\n\tif (svc_is_backchannel(rqstp))\n\t\treturn 1;\n\n\tpages = size / PAGE_SIZE + 1; /* extra page as we hold both request and reply.\n\t\t\t\t       * We assume one is at most one page\n\t\t\t\t       */\n\targhi = 0;\n\tWARN_ON_ONCE(pages > RPCSVC_MAXPAGES);\n\tif (pages > RPCSVC_MAXPAGES)\n\t\tpages = RPCSVC_MAXPAGES;\n\twhile (pages) {\n\t\tstruct page *p = alloc_pages_node(node, GFP_KERNEL, 0);\n\t\tif (!p)\n\t\t\tbreak;\n\t\trqstp->rq_pages[arghi++] = p;\n\t\tpages--;\n\t}\n\treturn pages == 0;\n}\n\n/*\n * Release an RPC server buffer\n */\nstatic void\nsvc_release_buffer(struct svc_rqst *rqstp)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ARRAY_SIZE(rqstp->rq_pages); i++)\n\t\tif (rqstp->rq_pages[i])\n\t\t\tput_page(rqstp->rq_pages[i]);\n}\n\nstruct svc_rqst *\nsvc_rqst_alloc(struct svc_serv *serv, struct svc_pool *pool, int node)\n{\n\tstruct svc_rqst\t*rqstp;\n\n\trqstp = kzalloc_node(sizeof(*rqstp), GFP_KERNEL, node);\n\tif (!rqstp)\n\t\treturn rqstp;\n\n\t__set_bit(RQ_BUSY, &rqstp->rq_flags);\n\tspin_lock_init(&rqstp->rq_lock);\n\trqstp->rq_server = serv;\n\trqstp->rq_pool = pool;\n\n\trqstp->rq_argp = kmalloc_node(serv->sv_xdrsize, GFP_KERNEL, node);\n\tif (!rqstp->rq_argp)\n\t\tgoto out_enomem;\n\n\trqstp->rq_resp = kmalloc_node(serv->sv_xdrsize, GFP_KERNEL, node);\n\tif (!rqstp->rq_resp)\n\t\tgoto out_enomem;\n\n\tif (!svc_init_buffer(rqstp, serv->sv_max_mesg, node))\n\t\tgoto out_enomem;\n\n\treturn rqstp;\nout_enomem:\n\tsvc_rqst_free(rqstp);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(svc_rqst_alloc);\n\nstruct svc_rqst *\nsvc_prepare_thread(struct svc_serv *serv, struct svc_pool *pool, int node)\n{\n\tstruct svc_rqst\t*rqstp;\n\n\trqstp = svc_rqst_alloc(serv, pool, node);\n\tif (!rqstp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tserv->sv_nrthreads++;\n\tspin_lock_bh(&pool->sp_lock);\n\tpool->sp_nrthreads++;\n\tlist_add_rcu(&rqstp->rq_all, &pool->sp_all_threads);\n\tspin_unlock_bh(&pool->sp_lock);\n\treturn rqstp;\n}\nEXPORT_SYMBOL_GPL(svc_prepare_thread);\n\n/*\n * Choose a pool in which to create a new thread, for svc_set_num_threads\n */\nstatic inline struct svc_pool *\nchoose_pool(struct svc_serv *serv, struct svc_pool *pool, unsigned int *state)\n{\n\tif (pool != NULL)\n\t\treturn pool;\n\n\treturn &serv->sv_pools[(*state)++ % serv->sv_nrpools];\n}\n\n/*\n * Choose a thread to kill, for svc_set_num_threads\n */\nstatic inline struct task_struct *\nchoose_victim(struct svc_serv *serv, struct svc_pool *pool, unsigned int *state)\n{\n\tunsigned int i;\n\tstruct task_struct *task = NULL;\n\n\tif (pool != NULL) {\n\t\tspin_lock_bh(&pool->sp_lock);\n\t} else {\n\t\t/* choose a pool in round-robin fashion */\n\t\tfor (i = 0; i < serv->sv_nrpools; i++) {\n\t\t\tpool = &serv->sv_pools[--(*state) % serv->sv_nrpools];\n\t\t\tspin_lock_bh(&pool->sp_lock);\n\t\t\tif (!list_empty(&pool->sp_all_threads))\n\t\t\t\tgoto found_pool;\n\t\t\tspin_unlock_bh(&pool->sp_lock);\n\t\t}\n\t\treturn NULL;\n\t}\n\nfound_pool:\n\tif (!list_empty(&pool->sp_all_threads)) {\n\t\tstruct svc_rqst *rqstp;\n\n\t\t/*\n\t\t * Remove from the pool->sp_all_threads list\n\t\t * so we don't try to kill it again.\n\t\t */\n\t\trqstp = list_entry(pool->sp_all_threads.next, struct svc_rqst, rq_all);\n\t\tset_bit(RQ_VICTIM, &rqstp->rq_flags);\n\t\tlist_del_rcu(&rqstp->rq_all);\n\t\ttask = rqstp->rq_task;\n\t}\n\tspin_unlock_bh(&pool->sp_lock);\n\n\treturn task;\n}\n\n/*\n * Create or destroy enough new threads to make the number\n * of threads the given number.  If `pool' is non-NULL, applies\n * only to threads in that pool, otherwise round-robins between\n * all pools.  Caller must ensure that mutual exclusion between this and\n * server startup or shutdown.\n *\n * Destroying threads relies on the service threads filling in\n * rqstp->rq_task, which only the nfs ones do.  Assumes the serv\n * has been created using svc_create_pooled().\n *\n * Based on code that used to be in nfsd_svc() but tweaked\n * to be pool-aware.\n */\nint\nsvc_set_num_threads(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n{\n\tstruct svc_rqst\t*rqstp;\n\tstruct task_struct *task;\n\tstruct svc_pool *chosen_pool;\n\tint error = 0;\n\tunsigned int state = serv->sv_nrthreads-1;\n\tint node;\n\n\tif (pool == NULL) {\n\t\t/* The -1 assumes caller has done a svc_get() */\n\t\tnrservs -= (serv->sv_nrthreads-1);\n\t} else {\n\t\tspin_lock_bh(&pool->sp_lock);\n\t\tnrservs -= pool->sp_nrthreads;\n\t\tspin_unlock_bh(&pool->sp_lock);\n\t}\n\n\t/* create new threads */\n\twhile (nrservs > 0) {\n\t\tnrservs--;\n\t\tchosen_pool = choose_pool(serv, pool, &state);\n\n\t\tnode = svc_pool_map_get_node(chosen_pool->sp_id);\n\t\trqstp = svc_prepare_thread(serv, chosen_pool, node);\n\t\tif (IS_ERR(rqstp)) {\n\t\t\terror = PTR_ERR(rqstp);\n\t\t\tbreak;\n\t\t}\n\n\t\t__module_get(serv->sv_ops->svo_module);\n\t\ttask = kthread_create_on_node(serv->sv_ops->svo_function, rqstp,\n\t\t\t\t\t      node, \"%s\", serv->sv_name);\n\t\tif (IS_ERR(task)) {\n\t\t\terror = PTR_ERR(task);\n\t\t\tmodule_put(serv->sv_ops->svo_module);\n\t\t\tsvc_exit_thread(rqstp);\n\t\t\tbreak;\n\t\t}\n\n\t\trqstp->rq_task = task;\n\t\tif (serv->sv_nrpools > 1)\n\t\t\tsvc_pool_map_set_cpumask(task, chosen_pool->sp_id);\n\n\t\tsvc_sock_update_bufs(serv);\n\t\twake_up_process(task);\n\t}\n\t/* destroy old threads */\n\twhile (nrservs < 0 &&\n\t       (task = choose_victim(serv, pool, &state)) != NULL) {\n\t\tsend_sig(SIGINT, task, 1);\n\t\tnrservs++;\n\t}\n\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(svc_set_num_threads);\n\n/*\n * Called from a server thread as it's exiting. Caller must hold the \"service\n * mutex\" for the service.\n */\nvoid\nsvc_rqst_free(struct svc_rqst *rqstp)\n{\n\tsvc_release_buffer(rqstp);\n\tkfree(rqstp->rq_resp);\n\tkfree(rqstp->rq_argp);\n\tkfree(rqstp->rq_auth_data);\n\tkfree_rcu(rqstp, rq_rcu_head);\n}\nEXPORT_SYMBOL_GPL(svc_rqst_free);\n\nvoid\nsvc_exit_thread(struct svc_rqst *rqstp)\n{\n\tstruct svc_serv\t*serv = rqstp->rq_server;\n\tstruct svc_pool\t*pool = rqstp->rq_pool;\n\n\tspin_lock_bh(&pool->sp_lock);\n\tpool->sp_nrthreads--;\n\tif (!test_and_set_bit(RQ_VICTIM, &rqstp->rq_flags))\n\t\tlist_del_rcu(&rqstp->rq_all);\n\tspin_unlock_bh(&pool->sp_lock);\n\n\tsvc_rqst_free(rqstp);\n\n\t/* Release the server */\n\tif (serv)\n\t\tsvc_destroy(serv);\n}\nEXPORT_SYMBOL_GPL(svc_exit_thread);\n\n/*\n * Register an \"inet\" protocol family netid with the local\n * rpcbind daemon via an rpcbind v4 SET request.\n *\n * No netconfig infrastructure is available in the kernel, so\n * we map IP_ protocol numbers to netids by hand.\n *\n * Returns zero on success; a negative errno value is returned\n * if any error occurs.\n */\nstatic int __svc_rpcb_register4(struct net *net, const u32 program,\n\t\t\t\tconst u32 version,\n\t\t\t\tconst unsigned short protocol,\n\t\t\t\tconst unsigned short port)\n{\n\tconst struct sockaddr_in sin = {\n\t\t.sin_family\t\t= AF_INET,\n\t\t.sin_addr.s_addr\t= htonl(INADDR_ANY),\n\t\t.sin_port\t\t= htons(port),\n\t};\n\tconst char *netid;\n\tint error;\n\n\tswitch (protocol) {\n\tcase IPPROTO_UDP:\n\t\tnetid = RPCBIND_NETID_UDP;\n\t\tbreak;\n\tcase IPPROTO_TCP:\n\t\tnetid = RPCBIND_NETID_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\terror = rpcb_v4_register(net, program, version,\n\t\t\t\t\t(const struct sockaddr *)&sin, netid);\n\n\t/*\n\t * User space didn't support rpcbind v4, so retry this\n\t * registration request with the legacy rpcbind v2 protocol.\n\t */\n\tif (error == -EPROTONOSUPPORT)\n\t\terror = rpcb_register(net, program, version, protocol, port);\n\n\treturn error;\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\n/*\n * Register an \"inet6\" protocol family netid with the local\n * rpcbind daemon via an rpcbind v4 SET request.\n *\n * No netconfig infrastructure is available in the kernel, so\n * we map IP_ protocol numbers to netids by hand.\n *\n * Returns zero on success; a negative errno value is returned\n * if any error occurs.\n */\nstatic int __svc_rpcb_register6(struct net *net, const u32 program,\n\t\t\t\tconst u32 version,\n\t\t\t\tconst unsigned short protocol,\n\t\t\t\tconst unsigned short port)\n{\n\tconst struct sockaddr_in6 sin6 = {\n\t\t.sin6_family\t\t= AF_INET6,\n\t\t.sin6_addr\t\t= IN6ADDR_ANY_INIT,\n\t\t.sin6_port\t\t= htons(port),\n\t};\n\tconst char *netid;\n\tint error;\n\n\tswitch (protocol) {\n\tcase IPPROTO_UDP:\n\t\tnetid = RPCBIND_NETID_UDP6;\n\t\tbreak;\n\tcase IPPROTO_TCP:\n\t\tnetid = RPCBIND_NETID_TCP6;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\terror = rpcb_v4_register(net, program, version,\n\t\t\t\t\t(const struct sockaddr *)&sin6, netid);\n\n\t/*\n\t * User space didn't support rpcbind version 4, so we won't\n\t * use a PF_INET6 listener.\n\t */\n\tif (error == -EPROTONOSUPPORT)\n\t\terror = -EAFNOSUPPORT;\n\n\treturn error;\n}\n#endif\t/* IS_ENABLED(CONFIG_IPV6) */\n\n/*\n * Register a kernel RPC service via rpcbind version 4.\n *\n * Returns zero on success; a negative errno value is returned\n * if any error occurs.\n */\nstatic int __svc_register(struct net *net, const char *progname,\n\t\t\t  const u32 program, const u32 version,\n\t\t\t  const int family,\n\t\t\t  const unsigned short protocol,\n\t\t\t  const unsigned short port)\n{\n\tint error = -EAFNOSUPPORT;\n\n\tswitch (family) {\n\tcase PF_INET:\n\t\terror = __svc_rpcb_register4(net, program, version,\n\t\t\t\t\t\tprotocol, port);\n\t\tbreak;\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase PF_INET6:\n\t\terror = __svc_rpcb_register6(net, program, version,\n\t\t\t\t\t\tprotocol, port);\n#endif\n\t}\n\n\treturn error;\n}\n\n/**\n * svc_register - register an RPC service with the local portmapper\n * @serv: svc_serv struct for the service to register\n * @net: net namespace for the service to register\n * @family: protocol family of service's listener socket\n * @proto: transport protocol number to advertise\n * @port: port to advertise\n *\n * Service is registered for any address in the passed-in protocol family\n */\nint svc_register(const struct svc_serv *serv, struct net *net,\n\t\t const int family, const unsigned short proto,\n\t\t const unsigned short port)\n{\n\tstruct svc_program\t*progp;\n\tstruct svc_version\t*vers;\n\tunsigned int\t\ti;\n\tint\t\t\terror = 0;\n\n\tWARN_ON_ONCE(proto == 0 && port == 0);\n\tif (proto == 0 && port == 0)\n\t\treturn -EINVAL;\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next) {\n\t\tfor (i = 0; i < progp->pg_nvers; i++) {\n\t\t\tvers = progp->pg_vers[i];\n\t\t\tif (vers == NULL)\n\t\t\t\tcontinue;\n\n\t\t\tdprintk(\"svc: svc_register(%sv%d, %s, %u, %u)%s\\n\",\n\t\t\t\t\tprogp->pg_name,\n\t\t\t\t\ti,\n\t\t\t\t\tproto == IPPROTO_UDP?  \"udp\" : \"tcp\",\n\t\t\t\t\tport,\n\t\t\t\t\tfamily,\n\t\t\t\t\tvers->vs_hidden ?\n\t\t\t\t\t\" (but not telling portmap)\" : \"\");\n\n\t\t\tif (vers->vs_hidden)\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * Don't register a UDP port if we need congestion\n\t\t\t * control.\n\t\t\t */\n\t\t\tif (vers->vs_need_cong_ctrl && proto == IPPROTO_UDP)\n\t\t\t\tcontinue;\n\n\t\t\terror = __svc_register(net, progp->pg_name, progp->pg_prog,\n\t\t\t\t\t\ti, family, proto, port);\n\n\t\t\tif (vers->vs_rpcb_optnl) {\n\t\t\t\terror = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (error < 0) {\n\t\t\t\tprintk(KERN_WARNING \"svc: failed to register \"\n\t\t\t\t\t\"%sv%u RPC service (errno %d).\\n\",\n\t\t\t\t\tprogp->pg_name, i, -error);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn error;\n}\n\n/*\n * If user space is running rpcbind, it should take the v4 UNSET\n * and clear everything for this [program, version].  If user space\n * is running portmap, it will reject the v4 UNSET, but won't have\n * any \"inet6\" entries anyway.  So a PMAP_UNSET should be sufficient\n * in this case to clear all existing entries for [program, version].\n */\nstatic void __svc_unregister(struct net *net, const u32 program, const u32 version,\n\t\t\t     const char *progname)\n{\n\tint error;\n\n\terror = rpcb_v4_register(net, program, version, NULL, \"\");\n\n\t/*\n\t * User space didn't support rpcbind v4, so retry this\n\t * request with the legacy rpcbind v2 protocol.\n\t */\n\tif (error == -EPROTONOSUPPORT)\n\t\terror = rpcb_register(net, program, version, 0, 0);\n\n\tdprintk(\"svc: %s(%sv%u), error %d\\n\",\n\t\t\t__func__, progname, version, error);\n}\n\n/*\n * All netids, bind addresses and ports registered for [program, version]\n * are removed from the local rpcbind database (if the service is not\n * hidden) to make way for a new instance of the service.\n *\n * The result of unregistration is reported via dprintk for those who want\n * verification of the result, but is otherwise not important.\n */\nstatic void svc_unregister(const struct svc_serv *serv, struct net *net)\n{\n\tstruct svc_program *progp;\n\tunsigned long flags;\n\tunsigned int i;\n\n\tclear_thread_flag(TIF_SIGPENDING);\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next) {\n\t\tfor (i = 0; i < progp->pg_nvers; i++) {\n\t\t\tif (progp->pg_vers[i] == NULL)\n\t\t\t\tcontinue;\n\t\t\tif (progp->pg_vers[i]->vs_hidden)\n\t\t\t\tcontinue;\n\n\t\t\tdprintk(\"svc: attempting to unregister %sv%u\\n\",\n\t\t\t\tprogp->pg_name, i);\n\t\t\t__svc_unregister(net, progp->pg_prog, i, progp->pg_name);\n\t\t}\n\t}\n\n\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\trecalc_sigpending();\n\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n}\n\n/*\n * dprintk the given error with the address of the client that caused it.\n */\n#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)\nstatic __printf(2, 3)\nvoid svc_printk(struct svc_rqst *rqstp, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\tchar \tbuf[RPC_MAX_ADDRBUFLEN];\n\n\tva_start(args, fmt);\n\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\n\tdprintk(\"svc: %s: %pV\", svc_print_addr(rqstp, buf, sizeof(buf)), &vaf);\n\n\tva_end(args);\n}\n#else\nstatic __printf(2,3) void svc_printk(struct svc_rqst *rqstp, const char *fmt, ...) {}\n#endif\n\n/*\n * Common routine for processing the RPC request.\n */\nstatic int\nsvc_process_common(struct svc_rqst *rqstp, struct kvec *argv, struct kvec *resv)\n{\n\tstruct svc_program\t*progp;\n\tstruct svc_version\t*versp = NULL;\t/* compiler food */\n\tstruct svc_procedure\t*procp = NULL;\n\tstruct svc_serv\t\t*serv = rqstp->rq_server;\n\tkxdrproc_t\t\txdr;\n\t__be32\t\t\t*statp;\n\tu32\t\t\tprog, vers, proc;\n\t__be32\t\t\tauth_stat, rpc_stat;\n\tint\t\t\tauth_res;\n\t__be32\t\t\t*reply_statp;\n\n\trpc_stat = rpc_success;\n\n\tif (argv->iov_len < 6*4)\n\t\tgoto err_short_len;\n\n\t/* Will be turned off only in gss privacy case: */\n\tset_bit(RQ_SPLICE_OK, &rqstp->rq_flags);\n\t/* Will be turned off only when NFSv4 Sessions are used */\n\tset_bit(RQ_USEDEFERRAL, &rqstp->rq_flags);\n\tclear_bit(RQ_DROPME, &rqstp->rq_flags);\n\n\t/* Setup reply header */\n\trqstp->rq_xprt->xpt_ops->xpo_prep_reply_hdr(rqstp);\n\n\tsvc_putu32(resv, rqstp->rq_xid);\n\n\tvers = svc_getnl(argv);\n\n\t/* First words of reply: */\n\tsvc_putnl(resv, 1);\t\t/* REPLY */\n\n\tif (vers != 2)\t\t/* RPC version number */\n\t\tgoto err_bad_rpc;\n\n\t/* Save position in case we later decide to reject: */\n\treply_statp = resv->iov_base + resv->iov_len;\n\n\tsvc_putnl(resv, 0);\t\t/* ACCEPT */\n\n\trqstp->rq_prog = prog = svc_getnl(argv);\t/* program number */\n\trqstp->rq_vers = vers = svc_getnl(argv);\t/* version number */\n\trqstp->rq_proc = proc = svc_getnl(argv);\t/* procedure number */\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next)\n\t\tif (prog == progp->pg_prog)\n\t\t\tbreak;\n\n\t/*\n\t * Decode auth data, and add verifier to reply buffer.\n\t * We do this before anything else in order to get a decent\n\t * auth verifier.\n\t */\n\tauth_res = svc_authenticate(rqstp, &auth_stat);\n\t/* Also give the program a chance to reject this call: */\n\tif (auth_res == SVC_OK && progp) {\n\t\tauth_stat = rpc_autherr_badcred;\n\t\tauth_res = progp->pg_authenticate(rqstp);\n\t}\n\tswitch (auth_res) {\n\tcase SVC_OK:\n\t\tbreak;\n\tcase SVC_GARBAGE:\n\t\tgoto err_garbage;\n\tcase SVC_SYSERR:\n\t\trpc_stat = rpc_system_err;\n\t\tgoto err_bad;\n\tcase SVC_DENIED:\n\t\tgoto err_bad_auth;\n\tcase SVC_CLOSE:\n\t\tgoto close;\n\tcase SVC_DROP:\n\t\tgoto dropit;\n\tcase SVC_COMPLETE:\n\t\tgoto sendit;\n\t}\n\n\tif (progp == NULL)\n\t\tgoto err_bad_prog;\n\n\tif (vers >= progp->pg_nvers ||\n\t  !(versp = progp->pg_vers[vers]))\n\t\tgoto err_bad_vers;\n\n\t/*\n\t * Some protocol versions (namely NFSv4) require some form of\n\t * congestion control.  (See RFC 7530 section 3.1 paragraph 2)\n\t * In other words, UDP is not allowed. We mark those when setting\n\t * up the svc_xprt, and verify that here.\n\t *\n\t * The spec is not very clear about what error should be returned\n\t * when someone tries to access a server that is listening on UDP\n\t * for lower versions. RPC_PROG_MISMATCH seems to be the closest\n\t * fit.\n\t */\n\tif (versp->vs_need_cong_ctrl &&\n\t    !test_bit(XPT_CONG_CTRL, &rqstp->rq_xprt->xpt_flags))\n\t\tgoto err_bad_vers;\n\n\tprocp = versp->vs_proc + proc;\n\tif (proc >= versp->vs_nproc || !procp->pc_func)\n\t\tgoto err_bad_proc;\n\trqstp->rq_procinfo = procp;\n\n\t/* Syntactic check complete */\n\tserv->sv_stats->rpccnt++;\n\n\t/* Build the reply header. */\n\tstatp = resv->iov_base +resv->iov_len;\n\tsvc_putnl(resv, RPC_SUCCESS);\n\n\t/* Bump per-procedure stats counter */\n\tprocp->pc_count++;\n\n\t/* Initialize storage for argp and resp */\n\tmemset(rqstp->rq_argp, 0, procp->pc_argsize);\n\tmemset(rqstp->rq_resp, 0, procp->pc_ressize);\n\n\t/* un-reserve some of the out-queue now that we have a\n\t * better idea of reply size\n\t */\n\tif (procp->pc_xdrressize)\n\t\tsvc_reserve_auth(rqstp, procp->pc_xdrressize<<2);\n\n\t/* Call the function that processes the request. */\n\tif (!versp->vs_dispatch) {\n\t\t/* Decode arguments */\n\t\txdr = procp->pc_decode;\n\t\tif (xdr && !xdr(rqstp, argv->iov_base, rqstp->rq_argp))\n\t\t\tgoto err_garbage;\n\n\t\t*statp = procp->pc_func(rqstp, rqstp->rq_argp, rqstp->rq_resp);\n\n\t\t/* Encode reply */\n\t\tif (*statp == rpc_drop_reply ||\n\t\t    test_bit(RQ_DROPME, &rqstp->rq_flags)) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp, NULL, rqstp->rq_resp);\n\t\t\tgoto dropit;\n\t\t}\n\t\tif (*statp == rpc_autherr_badcred) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp, NULL, rqstp->rq_resp);\n\t\t\tgoto err_bad_auth;\n\t\t}\n\t\tif (*statp == rpc_success &&\n\t\t    (xdr = procp->pc_encode) &&\n\t\t    !xdr(rqstp, resv->iov_base+resv->iov_len, rqstp->rq_resp)) {\n\t\t\tdprintk(\"svc: failed to encode reply\\n\");\n\t\t\t/* serv->sv_stats->rpcsystemerr++; */\n\t\t\t*statp = rpc_system_err;\n\t\t}\n\t} else {\n\t\tdprintk(\"svc: calling dispatcher\\n\");\n\t\tif (!versp->vs_dispatch(rqstp, statp)) {\n\t\t\t/* Release reply info */\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp, NULL, rqstp->rq_resp);\n\t\t\tgoto dropit;\n\t\t}\n\t}\n\n\t/* Check RPC status result */\n\tif (*statp != rpc_success)\n\t\tresv->iov_len = ((void*)statp)  - resv->iov_base + 4;\n\n\t/* Release reply info */\n\tif (procp->pc_release)\n\t\tprocp->pc_release(rqstp, NULL, rqstp->rq_resp);\n\n\tif (procp->pc_encode == NULL)\n\t\tgoto dropit;\n\n sendit:\n\tif (svc_authorise(rqstp))\n\t\tgoto close;\n\treturn 1;\t\t/* Caller can now send it */\n\n dropit:\n\tsvc_authorise(rqstp);\t/* doesn't hurt to call this twice */\n\tdprintk(\"svc: svc_process dropit\\n\");\n\treturn 0;\n\n close:\n\tif (test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))\n\t\tsvc_close_xprt(rqstp->rq_xprt);\n\tdprintk(\"svc: svc_process close\\n\");\n\treturn 0;\n\nerr_short_len:\n\tsvc_printk(rqstp, \"short len %zd, dropping request\\n\",\n\t\t\targv->iov_len);\n\tgoto close;\n\nerr_bad_rpc:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 0);\t/* RPC_MISMATCH */\n\tsvc_putnl(resv, 2);\t/* Only RPCv2 supported */\n\tsvc_putnl(resv, 2);\n\tgoto sendit;\n\nerr_bad_auth:\n\tdprintk(\"svc: authentication failed (%d)\\n\", ntohl(auth_stat));\n\tserv->sv_stats->rpcbadauth++;\n\t/* Restore write pointer to location of accept status: */\n\txdr_ressize_check(rqstp, reply_statp);\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 1);\t/* AUTH_ERROR */\n\tsvc_putnl(resv, ntohl(auth_stat));\t/* status */\n\tgoto sendit;\n\nerr_bad_prog:\n\tdprintk(\"svc: unknown program %d\\n\", prog);\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_UNAVAIL);\n\tgoto sendit;\n\nerr_bad_vers:\n\tsvc_printk(rqstp, \"unknown version (%d for prog %d, %s)\\n\",\n\t\t       vers, prog, progp->pg_name);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_MISMATCH);\n\tsvc_putnl(resv, progp->pg_lovers);\n\tsvc_putnl(resv, progp->pg_hivers);\n\tgoto sendit;\n\nerr_bad_proc:\n\tsvc_printk(rqstp, \"unknown procedure (%d)\\n\", proc);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROC_UNAVAIL);\n\tgoto sendit;\n\nerr_garbage:\n\tsvc_printk(rqstp, \"failed to decode args\\n\");\n\n\trpc_stat = rpc_garbage_args;\nerr_bad:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, ntohl(rpc_stat));\n\tgoto sendit;\n}\n\n/*\n * Process the RPC request.\n */\nint\nsvc_process(struct svc_rqst *rqstp)\n{\n\tstruct kvec\t\t*argv = &rqstp->rq_arg.head[0];\n\tstruct kvec\t\t*resv = &rqstp->rq_res.head[0];\n\tstruct svc_serv\t\t*serv = rqstp->rq_server;\n\tu32\t\t\tdir;\n\n\t/*\n\t * Setup response xdr_buf.\n\t * Initially it has just one page\n\t */\n\trqstp->rq_next_page = &rqstp->rq_respages[1];\n\tresv->iov_base = page_address(rqstp->rq_respages[0]);\n\tresv->iov_len = 0;\n\trqstp->rq_res.pages = rqstp->rq_respages + 1;\n\trqstp->rq_res.len = 0;\n\trqstp->rq_res.page_base = 0;\n\trqstp->rq_res.page_len = 0;\n\trqstp->rq_res.buflen = PAGE_SIZE;\n\trqstp->rq_res.tail[0].iov_base = NULL;\n\trqstp->rq_res.tail[0].iov_len = 0;\n\n\tdir  = svc_getnl(argv);\n\tif (dir != 0) {\n\t\t/* direction != CALL */\n\t\tsvc_printk(rqstp, \"bad direction %d, dropping request\\n\", dir);\n\t\tserv->sv_stats->rpcbadfmt++;\n\t\tgoto out_drop;\n\t}\n\n\t/* Returns 1 for send, 0 for drop */\n\tif (likely(svc_process_common(rqstp, argv, resv))) {\n\t\tint ret = svc_send(rqstp);\n\n\t\ttrace_svc_process(rqstp, ret);\n\t\treturn ret;\n\t}\nout_drop:\n\ttrace_svc_process(rqstp, 0);\n\tsvc_drop(rqstp);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(svc_process);\n\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\n/*\n * Process a backchannel RPC request that arrived over an existing\n * outbound connection\n */\nint\nbc_svc_process(struct svc_serv *serv, struct rpc_rqst *req,\n\t       struct svc_rqst *rqstp)\n{\n\tstruct kvec\t*argv = &rqstp->rq_arg.head[0];\n\tstruct kvec\t*resv = &rqstp->rq_res.head[0];\n\tstruct rpc_task *task;\n\tint proc_error;\n\tint error;\n\n\tdprintk(\"svc: %s(%p)\\n\", __func__, req);\n\n\t/* Build the svc_rqst used by the common processing routine */\n\trqstp->rq_xprt = serv->sv_bc_xprt;\n\trqstp->rq_xid = req->rq_xid;\n\trqstp->rq_prot = req->rq_xprt->prot;\n\trqstp->rq_server = serv;\n\n\trqstp->rq_addrlen = sizeof(req->rq_xprt->addr);\n\tmemcpy(&rqstp->rq_addr, &req->rq_xprt->addr, rqstp->rq_addrlen);\n\tmemcpy(&rqstp->rq_arg, &req->rq_rcv_buf, sizeof(rqstp->rq_arg));\n\tmemcpy(&rqstp->rq_res, &req->rq_snd_buf, sizeof(rqstp->rq_res));\n\n\t/* Adjust the argument buffer length */\n\trqstp->rq_arg.len = req->rq_private_buf.len;\n\tif (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len) {\n\t\trqstp->rq_arg.head[0].iov_len = rqstp->rq_arg.len;\n\t\trqstp->rq_arg.page_len = 0;\n\t} else if (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len)\n\t\trqstp->rq_arg.page_len = rqstp->rq_arg.len -\n\t\t\trqstp->rq_arg.head[0].iov_len;\n\telse\n\t\trqstp->rq_arg.len = rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len;\n\n\t/* reset result send buffer \"put\" position */\n\tresv->iov_len = 0;\n\n\t/*\n\t * Skip the next two words because they've already been\n\t * processed in the transport\n\t */\n\tsvc_getu32(argv);\t/* XID */\n\tsvc_getnl(argv);\t/* CALLDIR */\n\n\t/* Parse and execute the bc call */\n\tproc_error = svc_process_common(rqstp, argv, resv);\n\n\tatomic_inc(&req->rq_xprt->bc_free_slots);\n\tif (!proc_error) {\n\t\t/* Processing error: drop the request */\n\t\txprt_free_bc_request(req);\n\t\treturn 0;\n\t}\n\n\t/* Finally, send the reply synchronously */\n\tmemcpy(&req->rq_snd_buf, &rqstp->rq_res, sizeof(req->rq_snd_buf));\n\ttask = rpc_run_bc_task(req);\n\tif (IS_ERR(task)) {\n\t\terror = PTR_ERR(task);\n\t\tgoto out;\n\t}\n\n\tWARN_ON_ONCE(atomic_read(&task->tk_count) != 1);\n\terror = task->tk_status;\n\trpc_put_task(task);\n\nout:\n\tdprintk(\"svc: %s(), error=%d\\n\", __func__, error);\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(bc_svc_process);\n#endif /* CONFIG_SUNRPC_BACKCHANNEL */\n\n/*\n * Return (transport-specific) limit on the rpc payload.\n */\nu32 svc_max_payload(const struct svc_rqst *rqstp)\n{\n\tu32 max = rqstp->rq_xprt->xpt_class->xcl_max_payload;\n\n\tif (rqstp->rq_server->sv_max_payload < max)\n\t\tmax = rqstp->rq_server->sv_max_payload;\n\treturn max;\n}\nEXPORT_SYMBOL_GPL(svc_max_payload);\n", "obj-$(CONFIG_SUNRPC_XPRT_RDMA) += rpcrdma.o\n\nrpcrdma-y := transport.o rpc_rdma.o verbs.o \\\n\tfmr_ops.o frwr_ops.o \\\n\tsvc_rdma.o svc_rdma_backchannel.o svc_rdma_transport.o \\\n\tsvc_rdma_marshal.o svc_rdma_sendto.o svc_rdma_recvfrom.o \\\n\tmodule.o\nrpcrdma-$(CONFIG_SUNRPC_BACKCHANNEL) += backchannel.o\n", "/*\n * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n#include <linux/slab.h>\n#include <linux/fs.h>\n#include <linux/sysctl.h>\n#include <linux/workqueue.h>\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/sched.h>\n#include <linux/sunrpc/svc_rdma.h>\n#include \"xprt_rdma.h\"\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\n/* RPC/RDMA parameters */\nunsigned int svcrdma_ord = RPCRDMA_ORD;\nstatic unsigned int min_ord = 1;\nstatic unsigned int max_ord = 4096;\nunsigned int svcrdma_max_requests = RPCRDMA_MAX_REQUESTS;\nunsigned int svcrdma_max_bc_requests = RPCRDMA_MAX_BC_REQUESTS;\nstatic unsigned int min_max_requests = 4;\nstatic unsigned int max_max_requests = 16384;\nunsigned int svcrdma_max_req_size = RPCRDMA_MAX_REQ_SIZE;\nstatic unsigned int min_max_inline = 4096;\nstatic unsigned int max_max_inline = 65536;\n\natomic_t rdma_stat_recv;\natomic_t rdma_stat_read;\natomic_t rdma_stat_write;\natomic_t rdma_stat_sq_starve;\natomic_t rdma_stat_rq_starve;\natomic_t rdma_stat_rq_poll;\natomic_t rdma_stat_rq_prod;\natomic_t rdma_stat_sq_poll;\natomic_t rdma_stat_sq_prod;\n\nstruct workqueue_struct *svc_rdma_wq;\n\n/*\n * This function implements reading and resetting an atomic_t stat\n * variable through read/write to a proc file. Any write to the file\n * resets the associated statistic to zero. Any read returns it's\n * current value.\n */\nstatic int read_reset_stat(struct ctl_table *table, int write,\n\t\t\t   void __user *buffer, size_t *lenp,\n\t\t\t   loff_t *ppos)\n{\n\tatomic_t *stat = (atomic_t *)table->data;\n\n\tif (!stat)\n\t\treturn -EINVAL;\n\n\tif (write)\n\t\tatomic_set(stat, 0);\n\telse {\n\t\tchar str_buf[32];\n\t\tchar *data;\n\t\tint len = snprintf(str_buf, 32, \"%d\\n\", atomic_read(stat));\n\t\tif (len >= 32)\n\t\t\treturn -EFAULT;\n\t\tlen = strlen(str_buf);\n\t\tif (*ppos > len) {\n\t\t\t*lenp = 0;\n\t\t\treturn 0;\n\t\t}\n\t\tdata = &str_buf[*ppos];\n\t\tlen -= *ppos;\n\t\tif (len > *lenp)\n\t\t\tlen = *lenp;\n\t\tif (len && copy_to_user(buffer, str_buf, len))\n\t\t\treturn -EFAULT;\n\t\t*lenp = len;\n\t\t*ppos += len;\n\t}\n\treturn 0;\n}\n\nstatic struct ctl_table_header *svcrdma_table_header;\nstatic struct ctl_table svcrdma_parm_table[] = {\n\t{\n\t\t.procname\t= \"max_requests\",\n\t\t.data\t\t= &svcrdma_max_requests,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_max_requests,\n\t\t.extra2\t\t= &max_max_requests\n\t},\n\t{\n\t\t.procname\t= \"max_req_size\",\n\t\t.data\t\t= &svcrdma_max_req_size,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_max_inline,\n\t\t.extra2\t\t= &max_max_inline\n\t},\n\t{\n\t\t.procname\t= \"max_outbound_read_requests\",\n\t\t.data\t\t= &svcrdma_ord,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_ord,\n\t\t.extra2\t\t= &max_ord,\n\t},\n\n\t{\n\t\t.procname\t= \"rdma_stat_read\",\n\t\t.data\t\t= &rdma_stat_read,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_recv\",\n\t\t.data\t\t= &rdma_stat_recv,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_write\",\n\t\t.data\t\t= &rdma_stat_write,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_sq_starve\",\n\t\t.data\t\t= &rdma_stat_sq_starve,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_rq_starve\",\n\t\t.data\t\t= &rdma_stat_rq_starve,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_rq_poll\",\n\t\t.data\t\t= &rdma_stat_rq_poll,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_rq_prod\",\n\t\t.data\t\t= &rdma_stat_rq_prod,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_sq_poll\",\n\t\t.data\t\t= &rdma_stat_sq_poll,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_sq_prod\",\n\t\t.data\t\t= &rdma_stat_sq_prod,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{ },\n};\n\nstatic struct ctl_table svcrdma_table[] = {\n\t{\n\t\t.procname\t= \"svc_rdma\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= svcrdma_parm_table\n\t},\n\t{ },\n};\n\nstatic struct ctl_table svcrdma_root_table[] = {\n\t{\n\t\t.procname\t= \"sunrpc\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= svcrdma_table\n\t},\n\t{ },\n};\n\nvoid svc_rdma_cleanup(void)\n{\n\tdprintk(\"SVCRDMA Module Removed, deregister RPC RDMA transport\\n\");\n\tdestroy_workqueue(svc_rdma_wq);\n\tif (svcrdma_table_header) {\n\t\tunregister_sysctl_table(svcrdma_table_header);\n\t\tsvcrdma_table_header = NULL;\n\t}\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\n\tsvc_unreg_xprt_class(&svc_rdma_bc_class);\n#endif\n\tsvc_unreg_xprt_class(&svc_rdma_class);\n}\n\nint svc_rdma_init(void)\n{\n\tdprintk(\"SVCRDMA Module Init, register RPC RDMA transport\\n\");\n\tdprintk(\"\\tsvcrdma_ord      : %d\\n\", svcrdma_ord);\n\tdprintk(\"\\tmax_requests     : %u\\n\", svcrdma_max_requests);\n\tdprintk(\"\\tsq_depth         : %u\\n\",\n\t\tsvcrdma_max_requests * RPCRDMA_SQ_DEPTH_MULT);\n\tdprintk(\"\\tmax_bc_requests  : %u\\n\", svcrdma_max_bc_requests);\n\tdprintk(\"\\tmax_inline       : %d\\n\", svcrdma_max_req_size);\n\n\tsvc_rdma_wq = alloc_workqueue(\"svc_rdma\", 0, 0);\n\tif (!svc_rdma_wq)\n\t\treturn -ENOMEM;\n\n\tif (!svcrdma_table_header)\n\t\tsvcrdma_table_header =\n\t\t\tregister_sysctl_table(svcrdma_root_table);\n\n\t/* Register RDMA with the SVC transport switch */\n\tsvc_reg_xprt_class(&svc_rdma_class);\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\n\tsvc_reg_xprt_class(&svc_rdma_bc_class);\n#endif\n\treturn 0;\n}\n", "/*\n * Copyright (c) 2015 Oracle.  All rights reserved.\n *\n * Support for backward direction RPCs on RPC/RDMA (server-side).\n */\n\n#include <linux/module.h>\n#include <linux/sunrpc/svc_rdma.h>\n#include \"xprt_rdma.h\"\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\n#undef SVCRDMA_BACKCHANNEL_DEBUG\n\nint svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,\n\t\t\t     struct xdr_buf *rcvbuf)\n{\n\tstruct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);\n\tstruct kvec *dst, *src = &rcvbuf->head[0];\n\tstruct rpc_rqst *req;\n\tunsigned long cwnd;\n\tu32 credits;\n\tsize_t len;\n\t__be32 xid;\n\t__be32 *p;\n\tint ret;\n\n\tp = (__be32 *)src->iov_base;\n\tlen = src->iov_len;\n\txid = rmsgp->rm_xid;\n\n#ifdef SVCRDMA_BACKCHANNEL_DEBUG\n\tpr_info(\"%s: xid=%08x, length=%zu\\n\",\n\t\t__func__, be32_to_cpu(xid), len);\n\tpr_info(\"%s: RPC/RDMA: %*ph\\n\",\n\t\t__func__, (int)RPCRDMA_HDRLEN_MIN, rmsgp);\n\tpr_info(\"%s:      RPC: %*ph\\n\",\n\t\t__func__, (int)len, p);\n#endif\n\n\tret = -EAGAIN;\n\tif (src->iov_len < 24)\n\t\tgoto out_shortreply;\n\n\tspin_lock_bh(&xprt->transport_lock);\n\treq = xprt_lookup_rqst(xprt, xid);\n\tif (!req)\n\t\tgoto out_notfound;\n\n\tdst = &req->rq_private_buf.head[0];\n\tmemcpy(&req->rq_private_buf, &req->rq_rcv_buf, sizeof(struct xdr_buf));\n\tif (dst->iov_len < len)\n\t\tgoto out_unlock;\n\tmemcpy(dst->iov_base, p, len);\n\n\tcredits = be32_to_cpu(rmsgp->rm_credit);\n\tif (credits == 0)\n\t\tcredits = 1;\t/* don't deadlock */\n\telse if (credits > r_xprt->rx_buf.rb_bc_max_requests)\n\t\tcredits = r_xprt->rx_buf.rb_bc_max_requests;\n\n\tcwnd = xprt->cwnd;\n\txprt->cwnd = credits << RPC_CWNDSHIFT;\n\tif (xprt->cwnd > cwnd)\n\t\txprt_release_rqst_cong(req->rq_task);\n\n\tret = 0;\n\txprt_complete_rqst(req->rq_task, rcvbuf->len);\n\trcvbuf->len = 0;\n\nout_unlock:\n\tspin_unlock_bh(&xprt->transport_lock);\nout:\n\treturn ret;\n\nout_shortreply:\n\tdprintk(\"svcrdma: short bc reply: xprt=%p, len=%zu\\n\",\n\t\txprt, src->iov_len);\n\tgoto out;\n\nout_notfound:\n\tdprintk(\"svcrdma: unrecognized bc reply: xprt=%p, xid=%08x\\n\",\n\t\txprt, be32_to_cpu(xid));\n\n\tgoto out_unlock;\n}\n\n/* Send a backwards direction RPC call.\n *\n * Caller holds the connection's mutex and has already marshaled\n * the RPC/RDMA request.\n *\n * This is similar to svc_rdma_reply, but takes an rpc_rqst\n * instead, does not support chunks, and avoids blocking memory\n * allocation.\n *\n * XXX: There is still an opportunity to block in svc_rdma_send()\n * if there are no SQ entries to post the Send. This may occur if\n * the adapter has a small maximum SQ depth.\n */\nstatic int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,\n\t\t\t      struct rpc_rqst *rqst)\n{\n\tstruct xdr_buf *sndbuf = &rqst->rq_snd_buf;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\tstruct svc_rdma_req_map *vec;\n\tstruct ib_send_wr send_wr;\n\tint ret;\n\n\tvec = svc_rdma_get_req_map(rdma);\n\tret = svc_rdma_map_xdr(rdma, sndbuf, vec, false);\n\tif (ret)\n\t\tgoto out_err;\n\n\tret = svc_rdma_repost_recv(rdma, GFP_NOIO);\n\tif (ret)\n\t\tgoto out_err;\n\n\tctxt = svc_rdma_get_context(rdma);\n\tctxt->pages[0] = virt_to_page(rqst->rq_buffer);\n\tctxt->count = 1;\n\n\tctxt->direction = DMA_TO_DEVICE;\n\tctxt->sge[0].lkey = rdma->sc_pd->local_dma_lkey;\n\tctxt->sge[0].length = sndbuf->len;\n\tctxt->sge[0].addr =\n\t    ib_dma_map_page(rdma->sc_cm_id->device, ctxt->pages[0], 0,\n\t\t\t    sndbuf->len, DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(rdma->sc_cm_id->device, ctxt->sge[0].addr)) {\n\t\tret = -EIO;\n\t\tgoto out_unmap;\n\t}\n\tsvc_rdma_count_mappings(rdma, ctxt);\n\n\tmemset(&send_wr, 0, sizeof(send_wr));\n\tctxt->cqe.done = svc_rdma_wc_send;\n\tsend_wr.wr_cqe = &ctxt->cqe;\n\tsend_wr.sg_list = ctxt->sge;\n\tsend_wr.num_sge = 1;\n\tsend_wr.opcode = IB_WR_SEND;\n\tsend_wr.send_flags = IB_SEND_SIGNALED;\n\n\tret = svc_rdma_send(rdma, &send_wr);\n\tif (ret) {\n\t\tret = -EIO;\n\t\tgoto out_unmap;\n\t}\n\nout_err:\n\tsvc_rdma_put_req_map(rdma, vec);\n\tdprintk(\"svcrdma: %s returns %d\\n\", __func__, ret);\n\treturn ret;\n\nout_unmap:\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 1);\n\tgoto out_err;\n}\n\n/* Server-side transport endpoint wants a whole page for its send\n * buffer. The client RPC code constructs the RPC header in this\n * buffer before it invokes ->send_request.\n */\nstatic int\nxprt_rdma_bc_allocate(struct rpc_task *task)\n{\n\tstruct rpc_rqst *rqst = task->tk_rqstp;\n\tsize_t size = rqst->rq_callsize;\n\tstruct page *page;\n\n\tif (size > PAGE_SIZE) {\n\t\tWARN_ONCE(1, \"svcrdma: large bc buffer request (size %zu)\\n\",\n\t\t\t  size);\n\t\treturn -EINVAL;\n\t}\n\n\t/* svc_rdma_sendto releases this page */\n\tpage = alloc_page(RPCRDMA_DEF_GFP);\n\tif (!page)\n\t\treturn -ENOMEM;\n\trqst->rq_buffer = page_address(page);\n\n\trqst->rq_rbuffer = kmalloc(rqst->rq_rcvsize, RPCRDMA_DEF_GFP);\n\tif (!rqst->rq_rbuffer) {\n\t\tput_page(page);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic void\nxprt_rdma_bc_free(struct rpc_task *task)\n{\n\tstruct rpc_rqst *rqst = task->tk_rqstp;\n\n\tkfree(rqst->rq_rbuffer);\n}\n\nstatic int\nrpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)\n{\n\tstruct rpc_xprt *xprt = rqst->rq_xprt;\n\tstruct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);\n\t__be32 *p;\n\tint rc;\n\n\t/* Space in the send buffer for an RPC/RDMA header is reserved\n\t * via xprt->tsh_size.\n\t */\n\tp = rqst->rq_buffer;\n\t*p++ = rqst->rq_xid;\n\t*p++ = rpcrdma_version;\n\t*p++ = cpu_to_be32(r_xprt->rx_buf.rb_bc_max_requests);\n\t*p++ = rdma_msg;\n\t*p++ = xdr_zero;\n\t*p++ = xdr_zero;\n\t*p   = xdr_zero;\n\n#ifdef SVCRDMA_BACKCHANNEL_DEBUG\n\tpr_info(\"%s: %*ph\\n\", __func__, 64, rqst->rq_buffer);\n#endif\n\n\trc = svc_rdma_bc_sendto(rdma, rqst);\n\tif (rc)\n\t\tgoto drop_connection;\n\treturn rc;\n\ndrop_connection:\n\tdprintk(\"svcrdma: failed to send bc call\\n\");\n\txprt_disconnect_done(xprt);\n\treturn -ENOTCONN;\n}\n\n/* Send an RPC call on the passive end of a transport\n * connection.\n */\nstatic int\nxprt_rdma_bc_send_request(struct rpc_task *task)\n{\n\tstruct rpc_rqst *rqst = task->tk_rqstp;\n\tstruct svc_xprt *sxprt = rqst->rq_xprt->bc_xprt;\n\tstruct svcxprt_rdma *rdma;\n\tint ret;\n\n\tdprintk(\"svcrdma: sending bc call with xid: %08x\\n\",\n\t\tbe32_to_cpu(rqst->rq_xid));\n\n\tif (!mutex_trylock(&sxprt->xpt_mutex)) {\n\t\trpc_sleep_on(&sxprt->xpt_bc_pending, task, NULL);\n\t\tif (!mutex_trylock(&sxprt->xpt_mutex))\n\t\t\treturn -EAGAIN;\n\t\trpc_wake_up_queued_task(&sxprt->xpt_bc_pending, task);\n\t}\n\n\tret = -ENOTCONN;\n\trdma = container_of(sxprt, struct svcxprt_rdma, sc_xprt);\n\tif (!test_bit(XPT_DEAD, &sxprt->xpt_flags))\n\t\tret = rpcrdma_bc_send_request(rdma, rqst);\n\n\tmutex_unlock(&sxprt->xpt_mutex);\n\n\tif (ret < 0)\n\t\treturn ret;\n\treturn 0;\n}\n\nstatic void\nxprt_rdma_bc_close(struct rpc_xprt *xprt)\n{\n\tdprintk(\"svcrdma: %s: xprt %p\\n\", __func__, xprt);\n}\n\nstatic void\nxprt_rdma_bc_put(struct rpc_xprt *xprt)\n{\n\tdprintk(\"svcrdma: %s: xprt %p\\n\", __func__, xprt);\n\n\txprt_free(xprt);\n\tmodule_put(THIS_MODULE);\n}\n\nstatic struct rpc_xprt_ops xprt_rdma_bc_procs = {\n\t.reserve_xprt\t\t= xprt_reserve_xprt_cong,\n\t.release_xprt\t\t= xprt_release_xprt_cong,\n\t.alloc_slot\t\t= xprt_alloc_slot,\n\t.release_request\t= xprt_release_rqst_cong,\n\t.buf_alloc\t\t= xprt_rdma_bc_allocate,\n\t.buf_free\t\t= xprt_rdma_bc_free,\n\t.send_request\t\t= xprt_rdma_bc_send_request,\n\t.set_retrans_timeout\t= xprt_set_retrans_timeout_def,\n\t.close\t\t\t= xprt_rdma_bc_close,\n\t.destroy\t\t= xprt_rdma_bc_put,\n\t.print_stats\t\t= xprt_rdma_print_stats\n};\n\nstatic const struct rpc_timeout xprt_rdma_bc_timeout = {\n\t.to_initval = 60 * HZ,\n\t.to_maxval = 60 * HZ,\n};\n\n/* It shouldn't matter if the number of backchannel session slots\n * doesn't match the number of RPC/RDMA credits. That just means\n * one or the other will have extra slots that aren't used.\n */\nstatic struct rpc_xprt *\nxprt_setup_rdma_bc(struct xprt_create *args)\n{\n\tstruct rpc_xprt *xprt;\n\tstruct rpcrdma_xprt *new_xprt;\n\n\tif (args->addrlen > sizeof(xprt->addr)) {\n\t\tdprintk(\"RPC:       %s: address too large\\n\", __func__);\n\t\treturn ERR_PTR(-EBADF);\n\t}\n\n\txprt = xprt_alloc(args->net, sizeof(*new_xprt),\n\t\t\t  RPCRDMA_MAX_BC_REQUESTS,\n\t\t\t  RPCRDMA_MAX_BC_REQUESTS);\n\tif (!xprt) {\n\t\tdprintk(\"RPC:       %s: couldn't allocate rpc_xprt\\n\",\n\t\t\t__func__);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\txprt->timeout = &xprt_rdma_bc_timeout;\n\txprt_set_bound(xprt);\n\txprt_set_connected(xprt);\n\txprt->bind_timeout = RPCRDMA_BIND_TO;\n\txprt->reestablish_timeout = RPCRDMA_INIT_REEST_TO;\n\txprt->idle_timeout = RPCRDMA_IDLE_DISC_TO;\n\n\txprt->prot = XPRT_TRANSPORT_BC_RDMA;\n\txprt->tsh_size = RPCRDMA_HDRLEN_MIN / sizeof(__be32);\n\txprt->ops = &xprt_rdma_bc_procs;\n\n\tmemcpy(&xprt->addr, args->dstaddr, args->addrlen);\n\txprt->addrlen = args->addrlen;\n\txprt_rdma_format_addresses(xprt, (struct sockaddr *)&xprt->addr);\n\txprt->resvport = 0;\n\n\txprt->max_payload = xprt_rdma_max_inline_read;\n\n\tnew_xprt = rpcx_to_rdmax(xprt);\n\tnew_xprt->rx_buf.rb_bc_max_requests = xprt->max_reqs;\n\n\txprt_get(xprt);\n\targs->bc_xprt->xpt_bc_xprt = xprt;\n\txprt->bc_xprt = args->bc_xprt;\n\n\tif (!try_module_get(THIS_MODULE))\n\t\tgoto out_fail;\n\n\t/* Final put for backchannel xprt is in __svc_rdma_free */\n\txprt_get(xprt);\n\treturn xprt;\n\nout_fail:\n\txprt_rdma_free_addresses(xprt);\n\targs->bc_xprt->xpt_bc_xprt = NULL;\n\targs->bc_xprt->xpt_bc_xps = NULL;\n\txprt_put(xprt);\n\txprt_free(xprt);\n\treturn ERR_PTR(-EINVAL);\n}\n\nstruct xprt_class xprt_rdma_bc = {\n\t.list\t\t\t= LIST_HEAD_INIT(xprt_rdma_bc.list),\n\t.name\t\t\t= \"rdma backchannel\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.ident\t\t\t= XPRT_TRANSPORT_BC_RDMA,\n\t.setup\t\t\t= xprt_setup_rdma_bc,\n};\n", "/*\n * Copyright (c) 2016 Oracle. All rights reserved.\n * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n#include <linux/sunrpc/xdr.h>\n#include <linux/sunrpc/debug.h>\n#include <asm/unaligned.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <linux/sunrpc/svc_rdma.h>\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\nstatic __be32 *xdr_check_read_list(__be32 *p, __be32 *end)\n{\n\t__be32 *next;\n\n\twhile (*p++ != xdr_zero) {\n\t\tnext = p + rpcrdma_readchunk_maxsz - 1;\n\t\tif (next > end)\n\t\t\treturn NULL;\n\t\tp = next;\n\t}\n\treturn p;\n}\n\nstatic __be32 *xdr_check_write_list(__be32 *p, __be32 *end)\n{\n\t__be32 *next;\n\n\twhile (*p++ != xdr_zero) {\n\t\tnext = p + 1 + be32_to_cpup(p) * rpcrdma_segment_maxsz;\n\t\tif (next > end)\n\t\t\treturn NULL;\n\t\tp = next;\n\t}\n\treturn p;\n}\n\nstatic __be32 *xdr_check_reply_chunk(__be32 *p, __be32 *end)\n{\n\t__be32 *next;\n\n\tif (*p++ != xdr_zero) {\n\t\tnext = p + 1 + be32_to_cpup(p) * rpcrdma_segment_maxsz;\n\t\tif (next > end)\n\t\t\treturn NULL;\n\t\tp = next;\n\t}\n\treturn p;\n}\n\n/**\n * svc_rdma_xdr_decode_req - Parse incoming RPC-over-RDMA header\n * @rq_arg: Receive buffer\n *\n * On entry, xdr->head[0].iov_base points to first byte in the\n * RPC-over-RDMA header.\n *\n * On successful exit, head[0] points to first byte past the\n * RPC-over-RDMA header. For RDMA_MSG, this is the RPC message.\n * The length of the RPC-over-RDMA header is returned.\n */\nint svc_rdma_xdr_decode_req(struct xdr_buf *rq_arg)\n{\n\t__be32 *p, *end, *rdma_argp;\n\tunsigned int hdr_len;\n\n\t/* Verify that there's enough bytes for header + something */\n\tif (rq_arg->len <= RPCRDMA_HDRLEN_ERR)\n\t\tgoto out_short;\n\n\trdma_argp = rq_arg->head[0].iov_base;\n\tif (*(rdma_argp + 1) != rpcrdma_version)\n\t\tgoto out_version;\n\n\tswitch (*(rdma_argp + 3)) {\n\tcase rdma_msg:\n\tcase rdma_nomsg:\n\t\tbreak;\n\n\tcase rdma_done:\n\t\tgoto out_drop;\n\n\tcase rdma_error:\n\t\tgoto out_drop;\n\n\tdefault:\n\t\tgoto out_proc;\n\t}\n\n\tend = (__be32 *)((unsigned long)rdma_argp + rq_arg->len);\n\tp = xdr_check_read_list(rdma_argp + 4, end);\n\tif (!p)\n\t\tgoto out_inval;\n\tp = xdr_check_write_list(p, end);\n\tif (!p)\n\t\tgoto out_inval;\n\tp = xdr_check_reply_chunk(p, end);\n\tif (!p)\n\t\tgoto out_inval;\n\tif (p > end)\n\t\tgoto out_inval;\n\n\trq_arg->head[0].iov_base = p;\n\thdr_len = (unsigned long)p - (unsigned long)rdma_argp;\n\trq_arg->head[0].iov_len -= hdr_len;\n\treturn hdr_len;\n\nout_short:\n\tdprintk(\"svcrdma: header too short = %d\\n\", rq_arg->len);\n\treturn -EINVAL;\n\nout_version:\n\tdprintk(\"svcrdma: bad xprt version: %u\\n\",\n\t\tbe32_to_cpup(rdma_argp + 1));\n\treturn -EPROTONOSUPPORT;\n\nout_drop:\n\tdprintk(\"svcrdma: dropping RDMA_DONE/ERROR message\\n\");\n\treturn 0;\n\nout_proc:\n\tdprintk(\"svcrdma: bad rdma procedure (%u)\\n\",\n\t\tbe32_to_cpup(rdma_argp + 3));\n\treturn -EINVAL;\n\nout_inval:\n\tdprintk(\"svcrdma: failed to parse transport header\\n\");\n\treturn -EINVAL;\n}\n\nint svc_rdma_xdr_encode_error(struct svcxprt_rdma *xprt,\n\t\t\t      struct rpcrdma_msg *rmsgp,\n\t\t\t      enum rpcrdma_errcode err, __be32 *va)\n{\n\t__be32 *startp = va;\n\n\t*va++ = rmsgp->rm_xid;\n\t*va++ = rmsgp->rm_vers;\n\t*va++ = xprt->sc_fc_credits;\n\t*va++ = rdma_error;\n\t*va++ = cpu_to_be32(err);\n\tif (err == ERR_VERS) {\n\t\t*va++ = rpcrdma_version;\n\t\t*va++ = rpcrdma_version;\n\t}\n\n\treturn (int)((unsigned long)va - (unsigned long)startp);\n}\n\n/**\n * svc_rdma_xdr_get_reply_hdr_length - Get length of Reply transport header\n * @rdma_resp: buffer containing Reply transport header\n *\n * Returns length of transport header, in bytes.\n */\nunsigned int svc_rdma_xdr_get_reply_hdr_len(__be32 *rdma_resp)\n{\n\tunsigned int nsegs;\n\t__be32 *p;\n\n\tp = rdma_resp;\n\n\t/* RPC-over-RDMA V1 replies never have a Read list. */\n\tp += rpcrdma_fixed_maxsz + 1;\n\n\t/* Skip Write list. */\n\twhile (*p++ != xdr_zero) {\n\t\tnsegs = be32_to_cpup(p++);\n\t\tp += nsegs * rpcrdma_segment_maxsz;\n\t}\n\n\t/* Skip Reply chunk. */\n\tif (*p++ != xdr_zero) {\n\t\tnsegs = be32_to_cpup(p++);\n\t\tp += nsegs * rpcrdma_segment_maxsz;\n\t}\n\n\treturn (unsigned long)p - (unsigned long)rdma_resp;\n}\n\nvoid svc_rdma_xdr_encode_write_list(struct rpcrdma_msg *rmsgp, int chunks)\n{\n\tstruct rpcrdma_write_array *ary;\n\n\t/* no read-list */\n\trmsgp->rm_body.rm_chunks[0] = xdr_zero;\n\n\t/* write-array discrim */\n\tary = (struct rpcrdma_write_array *)\n\t\t&rmsgp->rm_body.rm_chunks[1];\n\tary->wc_discrim = xdr_one;\n\tary->wc_nchunks = cpu_to_be32(chunks);\n\n\t/* write-list terminator */\n\tary->wc_array[chunks].wc_target.rs_handle = xdr_zero;\n\n\t/* reply-array discriminator */\n\tary->wc_array[chunks].wc_target.rs_length = xdr_zero;\n}\n\nvoid svc_rdma_xdr_encode_reply_array(struct rpcrdma_write_array *ary,\n\t\t\t\t int chunks)\n{\n\tary->wc_discrim = xdr_one;\n\tary->wc_nchunks = cpu_to_be32(chunks);\n}\n\nvoid svc_rdma_xdr_encode_array_chunk(struct rpcrdma_write_array *ary,\n\t\t\t\t     int chunk_no,\n\t\t\t\t     __be32 rs_handle,\n\t\t\t\t     __be64 rs_offset,\n\t\t\t\t     u32 write_len)\n{\n\tstruct rpcrdma_segment *seg = &ary->wc_array[chunk_no].wc_target;\n\tseg->rs_handle = rs_handle;\n\tseg->rs_offset = rs_offset;\n\tseg->rs_length = cpu_to_be32(write_len);\n}\n", "/*\n * Copyright (c) 2014 Open Grid Computing, Inc. All rights reserved.\n * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n#include <linux/sunrpc/debug.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <linux/spinlock.h>\n#include <asm/unaligned.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n#include <linux/sunrpc/svc_rdma.h>\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\n/*\n * Replace the pages in the rq_argpages array with the pages from the SGE in\n * the RDMA_RECV completion. The SGL should contain full pages up until the\n * last one.\n */\nstatic void rdma_build_arg_xdr(struct svc_rqst *rqstp,\n\t\t\t       struct svc_rdma_op_ctxt *ctxt,\n\t\t\t       u32 byte_count)\n{\n\tstruct rpcrdma_msg *rmsgp;\n\tstruct page *page;\n\tu32 bc;\n\tint sge_no;\n\n\t/* Swap the page in the SGE with the page in argpages */\n\tpage = ctxt->pages[0];\n\tput_page(rqstp->rq_pages[0]);\n\trqstp->rq_pages[0] = page;\n\n\t/* Set up the XDR head */\n\trqstp->rq_arg.head[0].iov_base = page_address(page);\n\trqstp->rq_arg.head[0].iov_len =\n\t\tmin_t(size_t, byte_count, ctxt->sge[0].length);\n\trqstp->rq_arg.len = byte_count;\n\trqstp->rq_arg.buflen = byte_count;\n\n\t/* Compute bytes past head in the SGL */\n\tbc = byte_count - rqstp->rq_arg.head[0].iov_len;\n\n\t/* If data remains, store it in the pagelist */\n\trqstp->rq_arg.page_len = bc;\n\trqstp->rq_arg.page_base = 0;\n\n\t/* RDMA_NOMSG: RDMA READ data should land just after RDMA RECV data */\n\trmsgp = (struct rpcrdma_msg *)rqstp->rq_arg.head[0].iov_base;\n\tif (rmsgp->rm_type == rdma_nomsg)\n\t\trqstp->rq_arg.pages = &rqstp->rq_pages[0];\n\telse\n\t\trqstp->rq_arg.pages = &rqstp->rq_pages[1];\n\n\tsge_no = 1;\n\twhile (bc && sge_no < ctxt->count) {\n\t\tpage = ctxt->pages[sge_no];\n\t\tput_page(rqstp->rq_pages[sge_no]);\n\t\trqstp->rq_pages[sge_no] = page;\n\t\tbc -= min_t(u32, bc, ctxt->sge[sge_no].length);\n\t\trqstp->rq_arg.buflen += ctxt->sge[sge_no].length;\n\t\tsge_no++;\n\t}\n\trqstp->rq_respages = &rqstp->rq_pages[sge_no];\n\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\n\t/* If not all pages were used from the SGL, free the remaining ones */\n\tbc = sge_no;\n\twhile (sge_no < ctxt->count) {\n\t\tpage = ctxt->pages[sge_no++];\n\t\tput_page(page);\n\t}\n\tctxt->count = bc;\n\n\t/* Set up tail */\n\trqstp->rq_arg.tail[0].iov_base = NULL;\n\trqstp->rq_arg.tail[0].iov_len = 0;\n}\n\n/* Issue an RDMA_READ using the local lkey to map the data sink */\nint rdma_read_chunk_lcl(struct svcxprt_rdma *xprt,\n\t\t\tstruct svc_rqst *rqstp,\n\t\t\tstruct svc_rdma_op_ctxt *head,\n\t\t\tint *page_no,\n\t\t\tu32 *page_offset,\n\t\t\tu32 rs_handle,\n\t\t\tu32 rs_length,\n\t\t\tu64 rs_offset,\n\t\t\tbool last)\n{\n\tstruct ib_rdma_wr read_wr;\n\tint pages_needed = PAGE_ALIGN(*page_offset + rs_length) >> PAGE_SHIFT;\n\tstruct svc_rdma_op_ctxt *ctxt = svc_rdma_get_context(xprt);\n\tint ret, read, pno;\n\tu32 pg_off = *page_offset;\n\tu32 pg_no = *page_no;\n\n\tctxt->direction = DMA_FROM_DEVICE;\n\tctxt->read_hdr = head;\n\tpages_needed = min_t(int, pages_needed, xprt->sc_max_sge_rd);\n\tread = min_t(int, (pages_needed << PAGE_SHIFT) - *page_offset,\n\t\t     rs_length);\n\n\tfor (pno = 0; pno < pages_needed; pno++) {\n\t\tint len = min_t(int, rs_length, PAGE_SIZE - pg_off);\n\n\t\thead->arg.pages[pg_no] = rqstp->rq_arg.pages[pg_no];\n\t\thead->arg.page_len += len;\n\n\t\thead->arg.len += len;\n\t\tif (!pg_off)\n\t\t\thead->count++;\n\t\trqstp->rq_respages = &rqstp->rq_arg.pages[pg_no+1];\n\t\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\t\tctxt->sge[pno].addr =\n\t\t\tib_dma_map_page(xprt->sc_cm_id->device,\n\t\t\t\t\thead->arg.pages[pg_no], pg_off,\n\t\t\t\t\tPAGE_SIZE - pg_off,\n\t\t\t\t\tDMA_FROM_DEVICE);\n\t\tret = ib_dma_mapping_error(xprt->sc_cm_id->device,\n\t\t\t\t\t   ctxt->sge[pno].addr);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tsvc_rdma_count_mappings(xprt, ctxt);\n\n\t\tctxt->sge[pno].lkey = xprt->sc_pd->local_dma_lkey;\n\t\tctxt->sge[pno].length = len;\n\t\tctxt->count++;\n\n\t\t/* adjust offset and wrap to next page if needed */\n\t\tpg_off += len;\n\t\tif (pg_off == PAGE_SIZE) {\n\t\t\tpg_off = 0;\n\t\t\tpg_no++;\n\t\t}\n\t\trs_length -= len;\n\t}\n\n\tif (last && rs_length == 0)\n\t\tset_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\n\telse\n\t\tclear_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\n\n\tmemset(&read_wr, 0, sizeof(read_wr));\n\tctxt->cqe.done = svc_rdma_wc_read;\n\tread_wr.wr.wr_cqe = &ctxt->cqe;\n\tread_wr.wr.opcode = IB_WR_RDMA_READ;\n\tread_wr.wr.send_flags = IB_SEND_SIGNALED;\n\tread_wr.rkey = rs_handle;\n\tread_wr.remote_addr = rs_offset;\n\tread_wr.wr.sg_list = ctxt->sge;\n\tread_wr.wr.num_sge = pages_needed;\n\n\tret = svc_rdma_send(xprt, &read_wr.wr);\n\tif (ret) {\n\t\tpr_err(\"svcrdma: Error %d posting RDMA_READ\\n\", ret);\n\t\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\t\tgoto err;\n\t}\n\n\t/* return current location in page array */\n\t*page_no = pg_no;\n\t*page_offset = pg_off;\n\tret = read;\n\tatomic_inc(&rdma_stat_read);\n\treturn ret;\n err:\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 0);\n\treturn ret;\n}\n\n/* Issue an RDMA_READ using an FRMR to map the data sink */\nint rdma_read_chunk_frmr(struct svcxprt_rdma *xprt,\n\t\t\t struct svc_rqst *rqstp,\n\t\t\t struct svc_rdma_op_ctxt *head,\n\t\t\t int *page_no,\n\t\t\t u32 *page_offset,\n\t\t\t u32 rs_handle,\n\t\t\t u32 rs_length,\n\t\t\t u64 rs_offset,\n\t\t\t bool last)\n{\n\tstruct ib_rdma_wr read_wr;\n\tstruct ib_send_wr inv_wr;\n\tstruct ib_reg_wr reg_wr;\n\tu8 key;\n\tint nents = PAGE_ALIGN(*page_offset + rs_length) >> PAGE_SHIFT;\n\tstruct svc_rdma_op_ctxt *ctxt = svc_rdma_get_context(xprt);\n\tstruct svc_rdma_fastreg_mr *frmr = svc_rdma_get_frmr(xprt);\n\tint ret, read, pno, dma_nents, n;\n\tu32 pg_off = *page_offset;\n\tu32 pg_no = *page_no;\n\n\tif (IS_ERR(frmr))\n\t\treturn -ENOMEM;\n\n\tctxt->direction = DMA_FROM_DEVICE;\n\tctxt->frmr = frmr;\n\tnents = min_t(unsigned int, nents, xprt->sc_frmr_pg_list_len);\n\tread = min_t(int, (nents << PAGE_SHIFT) - *page_offset, rs_length);\n\n\tfrmr->direction = DMA_FROM_DEVICE;\n\tfrmr->access_flags = (IB_ACCESS_LOCAL_WRITE|IB_ACCESS_REMOTE_WRITE);\n\tfrmr->sg_nents = nents;\n\n\tfor (pno = 0; pno < nents; pno++) {\n\t\tint len = min_t(int, rs_length, PAGE_SIZE - pg_off);\n\n\t\thead->arg.pages[pg_no] = rqstp->rq_arg.pages[pg_no];\n\t\thead->arg.page_len += len;\n\t\thead->arg.len += len;\n\t\tif (!pg_off)\n\t\t\thead->count++;\n\n\t\tsg_set_page(&frmr->sg[pno], rqstp->rq_arg.pages[pg_no],\n\t\t\t    len, pg_off);\n\n\t\trqstp->rq_respages = &rqstp->rq_arg.pages[pg_no+1];\n\t\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\n\t\t/* adjust offset and wrap to next page if needed */\n\t\tpg_off += len;\n\t\tif (pg_off == PAGE_SIZE) {\n\t\t\tpg_off = 0;\n\t\t\tpg_no++;\n\t\t}\n\t\trs_length -= len;\n\t}\n\n\tif (last && rs_length == 0)\n\t\tset_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\n\telse\n\t\tclear_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\n\n\tdma_nents = ib_dma_map_sg(xprt->sc_cm_id->device,\n\t\t\t\t  frmr->sg, frmr->sg_nents,\n\t\t\t\t  frmr->direction);\n\tif (!dma_nents) {\n\t\tpr_err(\"svcrdma: failed to dma map sg %p\\n\",\n\t\t       frmr->sg);\n\t\treturn -ENOMEM;\n\t}\n\n\tn = ib_map_mr_sg(frmr->mr, frmr->sg, frmr->sg_nents, NULL, PAGE_SIZE);\n\tif (unlikely(n != frmr->sg_nents)) {\n\t\tpr_err(\"svcrdma: failed to map mr %p (%d/%d elements)\\n\",\n\t\t       frmr->mr, n, frmr->sg_nents);\n\t\treturn n < 0 ? n : -EINVAL;\n\t}\n\n\t/* Bump the key */\n\tkey = (u8)(frmr->mr->lkey & 0x000000FF);\n\tib_update_fast_reg_key(frmr->mr, ++key);\n\n\tctxt->sge[0].addr = frmr->mr->iova;\n\tctxt->sge[0].lkey = frmr->mr->lkey;\n\tctxt->sge[0].length = frmr->mr->length;\n\tctxt->count = 1;\n\tctxt->read_hdr = head;\n\n\t/* Prepare REG WR */\n\tctxt->reg_cqe.done = svc_rdma_wc_reg;\n\treg_wr.wr.wr_cqe = &ctxt->reg_cqe;\n\treg_wr.wr.opcode = IB_WR_REG_MR;\n\treg_wr.wr.send_flags = IB_SEND_SIGNALED;\n\treg_wr.wr.num_sge = 0;\n\treg_wr.mr = frmr->mr;\n\treg_wr.key = frmr->mr->lkey;\n\treg_wr.access = frmr->access_flags;\n\treg_wr.wr.next = &read_wr.wr;\n\n\t/* Prepare RDMA_READ */\n\tmemset(&read_wr, 0, sizeof(read_wr));\n\tctxt->cqe.done = svc_rdma_wc_read;\n\tread_wr.wr.wr_cqe = &ctxt->cqe;\n\tread_wr.wr.send_flags = IB_SEND_SIGNALED;\n\tread_wr.rkey = rs_handle;\n\tread_wr.remote_addr = rs_offset;\n\tread_wr.wr.sg_list = ctxt->sge;\n\tread_wr.wr.num_sge = 1;\n\tif (xprt->sc_dev_caps & SVCRDMA_DEVCAP_READ_W_INV) {\n\t\tread_wr.wr.opcode = IB_WR_RDMA_READ_WITH_INV;\n\t\tread_wr.wr.ex.invalidate_rkey = ctxt->frmr->mr->lkey;\n\t} else {\n\t\tread_wr.wr.opcode = IB_WR_RDMA_READ;\n\t\tread_wr.wr.next = &inv_wr;\n\t\t/* Prepare invalidate */\n\t\tmemset(&inv_wr, 0, sizeof(inv_wr));\n\t\tctxt->inv_cqe.done = svc_rdma_wc_inv;\n\t\tinv_wr.wr_cqe = &ctxt->inv_cqe;\n\t\tinv_wr.opcode = IB_WR_LOCAL_INV;\n\t\tinv_wr.send_flags = IB_SEND_SIGNALED | IB_SEND_FENCE;\n\t\tinv_wr.ex.invalidate_rkey = frmr->mr->lkey;\n\t}\n\n\t/* Post the chain */\n\tret = svc_rdma_send(xprt, &reg_wr.wr);\n\tif (ret) {\n\t\tpr_err(\"svcrdma: Error %d posting RDMA_READ\\n\", ret);\n\t\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\t\tgoto err;\n\t}\n\n\t/* return current location in page array */\n\t*page_no = pg_no;\n\t*page_offset = pg_off;\n\tret = read;\n\tatomic_inc(&rdma_stat_read);\n\treturn ret;\n err:\n\tsvc_rdma_put_context(ctxt, 0);\n\tsvc_rdma_put_frmr(xprt, frmr);\n\treturn ret;\n}\n\nstatic unsigned int\nrdma_rcl_chunk_count(struct rpcrdma_read_chunk *ch)\n{\n\tunsigned int count;\n\n\tfor (count = 0; ch->rc_discrim != xdr_zero; ch++)\n\t\tcount++;\n\treturn count;\n}\n\n/* If there was additional inline content, append it to the end of arg.pages.\n * Tail copy has to be done after the reader function has determined how many\n * pages are needed for RDMA READ.\n */\nstatic int\nrdma_copy_tail(struct svc_rqst *rqstp, struct svc_rdma_op_ctxt *head,\n\t       u32 position, u32 byte_count, u32 page_offset, int page_no)\n{\n\tchar *srcp, *destp;\n\n\tsrcp = head->arg.head[0].iov_base + position;\n\tbyte_count = head->arg.head[0].iov_len - position;\n\tif (byte_count > PAGE_SIZE) {\n\t\tdprintk(\"svcrdma: large tail unsupported\\n\");\n\t\treturn 0;\n\t}\n\n\t/* Fit as much of the tail on the current page as possible */\n\tif (page_offset != PAGE_SIZE) {\n\t\tdestp = page_address(rqstp->rq_arg.pages[page_no]);\n\t\tdestp += page_offset;\n\t\twhile (byte_count--) {\n\t\t\t*destp++ = *srcp++;\n\t\t\tpage_offset++;\n\t\t\tif (page_offset == PAGE_SIZE && byte_count)\n\t\t\t\tgoto more;\n\t\t}\n\t\tgoto done;\n\t}\n\nmore:\n\t/* Fit the rest on the next page */\n\tpage_no++;\n\tdestp = page_address(rqstp->rq_arg.pages[page_no]);\n\twhile (byte_count--)\n\t\t*destp++ = *srcp++;\n\n\trqstp->rq_respages = &rqstp->rq_arg.pages[page_no+1];\n\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\ndone:\n\tbyte_count = head->arg.head[0].iov_len - position;\n\thead->arg.page_len += byte_count;\n\thead->arg.len += byte_count;\n\thead->arg.buflen += byte_count;\n\treturn 1;\n}\n\n/* Returns the address of the first read chunk or <nul> if no read chunk\n * is present\n */\nstatic struct rpcrdma_read_chunk *\nsvc_rdma_get_read_chunk(struct rpcrdma_msg *rmsgp)\n{\n\tstruct rpcrdma_read_chunk *ch =\n\t\t(struct rpcrdma_read_chunk *)&rmsgp->rm_body.rm_chunks[0];\n\n\tif (ch->rc_discrim == xdr_zero)\n\t\treturn NULL;\n\treturn ch;\n}\n\nstatic int rdma_read_chunks(struct svcxprt_rdma *xprt,\n\t\t\t    struct rpcrdma_msg *rmsgp,\n\t\t\t    struct svc_rqst *rqstp,\n\t\t\t    struct svc_rdma_op_ctxt *head)\n{\n\tint page_no, ret;\n\tstruct rpcrdma_read_chunk *ch;\n\tu32 handle, page_offset, byte_count;\n\tu32 position;\n\tu64 rs_offset;\n\tbool last;\n\n\t/* If no read list is present, return 0 */\n\tch = svc_rdma_get_read_chunk(rmsgp);\n\tif (!ch)\n\t\treturn 0;\n\n\tif (rdma_rcl_chunk_count(ch) > RPCSVC_MAXPAGES)\n\t\treturn -EINVAL;\n\n\t/* The request is completed when the RDMA_READs complete. The\n\t * head context keeps all the pages that comprise the\n\t * request.\n\t */\n\thead->arg.head[0] = rqstp->rq_arg.head[0];\n\thead->arg.tail[0] = rqstp->rq_arg.tail[0];\n\thead->hdr_count = head->count;\n\thead->arg.page_base = 0;\n\thead->arg.page_len = 0;\n\thead->arg.len = rqstp->rq_arg.len;\n\thead->arg.buflen = rqstp->rq_arg.buflen;\n\n\t/* RDMA_NOMSG: RDMA READ data should land just after RDMA RECV data */\n\tposition = be32_to_cpu(ch->rc_position);\n\tif (position == 0) {\n\t\thead->arg.pages = &head->pages[0];\n\t\tpage_offset = head->byte_len;\n\t} else {\n\t\thead->arg.pages = &head->pages[head->count];\n\t\tpage_offset = 0;\n\t}\n\n\tret = 0;\n\tpage_no = 0;\n\tfor (; ch->rc_discrim != xdr_zero; ch++) {\n\t\tif (be32_to_cpu(ch->rc_position) != position)\n\t\t\tgoto err;\n\n\t\thandle = be32_to_cpu(ch->rc_target.rs_handle),\n\t\tbyte_count = be32_to_cpu(ch->rc_target.rs_length);\n\t\txdr_decode_hyper((__be32 *)&ch->rc_target.rs_offset,\n\t\t\t\t &rs_offset);\n\n\t\twhile (byte_count > 0) {\n\t\t\tlast = (ch + 1)->rc_discrim == xdr_zero;\n\t\t\tret = xprt->sc_reader(xprt, rqstp, head,\n\t\t\t\t\t      &page_no, &page_offset,\n\t\t\t\t\t      handle, byte_count,\n\t\t\t\t\t      rs_offset, last);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto err;\n\t\t\tbyte_count -= ret;\n\t\t\trs_offset += ret;\n\t\t\thead->arg.buflen += ret;\n\t\t}\n\t}\n\n\t/* Read list may need XDR round-up (see RFC 5666, s. 3.7) */\n\tif (page_offset & 3) {\n\t\tu32 pad = 4 - (page_offset & 3);\n\n\t\thead->arg.tail[0].iov_len += pad;\n\t\thead->arg.len += pad;\n\t\thead->arg.buflen += pad;\n\t\tpage_offset += pad;\n\t}\n\n\tret = 1;\n\tif (position && position < head->arg.head[0].iov_len)\n\t\tret = rdma_copy_tail(rqstp, head, position,\n\t\t\t\t     byte_count, page_offset, page_no);\n\thead->arg.head[0].iov_len = position;\n\thead->position = position;\n\n err:\n\t/* Detach arg pages. svc_recv will replenish them */\n\tfor (page_no = 0;\n\t     &rqstp->rq_pages[page_no] < rqstp->rq_respages; page_no++)\n\t\trqstp->rq_pages[page_no] = NULL;\n\n\treturn ret;\n}\n\nstatic void rdma_read_complete(struct svc_rqst *rqstp,\n\t\t\t       struct svc_rdma_op_ctxt *head)\n{\n\tint page_no;\n\n\t/* Copy RPC pages */\n\tfor (page_no = 0; page_no < head->count; page_no++) {\n\t\tput_page(rqstp->rq_pages[page_no]);\n\t\trqstp->rq_pages[page_no] = head->pages[page_no];\n\t}\n\n\t/* Adjustments made for RDMA_NOMSG type requests */\n\tif (head->position == 0) {\n\t\tif (head->arg.len <= head->sge[0].length) {\n\t\t\thead->arg.head[0].iov_len = head->arg.len -\n\t\t\t\t\t\t\thead->byte_len;\n\t\t\thead->arg.page_len = 0;\n\t\t} else {\n\t\t\thead->arg.head[0].iov_len = head->sge[0].length -\n\t\t\t\t\t\t\t\thead->byte_len;\n\t\t\thead->arg.page_len = head->arg.len -\n\t\t\t\t\t\thead->sge[0].length;\n\t\t}\n\t}\n\n\t/* Point rq_arg.pages past header */\n\trqstp->rq_arg.pages = &rqstp->rq_pages[head->hdr_count];\n\trqstp->rq_arg.page_len = head->arg.page_len;\n\trqstp->rq_arg.page_base = head->arg.page_base;\n\n\t/* rq_respages starts after the last arg page */\n\trqstp->rq_respages = &rqstp->rq_pages[page_no];\n\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\n\t/* Rebuild rq_arg head and tail. */\n\trqstp->rq_arg.head[0] = head->arg.head[0];\n\trqstp->rq_arg.tail[0] = head->arg.tail[0];\n\trqstp->rq_arg.len = head->arg.len;\n\trqstp->rq_arg.buflen = head->arg.buflen;\n}\n\n/* By convention, backchannel calls arrive via rdma_msg type\n * messages, and never populate the chunk lists. This makes\n * the RPC/RDMA header small and fixed in size, so it is\n * straightforward to check the RPC header's direction field.\n */\nstatic bool\nsvc_rdma_is_backchannel_reply(struct svc_xprt *xprt, struct rpcrdma_msg *rmsgp)\n{\n\t__be32 *p = (__be32 *)rmsgp;\n\n\tif (!xprt->xpt_bc_xprt)\n\t\treturn false;\n\n\tif (rmsgp->rm_type != rdma_msg)\n\t\treturn false;\n\tif (rmsgp->rm_body.rm_chunks[0] != xdr_zero)\n\t\treturn false;\n\tif (rmsgp->rm_body.rm_chunks[1] != xdr_zero)\n\t\treturn false;\n\tif (rmsgp->rm_body.rm_chunks[2] != xdr_zero)\n\t\treturn false;\n\n\t/* sanity */\n\tif (p[7] != rmsgp->rm_xid)\n\t\treturn false;\n\t/* call direction */\n\tif (p[8] == cpu_to_be32(RPC_CALL))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * Set up the rqstp thread context to point to the RQ buffer. If\n * necessary, pull additional data from the client with an RDMA_READ\n * request.\n */\nint svc_rdma_recvfrom(struct svc_rqst *rqstp)\n{\n\tstruct svc_xprt *xprt = rqstp->rq_xprt;\n\tstruct svcxprt_rdma *rdma_xprt =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tstruct svc_rdma_op_ctxt *ctxt = NULL;\n\tstruct rpcrdma_msg *rmsgp;\n\tint ret = 0;\n\n\tdprintk(\"svcrdma: rqstp=%p\\n\", rqstp);\n\n\tspin_lock(&rdma_xprt->sc_rq_dto_lock);\n\tif (!list_empty(&rdma_xprt->sc_read_complete_q)) {\n\t\tctxt = list_first_entry(&rdma_xprt->sc_read_complete_q,\n\t\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\t\tlist_del(&ctxt->list);\n\t\tspin_unlock(&rdma_xprt->sc_rq_dto_lock);\n\t\trdma_read_complete(rqstp, ctxt);\n\t\tgoto complete;\n\t} else if (!list_empty(&rdma_xprt->sc_rq_dto_q)) {\n\t\tctxt = list_first_entry(&rdma_xprt->sc_rq_dto_q,\n\t\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\t\tlist_del(&ctxt->list);\n\t} else {\n\t\tatomic_inc(&rdma_stat_rq_starve);\n\t\tclear_bit(XPT_DATA, &xprt->xpt_flags);\n\t\tctxt = NULL;\n\t}\n\tspin_unlock(&rdma_xprt->sc_rq_dto_lock);\n\tif (!ctxt) {\n\t\t/* This is the EAGAIN path. The svc_recv routine will\n\t\t * return -EAGAIN, the nfsd thread will go to call into\n\t\t * svc_recv again and we shouldn't be on the active\n\t\t * transport list\n\t\t */\n\t\tif (test_bit(XPT_CLOSE, &xprt->xpt_flags))\n\t\t\tgoto defer;\n\t\tgoto out;\n\t}\n\tdprintk(\"svcrdma: processing ctxt=%p on xprt=%p, rqstp=%p\\n\",\n\t\tctxt, rdma_xprt, rqstp);\n\tatomic_inc(&rdma_stat_recv);\n\n\t/* Build up the XDR from the receive buffers. */\n\trdma_build_arg_xdr(rqstp, ctxt, ctxt->byte_len);\n\n\t/* Decode the RDMA header. */\n\trmsgp = (struct rpcrdma_msg *)rqstp->rq_arg.head[0].iov_base;\n\tret = svc_rdma_xdr_decode_req(&rqstp->rq_arg);\n\tif (ret < 0)\n\t\tgoto out_err;\n\tif (ret == 0)\n\t\tgoto out_drop;\n\trqstp->rq_xprt_hlen = ret;\n\n\tif (svc_rdma_is_backchannel_reply(xprt, rmsgp)) {\n\t\tret = svc_rdma_handle_bc_reply(xprt->xpt_bc_xprt, rmsgp,\n\t\t\t\t\t       &rqstp->rq_arg);\n\t\tsvc_rdma_put_context(ctxt, 0);\n\t\tif (ret)\n\t\t\tgoto repost;\n\t\treturn ret;\n\t}\n\n\t/* Read read-list data. */\n\tret = rdma_read_chunks(rdma_xprt, rmsgp, rqstp, ctxt);\n\tif (ret > 0) {\n\t\t/* read-list posted, defer until data received from client. */\n\t\tgoto defer;\n\t} else if (ret < 0) {\n\t\t/* Post of read-list failed, free context. */\n\t\tsvc_rdma_put_context(ctxt, 1);\n\t\treturn 0;\n\t}\n\ncomplete:\n\tret = rqstp->rq_arg.head[0].iov_len\n\t\t+ rqstp->rq_arg.page_len\n\t\t+ rqstp->rq_arg.tail[0].iov_len;\n\tsvc_rdma_put_context(ctxt, 0);\n out:\n\tdprintk(\"svcrdma: ret=%d, rq_arg.len=%u, \"\n\t\t\"rq_arg.head[0].iov_base=%p, rq_arg.head[0].iov_len=%zd\\n\",\n\t\tret, rqstp->rq_arg.len,\n\t\trqstp->rq_arg.head[0].iov_base,\n\t\trqstp->rq_arg.head[0].iov_len);\n\trqstp->rq_prot = IPPROTO_MAX;\n\tsvc_xprt_copy_addrs(rqstp, xprt);\n\treturn ret;\n\nout_err:\n\tsvc_rdma_send_error(rdma_xprt, rmsgp, ret);\n\tsvc_rdma_put_context(ctxt, 0);\n\treturn 0;\n\ndefer:\n\treturn 0;\n\nout_drop:\n\tsvc_rdma_put_context(ctxt, 1);\nrepost:\n\treturn svc_rdma_repost_recv(rdma_xprt, GFP_KERNEL);\n}\n", "Couldn't find the requested file /net/sunrpc/xprtrdma/svc_rdma_rw.c in torvalds/linux.", "/*\n * Copyright (c) 2014 Open Grid Computing, Inc. All rights reserved.\n * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n#include <linux/sunrpc/debug.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <linux/spinlock.h>\n#include <asm/unaligned.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n#include <linux/sunrpc/svc_rdma.h>\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\nstatic u32 xdr_padsize(u32 len)\n{\n\treturn (len & 3) ? (4 - (len & 3)) : 0;\n}\n\nint svc_rdma_map_xdr(struct svcxprt_rdma *xprt,\n\t\t     struct xdr_buf *xdr,\n\t\t     struct svc_rdma_req_map *vec,\n\t\t     bool write_chunk_present)\n{\n\tint sge_no;\n\tu32 sge_bytes;\n\tu32 page_bytes;\n\tu32 page_off;\n\tint page_no;\n\n\tif (xdr->len !=\n\t    (xdr->head[0].iov_len + xdr->page_len + xdr->tail[0].iov_len)) {\n\t\tpr_err(\"svcrdma: %s: XDR buffer length error\\n\", __func__);\n\t\treturn -EIO;\n\t}\n\n\t/* Skip the first sge, this is for the RPCRDMA header */\n\tsge_no = 1;\n\n\t/* Head SGE */\n\tvec->sge[sge_no].iov_base = xdr->head[0].iov_base;\n\tvec->sge[sge_no].iov_len = xdr->head[0].iov_len;\n\tsge_no++;\n\n\t/* pages SGE */\n\tpage_no = 0;\n\tpage_bytes = xdr->page_len;\n\tpage_off = xdr->page_base;\n\twhile (page_bytes) {\n\t\tvec->sge[sge_no].iov_base =\n\t\t\tpage_address(xdr->pages[page_no]) + page_off;\n\t\tsge_bytes = min_t(u32, page_bytes, (PAGE_SIZE - page_off));\n\t\tpage_bytes -= sge_bytes;\n\t\tvec->sge[sge_no].iov_len = sge_bytes;\n\n\t\tsge_no++;\n\t\tpage_no++;\n\t\tpage_off = 0; /* reset for next time through loop */\n\t}\n\n\t/* Tail SGE */\n\tif (xdr->tail[0].iov_len) {\n\t\tunsigned char *base = xdr->tail[0].iov_base;\n\t\tsize_t len = xdr->tail[0].iov_len;\n\t\tu32 xdr_pad = xdr_padsize(xdr->page_len);\n\n\t\tif (write_chunk_present && xdr_pad) {\n\t\t\tbase += xdr_pad;\n\t\t\tlen -= xdr_pad;\n\t\t}\n\n\t\tif (len) {\n\t\t\tvec->sge[sge_no].iov_base = base;\n\t\t\tvec->sge[sge_no].iov_len = len;\n\t\t\tsge_no++;\n\t\t}\n\t}\n\n\tdprintk(\"svcrdma: %s: sge_no %d page_no %d \"\n\t\t\"page_base %u page_len %u head_len %zu tail_len %zu\\n\",\n\t\t__func__, sge_no, page_no, xdr->page_base, xdr->page_len,\n\t\txdr->head[0].iov_len, xdr->tail[0].iov_len);\n\n\tvec->count = sge_no;\n\treturn 0;\n}\n\nstatic dma_addr_t dma_map_xdr(struct svcxprt_rdma *xprt,\n\t\t\t      struct xdr_buf *xdr,\n\t\t\t      u32 xdr_off, size_t len, int dir)\n{\n\tstruct page *page;\n\tdma_addr_t dma_addr;\n\tif (xdr_off < xdr->head[0].iov_len) {\n\t\t/* This offset is in the head */\n\t\txdr_off += (unsigned long)xdr->head[0].iov_base & ~PAGE_MASK;\n\t\tpage = virt_to_page(xdr->head[0].iov_base);\n\t} else {\n\t\txdr_off -= xdr->head[0].iov_len;\n\t\tif (xdr_off < xdr->page_len) {\n\t\t\t/* This offset is in the page list */\n\t\t\txdr_off += xdr->page_base;\n\t\t\tpage = xdr->pages[xdr_off >> PAGE_SHIFT];\n\t\t\txdr_off &= ~PAGE_MASK;\n\t\t} else {\n\t\t\t/* This offset is in the tail */\n\t\t\txdr_off -= xdr->page_len;\n\t\t\txdr_off += (unsigned long)\n\t\t\t\txdr->tail[0].iov_base & ~PAGE_MASK;\n\t\t\tpage = virt_to_page(xdr->tail[0].iov_base);\n\t\t}\n\t}\n\tdma_addr = ib_dma_map_page(xprt->sc_cm_id->device, page, xdr_off,\n\t\t\t\t   min_t(size_t, PAGE_SIZE, len), dir);\n\treturn dma_addr;\n}\n\n/* Parse the RPC Call's transport header.\n */\nstatic void svc_rdma_get_write_arrays(struct rpcrdma_msg *rmsgp,\n\t\t\t\t      struct rpcrdma_write_array **write,\n\t\t\t\t      struct rpcrdma_write_array **reply)\n{\n\t__be32 *p;\n\n\tp = (__be32 *)&rmsgp->rm_body.rm_chunks[0];\n\n\t/* Read list */\n\twhile (*p++ != xdr_zero)\n\t\tp += 5;\n\n\t/* Write list */\n\tif (*p != xdr_zero) {\n\t\t*write = (struct rpcrdma_write_array *)p;\n\t\twhile (*p++ != xdr_zero)\n\t\t\tp += 1 + be32_to_cpu(*p) * 4;\n\t} else {\n\t\t*write = NULL;\n\t\tp++;\n\t}\n\n\t/* Reply chunk */\n\tif (*p != xdr_zero)\n\t\t*reply = (struct rpcrdma_write_array *)p;\n\telse\n\t\t*reply = NULL;\n}\n\n/* RPC-over-RDMA Version One private extension: Remote Invalidation.\n * Responder's choice: requester signals it can handle Send With\n * Invalidate, and responder chooses one rkey to invalidate.\n *\n * Find a candidate rkey to invalidate when sending a reply.  Picks the\n * first rkey it finds in the chunks lists.\n *\n * Returns zero if RPC's chunk lists are empty.\n */\nstatic u32 svc_rdma_get_inv_rkey(struct rpcrdma_msg *rdma_argp,\n\t\t\t\t struct rpcrdma_write_array *wr_ary,\n\t\t\t\t struct rpcrdma_write_array *rp_ary)\n{\n\tstruct rpcrdma_read_chunk *rd_ary;\n\tstruct rpcrdma_segment *arg_ch;\n\n\trd_ary = (struct rpcrdma_read_chunk *)&rdma_argp->rm_body.rm_chunks[0];\n\tif (rd_ary->rc_discrim != xdr_zero)\n\t\treturn be32_to_cpu(rd_ary->rc_target.rs_handle);\n\n\tif (wr_ary && be32_to_cpu(wr_ary->wc_nchunks)) {\n\t\targ_ch = &wr_ary->wc_array[0].wc_target;\n\t\treturn be32_to_cpu(arg_ch->rs_handle);\n\t}\n\n\tif (rp_ary && be32_to_cpu(rp_ary->wc_nchunks)) {\n\t\targ_ch = &rp_ary->wc_array[0].wc_target;\n\t\treturn be32_to_cpu(arg_ch->rs_handle);\n\t}\n\n\treturn 0;\n}\n\n/* Assumptions:\n * - The specified write_len can be represented in sc_max_sge * PAGE_SIZE\n */\nstatic int send_write(struct svcxprt_rdma *xprt, struct svc_rqst *rqstp,\n\t\t      u32 rmr, u64 to,\n\t\t      u32 xdr_off, int write_len,\n\t\t      struct svc_rdma_req_map *vec)\n{\n\tstruct ib_rdma_wr write_wr;\n\tstruct ib_sge *sge;\n\tint xdr_sge_no;\n\tint sge_no;\n\tint sge_bytes;\n\tint sge_off;\n\tint bc;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\n\tif (vec->count > RPCSVC_MAXPAGES) {\n\t\tpr_err(\"svcrdma: Too many pages (%lu)\\n\", vec->count);\n\t\treturn -EIO;\n\t}\n\n\tdprintk(\"svcrdma: RDMA_WRITE rmr=%x, to=%llx, xdr_off=%d, \"\n\t\t\"write_len=%d, vec->sge=%p, vec->count=%lu\\n\",\n\t\trmr, (unsigned long long)to, xdr_off,\n\t\twrite_len, vec->sge, vec->count);\n\n\tctxt = svc_rdma_get_context(xprt);\n\tctxt->direction = DMA_TO_DEVICE;\n\tsge = ctxt->sge;\n\n\t/* Find the SGE associated with xdr_off */\n\tfor (bc = xdr_off, xdr_sge_no = 1; bc && xdr_sge_no < vec->count;\n\t     xdr_sge_no++) {\n\t\tif (vec->sge[xdr_sge_no].iov_len > bc)\n\t\t\tbreak;\n\t\tbc -= vec->sge[xdr_sge_no].iov_len;\n\t}\n\n\tsge_off = bc;\n\tbc = write_len;\n\tsge_no = 0;\n\n\t/* Copy the remaining SGE */\n\twhile (bc != 0) {\n\t\tsge_bytes = min_t(size_t,\n\t\t\t  bc, vec->sge[xdr_sge_no].iov_len-sge_off);\n\t\tsge[sge_no].length = sge_bytes;\n\t\tsge[sge_no].addr =\n\t\t\tdma_map_xdr(xprt, &rqstp->rq_res, xdr_off,\n\t\t\t\t    sge_bytes, DMA_TO_DEVICE);\n\t\txdr_off += sge_bytes;\n\t\tif (ib_dma_mapping_error(xprt->sc_cm_id->device,\n\t\t\t\t\t sge[sge_no].addr))\n\t\t\tgoto err;\n\t\tsvc_rdma_count_mappings(xprt, ctxt);\n\t\tsge[sge_no].lkey = xprt->sc_pd->local_dma_lkey;\n\t\tctxt->count++;\n\t\tsge_off = 0;\n\t\tsge_no++;\n\t\txdr_sge_no++;\n\t\tif (xdr_sge_no > vec->count) {\n\t\t\tpr_err(\"svcrdma: Too many sges (%d)\\n\", xdr_sge_no);\n\t\t\tgoto err;\n\t\t}\n\t\tbc -= sge_bytes;\n\t\tif (sge_no == xprt->sc_max_sge)\n\t\t\tbreak;\n\t}\n\n\t/* Prepare WRITE WR */\n\tmemset(&write_wr, 0, sizeof write_wr);\n\tctxt->cqe.done = svc_rdma_wc_write;\n\twrite_wr.wr.wr_cqe = &ctxt->cqe;\n\twrite_wr.wr.sg_list = &sge[0];\n\twrite_wr.wr.num_sge = sge_no;\n\twrite_wr.wr.opcode = IB_WR_RDMA_WRITE;\n\twrite_wr.wr.send_flags = IB_SEND_SIGNALED;\n\twrite_wr.rkey = rmr;\n\twrite_wr.remote_addr = to;\n\n\t/* Post It */\n\tatomic_inc(&rdma_stat_write);\n\tif (svc_rdma_send(xprt, &write_wr.wr))\n\t\tgoto err;\n\treturn write_len - bc;\n err:\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 0);\n\treturn -EIO;\n}\n\nnoinline\nstatic int send_write_chunks(struct svcxprt_rdma *xprt,\n\t\t\t     struct rpcrdma_write_array *wr_ary,\n\t\t\t     struct rpcrdma_msg *rdma_resp,\n\t\t\t     struct svc_rqst *rqstp,\n\t\t\t     struct svc_rdma_req_map *vec)\n{\n\tu32 xfer_len = rqstp->rq_res.page_len;\n\tint write_len;\n\tu32 xdr_off;\n\tint chunk_off;\n\tint chunk_no;\n\tint nchunks;\n\tstruct rpcrdma_write_array *res_ary;\n\tint ret;\n\n\tres_ary = (struct rpcrdma_write_array *)\n\t\t&rdma_resp->rm_body.rm_chunks[1];\n\n\t/* Write chunks start at the pagelist */\n\tnchunks = be32_to_cpu(wr_ary->wc_nchunks);\n\tfor (xdr_off = rqstp->rq_res.head[0].iov_len, chunk_no = 0;\n\t     xfer_len && chunk_no < nchunks;\n\t     chunk_no++) {\n\t\tstruct rpcrdma_segment *arg_ch;\n\t\tu64 rs_offset;\n\n\t\targ_ch = &wr_ary->wc_array[chunk_no].wc_target;\n\t\twrite_len = min(xfer_len, be32_to_cpu(arg_ch->rs_length));\n\n\t\t/* Prepare the response chunk given the length actually\n\t\t * written */\n\t\txdr_decode_hyper((__be32 *)&arg_ch->rs_offset, &rs_offset);\n\t\tsvc_rdma_xdr_encode_array_chunk(res_ary, chunk_no,\n\t\t\t\t\t\targ_ch->rs_handle,\n\t\t\t\t\t\targ_ch->rs_offset,\n\t\t\t\t\t\twrite_len);\n\t\tchunk_off = 0;\n\t\twhile (write_len) {\n\t\t\tret = send_write(xprt, rqstp,\n\t\t\t\t\t be32_to_cpu(arg_ch->rs_handle),\n\t\t\t\t\t rs_offset + chunk_off,\n\t\t\t\t\t xdr_off,\n\t\t\t\t\t write_len,\n\t\t\t\t\t vec);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto out_err;\n\t\t\tchunk_off += ret;\n\t\t\txdr_off += ret;\n\t\t\txfer_len -= ret;\n\t\t\twrite_len -= ret;\n\t\t}\n\t}\n\t/* Update the req with the number of chunks actually used */\n\tsvc_rdma_xdr_encode_write_list(rdma_resp, chunk_no);\n\n\treturn rqstp->rq_res.page_len;\n\nout_err:\n\tpr_err(\"svcrdma: failed to send write chunks, rc=%d\\n\", ret);\n\treturn -EIO;\n}\n\nnoinline\nstatic int send_reply_chunks(struct svcxprt_rdma *xprt,\n\t\t\t     struct rpcrdma_write_array *rp_ary,\n\t\t\t     struct rpcrdma_msg *rdma_resp,\n\t\t\t     struct svc_rqst *rqstp,\n\t\t\t     struct svc_rdma_req_map *vec)\n{\n\tu32 xfer_len = rqstp->rq_res.len;\n\tint write_len;\n\tu32 xdr_off;\n\tint chunk_no;\n\tint chunk_off;\n\tint nchunks;\n\tstruct rpcrdma_segment *ch;\n\tstruct rpcrdma_write_array *res_ary;\n\tint ret;\n\n\t/* XXX: need to fix when reply lists occur with read-list and or\n\t * write-list */\n\tres_ary = (struct rpcrdma_write_array *)\n\t\t&rdma_resp->rm_body.rm_chunks[2];\n\n\t/* xdr offset starts at RPC message */\n\tnchunks = be32_to_cpu(rp_ary->wc_nchunks);\n\tfor (xdr_off = 0, chunk_no = 0;\n\t     xfer_len && chunk_no < nchunks;\n\t     chunk_no++) {\n\t\tu64 rs_offset;\n\t\tch = &rp_ary->wc_array[chunk_no].wc_target;\n\t\twrite_len = min(xfer_len, be32_to_cpu(ch->rs_length));\n\n\t\t/* Prepare the reply chunk given the length actually\n\t\t * written */\n\t\txdr_decode_hyper((__be32 *)&ch->rs_offset, &rs_offset);\n\t\tsvc_rdma_xdr_encode_array_chunk(res_ary, chunk_no,\n\t\t\t\t\t\tch->rs_handle, ch->rs_offset,\n\t\t\t\t\t\twrite_len);\n\t\tchunk_off = 0;\n\t\twhile (write_len) {\n\t\t\tret = send_write(xprt, rqstp,\n\t\t\t\t\t be32_to_cpu(ch->rs_handle),\n\t\t\t\t\t rs_offset + chunk_off,\n\t\t\t\t\t xdr_off,\n\t\t\t\t\t write_len,\n\t\t\t\t\t vec);\n\t\t\tif (ret <= 0)\n\t\t\t\tgoto out_err;\n\t\t\tchunk_off += ret;\n\t\t\txdr_off += ret;\n\t\t\txfer_len -= ret;\n\t\t\twrite_len -= ret;\n\t\t}\n\t}\n\t/* Update the req with the number of chunks actually used */\n\tsvc_rdma_xdr_encode_reply_array(res_ary, chunk_no);\n\n\treturn rqstp->rq_res.len;\n\nout_err:\n\tpr_err(\"svcrdma: failed to send reply chunks, rc=%d\\n\", ret);\n\treturn -EIO;\n}\n\n/* This function prepares the portion of the RPCRDMA message to be\n * sent in the RDMA_SEND. This function is called after data sent via\n * RDMA has already been transmitted. There are three cases:\n * - The RPCRDMA header, RPC header, and payload are all sent in a\n *   single RDMA_SEND. This is the \"inline\" case.\n * - The RPCRDMA header and some portion of the RPC header and data\n *   are sent via this RDMA_SEND and another portion of the data is\n *   sent via RDMA.\n * - The RPCRDMA header [NOMSG] is sent in this RDMA_SEND and the RPC\n *   header and data are all transmitted via RDMA.\n * In all three cases, this function prepares the RPCRDMA header in\n * sge[0], the 'type' parameter indicates the type to place in the\n * RPCRDMA header, and the 'byte_count' field indicates how much of\n * the XDR to include in this RDMA_SEND. NB: The offset of the payload\n * to send is zero in the XDR.\n */\nstatic int send_reply(struct svcxprt_rdma *rdma,\n\t\t      struct svc_rqst *rqstp,\n\t\t      struct page *page,\n\t\t      struct rpcrdma_msg *rdma_resp,\n\t\t      struct svc_rdma_req_map *vec,\n\t\t      int byte_count,\n\t\t      u32 inv_rkey)\n{\n\tstruct svc_rdma_op_ctxt *ctxt;\n\tstruct ib_send_wr send_wr;\n\tu32 xdr_off;\n\tint sge_no;\n\tint sge_bytes;\n\tint page_no;\n\tint pages;\n\tint ret = -EIO;\n\n\t/* Prepare the context */\n\tctxt = svc_rdma_get_context(rdma);\n\tctxt->direction = DMA_TO_DEVICE;\n\tctxt->pages[0] = page;\n\tctxt->count = 1;\n\n\t/* Prepare the SGE for the RPCRDMA Header */\n\tctxt->sge[0].lkey = rdma->sc_pd->local_dma_lkey;\n\tctxt->sge[0].length =\n\t    svc_rdma_xdr_get_reply_hdr_len((__be32 *)rdma_resp);\n\tctxt->sge[0].addr =\n\t    ib_dma_map_page(rdma->sc_cm_id->device, page, 0,\n\t\t\t    ctxt->sge[0].length, DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(rdma->sc_cm_id->device, ctxt->sge[0].addr))\n\t\tgoto err;\n\tsvc_rdma_count_mappings(rdma, ctxt);\n\n\tctxt->direction = DMA_TO_DEVICE;\n\n\t/* Map the payload indicated by 'byte_count' */\n\txdr_off = 0;\n\tfor (sge_no = 1; byte_count && sge_no < vec->count; sge_no++) {\n\t\tsge_bytes = min_t(size_t, vec->sge[sge_no].iov_len, byte_count);\n\t\tbyte_count -= sge_bytes;\n\t\tctxt->sge[sge_no].addr =\n\t\t\tdma_map_xdr(rdma, &rqstp->rq_res, xdr_off,\n\t\t\t\t    sge_bytes, DMA_TO_DEVICE);\n\t\txdr_off += sge_bytes;\n\t\tif (ib_dma_mapping_error(rdma->sc_cm_id->device,\n\t\t\t\t\t ctxt->sge[sge_no].addr))\n\t\t\tgoto err;\n\t\tsvc_rdma_count_mappings(rdma, ctxt);\n\t\tctxt->sge[sge_no].lkey = rdma->sc_pd->local_dma_lkey;\n\t\tctxt->sge[sge_no].length = sge_bytes;\n\t}\n\tif (byte_count != 0) {\n\t\tpr_err(\"svcrdma: Could not map %d bytes\\n\", byte_count);\n\t\tgoto err;\n\t}\n\n\t/* Save all respages in the ctxt and remove them from the\n\t * respages array. They are our pages until the I/O\n\t * completes.\n\t */\n\tpages = rqstp->rq_next_page - rqstp->rq_respages;\n\tfor (page_no = 0; page_no < pages; page_no++) {\n\t\tctxt->pages[page_no+1] = rqstp->rq_respages[page_no];\n\t\tctxt->count++;\n\t\trqstp->rq_respages[page_no] = NULL;\n\t}\n\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\n\tif (sge_no > rdma->sc_max_sge) {\n\t\tpr_err(\"svcrdma: Too many sges (%d)\\n\", sge_no);\n\t\tgoto err;\n\t}\n\tmemset(&send_wr, 0, sizeof send_wr);\n\tctxt->cqe.done = svc_rdma_wc_send;\n\tsend_wr.wr_cqe = &ctxt->cqe;\n\tsend_wr.sg_list = ctxt->sge;\n\tsend_wr.num_sge = sge_no;\n\tif (inv_rkey) {\n\t\tsend_wr.opcode = IB_WR_SEND_WITH_INV;\n\t\tsend_wr.ex.invalidate_rkey = inv_rkey;\n\t} else\n\t\tsend_wr.opcode = IB_WR_SEND;\n\tsend_wr.send_flags =  IB_SEND_SIGNALED;\n\n\tret = svc_rdma_send(rdma, &send_wr);\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\n\n err:\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 1);\n\treturn ret;\n}\n\nvoid svc_rdma_prep_reply_hdr(struct svc_rqst *rqstp)\n{\n}\n\nint svc_rdma_sendto(struct svc_rqst *rqstp)\n{\n\tstruct svc_xprt *xprt = rqstp->rq_xprt;\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tstruct rpcrdma_msg *rdma_argp;\n\tstruct rpcrdma_msg *rdma_resp;\n\tstruct rpcrdma_write_array *wr_ary, *rp_ary;\n\tint ret;\n\tint inline_bytes;\n\tstruct page *res_page;\n\tstruct svc_rdma_req_map *vec;\n\tu32 inv_rkey;\n\t__be32 *p;\n\n\tdprintk(\"svcrdma: sending response for rqstp=%p\\n\", rqstp);\n\n\t/* Get the RDMA request header. The receive logic always\n\t * places this at the start of page 0.\n\t */\n\trdma_argp = page_address(rqstp->rq_pages[0]);\n\tsvc_rdma_get_write_arrays(rdma_argp, &wr_ary, &rp_ary);\n\n\tinv_rkey = 0;\n\tif (rdma->sc_snd_w_inv)\n\t\tinv_rkey = svc_rdma_get_inv_rkey(rdma_argp, wr_ary, rp_ary);\n\n\t/* Build an req vec for the XDR */\n\tvec = svc_rdma_get_req_map(rdma);\n\tret = svc_rdma_map_xdr(rdma, &rqstp->rq_res, vec, wr_ary != NULL);\n\tif (ret)\n\t\tgoto err0;\n\tinline_bytes = rqstp->rq_res.len;\n\n\t/* Create the RDMA response header. xprt->xpt_mutex,\n\t * acquired in svc_send(), serializes RPC replies. The\n\t * code path below that inserts the credit grant value\n\t * into each transport header runs only inside this\n\t * critical section.\n\t */\n\tret = -ENOMEM;\n\tres_page = alloc_page(GFP_KERNEL);\n\tif (!res_page)\n\t\tgoto err0;\n\trdma_resp = page_address(res_page);\n\n\tp = &rdma_resp->rm_xid;\n\t*p++ = rdma_argp->rm_xid;\n\t*p++ = rdma_argp->rm_vers;\n\t*p++ = rdma->sc_fc_credits;\n\t*p++ = rp_ary ? rdma_nomsg : rdma_msg;\n\n\t/* Start with empty chunks */\n\t*p++ = xdr_zero;\n\t*p++ = xdr_zero;\n\t*p   = xdr_zero;\n\n\t/* Send any write-chunk data and build resp write-list */\n\tif (wr_ary) {\n\t\tret = send_write_chunks(rdma, wr_ary, rdma_resp, rqstp, vec);\n\t\tif (ret < 0)\n\t\t\tgoto err1;\n\t\tinline_bytes -= ret + xdr_padsize(ret);\n\t}\n\n\t/* Send any reply-list data and update resp reply-list */\n\tif (rp_ary) {\n\t\tret = send_reply_chunks(rdma, rp_ary, rdma_resp, rqstp, vec);\n\t\tif (ret < 0)\n\t\t\tgoto err1;\n\t\tinline_bytes -= ret;\n\t}\n\n\t/* Post a fresh Receive buffer _before_ sending the reply */\n\tret = svc_rdma_post_recv(rdma, GFP_KERNEL);\n\tif (ret)\n\t\tgoto err1;\n\n\tret = send_reply(rdma, rqstp, res_page, rdma_resp, vec,\n\t\t\t inline_bytes, inv_rkey);\n\tif (ret < 0)\n\t\tgoto err0;\n\n\tsvc_rdma_put_req_map(rdma, vec);\n\tdprintk(\"svcrdma: send_reply returns %d\\n\", ret);\n\treturn ret;\n\n err1:\n\tput_page(res_page);\n err0:\n\tsvc_rdma_put_req_map(rdma, vec);\n\tpr_err(\"svcrdma: Could not send reply, err=%d. Closing transport.\\n\",\n\t       ret);\n\tset_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);\n\treturn -ENOTCONN;\n}\n\nvoid svc_rdma_send_error(struct svcxprt_rdma *xprt, struct rpcrdma_msg *rmsgp,\n\t\t\t int status)\n{\n\tstruct ib_send_wr err_wr;\n\tstruct page *p;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\tenum rpcrdma_errcode err;\n\t__be32 *va;\n\tint length;\n\tint ret;\n\n\tret = svc_rdma_repost_recv(xprt, GFP_KERNEL);\n\tif (ret)\n\t\treturn;\n\n\tp = alloc_page(GFP_KERNEL);\n\tif (!p)\n\t\treturn;\n\tva = page_address(p);\n\n\t/* XDR encode an error reply */\n\terr = ERR_CHUNK;\n\tif (status == -EPROTONOSUPPORT)\n\t\terr = ERR_VERS;\n\tlength = svc_rdma_xdr_encode_error(xprt, rmsgp, err, va);\n\n\tctxt = svc_rdma_get_context(xprt);\n\tctxt->direction = DMA_TO_DEVICE;\n\tctxt->count = 1;\n\tctxt->pages[0] = p;\n\n\t/* Prepare SGE for local address */\n\tctxt->sge[0].lkey = xprt->sc_pd->local_dma_lkey;\n\tctxt->sge[0].length = length;\n\tctxt->sge[0].addr = ib_dma_map_page(xprt->sc_cm_id->device,\n\t\t\t\t\t    p, 0, length, DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(xprt->sc_cm_id->device, ctxt->sge[0].addr)) {\n\t\tdprintk(\"svcrdma: Error mapping buffer for protocol error\\n\");\n\t\tsvc_rdma_put_context(ctxt, 1);\n\t\treturn;\n\t}\n\tsvc_rdma_count_mappings(xprt, ctxt);\n\n\t/* Prepare SEND WR */\n\tmemset(&err_wr, 0, sizeof(err_wr));\n\tctxt->cqe.done = svc_rdma_wc_send;\n\terr_wr.wr_cqe = &ctxt->cqe;\n\terr_wr.sg_list = ctxt->sge;\n\terr_wr.num_sge = 1;\n\terr_wr.opcode = IB_WR_SEND;\n\terr_wr.send_flags = IB_SEND_SIGNALED;\n\n\t/* Post It */\n\tret = svc_rdma_send(xprt, &err_wr);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: Error %d posting send for protocol error\\n\",\n\t\t\tret);\n\t\tsvc_rdma_unmap_dma(ctxt);\n\t\tsvc_rdma_put_context(ctxt, 1);\n\t}\n}\n", "/*\n * Copyright (c) 2014 Open Grid Computing, Inc. All rights reserved.\n * Copyright (c) 2005-2007 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n#include <linux/sunrpc/svc_xprt.h>\n#include <linux/sunrpc/addr.h>\n#include <linux/sunrpc/debug.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <linux/interrupt.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/workqueue.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n#include <linux/sunrpc/svc_rdma.h>\n#include <linux/export.h>\n#include \"xprt_rdma.h\"\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\nstatic struct svcxprt_rdma *rdma_create_xprt(struct svc_serv *, int);\nstatic struct svc_xprt *svc_rdma_create(struct svc_serv *serv,\n\t\t\t\t\tstruct net *net,\n\t\t\t\t\tstruct sockaddr *sa, int salen,\n\t\t\t\t\tint flags);\nstatic struct svc_xprt *svc_rdma_accept(struct svc_xprt *xprt);\nstatic void svc_rdma_release_rqst(struct svc_rqst *);\nstatic void svc_rdma_detach(struct svc_xprt *xprt);\nstatic void svc_rdma_free(struct svc_xprt *xprt);\nstatic int svc_rdma_has_wspace(struct svc_xprt *xprt);\nstatic int svc_rdma_secure_port(struct svc_rqst *);\nstatic void svc_rdma_kill_temp_xprt(struct svc_xprt *);\n\nstatic struct svc_xprt_ops svc_rdma_ops = {\n\t.xpo_create = svc_rdma_create,\n\t.xpo_recvfrom = svc_rdma_recvfrom,\n\t.xpo_sendto = svc_rdma_sendto,\n\t.xpo_release_rqst = svc_rdma_release_rqst,\n\t.xpo_detach = svc_rdma_detach,\n\t.xpo_free = svc_rdma_free,\n\t.xpo_prep_reply_hdr = svc_rdma_prep_reply_hdr,\n\t.xpo_has_wspace = svc_rdma_has_wspace,\n\t.xpo_accept = svc_rdma_accept,\n\t.xpo_secure_port = svc_rdma_secure_port,\n\t.xpo_kill_temp_xprt = svc_rdma_kill_temp_xprt,\n};\n\nstruct svc_xprt_class svc_rdma_class = {\n\t.xcl_name = \"rdma\",\n\t.xcl_owner = THIS_MODULE,\n\t.xcl_ops = &svc_rdma_ops,\n\t.xcl_max_payload = RPCSVC_MAXPAYLOAD_RDMA,\n\t.xcl_ident = XPRT_TRANSPORT_RDMA,\n};\n\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\nstatic struct svc_xprt *svc_rdma_bc_create(struct svc_serv *, struct net *,\n\t\t\t\t\t   struct sockaddr *, int, int);\nstatic void svc_rdma_bc_detach(struct svc_xprt *);\nstatic void svc_rdma_bc_free(struct svc_xprt *);\n\nstatic struct svc_xprt_ops svc_rdma_bc_ops = {\n\t.xpo_create = svc_rdma_bc_create,\n\t.xpo_detach = svc_rdma_bc_detach,\n\t.xpo_free = svc_rdma_bc_free,\n\t.xpo_prep_reply_hdr = svc_rdma_prep_reply_hdr,\n\t.xpo_secure_port = svc_rdma_secure_port,\n};\n\nstruct svc_xprt_class svc_rdma_bc_class = {\n\t.xcl_name = \"rdma-bc\",\n\t.xcl_owner = THIS_MODULE,\n\t.xcl_ops = &svc_rdma_bc_ops,\n\t.xcl_max_payload = (1024 - RPCRDMA_HDRLEN_MIN)\n};\n\nstatic struct svc_xprt *svc_rdma_bc_create(struct svc_serv *serv,\n\t\t\t\t\t   struct net *net,\n\t\t\t\t\t   struct sockaddr *sa, int salen,\n\t\t\t\t\t   int flags)\n{\n\tstruct svcxprt_rdma *cma_xprt;\n\tstruct svc_xprt *xprt;\n\n\tcma_xprt = rdma_create_xprt(serv, 0);\n\tif (!cma_xprt)\n\t\treturn ERR_PTR(-ENOMEM);\n\txprt = &cma_xprt->sc_xprt;\n\n\tsvc_xprt_init(net, &svc_rdma_bc_class, xprt, serv);\n\tset_bit(XPT_CONG_CTRL, &xprt->xpt_flags);\n\tserv->sv_bc_xprt = xprt;\n\n\tdprintk(\"svcrdma: %s(%p)\\n\", __func__, xprt);\n\treturn xprt;\n}\n\nstatic void svc_rdma_bc_detach(struct svc_xprt *xprt)\n{\n\tdprintk(\"svcrdma: %s(%p)\\n\", __func__, xprt);\n}\n\nstatic void svc_rdma_bc_free(struct svc_xprt *xprt)\n{\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\n\tdprintk(\"svcrdma: %s(%p)\\n\", __func__, xprt);\n\tif (xprt)\n\t\tkfree(rdma);\n}\n#endif\t/* CONFIG_SUNRPC_BACKCHANNEL */\n\nstatic struct svc_rdma_op_ctxt *alloc_ctxt(struct svcxprt_rdma *xprt,\n\t\t\t\t\t   gfp_t flags)\n{\n\tstruct svc_rdma_op_ctxt *ctxt;\n\n\tctxt = kmalloc(sizeof(*ctxt), flags);\n\tif (ctxt) {\n\t\tctxt->xprt = xprt;\n\t\tINIT_LIST_HEAD(&ctxt->list);\n\t}\n\treturn ctxt;\n}\n\nstatic bool svc_rdma_prealloc_ctxts(struct svcxprt_rdma *xprt)\n{\n\tunsigned int i;\n\n\t/* Each RPC/RDMA credit can consume a number of send\n\t * and receive WQEs. One ctxt is allocated for each.\n\t */\n\ti = xprt->sc_sq_depth + xprt->sc_rq_depth;\n\n\twhile (i--) {\n\t\tstruct svc_rdma_op_ctxt *ctxt;\n\n\t\tctxt = alloc_ctxt(xprt, GFP_KERNEL);\n\t\tif (!ctxt) {\n\t\t\tdprintk(\"svcrdma: No memory for RDMA ctxt\\n\");\n\t\t\treturn false;\n\t\t}\n\t\tlist_add(&ctxt->list, &xprt->sc_ctxts);\n\t}\n\treturn true;\n}\n\nstruct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *xprt)\n{\n\tstruct svc_rdma_op_ctxt *ctxt = NULL;\n\n\tspin_lock(&xprt->sc_ctxt_lock);\n\txprt->sc_ctxt_used++;\n\tif (list_empty(&xprt->sc_ctxts))\n\t\tgoto out_empty;\n\n\tctxt = list_first_entry(&xprt->sc_ctxts,\n\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\tlist_del(&ctxt->list);\n\tspin_unlock(&xprt->sc_ctxt_lock);\n\nout:\n\tctxt->count = 0;\n\tctxt->mapped_sges = 0;\n\tctxt->frmr = NULL;\n\treturn ctxt;\n\nout_empty:\n\t/* Either pre-allocation missed the mark, or send\n\t * queue accounting is broken.\n\t */\n\tspin_unlock(&xprt->sc_ctxt_lock);\n\n\tctxt = alloc_ctxt(xprt, GFP_NOIO);\n\tif (ctxt)\n\t\tgoto out;\n\n\tspin_lock(&xprt->sc_ctxt_lock);\n\txprt->sc_ctxt_used--;\n\tspin_unlock(&xprt->sc_ctxt_lock);\n\tWARN_ONCE(1, \"svcrdma: empty RDMA ctxt list?\\n\");\n\treturn NULL;\n}\n\nvoid svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt)\n{\n\tstruct svcxprt_rdma *xprt = ctxt->xprt;\n\tstruct ib_device *device = xprt->sc_cm_id->device;\n\tu32 lkey = xprt->sc_pd->local_dma_lkey;\n\tunsigned int i;\n\n\tfor (i = 0; i < ctxt->mapped_sges; i++) {\n\t\t/*\n\t\t * Unmap the DMA addr in the SGE if the lkey matches\n\t\t * the local_dma_lkey, otherwise, ignore it since it is\n\t\t * an FRMR lkey and will be unmapped later when the\n\t\t * last WR that uses it completes.\n\t\t */\n\t\tif (ctxt->sge[i].lkey == lkey)\n\t\t\tib_dma_unmap_page(device,\n\t\t\t\t\t    ctxt->sge[i].addr,\n\t\t\t\t\t    ctxt->sge[i].length,\n\t\t\t\t\t    ctxt->direction);\n\t}\n\tctxt->mapped_sges = 0;\n}\n\nvoid svc_rdma_put_context(struct svc_rdma_op_ctxt *ctxt, int free_pages)\n{\n\tstruct svcxprt_rdma *xprt = ctxt->xprt;\n\tint i;\n\n\tif (free_pages)\n\t\tfor (i = 0; i < ctxt->count; i++)\n\t\t\tput_page(ctxt->pages[i]);\n\n\tspin_lock(&xprt->sc_ctxt_lock);\n\txprt->sc_ctxt_used--;\n\tlist_add(&ctxt->list, &xprt->sc_ctxts);\n\tspin_unlock(&xprt->sc_ctxt_lock);\n}\n\nstatic void svc_rdma_destroy_ctxts(struct svcxprt_rdma *xprt)\n{\n\twhile (!list_empty(&xprt->sc_ctxts)) {\n\t\tstruct svc_rdma_op_ctxt *ctxt;\n\n\t\tctxt = list_first_entry(&xprt->sc_ctxts,\n\t\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\t\tlist_del(&ctxt->list);\n\t\tkfree(ctxt);\n\t}\n}\n\nstatic struct svc_rdma_req_map *alloc_req_map(gfp_t flags)\n{\n\tstruct svc_rdma_req_map *map;\n\n\tmap = kmalloc(sizeof(*map), flags);\n\tif (map)\n\t\tINIT_LIST_HEAD(&map->free);\n\treturn map;\n}\n\nstatic bool svc_rdma_prealloc_maps(struct svcxprt_rdma *xprt)\n{\n\tunsigned int i;\n\n\t/* One for each receive buffer on this connection. */\n\ti = xprt->sc_max_requests;\n\n\twhile (i--) {\n\t\tstruct svc_rdma_req_map *map;\n\n\t\tmap = alloc_req_map(GFP_KERNEL);\n\t\tif (!map) {\n\t\t\tdprintk(\"svcrdma: No memory for request map\\n\");\n\t\t\treturn false;\n\t\t}\n\t\tlist_add(&map->free, &xprt->sc_maps);\n\t}\n\treturn true;\n}\n\nstruct svc_rdma_req_map *svc_rdma_get_req_map(struct svcxprt_rdma *xprt)\n{\n\tstruct svc_rdma_req_map *map = NULL;\n\n\tspin_lock(&xprt->sc_map_lock);\n\tif (list_empty(&xprt->sc_maps))\n\t\tgoto out_empty;\n\n\tmap = list_first_entry(&xprt->sc_maps,\n\t\t\t       struct svc_rdma_req_map, free);\n\tlist_del_init(&map->free);\n\tspin_unlock(&xprt->sc_map_lock);\n\nout:\n\tmap->count = 0;\n\treturn map;\n\nout_empty:\n\tspin_unlock(&xprt->sc_map_lock);\n\n\t/* Pre-allocation amount was incorrect */\n\tmap = alloc_req_map(GFP_NOIO);\n\tif (map)\n\t\tgoto out;\n\n\tWARN_ONCE(1, \"svcrdma: empty request map list?\\n\");\n\treturn NULL;\n}\n\nvoid svc_rdma_put_req_map(struct svcxprt_rdma *xprt,\n\t\t\t  struct svc_rdma_req_map *map)\n{\n\tspin_lock(&xprt->sc_map_lock);\n\tlist_add(&map->free, &xprt->sc_maps);\n\tspin_unlock(&xprt->sc_map_lock);\n}\n\nstatic void svc_rdma_destroy_maps(struct svcxprt_rdma *xprt)\n{\n\twhile (!list_empty(&xprt->sc_maps)) {\n\t\tstruct svc_rdma_req_map *map;\n\n\t\tmap = list_first_entry(&xprt->sc_maps,\n\t\t\t\t       struct svc_rdma_req_map, free);\n\t\tlist_del(&map->free);\n\t\tkfree(map);\n\t}\n}\n\n/* QP event handler */\nstatic void qp_event_handler(struct ib_event *event, void *context)\n{\n\tstruct svc_xprt *xprt = context;\n\n\tswitch (event->event) {\n\t/* These are considered benign events */\n\tcase IB_EVENT_PATH_MIG:\n\tcase IB_EVENT_COMM_EST:\n\tcase IB_EVENT_SQ_DRAINED:\n\tcase IB_EVENT_QP_LAST_WQE_REACHED:\n\t\tdprintk(\"svcrdma: QP event %s (%d) received for QP=%p\\n\",\n\t\t\tib_event_msg(event->event), event->event,\n\t\t\tevent->element.qp);\n\t\tbreak;\n\t/* These are considered fatal events */\n\tcase IB_EVENT_PATH_MIG_ERR:\n\tcase IB_EVENT_QP_FATAL:\n\tcase IB_EVENT_QP_REQ_ERR:\n\tcase IB_EVENT_QP_ACCESS_ERR:\n\tcase IB_EVENT_DEVICE_FATAL:\n\tdefault:\n\t\tdprintk(\"svcrdma: QP ERROR event %s (%d) received for QP=%p, \"\n\t\t\t\"closing transport\\n\",\n\t\t\tib_event_msg(event->event), event->event,\n\t\t\tevent->element.qp);\n\t\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\t\tbreak;\n\t}\n}\n\n/**\n * svc_rdma_wc_receive - Invoked by RDMA provider for each polled Receive WC\n * @cq:        completion queue\n * @wc:        completed WR\n *\n */\nstatic void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct svcxprt_rdma *xprt = cq->cq_context;\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\n\t/* WARNING: Only wc->wr_cqe and wc->status are reliable */\n\tctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\n\tsvc_rdma_unmap_dma(ctxt);\n\n\tif (wc->status != IB_WC_SUCCESS)\n\t\tgoto flushed;\n\n\t/* All wc fields are now known to be valid */\n\tctxt->byte_len = wc->byte_len;\n\tspin_lock(&xprt->sc_rq_dto_lock);\n\tlist_add_tail(&ctxt->list, &xprt->sc_rq_dto_q);\n\tspin_unlock(&xprt->sc_rq_dto_lock);\n\n\tset_bit(XPT_DATA, &xprt->sc_xprt.xpt_flags);\n\tif (test_bit(RDMAXPRT_CONN_PENDING, &xprt->sc_flags))\n\t\tgoto out;\n\tsvc_xprt_enqueue(&xprt->sc_xprt);\n\tgoto out;\n\nflushed:\n\tif (wc->status != IB_WC_WR_FLUSH_ERR)\n\t\tpr_warn(\"svcrdma: receive: %s (%u/0x%x)\\n\",\n\t\t\tib_wc_status_msg(wc->status),\n\t\t\twc->status, wc->vendor_err);\n\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\tsvc_rdma_put_context(ctxt, 1);\n\nout:\n\tsvc_xprt_put(&xprt->sc_xprt);\n}\n\nstatic void svc_rdma_send_wc_common(struct svcxprt_rdma *xprt,\n\t\t\t\t    struct ib_wc *wc,\n\t\t\t\t    const char *opname)\n{\n\tif (wc->status != IB_WC_SUCCESS)\n\t\tgoto err;\n\nout:\n\tatomic_inc(&xprt->sc_sq_avail);\n\twake_up(&xprt->sc_send_wait);\n\treturn;\n\nerr:\n\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\tif (wc->status != IB_WC_WR_FLUSH_ERR)\n\t\tpr_err(\"svcrdma: %s: %s (%u/0x%x)\\n\",\n\t\t       opname, ib_wc_status_msg(wc->status),\n\t\t       wc->status, wc->vendor_err);\n\tgoto out;\n}\n\nstatic void svc_rdma_send_wc_common_put(struct ib_cq *cq, struct ib_wc *wc,\n\t\t\t\t\tconst char *opname)\n{\n\tstruct svcxprt_rdma *xprt = cq->cq_context;\n\n\tsvc_rdma_send_wc_common(xprt, wc, opname);\n\tsvc_xprt_put(&xprt->sc_xprt);\n}\n\n/**\n * svc_rdma_wc_send - Invoked by RDMA provider for each polled Send WC\n * @cq:        completion queue\n * @wc:        completed WR\n *\n */\nvoid svc_rdma_wc_send(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\n\tsvc_rdma_send_wc_common_put(cq, wc, \"send\");\n\n\tctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 1);\n}\n\n/**\n * svc_rdma_wc_write - Invoked by RDMA provider for each polled Write WC\n * @cq:        completion queue\n * @wc:        completed WR\n *\n */\nvoid svc_rdma_wc_write(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\n\tsvc_rdma_send_wc_common_put(cq, wc, \"write\");\n\n\tctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 0);\n}\n\n/**\n * svc_rdma_wc_reg - Invoked by RDMA provider for each polled FASTREG WC\n * @cq:        completion queue\n * @wc:        completed WR\n *\n */\nvoid svc_rdma_wc_reg(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tsvc_rdma_send_wc_common_put(cq, wc, \"fastreg\");\n}\n\n/**\n * svc_rdma_wc_read - Invoked by RDMA provider for each polled Read WC\n * @cq:        completion queue\n * @wc:        completed WR\n *\n */\nvoid svc_rdma_wc_read(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct svcxprt_rdma *xprt = cq->cq_context;\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\n\tsvc_rdma_send_wc_common(xprt, wc, \"read\");\n\n\tctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_frmr(xprt, ctxt->frmr);\n\n\tif (test_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags)) {\n\t\tstruct svc_rdma_op_ctxt *read_hdr;\n\n\t\tread_hdr = ctxt->read_hdr;\n\t\tspin_lock(&xprt->sc_rq_dto_lock);\n\t\tlist_add_tail(&read_hdr->list,\n\t\t\t      &xprt->sc_read_complete_q);\n\t\tspin_unlock(&xprt->sc_rq_dto_lock);\n\n\t\tset_bit(XPT_DATA, &xprt->sc_xprt.xpt_flags);\n\t\tsvc_xprt_enqueue(&xprt->sc_xprt);\n\t}\n\n\tsvc_rdma_put_context(ctxt, 0);\n\tsvc_xprt_put(&xprt->sc_xprt);\n}\n\n/**\n * svc_rdma_wc_inv - Invoked by RDMA provider for each polled LOCAL_INV WC\n * @cq:        completion queue\n * @wc:        completed WR\n *\n */\nvoid svc_rdma_wc_inv(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tsvc_rdma_send_wc_common_put(cq, wc, \"localInv\");\n}\n\nstatic struct svcxprt_rdma *rdma_create_xprt(struct svc_serv *serv,\n\t\t\t\t\t     int listener)\n{\n\tstruct svcxprt_rdma *cma_xprt = kzalloc(sizeof *cma_xprt, GFP_KERNEL);\n\n\tif (!cma_xprt)\n\t\treturn NULL;\n\tsvc_xprt_init(&init_net, &svc_rdma_class, &cma_xprt->sc_xprt, serv);\n\tINIT_LIST_HEAD(&cma_xprt->sc_accept_q);\n\tINIT_LIST_HEAD(&cma_xprt->sc_rq_dto_q);\n\tINIT_LIST_HEAD(&cma_xprt->sc_read_complete_q);\n\tINIT_LIST_HEAD(&cma_xprt->sc_frmr_q);\n\tINIT_LIST_HEAD(&cma_xprt->sc_ctxts);\n\tINIT_LIST_HEAD(&cma_xprt->sc_maps);\n\tinit_waitqueue_head(&cma_xprt->sc_send_wait);\n\n\tspin_lock_init(&cma_xprt->sc_lock);\n\tspin_lock_init(&cma_xprt->sc_rq_dto_lock);\n\tspin_lock_init(&cma_xprt->sc_frmr_q_lock);\n\tspin_lock_init(&cma_xprt->sc_ctxt_lock);\n\tspin_lock_init(&cma_xprt->sc_map_lock);\n\n\t/*\n\t * Note that this implies that the underlying transport support\n\t * has some form of congestion control (see RFC 7530 section 3.1\n\t * paragraph 2). For now, we assume that all supported RDMA\n\t * transports are suitable here.\n\t */\n\tset_bit(XPT_CONG_CTRL, &cma_xprt->sc_xprt.xpt_flags);\n\n\tif (listener)\n\t\tset_bit(XPT_LISTENER, &cma_xprt->sc_xprt.xpt_flags);\n\n\treturn cma_xprt;\n}\n\nint svc_rdma_post_recv(struct svcxprt_rdma *xprt, gfp_t flags)\n{\n\tstruct ib_recv_wr recv_wr, *bad_recv_wr;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\tstruct page *page;\n\tdma_addr_t pa;\n\tint sge_no;\n\tint buflen;\n\tint ret;\n\n\tctxt = svc_rdma_get_context(xprt);\n\tbuflen = 0;\n\tctxt->direction = DMA_FROM_DEVICE;\n\tctxt->cqe.done = svc_rdma_wc_receive;\n\tfor (sge_no = 0; buflen < xprt->sc_max_req_size; sge_no++) {\n\t\tif (sge_no >= xprt->sc_max_sge) {\n\t\t\tpr_err(\"svcrdma: Too many sges (%d)\\n\", sge_no);\n\t\t\tgoto err_put_ctxt;\n\t\t}\n\t\tpage = alloc_page(flags);\n\t\tif (!page)\n\t\t\tgoto err_put_ctxt;\n\t\tctxt->pages[sge_no] = page;\n\t\tpa = ib_dma_map_page(xprt->sc_cm_id->device,\n\t\t\t\t     page, 0, PAGE_SIZE,\n\t\t\t\t     DMA_FROM_DEVICE);\n\t\tif (ib_dma_mapping_error(xprt->sc_cm_id->device, pa))\n\t\t\tgoto err_put_ctxt;\n\t\tsvc_rdma_count_mappings(xprt, ctxt);\n\t\tctxt->sge[sge_no].addr = pa;\n\t\tctxt->sge[sge_no].length = PAGE_SIZE;\n\t\tctxt->sge[sge_no].lkey = xprt->sc_pd->local_dma_lkey;\n\t\tctxt->count = sge_no + 1;\n\t\tbuflen += PAGE_SIZE;\n\t}\n\trecv_wr.next = NULL;\n\trecv_wr.sg_list = &ctxt->sge[0];\n\trecv_wr.num_sge = ctxt->count;\n\trecv_wr.wr_cqe = &ctxt->cqe;\n\n\tsvc_xprt_get(&xprt->sc_xprt);\n\tret = ib_post_recv(xprt->sc_qp, &recv_wr, &bad_recv_wr);\n\tif (ret) {\n\t\tsvc_rdma_unmap_dma(ctxt);\n\t\tsvc_rdma_put_context(ctxt, 1);\n\t\tsvc_xprt_put(&xprt->sc_xprt);\n\t}\n\treturn ret;\n\n err_put_ctxt:\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 1);\n\treturn -ENOMEM;\n}\n\nint svc_rdma_repost_recv(struct svcxprt_rdma *xprt, gfp_t flags)\n{\n\tint ret = 0;\n\n\tret = svc_rdma_post_recv(xprt, flags);\n\tif (ret) {\n\t\tpr_err(\"svcrdma: could not post a receive buffer, err=%d.\\n\",\n\t\t       ret);\n\t\tpr_err(\"svcrdma: closing transport %p.\\n\", xprt);\n\t\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\t\tret = -ENOTCONN;\n\t}\n\treturn ret;\n}\n\nstatic void\nsvc_rdma_parse_connect_private(struct svcxprt_rdma *newxprt,\n\t\t\t       struct rdma_conn_param *param)\n{\n\tconst struct rpcrdma_connect_private *pmsg = param->private_data;\n\n\tif (pmsg &&\n\t    pmsg->cp_magic == rpcrdma_cmp_magic &&\n\t    pmsg->cp_version == RPCRDMA_CMP_VERSION) {\n\t\tnewxprt->sc_snd_w_inv = pmsg->cp_flags &\n\t\t\t\t\tRPCRDMA_CMP_F_SND_W_INV_OK;\n\n\t\tdprintk(\"svcrdma: client send_size %u, recv_size %u \"\n\t\t\t\"remote inv %ssupported\\n\",\n\t\t\trpcrdma_decode_buffer_size(pmsg->cp_send_size),\n\t\t\trpcrdma_decode_buffer_size(pmsg->cp_recv_size),\n\t\t\tnewxprt->sc_snd_w_inv ? \"\" : \"un\");\n\t}\n}\n\n/*\n * This function handles the CONNECT_REQUEST event on a listening\n * endpoint. It is passed the cma_id for the _new_ connection. The context in\n * this cma_id is inherited from the listening cma_id and is the svc_xprt\n * structure for the listening endpoint.\n *\n * This function creates a new xprt for the new connection and enqueues it on\n * the accept queue for the listent xprt. When the listen thread is kicked, it\n * will call the recvfrom method on the listen xprt which will accept the new\n * connection.\n */\nstatic void handle_connect_req(struct rdma_cm_id *new_cma_id,\n\t\t\t       struct rdma_conn_param *param)\n{\n\tstruct svcxprt_rdma *listen_xprt = new_cma_id->context;\n\tstruct svcxprt_rdma *newxprt;\n\tstruct sockaddr *sa;\n\n\t/* Create a new transport */\n\tnewxprt = rdma_create_xprt(listen_xprt->sc_xprt.xpt_server, 0);\n\tif (!newxprt) {\n\t\tdprintk(\"svcrdma: failed to create new transport\\n\");\n\t\treturn;\n\t}\n\tnewxprt->sc_cm_id = new_cma_id;\n\tnew_cma_id->context = newxprt;\n\tdprintk(\"svcrdma: Creating newxprt=%p, cm_id=%p, listenxprt=%p\\n\",\n\t\tnewxprt, newxprt->sc_cm_id, listen_xprt);\n\tsvc_rdma_parse_connect_private(newxprt, param);\n\n\t/* Save client advertised inbound read limit for use later in accept. */\n\tnewxprt->sc_ord = param->initiator_depth;\n\n\t/* Set the local and remote addresses in the transport */\n\tsa = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.dst_addr;\n\tsvc_xprt_set_remote(&newxprt->sc_xprt, sa, svc_addr_len(sa));\n\tsa = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.src_addr;\n\tsvc_xprt_set_local(&newxprt->sc_xprt, sa, svc_addr_len(sa));\n\n\t/*\n\t * Enqueue the new transport on the accept queue of the listening\n\t * transport\n\t */\n\tspin_lock_bh(&listen_xprt->sc_lock);\n\tlist_add_tail(&newxprt->sc_accept_q, &listen_xprt->sc_accept_q);\n\tspin_unlock_bh(&listen_xprt->sc_lock);\n\n\tset_bit(XPT_CONN, &listen_xprt->sc_xprt.xpt_flags);\n\tsvc_xprt_enqueue(&listen_xprt->sc_xprt);\n}\n\n/*\n * Handles events generated on the listening endpoint. These events will be\n * either be incoming connect requests or adapter removal  events.\n */\nstatic int rdma_listen_handler(struct rdma_cm_id *cma_id,\n\t\t\t       struct rdma_cm_event *event)\n{\n\tstruct svcxprt_rdma *xprt = cma_id->context;\n\tint ret = 0;\n\n\tswitch (event->event) {\n\tcase RDMA_CM_EVENT_CONNECT_REQUEST:\n\t\tdprintk(\"svcrdma: Connect request on cma_id=%p, xprt = %p, \"\n\t\t\t\"event = %s (%d)\\n\", cma_id, cma_id->context,\n\t\t\trdma_event_msg(event->event), event->event);\n\t\thandle_connect_req(cma_id, &event->param.conn);\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\t/* Accept complete */\n\t\tdprintk(\"svcrdma: Connection completed on LISTEN xprt=%p, \"\n\t\t\t\"cm_id=%p\\n\", xprt, cma_id);\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\t\tdprintk(\"svcrdma: Device removal xprt=%p, cm_id=%p\\n\",\n\t\t\txprt, cma_id);\n\t\tif (xprt)\n\t\t\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\t\tbreak;\n\n\tdefault:\n\t\tdprintk(\"svcrdma: Unexpected event on listening endpoint %p, \"\n\t\t\t\"event = %s (%d)\\n\", cma_id,\n\t\t\trdma_event_msg(event->event), event->event);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int rdma_cma_handler(struct rdma_cm_id *cma_id,\n\t\t\t    struct rdma_cm_event *event)\n{\n\tstruct svc_xprt *xprt = cma_id->context;\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tswitch (event->event) {\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\t/* Accept complete */\n\t\tsvc_xprt_get(xprt);\n\t\tdprintk(\"svcrdma: Connection completed on DTO xprt=%p, \"\n\t\t\t\"cm_id=%p\\n\", xprt, cma_id);\n\t\tclear_bit(RDMAXPRT_CONN_PENDING, &rdma->sc_flags);\n\t\tsvc_xprt_enqueue(xprt);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\t\tdprintk(\"svcrdma: Disconnect on DTO xprt=%p, cm_id=%p\\n\",\n\t\t\txprt, cma_id);\n\t\tif (xprt) {\n\t\t\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\t\t\tsvc_xprt_enqueue(xprt);\n\t\t\tsvc_xprt_put(xprt);\n\t\t}\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\t\tdprintk(\"svcrdma: Device removal cma_id=%p, xprt = %p, \"\n\t\t\t\"event = %s (%d)\\n\", cma_id, xprt,\n\t\t\trdma_event_msg(event->event), event->event);\n\t\tif (xprt) {\n\t\t\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\t\t\tsvc_xprt_enqueue(xprt);\n\t\t\tsvc_xprt_put(xprt);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"svcrdma: Unexpected event on DTO endpoint %p, \"\n\t\t\t\"event = %s (%d)\\n\", cma_id,\n\t\t\trdma_event_msg(event->event), event->event);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n/*\n * Create a listening RDMA service endpoint.\n */\nstatic struct svc_xprt *svc_rdma_create(struct svc_serv *serv,\n\t\t\t\t\tstruct net *net,\n\t\t\t\t\tstruct sockaddr *sa, int salen,\n\t\t\t\t\tint flags)\n{\n\tstruct rdma_cm_id *listen_id;\n\tstruct svcxprt_rdma *cma_xprt;\n\tint ret;\n\n\tdprintk(\"svcrdma: Creating RDMA socket\\n\");\n\tif ((sa->sa_family != AF_INET) && (sa->sa_family != AF_INET6)) {\n\t\tdprintk(\"svcrdma: Address family %d is not supported.\\n\", sa->sa_family);\n\t\treturn ERR_PTR(-EAFNOSUPPORT);\n\t}\n\tcma_xprt = rdma_create_xprt(serv, 1);\n\tif (!cma_xprt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tlisten_id = rdma_create_id(&init_net, rdma_listen_handler, cma_xprt,\n\t\t\t\t   RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(listen_id)) {\n\t\tret = PTR_ERR(listen_id);\n\t\tdprintk(\"svcrdma: rdma_create_id failed = %d\\n\", ret);\n\t\tgoto err0;\n\t}\n\n\t/* Allow both IPv4 and IPv6 sockets to bind a single port\n\t * at the same time.\n\t */\n#if IS_ENABLED(CONFIG_IPV6)\n\tret = rdma_set_afonly(listen_id, 1);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: rdma_set_afonly failed = %d\\n\", ret);\n\t\tgoto err1;\n\t}\n#endif\n\tret = rdma_bind_addr(listen_id, sa);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: rdma_bind_addr failed = %d\\n\", ret);\n\t\tgoto err1;\n\t}\n\tcma_xprt->sc_cm_id = listen_id;\n\n\tret = rdma_listen(listen_id, RPCRDMA_LISTEN_BACKLOG);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: rdma_listen failed = %d\\n\", ret);\n\t\tgoto err1;\n\t}\n\n\t/*\n\t * We need to use the address from the cm_id in case the\n\t * caller specified 0 for the port number.\n\t */\n\tsa = (struct sockaddr *)&cma_xprt->sc_cm_id->route.addr.src_addr;\n\tsvc_xprt_set_local(&cma_xprt->sc_xprt, sa, salen);\n\n\treturn &cma_xprt->sc_xprt;\n\n err1:\n\trdma_destroy_id(listen_id);\n err0:\n\tkfree(cma_xprt);\n\treturn ERR_PTR(ret);\n}\n\nstatic struct svc_rdma_fastreg_mr *rdma_alloc_frmr(struct svcxprt_rdma *xprt)\n{\n\tstruct ib_mr *mr;\n\tstruct scatterlist *sg;\n\tstruct svc_rdma_fastreg_mr *frmr;\n\tu32 num_sg;\n\n\tfrmr = kmalloc(sizeof(*frmr), GFP_KERNEL);\n\tif (!frmr)\n\t\tgoto err;\n\n\tnum_sg = min_t(u32, RPCSVC_MAXPAGES, xprt->sc_frmr_pg_list_len);\n\tmr = ib_alloc_mr(xprt->sc_pd, IB_MR_TYPE_MEM_REG, num_sg);\n\tif (IS_ERR(mr))\n\t\tgoto err_free_frmr;\n\n\tsg = kcalloc(RPCSVC_MAXPAGES, sizeof(*sg), GFP_KERNEL);\n\tif (!sg)\n\t\tgoto err_free_mr;\n\n\tsg_init_table(sg, RPCSVC_MAXPAGES);\n\n\tfrmr->mr = mr;\n\tfrmr->sg = sg;\n\tINIT_LIST_HEAD(&frmr->frmr_list);\n\treturn frmr;\n\n err_free_mr:\n\tib_dereg_mr(mr);\n err_free_frmr:\n\tkfree(frmr);\n err:\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic void rdma_dealloc_frmr_q(struct svcxprt_rdma *xprt)\n{\n\tstruct svc_rdma_fastreg_mr *frmr;\n\n\twhile (!list_empty(&xprt->sc_frmr_q)) {\n\t\tfrmr = list_entry(xprt->sc_frmr_q.next,\n\t\t\t\t  struct svc_rdma_fastreg_mr, frmr_list);\n\t\tlist_del_init(&frmr->frmr_list);\n\t\tkfree(frmr->sg);\n\t\tib_dereg_mr(frmr->mr);\n\t\tkfree(frmr);\n\t}\n}\n\nstruct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *rdma)\n{\n\tstruct svc_rdma_fastreg_mr *frmr = NULL;\n\n\tspin_lock(&rdma->sc_frmr_q_lock);\n\tif (!list_empty(&rdma->sc_frmr_q)) {\n\t\tfrmr = list_entry(rdma->sc_frmr_q.next,\n\t\t\t\t  struct svc_rdma_fastreg_mr, frmr_list);\n\t\tlist_del_init(&frmr->frmr_list);\n\t\tfrmr->sg_nents = 0;\n\t}\n\tspin_unlock(&rdma->sc_frmr_q_lock);\n\tif (frmr)\n\t\treturn frmr;\n\n\treturn rdma_alloc_frmr(rdma);\n}\n\nvoid svc_rdma_put_frmr(struct svcxprt_rdma *rdma,\n\t\t       struct svc_rdma_fastreg_mr *frmr)\n{\n\tif (frmr) {\n\t\tib_dma_unmap_sg(rdma->sc_cm_id->device,\n\t\t\t\tfrmr->sg, frmr->sg_nents, frmr->direction);\n\t\tspin_lock(&rdma->sc_frmr_q_lock);\n\t\tWARN_ON_ONCE(!list_empty(&frmr->frmr_list));\n\t\tlist_add(&frmr->frmr_list, &rdma->sc_frmr_q);\n\t\tspin_unlock(&rdma->sc_frmr_q_lock);\n\t}\n}\n\n/*\n * This is the xpo_recvfrom function for listening endpoints. Its\n * purpose is to accept incoming connections. The CMA callback handler\n * has already created a new transport and attached it to the new CMA\n * ID.\n *\n * There is a queue of pending connections hung on the listening\n * transport. This queue contains the new svc_xprt structure. This\n * function takes svc_xprt structures off the accept_q and completes\n * the connection.\n */\nstatic struct svc_xprt *svc_rdma_accept(struct svc_xprt *xprt)\n{\n\tstruct svcxprt_rdma *listen_rdma;\n\tstruct svcxprt_rdma *newxprt = NULL;\n\tstruct rdma_conn_param conn_param;\n\tstruct rpcrdma_connect_private pmsg;\n\tstruct ib_qp_init_attr qp_attr;\n\tstruct ib_device *dev;\n\tstruct sockaddr *sap;\n\tunsigned int i;\n\tint ret = 0;\n\n\tlisten_rdma = container_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tclear_bit(XPT_CONN, &xprt->xpt_flags);\n\t/* Get the next entry off the accept list */\n\tspin_lock_bh(&listen_rdma->sc_lock);\n\tif (!list_empty(&listen_rdma->sc_accept_q)) {\n\t\tnewxprt = list_entry(listen_rdma->sc_accept_q.next,\n\t\t\t\t     struct svcxprt_rdma, sc_accept_q);\n\t\tlist_del_init(&newxprt->sc_accept_q);\n\t}\n\tif (!list_empty(&listen_rdma->sc_accept_q))\n\t\tset_bit(XPT_CONN, &listen_rdma->sc_xprt.xpt_flags);\n\tspin_unlock_bh(&listen_rdma->sc_lock);\n\tif (!newxprt)\n\t\treturn NULL;\n\n\tdprintk(\"svcrdma: newxprt from accept queue = %p, cm_id=%p\\n\",\n\t\tnewxprt, newxprt->sc_cm_id);\n\n\tdev = newxprt->sc_cm_id->device;\n\n\t/* Qualify the transport resource defaults with the\n\t * capabilities of this particular device */\n\tnewxprt->sc_max_sge = min((size_t)dev->attrs.max_sge,\n\t\t\t\t  (size_t)RPCSVC_MAXPAGES);\n\tnewxprt->sc_max_sge_rd = min_t(size_t, dev->attrs.max_sge_rd,\n\t\t\t\t       RPCSVC_MAXPAGES);\n\tnewxprt->sc_max_req_size = svcrdma_max_req_size;\n\tnewxprt->sc_max_requests = min_t(u32, dev->attrs.max_qp_wr,\n\t\t\t\t\t svcrdma_max_requests);\n\tnewxprt->sc_fc_credits = cpu_to_be32(newxprt->sc_max_requests);\n\tnewxprt->sc_max_bc_requests = min_t(u32, dev->attrs.max_qp_wr,\n\t\t\t\t\t    svcrdma_max_bc_requests);\n\tnewxprt->sc_rq_depth = newxprt->sc_max_requests +\n\t\t\t       newxprt->sc_max_bc_requests;\n\tnewxprt->sc_sq_depth = RPCRDMA_SQ_DEPTH_MULT * newxprt->sc_rq_depth;\n\tatomic_set(&newxprt->sc_sq_avail, newxprt->sc_sq_depth);\n\n\tif (!svc_rdma_prealloc_ctxts(newxprt))\n\t\tgoto errout;\n\tif (!svc_rdma_prealloc_maps(newxprt))\n\t\tgoto errout;\n\n\t/*\n\t * Limit ORD based on client limit, local device limit, and\n\t * configured svcrdma limit.\n\t */\n\tnewxprt->sc_ord = min_t(size_t, dev->attrs.max_qp_rd_atom, newxprt->sc_ord);\n\tnewxprt->sc_ord = min_t(size_t,\tsvcrdma_ord, newxprt->sc_ord);\n\n\tnewxprt->sc_pd = ib_alloc_pd(dev, 0);\n\tif (IS_ERR(newxprt->sc_pd)) {\n\t\tdprintk(\"svcrdma: error creating PD for connect request\\n\");\n\t\tgoto errout;\n\t}\n\tnewxprt->sc_sq_cq = ib_alloc_cq(dev, newxprt, newxprt->sc_sq_depth,\n\t\t\t\t\t0, IB_POLL_WORKQUEUE);\n\tif (IS_ERR(newxprt->sc_sq_cq)) {\n\t\tdprintk(\"svcrdma: error creating SQ CQ for connect request\\n\");\n\t\tgoto errout;\n\t}\n\tnewxprt->sc_rq_cq = ib_alloc_cq(dev, newxprt, newxprt->sc_rq_depth,\n\t\t\t\t\t0, IB_POLL_WORKQUEUE);\n\tif (IS_ERR(newxprt->sc_rq_cq)) {\n\t\tdprintk(\"svcrdma: error creating RQ CQ for connect request\\n\");\n\t\tgoto errout;\n\t}\n\n\tmemset(&qp_attr, 0, sizeof qp_attr);\n\tqp_attr.event_handler = qp_event_handler;\n\tqp_attr.qp_context = &newxprt->sc_xprt;\n\tqp_attr.cap.max_send_wr = newxprt->sc_sq_depth;\n\tqp_attr.cap.max_recv_wr = newxprt->sc_rq_depth;\n\tqp_attr.cap.max_send_sge = newxprt->sc_max_sge;\n\tqp_attr.cap.max_recv_sge = newxprt->sc_max_sge;\n\tqp_attr.sq_sig_type = IB_SIGNAL_REQ_WR;\n\tqp_attr.qp_type = IB_QPT_RC;\n\tqp_attr.send_cq = newxprt->sc_sq_cq;\n\tqp_attr.recv_cq = newxprt->sc_rq_cq;\n\tdprintk(\"svcrdma: newxprt->sc_cm_id=%p, newxprt->sc_pd=%p\\n\",\n\t\tnewxprt->sc_cm_id, newxprt->sc_pd);\n\tdprintk(\"    cap.max_send_wr = %d, cap.max_recv_wr = %d\\n\",\n\t\tqp_attr.cap.max_send_wr, qp_attr.cap.max_recv_wr);\n\tdprintk(\"    cap.max_send_sge = %d, cap.max_recv_sge = %d\\n\",\n\t\tqp_attr.cap.max_send_sge, qp_attr.cap.max_recv_sge);\n\n\tret = rdma_create_qp(newxprt->sc_cm_id, newxprt->sc_pd, &qp_attr);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: failed to create QP, ret=%d\\n\", ret);\n\t\tgoto errout;\n\t}\n\tnewxprt->sc_qp = newxprt->sc_cm_id->qp;\n\n\t/*\n\t * Use the most secure set of MR resources based on the\n\t * transport type and available memory management features in\n\t * the device. Here's the table implemented below:\n\t *\n\t *\t\tFast\tGlobal\tDMA\tRemote WR\n\t *\t\tReg\tLKEY\tMR\tAccess\n\t *\t\tSup'd\tSup'd\tNeeded\tNeeded\n\t *\n\t * IWARP\tN\tN\tY\tY\n\t *\t\tN\tY\tY\tY\n\t *\t\tY\tN\tY\tN\n\t *\t\tY\tY\tN\t-\n\t *\n\t * IB\t\tN\tN\tY\tN\n\t *\t\tN\tY\tN\t-\n\t *\t\tY\tN\tY\tN\n\t *\t\tY\tY\tN\t-\n\t *\n\t * NB:\tiWARP requires remote write access for the data sink\n\t *\tof an RDMA_READ. IB does not.\n\t */\n\tnewxprt->sc_reader = rdma_read_chunk_lcl;\n\tif (dev->attrs.device_cap_flags & IB_DEVICE_MEM_MGT_EXTENSIONS) {\n\t\tnewxprt->sc_frmr_pg_list_len =\n\t\t\tdev->attrs.max_fast_reg_page_list_len;\n\t\tnewxprt->sc_dev_caps |= SVCRDMA_DEVCAP_FAST_REG;\n\t\tnewxprt->sc_reader = rdma_read_chunk_frmr;\n\t} else\n\t\tnewxprt->sc_snd_w_inv = false;\n\n\t/*\n\t * Determine if a DMA MR is required and if so, what privs are required\n\t */\n\tif (!rdma_protocol_iwarp(dev, newxprt->sc_cm_id->port_num) &&\n\t    !rdma_ib_or_roce(dev, newxprt->sc_cm_id->port_num))\n\t\tgoto errout;\n\n\tif (rdma_protocol_iwarp(dev, newxprt->sc_cm_id->port_num))\n\t\tnewxprt->sc_dev_caps |= SVCRDMA_DEVCAP_READ_W_INV;\n\n\t/* Post receive buffers */\n\tfor (i = 0; i < newxprt->sc_max_requests; i++) {\n\t\tret = svc_rdma_post_recv(newxprt, GFP_KERNEL);\n\t\tif (ret) {\n\t\t\tdprintk(\"svcrdma: failure posting receive buffers\\n\");\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\t/* Swap out the handler */\n\tnewxprt->sc_cm_id->event_handler = rdma_cma_handler;\n\n\t/* Construct RDMA-CM private message */\n\tpmsg.cp_magic = rpcrdma_cmp_magic;\n\tpmsg.cp_version = RPCRDMA_CMP_VERSION;\n\tpmsg.cp_flags = 0;\n\tpmsg.cp_send_size = pmsg.cp_recv_size =\n\t\trpcrdma_encode_buffer_size(newxprt->sc_max_req_size);\n\n\t/* Accept Connection */\n\tset_bit(RDMAXPRT_CONN_PENDING, &newxprt->sc_flags);\n\tmemset(&conn_param, 0, sizeof conn_param);\n\tconn_param.responder_resources = 0;\n\tconn_param.initiator_depth = newxprt->sc_ord;\n\tconn_param.private_data = &pmsg;\n\tconn_param.private_data_len = sizeof(pmsg);\n\tret = rdma_accept(newxprt->sc_cm_id, &conn_param);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: failed to accept new connection, ret=%d\\n\",\n\t\t       ret);\n\t\tgoto errout;\n\t}\n\n\tdprintk(\"svcrdma: new connection %p accepted:\\n\", newxprt);\n\tsap = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.src_addr;\n\tdprintk(\"    local address   : %pIS:%u\\n\", sap, rpc_get_port(sap));\n\tsap = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.dst_addr;\n\tdprintk(\"    remote address  : %pIS:%u\\n\", sap, rpc_get_port(sap));\n\tdprintk(\"    max_sge         : %d\\n\", newxprt->sc_max_sge);\n\tdprintk(\"    max_sge_rd      : %d\\n\", newxprt->sc_max_sge_rd);\n\tdprintk(\"    sq_depth        : %d\\n\", newxprt->sc_sq_depth);\n\tdprintk(\"    max_requests    : %d\\n\", newxprt->sc_max_requests);\n\tdprintk(\"    ord             : %d\\n\", newxprt->sc_ord);\n\n\treturn &newxprt->sc_xprt;\n\n errout:\n\tdprintk(\"svcrdma: failure accepting new connection rc=%d.\\n\", ret);\n\t/* Take a reference in case the DTO handler runs */\n\tsvc_xprt_get(&newxprt->sc_xprt);\n\tif (newxprt->sc_qp && !IS_ERR(newxprt->sc_qp))\n\t\tib_destroy_qp(newxprt->sc_qp);\n\trdma_destroy_id(newxprt->sc_cm_id);\n\t/* This call to put will destroy the transport */\n\tsvc_xprt_put(&newxprt->sc_xprt);\n\treturn NULL;\n}\n\nstatic void svc_rdma_release_rqst(struct svc_rqst *rqstp)\n{\n}\n\n/*\n * When connected, an svc_xprt has at least two references:\n *\n * - A reference held by the cm_id between the ESTABLISHED and\n *   DISCONNECTED events. If the remote peer disconnected first, this\n *   reference could be gone.\n *\n * - A reference held by the svc_recv code that called this function\n *   as part of close processing.\n *\n * At a minimum one references should still be held.\n */\nstatic void svc_rdma_detach(struct svc_xprt *xprt)\n{\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tdprintk(\"svc: svc_rdma_detach(%p)\\n\", xprt);\n\n\t/* Disconnect and flush posted WQE */\n\trdma_disconnect(rdma->sc_cm_id);\n}\n\nstatic void __svc_rdma_free(struct work_struct *work)\n{\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(work, struct svcxprt_rdma, sc_work);\n\tstruct svc_xprt *xprt = &rdma->sc_xprt;\n\n\tdprintk(\"svcrdma: %s(%p)\\n\", __func__, rdma);\n\n\tif (rdma->sc_qp && !IS_ERR(rdma->sc_qp))\n\t\tib_drain_qp(rdma->sc_qp);\n\n\t/* We should only be called from kref_put */\n\tif (kref_read(&xprt->xpt_ref) != 0)\n\t\tpr_err(\"svcrdma: sc_xprt still in use? (%d)\\n\",\n\t\t       kref_read(&xprt->xpt_ref));\n\n\t/*\n\t * Destroy queued, but not processed read completions. Note\n\t * that this cleanup has to be done before destroying the\n\t * cm_id because the device ptr is needed to unmap the dma in\n\t * svc_rdma_put_context.\n\t */\n\twhile (!list_empty(&rdma->sc_read_complete_q)) {\n\t\tstruct svc_rdma_op_ctxt *ctxt;\n\t\tctxt = list_first_entry(&rdma->sc_read_complete_q,\n\t\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\t\tlist_del(&ctxt->list);\n\t\tsvc_rdma_put_context(ctxt, 1);\n\t}\n\n\t/* Destroy queued, but not processed recv completions */\n\twhile (!list_empty(&rdma->sc_rq_dto_q)) {\n\t\tstruct svc_rdma_op_ctxt *ctxt;\n\t\tctxt = list_first_entry(&rdma->sc_rq_dto_q,\n\t\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\t\tlist_del(&ctxt->list);\n\t\tsvc_rdma_put_context(ctxt, 1);\n\t}\n\n\t/* Warn if we leaked a resource or under-referenced */\n\tif (rdma->sc_ctxt_used != 0)\n\t\tpr_err(\"svcrdma: ctxt still in use? (%d)\\n\",\n\t\t       rdma->sc_ctxt_used);\n\n\t/* Final put of backchannel client transport */\n\tif (xprt->xpt_bc_xprt) {\n\t\txprt_put(xprt->xpt_bc_xprt);\n\t\txprt->xpt_bc_xprt = NULL;\n\t}\n\n\trdma_dealloc_frmr_q(rdma);\n\tsvc_rdma_destroy_ctxts(rdma);\n\tsvc_rdma_destroy_maps(rdma);\n\n\t/* Destroy the QP if present (not a listener) */\n\tif (rdma->sc_qp && !IS_ERR(rdma->sc_qp))\n\t\tib_destroy_qp(rdma->sc_qp);\n\n\tif (rdma->sc_sq_cq && !IS_ERR(rdma->sc_sq_cq))\n\t\tib_free_cq(rdma->sc_sq_cq);\n\n\tif (rdma->sc_rq_cq && !IS_ERR(rdma->sc_rq_cq))\n\t\tib_free_cq(rdma->sc_rq_cq);\n\n\tif (rdma->sc_pd && !IS_ERR(rdma->sc_pd))\n\t\tib_dealloc_pd(rdma->sc_pd);\n\n\t/* Destroy the CM ID */\n\trdma_destroy_id(rdma->sc_cm_id);\n\n\tkfree(rdma);\n}\n\nstatic void svc_rdma_free(struct svc_xprt *xprt)\n{\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tINIT_WORK(&rdma->sc_work, __svc_rdma_free);\n\tqueue_work(svc_rdma_wq, &rdma->sc_work);\n}\n\nstatic int svc_rdma_has_wspace(struct svc_xprt *xprt)\n{\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\n\t/*\n\t * If there are already waiters on the SQ,\n\t * return false.\n\t */\n\tif (waitqueue_active(&rdma->sc_send_wait))\n\t\treturn 0;\n\n\t/* Otherwise return true. */\n\treturn 1;\n}\n\nstatic int svc_rdma_secure_port(struct svc_rqst *rqstp)\n{\n\treturn 1;\n}\n\nstatic void svc_rdma_kill_temp_xprt(struct svc_xprt *xprt)\n{\n}\n\nint svc_rdma_send(struct svcxprt_rdma *xprt, struct ib_send_wr *wr)\n{\n\tstruct ib_send_wr *bad_wr, *n_wr;\n\tint wr_count;\n\tint i;\n\tint ret;\n\n\tif (test_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags))\n\t\treturn -ENOTCONN;\n\n\twr_count = 1;\n\tfor (n_wr = wr->next; n_wr; n_wr = n_wr->next)\n\t\twr_count++;\n\n\t/* If the SQ is full, wait until an SQ entry is available */\n\twhile (1) {\n\t\tif ((atomic_sub_return(wr_count, &xprt->sc_sq_avail) < 0)) {\n\t\t\tatomic_inc(&rdma_stat_sq_starve);\n\n\t\t\t/* Wait until SQ WR available if SQ still full */\n\t\t\tatomic_add(wr_count, &xprt->sc_sq_avail);\n\t\t\twait_event(xprt->sc_send_wait,\n\t\t\t\t   atomic_read(&xprt->sc_sq_avail) > wr_count);\n\t\t\tif (test_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags))\n\t\t\t\treturn -ENOTCONN;\n\t\t\tcontinue;\n\t\t}\n\t\t/* Take a transport ref for each WR posted */\n\t\tfor (i = 0; i < wr_count; i++)\n\t\t\tsvc_xprt_get(&xprt->sc_xprt);\n\n\t\t/* Bump used SQ WR count and post */\n\t\tret = ib_post_send(xprt->sc_qp, wr, &bad_wr);\n\t\tif (ret) {\n\t\t\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\t\t\tfor (i = 0; i < wr_count; i ++)\n\t\t\t\tsvc_xprt_put(&xprt->sc_xprt);\n\t\t\tdprintk(\"svcrdma: failed to post SQ WR rc=%d\\n\", ret);\n\t\t\tdprintk(\"    sc_sq_avail=%d, sc_sq_depth=%d\\n\",\n\t\t\t\tatomic_read(&xprt->sc_sq_avail),\n\t\t\t\txprt->sc_sq_depth);\n\t\t\twake_up(&xprt->sc_send_wait);\n\t\t}\n\t\tbreak;\n\t}\n\treturn ret;\n}\n"], "fixing_code": ["/*\n * linux/fs/lockd/svc.c\n *\n * This is the central lockd service.\n *\n * FIXME: Separate the lockd NFS server functionality from the lockd NFS\n * \t  client functionality. Oh why didn't Sun create two separate\n *\t  services in the first place?\n *\n * Authors:\tOlaf Kirch (okir@monad.swb.de)\n *\n * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>\n */\n\n#include <linux/module.h>\n#include <linux/init.h>\n#include <linux/sysctl.h>\n#include <linux/moduleparam.h>\n\n#include <linux/sched/signal.h>\n#include <linux/errno.h>\n#include <linux/in.h>\n#include <linux/uio.h>\n#include <linux/smp.h>\n#include <linux/mutex.h>\n#include <linux/kthread.h>\n#include <linux/freezer.h>\n#include <linux/inetdevice.h>\n\n#include <linux/sunrpc/types.h>\n#include <linux/sunrpc/stats.h>\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/svc.h>\n#include <linux/sunrpc/svcsock.h>\n#include <linux/sunrpc/svc_xprt.h>\n#include <net/ip.h>\n#include <net/addrconf.h>\n#include <net/ipv6.h>\n#include <linux/lockd/lockd.h>\n#include <linux/nfs.h>\n\n#include \"netns.h\"\n#include \"procfs.h\"\n\n#define NLMDBG_FACILITY\t\tNLMDBG_SVC\n#define LOCKD_BUFSIZE\t\t(1024 + NLMSVC_XDRSIZE)\n#define ALLOWED_SIGS\t\t(sigmask(SIGKILL))\n\nstatic struct svc_program\tnlmsvc_program;\n\nconst struct nlmsvc_binding\t*nlmsvc_ops;\nEXPORT_SYMBOL_GPL(nlmsvc_ops);\n\nstatic DEFINE_MUTEX(nlmsvc_mutex);\nstatic unsigned int\t\tnlmsvc_users;\nstatic struct task_struct\t*nlmsvc_task;\nstatic struct svc_rqst\t\t*nlmsvc_rqst;\nunsigned long\t\t\tnlmsvc_timeout;\n\nunsigned int lockd_net_id;\n\n/*\n * These can be set at insmod time (useful for NFS as root filesystem),\n * and also changed through the sysctl interface.  -- Jamie Lokier, Aug 2003\n */\nstatic unsigned long\t\tnlm_grace_period;\nstatic unsigned long\t\tnlm_timeout = LOCKD_DFLT_TIMEO;\nstatic int\t\t\tnlm_udpport, nlm_tcpport;\n\n/* RLIM_NOFILE defaults to 1024. That seems like a reasonable default here. */\nstatic unsigned int\t\tnlm_max_connections = 1024;\n\n/*\n * Constants needed for the sysctl interface.\n */\nstatic const unsigned long\tnlm_grace_period_min = 0;\nstatic const unsigned long\tnlm_grace_period_max = 240;\nstatic const unsigned long\tnlm_timeout_min = 3;\nstatic const unsigned long\tnlm_timeout_max = 20;\nstatic const int\t\tnlm_port_min = 0, nlm_port_max = 65535;\n\n#ifdef CONFIG_SYSCTL\nstatic struct ctl_table_header * nlm_sysctl_table;\n#endif\n\nstatic unsigned long get_lockd_grace_period(void)\n{\n\t/* Note: nlm_timeout should always be nonzero */\n\tif (nlm_grace_period)\n\t\treturn roundup(nlm_grace_period, nlm_timeout) * HZ;\n\telse\n\t\treturn nlm_timeout * 5 * HZ;\n}\n\nstatic void grace_ender(struct work_struct *grace)\n{\n\tstruct delayed_work *dwork = to_delayed_work(grace);\n\tstruct lockd_net *ln = container_of(dwork, struct lockd_net,\n\t\t\t\t\t    grace_period_end);\n\n\tlocks_end_grace(&ln->lockd_manager);\n}\n\nstatic void set_grace_period(struct net *net)\n{\n\tunsigned long grace_period = get_lockd_grace_period();\n\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n\n\tlocks_start_grace(net, &ln->lockd_manager);\n\tcancel_delayed_work_sync(&ln->grace_period_end);\n\tschedule_delayed_work(&ln->grace_period_end, grace_period);\n}\n\nstatic void restart_grace(void)\n{\n\tif (nlmsvc_ops) {\n\t\tstruct net *net = &init_net;\n\t\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n\n\t\tcancel_delayed_work_sync(&ln->grace_period_end);\n\t\tlocks_end_grace(&ln->lockd_manager);\n\t\tnlmsvc_invalidate_all();\n\t\tset_grace_period(net);\n\t}\n}\n\n/*\n * This is the lockd kernel thread\n */\nstatic int\nlockd(void *vrqstp)\n{\n\tint\t\terr = 0;\n\tstruct svc_rqst *rqstp = vrqstp;\n\tstruct net *net = &init_net;\n\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n\n\t/* try_to_freeze() is called from svc_recv() */\n\tset_freezable();\n\n\t/* Allow SIGKILL to tell lockd to drop all of its locks */\n\tallow_signal(SIGKILL);\n\n\tdprintk(\"NFS locking service started (ver \" LOCKD_VERSION \").\\n\");\n\n\t/*\n\t * The main request loop. We don't terminate until the last\n\t * NFS mount or NFS daemon has gone away.\n\t */\n\twhile (!kthread_should_stop()) {\n\t\tlong timeout = MAX_SCHEDULE_TIMEOUT;\n\t\tRPC_IFDEBUG(char buf[RPC_MAX_ADDRBUFLEN]);\n\n\t\t/* update sv_maxconn if it has changed */\n\t\trqstp->rq_server->sv_maxconn = nlm_max_connections;\n\n\t\tif (signalled()) {\n\t\t\tflush_signals(current);\n\t\t\trestart_grace();\n\t\t\tcontinue;\n\t\t}\n\n\t\ttimeout = nlmsvc_retry_blocked();\n\n\t\t/*\n\t\t * Find a socket with data available and call its\n\t\t * recvfrom routine.\n\t\t */\n\t\terr = svc_recv(rqstp, timeout);\n\t\tif (err == -EAGAIN || err == -EINTR)\n\t\t\tcontinue;\n\t\tdprintk(\"lockd: request from %s\\n\",\n\t\t\t\tsvc_print_addr(rqstp, buf, sizeof(buf)));\n\n\t\tsvc_process(rqstp);\n\t}\n\tflush_signals(current);\n\tif (nlmsvc_ops)\n\t\tnlmsvc_invalidate_all();\n\tnlm_shutdown_hosts();\n\tcancel_delayed_work_sync(&ln->grace_period_end);\n\tlocks_end_grace(&ln->lockd_manager);\n\treturn 0;\n}\n\nstatic int create_lockd_listener(struct svc_serv *serv, const char *name,\n\t\t\t\t struct net *net, const int family,\n\t\t\t\t const unsigned short port)\n{\n\tstruct svc_xprt *xprt;\n\n\txprt = svc_find_xprt(serv, name, net, family, 0);\n\tif (xprt == NULL)\n\t\treturn svc_create_xprt(serv, name, net, family, port,\n\t\t\t\t\t\tSVC_SOCK_DEFAULTS);\n\tsvc_xprt_put(xprt);\n\treturn 0;\n}\n\nstatic int create_lockd_family(struct svc_serv *serv, struct net *net,\n\t\t\t       const int family)\n{\n\tint err;\n\n\terr = create_lockd_listener(serv, \"udp\", net, family, nlm_udpport);\n\tif (err < 0)\n\t\treturn err;\n\n\treturn create_lockd_listener(serv, \"tcp\", net, family, nlm_tcpport);\n}\n\n/*\n * Ensure there are active UDP and TCP listeners for lockd.\n *\n * Even if we have only TCP NFS mounts and/or TCP NFSDs, some\n * local services (such as rpc.statd) still require UDP, and\n * some NFS servers do not yet support NLM over TCP.\n *\n * Returns zero if all listeners are available; otherwise a\n * negative errno value is returned.\n */\nstatic int make_socks(struct svc_serv *serv, struct net *net)\n{\n\tstatic int warned;\n\tint err;\n\n\terr = create_lockd_family(serv, net, PF_INET);\n\tif (err < 0)\n\t\tgoto out_err;\n\n\terr = create_lockd_family(serv, net, PF_INET6);\n\tif (err < 0 && err != -EAFNOSUPPORT)\n\t\tgoto out_err;\n\n\twarned = 0;\n\treturn 0;\n\nout_err:\n\tif (warned++ == 0)\n\t\tprintk(KERN_WARNING\n\t\t\t\"lockd_up: makesock failed, error=%d\\n\", err);\n\tsvc_shutdown_net(serv, net);\n\treturn err;\n}\n\nstatic int lockd_up_net(struct svc_serv *serv, struct net *net)\n{\n\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n\tint error;\n\n\tif (ln->nlmsvc_users++)\n\t\treturn 0;\n\n\terror = svc_bind(serv, net);\n\tif (error)\n\t\tgoto err_bind;\n\n\terror = make_socks(serv, net);\n\tif (error < 0)\n\t\tgoto err_bind;\n\tset_grace_period(net);\n\tdprintk(\"lockd_up_net: per-net data created; net=%p\\n\", net);\n\treturn 0;\n\nerr_bind:\n\tln->nlmsvc_users--;\n\treturn error;\n}\n\nstatic void lockd_down_net(struct svc_serv *serv, struct net *net)\n{\n\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n\n\tif (ln->nlmsvc_users) {\n\t\tif (--ln->nlmsvc_users == 0) {\n\t\t\tnlm_shutdown_hosts_net(net);\n\t\t\tsvc_shutdown_net(serv, net);\n\t\t\tdprintk(\"lockd_down_net: per-net data destroyed; net=%p\\n\", net);\n\t\t}\n\t} else {\n\t\tprintk(KERN_ERR \"lockd_down_net: no users! task=%p, net=%p\\n\",\n\t\t\t\tnlmsvc_task, net);\n\t\tBUG();\n\t}\n}\n\nstatic int lockd_inetaddr_event(struct notifier_block *this,\n\tunsigned long event, void *ptr)\n{\n\tstruct in_ifaddr *ifa = (struct in_ifaddr *)ptr;\n\tstruct sockaddr_in sin;\n\n\tif (event != NETDEV_DOWN)\n\t\tgoto out;\n\n\tif (nlmsvc_rqst) {\n\t\tdprintk(\"lockd_inetaddr_event: removed %pI4\\n\",\n\t\t\t&ifa->ifa_local);\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = ifa->ifa_local;\n\t\tsvc_age_temp_xprts_now(nlmsvc_rqst->rq_server,\n\t\t\t(struct sockaddr *)&sin);\n\t}\n\nout:\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block lockd_inetaddr_notifier = {\n\t.notifier_call = lockd_inetaddr_event,\n};\n\n#if IS_ENABLED(CONFIG_IPV6)\nstatic int lockd_inet6addr_event(struct notifier_block *this,\n\tunsigned long event, void *ptr)\n{\n\tstruct inet6_ifaddr *ifa = (struct inet6_ifaddr *)ptr;\n\tstruct sockaddr_in6 sin6;\n\n\tif (event != NETDEV_DOWN)\n\t\tgoto out;\n\n\tif (nlmsvc_rqst) {\n\t\tdprintk(\"lockd_inet6addr_event: removed %pI6\\n\", &ifa->addr);\n\t\tsin6.sin6_family = AF_INET6;\n\t\tsin6.sin6_addr = ifa->addr;\n\t\tif (ipv6_addr_type(&sin6.sin6_addr) & IPV6_ADDR_LINKLOCAL)\n\t\t\tsin6.sin6_scope_id = ifa->idev->dev->ifindex;\n\t\tsvc_age_temp_xprts_now(nlmsvc_rqst->rq_server,\n\t\t\t(struct sockaddr *)&sin6);\n\t}\n\nout:\n\treturn NOTIFY_DONE;\n}\n\nstatic struct notifier_block lockd_inet6addr_notifier = {\n\t.notifier_call = lockd_inet6addr_event,\n};\n#endif\n\nstatic void lockd_unregister_notifiers(void)\n{\n\tunregister_inetaddr_notifier(&lockd_inetaddr_notifier);\n#if IS_ENABLED(CONFIG_IPV6)\n\tunregister_inet6addr_notifier(&lockd_inet6addr_notifier);\n#endif\n}\n\nstatic void lockd_svc_exit_thread(void)\n{\n\tlockd_unregister_notifiers();\n\tsvc_exit_thread(nlmsvc_rqst);\n}\n\nstatic int lockd_start_svc(struct svc_serv *serv)\n{\n\tint error;\n\n\tif (nlmsvc_rqst)\n\t\treturn 0;\n\n\t/*\n\t * Create the kernel thread and wait for it to start.\n\t */\n\tnlmsvc_rqst = svc_prepare_thread(serv, &serv->sv_pools[0], NUMA_NO_NODE);\n\tif (IS_ERR(nlmsvc_rqst)) {\n\t\terror = PTR_ERR(nlmsvc_rqst);\n\t\tprintk(KERN_WARNING\n\t\t\t\"lockd_up: svc_rqst allocation failed, error=%d\\n\",\n\t\t\terror);\n\t\tgoto out_rqst;\n\t}\n\n\tsvc_sock_update_bufs(serv);\n\tserv->sv_maxconn = nlm_max_connections;\n\n\tnlmsvc_task = kthread_create(lockd, nlmsvc_rqst, \"%s\", serv->sv_name);\n\tif (IS_ERR(nlmsvc_task)) {\n\t\terror = PTR_ERR(nlmsvc_task);\n\t\tprintk(KERN_WARNING\n\t\t\t\"lockd_up: kthread_run failed, error=%d\\n\", error);\n\t\tgoto out_task;\n\t}\n\tnlmsvc_rqst->rq_task = nlmsvc_task;\n\twake_up_process(nlmsvc_task);\n\n\tdprintk(\"lockd_up: service started\\n\");\n\treturn 0;\n\nout_task:\n\tlockd_svc_exit_thread();\n\tnlmsvc_task = NULL;\nout_rqst:\n\tnlmsvc_rqst = NULL;\n\treturn error;\n}\n\nstatic struct svc_serv_ops lockd_sv_ops = {\n\t.svo_shutdown\t\t= svc_rpcb_cleanup,\n\t.svo_enqueue_xprt\t= svc_xprt_do_enqueue,\n};\n\nstatic struct svc_serv *lockd_create_svc(void)\n{\n\tstruct svc_serv *serv;\n\n\t/*\n\t * Check whether we're already up and running.\n\t */\n\tif (nlmsvc_rqst) {\n\t\t/*\n\t\t * Note: increase service usage, because later in case of error\n\t\t * svc_destroy() will be called.\n\t\t */\n\t\tsvc_get(nlmsvc_rqst->rq_server);\n\t\treturn nlmsvc_rqst->rq_server;\n\t}\n\n\t/*\n\t * Sanity check: if there's no pid,\n\t * we should be the first user ...\n\t */\n\tif (nlmsvc_users)\n\t\tprintk(KERN_WARNING\n\t\t\t\"lockd_up: no pid, %d users??\\n\", nlmsvc_users);\n\n\tif (!nlm_timeout)\n\t\tnlm_timeout = LOCKD_DFLT_TIMEO;\n\tnlmsvc_timeout = nlm_timeout * HZ;\n\n\tserv = svc_create(&nlmsvc_program, LOCKD_BUFSIZE, &lockd_sv_ops);\n\tif (!serv) {\n\t\tprintk(KERN_WARNING \"lockd_up: create service failed\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tregister_inetaddr_notifier(&lockd_inetaddr_notifier);\n#if IS_ENABLED(CONFIG_IPV6)\n\tregister_inet6addr_notifier(&lockd_inet6addr_notifier);\n#endif\n\tdprintk(\"lockd_up: service created\\n\");\n\treturn serv;\n}\n\n/*\n * Bring up the lockd process if it's not already up.\n */\nint lockd_up(struct net *net)\n{\n\tstruct svc_serv *serv;\n\tint error;\n\n\tmutex_lock(&nlmsvc_mutex);\n\n\tserv = lockd_create_svc();\n\tif (IS_ERR(serv)) {\n\t\terror = PTR_ERR(serv);\n\t\tgoto err_create;\n\t}\n\n\terror = lockd_up_net(serv, net);\n\tif (error < 0)\n\t\tgoto err_net;\n\n\terror = lockd_start_svc(serv);\n\tif (error < 0)\n\t\tgoto err_start;\n\n\tnlmsvc_users++;\n\t/*\n\t * Note: svc_serv structures have an initial use count of 1,\n\t * so we exit through here on both success and failure.\n\t */\nerr_put:\n\tsvc_destroy(serv);\nerr_create:\n\tmutex_unlock(&nlmsvc_mutex);\n\treturn error;\n\nerr_start:\n\tlockd_down_net(serv, net);\nerr_net:\n\tlockd_unregister_notifiers();\n\tgoto err_put;\n}\nEXPORT_SYMBOL_GPL(lockd_up);\n\n/*\n * Decrement the user count and bring down lockd if we're the last.\n */\nvoid\nlockd_down(struct net *net)\n{\n\tmutex_lock(&nlmsvc_mutex);\n\tlockd_down_net(nlmsvc_rqst->rq_server, net);\n\tif (nlmsvc_users) {\n\t\tif (--nlmsvc_users)\n\t\t\tgoto out;\n\t} else {\n\t\tprintk(KERN_ERR \"lockd_down: no users! task=%p\\n\",\n\t\t\tnlmsvc_task);\n\t\tBUG();\n\t}\n\n\tif (!nlmsvc_task) {\n\t\tprintk(KERN_ERR \"lockd_down: no lockd running.\\n\");\n\t\tBUG();\n\t}\n\tkthread_stop(nlmsvc_task);\n\tdprintk(\"lockd_down: service stopped\\n\");\n\tlockd_svc_exit_thread();\n\tdprintk(\"lockd_down: service destroyed\\n\");\n\tnlmsvc_task = NULL;\n\tnlmsvc_rqst = NULL;\nout:\n\tmutex_unlock(&nlmsvc_mutex);\n}\nEXPORT_SYMBOL_GPL(lockd_down);\n\n#ifdef CONFIG_SYSCTL\n\n/*\n * Sysctl parameters (same as module parameters, different interface).\n */\n\nstatic struct ctl_table nlm_sysctls[] = {\n\t{\n\t\t.procname\t= \"nlm_grace_period\",\n\t\t.data\t\t= &nlm_grace_period,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t\t.extra1\t\t= (unsigned long *) &nlm_grace_period_min,\n\t\t.extra2\t\t= (unsigned long *) &nlm_grace_period_max,\n\t},\n\t{\n\t\t.procname\t= \"nlm_timeout\",\n\t\t.data\t\t= &nlm_timeout,\n\t\t.maxlen\t\t= sizeof(unsigned long),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_doulongvec_minmax,\n\t\t.extra1\t\t= (unsigned long *) &nlm_timeout_min,\n\t\t.extra2\t\t= (unsigned long *) &nlm_timeout_max,\n\t},\n\t{\n\t\t.procname\t= \"nlm_udpport\",\n\t\t.data\t\t= &nlm_udpport,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= (int *) &nlm_port_min,\n\t\t.extra2\t\t= (int *) &nlm_port_max,\n\t},\n\t{\n\t\t.procname\t= \"nlm_tcpport\",\n\t\t.data\t\t= &nlm_tcpport,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= (int *) &nlm_port_min,\n\t\t.extra2\t\t= (int *) &nlm_port_max,\n\t},\n\t{\n\t\t.procname\t= \"nsm_use_hostnames\",\n\t\t.data\t\t= &nsm_use_hostnames,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{\n\t\t.procname\t= \"nsm_local_state\",\n\t\t.data\t\t= &nsm_local_state,\n\t\t.maxlen\t\t= sizeof(int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec,\n\t},\n\t{ }\n};\n\nstatic struct ctl_table nlm_sysctl_dir[] = {\n\t{\n\t\t.procname\t= \"nfs\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= nlm_sysctls,\n\t},\n\t{ }\n};\n\nstatic struct ctl_table nlm_sysctl_root[] = {\n\t{\n\t\t.procname\t= \"fs\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= nlm_sysctl_dir,\n\t},\n\t{ }\n};\n\n#endif\t/* CONFIG_SYSCTL */\n\n/*\n * Module (and sysfs) parameters.\n */\n\n#define param_set_min_max(name, type, which_strtol, min, max)\t\t\\\nstatic int param_set_##name(const char *val, struct kernel_param *kp)\t\\\n{\t\t\t\t\t\t\t\t\t\\\n\tchar *endp;\t\t\t\t\t\t\t\\\n\t__typeof__(type) num = which_strtol(val, &endp, 0);\t\t\\\n\tif (endp == val || *endp || num < (min) || num > (max))\t\t\\\n\t\treturn -EINVAL;\t\t\t\t\t\t\\\n\t*((type *) kp->arg) = num;\t\t\t\t\t\\\n\treturn 0;\t\t\t\t\t\t\t\\\n}\n\nstatic inline int is_callback(u32 proc)\n{\n\treturn proc == NLMPROC_GRANTED\n\t\t|| proc == NLMPROC_GRANTED_MSG\n\t\t|| proc == NLMPROC_TEST_RES\n\t\t|| proc == NLMPROC_LOCK_RES\n\t\t|| proc == NLMPROC_CANCEL_RES\n\t\t|| proc == NLMPROC_UNLOCK_RES\n\t\t|| proc == NLMPROC_NSM_NOTIFY;\n}\n\n\nstatic int lockd_authenticate(struct svc_rqst *rqstp)\n{\n\trqstp->rq_client = NULL;\n\tswitch (rqstp->rq_authop->flavour) {\n\t\tcase RPC_AUTH_NULL:\n\t\tcase RPC_AUTH_UNIX:\n\t\t\tif (rqstp->rq_proc == 0)\n\t\t\t\treturn SVC_OK;\n\t\t\tif (is_callback(rqstp->rq_proc)) {\n\t\t\t\t/* Leave it to individual procedures to\n\t\t\t\t * call nlmsvc_lookup_host(rqstp)\n\t\t\t\t */\n\t\t\t\treturn SVC_OK;\n\t\t\t}\n\t\t\treturn svc_set_client(rqstp);\n\t}\n\treturn SVC_DENIED;\n}\n\n\nparam_set_min_max(port, int, simple_strtol, 0, 65535)\nparam_set_min_max(grace_period, unsigned long, simple_strtoul,\n\t\t  nlm_grace_period_min, nlm_grace_period_max)\nparam_set_min_max(timeout, unsigned long, simple_strtoul,\n\t\t  nlm_timeout_min, nlm_timeout_max)\n\nMODULE_AUTHOR(\"Olaf Kirch <okir@monad.swb.de>\");\nMODULE_DESCRIPTION(\"NFS file locking service version \" LOCKD_VERSION \".\");\nMODULE_LICENSE(\"GPL\");\n\nmodule_param_call(nlm_grace_period, param_set_grace_period, param_get_ulong,\n\t\t  &nlm_grace_period, 0644);\nmodule_param_call(nlm_timeout, param_set_timeout, param_get_ulong,\n\t\t  &nlm_timeout, 0644);\nmodule_param_call(nlm_udpport, param_set_port, param_get_int,\n\t\t  &nlm_udpport, 0644);\nmodule_param_call(nlm_tcpport, param_set_port, param_get_int,\n\t\t  &nlm_tcpport, 0644);\nmodule_param(nsm_use_hostnames, bool, 0644);\nmodule_param(nlm_max_connections, uint, 0644);\n\nstatic int lockd_init_net(struct net *net)\n{\n\tstruct lockd_net *ln = net_generic(net, lockd_net_id);\n\n\tINIT_DELAYED_WORK(&ln->grace_period_end, grace_ender);\n\tINIT_LIST_HEAD(&ln->lockd_manager.list);\n\tln->lockd_manager.block_opens = false;\n\tINIT_LIST_HEAD(&ln->nsm_handles);\n\treturn 0;\n}\n\nstatic void lockd_exit_net(struct net *net)\n{\n}\n\nstatic struct pernet_operations lockd_net_ops = {\n\t.init = lockd_init_net,\n\t.exit = lockd_exit_net,\n\t.id = &lockd_net_id,\n\t.size = sizeof(struct lockd_net),\n};\n\n\n/*\n * Initialising and terminating the module.\n */\n\nstatic int __init init_nlm(void)\n{\n\tint err;\n\n#ifdef CONFIG_SYSCTL\n\terr = -ENOMEM;\n\tnlm_sysctl_table = register_sysctl_table(nlm_sysctl_root);\n\tif (nlm_sysctl_table == NULL)\n\t\tgoto err_sysctl;\n#endif\n\terr = register_pernet_subsys(&lockd_net_ops);\n\tif (err)\n\t\tgoto err_pernet;\n\n\terr = lockd_create_procfs();\n\tif (err)\n\t\tgoto err_procfs;\n\n\treturn 0;\n\nerr_procfs:\n\tunregister_pernet_subsys(&lockd_net_ops);\nerr_pernet:\n#ifdef CONFIG_SYSCTL\n\tunregister_sysctl_table(nlm_sysctl_table);\nerr_sysctl:\n#endif\n\treturn err;\n}\n\nstatic void __exit exit_nlm(void)\n{\n\t/* FIXME: delete all NLM clients */\n\tnlm_shutdown_hosts();\n\tlockd_remove_procfs();\n\tunregister_pernet_subsys(&lockd_net_ops);\n#ifdef CONFIG_SYSCTL\n\tunregister_sysctl_table(nlm_sysctl_table);\n#endif\n}\n\nmodule_init(init_nlm);\nmodule_exit(exit_nlm);\n\n/*\n * Define NLM program and procedures\n */\nstatic struct svc_version\tnlmsvc_version1 = {\n\t\t.vs_vers\t= 1,\n\t\t.vs_nproc\t= 17,\n\t\t.vs_proc\t= nlmsvc_procedures,\n\t\t.vs_xdrsize\t= NLMSVC_XDRSIZE,\n};\nstatic struct svc_version\tnlmsvc_version3 = {\n\t\t.vs_vers\t= 3,\n\t\t.vs_nproc\t= 24,\n\t\t.vs_proc\t= nlmsvc_procedures,\n\t\t.vs_xdrsize\t= NLMSVC_XDRSIZE,\n};\n#ifdef CONFIG_LOCKD_V4\nstatic struct svc_version\tnlmsvc_version4 = {\n\t\t.vs_vers\t= 4,\n\t\t.vs_nproc\t= 24,\n\t\t.vs_proc\t= nlmsvc_procedures4,\n\t\t.vs_xdrsize\t= NLMSVC_XDRSIZE,\n};\n#endif\nstatic struct svc_version *\tnlmsvc_version[] = {\n\t[1] = &nlmsvc_version1,\n\t[3] = &nlmsvc_version3,\n#ifdef CONFIG_LOCKD_V4\n\t[4] = &nlmsvc_version4,\n#endif\n};\n\nstatic struct svc_stat\t\tnlmsvc_stats;\n\n#define NLM_NRVERS\tARRAY_SIZE(nlmsvc_version)\nstatic struct svc_program\tnlmsvc_program = {\n\t.pg_prog\t\t= NLM_PROGRAM,\t\t/* program number */\n\t.pg_nvers\t\t= NLM_NRVERS,\t\t/* number of entries in nlmsvc_version */\n\t.pg_vers\t\t= nlmsvc_version,\t/* version table */\n\t.pg_name\t\t= \"lockd\",\t\t/* service name */\n\t.pg_class\t\t= \"nfsd\",\t\t/* share authentication with nfsd */\n\t.pg_stats\t\t= &nlmsvc_stats,\t/* stats table */\n\t.pg_authenticate = &lockd_authenticate\t/* export authentication */\n};\n", "/*\n * linux/fs/lockd/svclock.c\n *\n * Handling of server-side locks, mostly of the blocked variety.\n * This is the ugliest part of lockd because we tread on very thin ice.\n * GRANT and CANCEL calls may get stuck, meet in mid-flight, etc.\n * IMNSHO introducing the grant callback into the NLM protocol was one\n * of the worst ideas Sun ever had. Except maybe for the idea of doing\n * NFS file locking at all.\n *\n * I'm trying hard to avoid race conditions by protecting most accesses\n * to a file's list of blocked locks through a semaphore. The global\n * list of blocked locks is not protected in this fashion however.\n * Therefore, some functions (such as the RPC callback for the async grant\n * call) move blocked locks towards the head of the list *while some other\n * process might be traversing it*. This should not be a problem in\n * practice, because this will only cause functions traversing the list\n * to visit some blocks twice.\n *\n * Copyright (C) 1996, Olaf Kirch <okir@monad.swb.de>\n */\n\n#include <linux/types.h>\n#include <linux/slab.h>\n#include <linux/errno.h>\n#include <linux/kernel.h>\n#include <linux/sched.h>\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/svc_xprt.h>\n#include <linux/lockd/nlm.h>\n#include <linux/lockd/lockd.h>\n#include <linux/kthread.h>\n\n#define NLMDBG_FACILITY\t\tNLMDBG_SVCLOCK\n\n#ifdef CONFIG_LOCKD_V4\n#define nlm_deadlock\tnlm4_deadlock\n#else\n#define nlm_deadlock\tnlm_lck_denied\n#endif\n\nstatic void nlmsvc_release_block(struct nlm_block *block);\nstatic void\tnlmsvc_insert_block(struct nlm_block *block, unsigned long);\nstatic void\tnlmsvc_remove_block(struct nlm_block *block);\n\nstatic int nlmsvc_setgrantargs(struct nlm_rqst *call, struct nlm_lock *lock);\nstatic void nlmsvc_freegrantargs(struct nlm_rqst *call);\nstatic const struct rpc_call_ops nlmsvc_grant_ops;\n\n/*\n * The list of blocked locks to retry\n */\nstatic LIST_HEAD(nlm_blocked);\nstatic DEFINE_SPINLOCK(nlm_blocked_lock);\n\n#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)\nstatic const char *nlmdbg_cookie2a(const struct nlm_cookie *cookie)\n{\n\t/*\n\t * We can get away with a static buffer because this is only called\n\t * from lockd, which is single-threaded.\n\t */\n\tstatic char buf[2*NLM_MAXCOOKIELEN+1];\n\tunsigned int i, len = sizeof(buf);\n\tchar *p = buf;\n\n\tlen--;\t/* allow for trailing \\0 */\n\tif (len < 3)\n\t\treturn \"???\";\n\tfor (i = 0 ; i < cookie->len ; i++) {\n\t\tif (len < 2) {\n\t\t\tstrcpy(p-3, \"...\");\n\t\t\tbreak;\n\t\t}\n\t\tsprintf(p, \"%02x\", cookie->data[i]);\n\t\tp += 2;\n\t\tlen -= 2;\n\t}\n\t*p = '\\0';\n\n\treturn buf;\n}\n#endif\n\n/*\n * Insert a blocked lock into the global list\n */\nstatic void\nnlmsvc_insert_block_locked(struct nlm_block *block, unsigned long when)\n{\n\tstruct nlm_block *b;\n\tstruct list_head *pos;\n\n\tdprintk(\"lockd: nlmsvc_insert_block(%p, %ld)\\n\", block, when);\n\tif (list_empty(&block->b_list)) {\n\t\tkref_get(&block->b_count);\n\t} else {\n\t\tlist_del_init(&block->b_list);\n\t}\n\n\tpos = &nlm_blocked;\n\tif (when != NLM_NEVER) {\n\t\tif ((when += jiffies) == NLM_NEVER)\n\t\t\twhen ++;\n\t\tlist_for_each(pos, &nlm_blocked) {\n\t\t\tb = list_entry(pos, struct nlm_block, b_list);\n\t\t\tif (time_after(b->b_when,when) || b->b_when == NLM_NEVER)\n\t\t\t\tbreak;\n\t\t}\n\t\t/* On normal exit from the loop, pos == &nlm_blocked,\n\t\t * so we will be adding to the end of the list - good\n\t\t */\n\t}\n\n\tlist_add_tail(&block->b_list, pos);\n\tblock->b_when = when;\n}\n\nstatic void nlmsvc_insert_block(struct nlm_block *block, unsigned long when)\n{\n\tspin_lock(&nlm_blocked_lock);\n\tnlmsvc_insert_block_locked(block, when);\n\tspin_unlock(&nlm_blocked_lock);\n}\n\n/*\n * Remove a block from the global list\n */\nstatic inline void\nnlmsvc_remove_block(struct nlm_block *block)\n{\n\tif (!list_empty(&block->b_list)) {\n\t\tspin_lock(&nlm_blocked_lock);\n\t\tlist_del_init(&block->b_list);\n\t\tspin_unlock(&nlm_blocked_lock);\n\t\tnlmsvc_release_block(block);\n\t}\n}\n\n/*\n * Find a block for a given lock\n */\nstatic struct nlm_block *\nnlmsvc_lookup_block(struct nlm_file *file, struct nlm_lock *lock)\n{\n\tstruct nlm_block\t*block;\n\tstruct file_lock\t*fl;\n\n\tdprintk(\"lockd: nlmsvc_lookup_block f=%p pd=%d %Ld-%Ld ty=%d\\n\",\n\t\t\t\tfile, lock->fl.fl_pid,\n\t\t\t\t(long long)lock->fl.fl_start,\n\t\t\t\t(long long)lock->fl.fl_end, lock->fl.fl_type);\n\tlist_for_each_entry(block, &nlm_blocked, b_list) {\n\t\tfl = &block->b_call->a_args.lock.fl;\n\t\tdprintk(\"lockd: check f=%p pd=%d %Ld-%Ld ty=%d cookie=%s\\n\",\n\t\t\t\tblock->b_file, fl->fl_pid,\n\t\t\t\t(long long)fl->fl_start,\n\t\t\t\t(long long)fl->fl_end, fl->fl_type,\n\t\t\t\tnlmdbg_cookie2a(&block->b_call->a_args.cookie));\n\t\tif (block->b_file == file && nlm_compare_locks(fl, &lock->fl)) {\n\t\t\tkref_get(&block->b_count);\n\t\t\treturn block;\n\t\t}\n\t}\n\n\treturn NULL;\n}\n\nstatic inline int nlm_cookie_match(struct nlm_cookie *a, struct nlm_cookie *b)\n{\n\tif (a->len != b->len)\n\t\treturn 0;\n\tif (memcmp(a->data, b->data, a->len))\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * Find a block with a given NLM cookie.\n */\nstatic inline struct nlm_block *\nnlmsvc_find_block(struct nlm_cookie *cookie)\n{\n\tstruct nlm_block *block;\n\n\tlist_for_each_entry(block, &nlm_blocked, b_list) {\n\t\tif (nlm_cookie_match(&block->b_call->a_args.cookie,cookie))\n\t\t\tgoto found;\n\t}\n\n\treturn NULL;\n\nfound:\n\tdprintk(\"nlmsvc_find_block(%s): block=%p\\n\", nlmdbg_cookie2a(cookie), block);\n\tkref_get(&block->b_count);\n\treturn block;\n}\n\n/*\n * Create a block and initialize it.\n *\n * Note: we explicitly set the cookie of the grant reply to that of\n * the blocked lock request. The spec explicitly mentions that the client\n * should _not_ rely on the callback containing the same cookie as the\n * request, but (as I found out later) that's because some implementations\n * do just this. Never mind the standards comittees, they support our\n * logging industries.\n *\n * 10 years later: I hope we can safely ignore these old and broken\n * clients by now. Let's fix this so we can uniquely identify an incoming\n * GRANTED_RES message by cookie, without having to rely on the client's IP\n * address. --okir\n */\nstatic struct nlm_block *\nnlmsvc_create_block(struct svc_rqst *rqstp, struct nlm_host *host,\n\t\t    struct nlm_file *file, struct nlm_lock *lock,\n\t\t    struct nlm_cookie *cookie)\n{\n\tstruct nlm_block\t*block;\n\tstruct nlm_rqst\t\t*call = NULL;\n\n\tcall = nlm_alloc_call(host);\n\tif (call == NULL)\n\t\treturn NULL;\n\n\t/* Allocate memory for block, and initialize arguments */\n\tblock = kzalloc(sizeof(*block), GFP_KERNEL);\n\tif (block == NULL)\n\t\tgoto failed;\n\tkref_init(&block->b_count);\n\tINIT_LIST_HEAD(&block->b_list);\n\tINIT_LIST_HEAD(&block->b_flist);\n\n\tif (!nlmsvc_setgrantargs(call, lock))\n\t\tgoto failed_free;\n\n\t/* Set notifier function for VFS, and init args */\n\tcall->a_args.lock.fl.fl_flags |= FL_SLEEP;\n\tcall->a_args.lock.fl.fl_lmops = &nlmsvc_lock_operations;\n\tnlmclnt_next_cookie(&call->a_args.cookie);\n\n\tdprintk(\"lockd: created block %p...\\n\", block);\n\n\t/* Create and initialize the block */\n\tblock->b_daemon = rqstp->rq_server;\n\tblock->b_host   = host;\n\tblock->b_file   = file;\n\tfile->f_count++;\n\n\t/* Add to file's list of blocks */\n\tlist_add(&block->b_flist, &file->f_blocks);\n\n\t/* Set up RPC arguments for callback */\n\tblock->b_call = call;\n\tcall->a_flags   = RPC_TASK_ASYNC;\n\tcall->a_block = block;\n\n\treturn block;\n\nfailed_free:\n\tkfree(block);\nfailed:\n\tnlmsvc_release_call(call);\n\treturn NULL;\n}\n\n/*\n * Delete a block.\n * It is the caller's responsibility to check whether the file\n * can be closed hereafter.\n */\nstatic int nlmsvc_unlink_block(struct nlm_block *block)\n{\n\tint status;\n\tdprintk(\"lockd: unlinking block %p...\\n\", block);\n\n\t/* Remove block from list */\n\tstatus = posix_unblock_lock(&block->b_call->a_args.lock.fl);\n\tnlmsvc_remove_block(block);\n\treturn status;\n}\n\nstatic void nlmsvc_free_block(struct kref *kref)\n{\n\tstruct nlm_block *block = container_of(kref, struct nlm_block, b_count);\n\tstruct nlm_file\t\t*file = block->b_file;\n\n\tdprintk(\"lockd: freeing block %p...\\n\", block);\n\n\t/* Remove block from file's list of blocks */\n\tlist_del_init(&block->b_flist);\n\tmutex_unlock(&file->f_mutex);\n\n\tnlmsvc_freegrantargs(block->b_call);\n\tnlmsvc_release_call(block->b_call);\n\tnlm_release_file(block->b_file);\n\tkfree(block);\n}\n\nstatic void nlmsvc_release_block(struct nlm_block *block)\n{\n\tif (block != NULL)\n\t\tkref_put_mutex(&block->b_count, nlmsvc_free_block, &block->b_file->f_mutex);\n}\n\n/*\n * Loop over all blocks and delete blocks held by\n * a matching host.\n */\nvoid nlmsvc_traverse_blocks(struct nlm_host *host,\n\t\t\tstruct nlm_file *file,\n\t\t\tnlm_host_match_fn_t match)\n{\n\tstruct nlm_block *block, *next;\n\nrestart:\n\tmutex_lock(&file->f_mutex);\n\tlist_for_each_entry_safe(block, next, &file->f_blocks, b_flist) {\n\t\tif (!match(block->b_host, host))\n\t\t\tcontinue;\n\t\t/* Do not destroy blocks that are not on\n\t\t * the global retry list - why? */\n\t\tif (list_empty(&block->b_list))\n\t\t\tcontinue;\n\t\tkref_get(&block->b_count);\n\t\tmutex_unlock(&file->f_mutex);\n\t\tnlmsvc_unlink_block(block);\n\t\tnlmsvc_release_block(block);\n\t\tgoto restart;\n\t}\n\tmutex_unlock(&file->f_mutex);\n}\n\n/*\n * Initialize arguments for GRANTED call. The nlm_rqst structure\n * has been cleared already.\n */\nstatic int nlmsvc_setgrantargs(struct nlm_rqst *call, struct nlm_lock *lock)\n{\n\tlocks_copy_lock(&call->a_args.lock.fl, &lock->fl);\n\tmemcpy(&call->a_args.lock.fh, &lock->fh, sizeof(call->a_args.lock.fh));\n\tcall->a_args.lock.caller = utsname()->nodename;\n\tcall->a_args.lock.oh.len = lock->oh.len;\n\n\t/* set default data area */\n\tcall->a_args.lock.oh.data = call->a_owner;\n\tcall->a_args.lock.svid = lock->fl.fl_pid;\n\n\tif (lock->oh.len > NLMCLNT_OHSIZE) {\n\t\tvoid *data = kmalloc(lock->oh.len, GFP_KERNEL);\n\t\tif (!data)\n\t\t\treturn 0;\n\t\tcall->a_args.lock.oh.data = (u8 *) data;\n\t}\n\n\tmemcpy(call->a_args.lock.oh.data, lock->oh.data, lock->oh.len);\n\treturn 1;\n}\n\nstatic void nlmsvc_freegrantargs(struct nlm_rqst *call)\n{\n\tif (call->a_args.lock.oh.data != call->a_owner)\n\t\tkfree(call->a_args.lock.oh.data);\n\n\tlocks_release_private(&call->a_args.lock.fl);\n}\n\n/*\n * Deferred lock request handling for non-blocking lock\n */\nstatic __be32\nnlmsvc_defer_lock_rqst(struct svc_rqst *rqstp, struct nlm_block *block)\n{\n\t__be32 status = nlm_lck_denied_nolocks;\n\n\tblock->b_flags |= B_QUEUED;\n\n\tnlmsvc_insert_block(block, NLM_TIMEOUT);\n\n\tblock->b_cache_req = &rqstp->rq_chandle;\n\tif (rqstp->rq_chandle.defer) {\n\t\tblock->b_deferred_req =\n\t\t\trqstp->rq_chandle.defer(block->b_cache_req);\n\t\tif (block->b_deferred_req != NULL)\n\t\t\tstatus = nlm_drop_reply;\n\t}\n\tdprintk(\"lockd: nlmsvc_defer_lock_rqst block %p flags %d status %d\\n\",\n\t\tblock, block->b_flags, ntohl(status));\n\n\treturn status;\n}\n\n/*\n * Attempt to establish a lock, and if it can't be granted, block it\n * if required.\n */\n__be32\nnlmsvc_lock(struct svc_rqst *rqstp, struct nlm_file *file,\n\t    struct nlm_host *host, struct nlm_lock *lock, int wait,\n\t    struct nlm_cookie *cookie, int reclaim)\n{\n\tstruct nlm_block\t*block = NULL;\n\tint\t\t\terror;\n\t__be32\t\t\tret;\n\n\tdprintk(\"lockd: nlmsvc_lock(%s/%ld, ty=%d, pi=%d, %Ld-%Ld, bl=%d)\\n\",\n\t\t\t\tfile_inode(file->f_file)->i_sb->s_id,\n\t\t\t\tfile_inode(file->f_file)->i_ino,\n\t\t\t\tlock->fl.fl_type, lock->fl.fl_pid,\n\t\t\t\t(long long)lock->fl.fl_start,\n\t\t\t\t(long long)lock->fl.fl_end,\n\t\t\t\twait);\n\n\t/* Lock file against concurrent access */\n\tmutex_lock(&file->f_mutex);\n\t/* Get existing block (in case client is busy-waiting)\n\t * or create new block\n\t */\n\tblock = nlmsvc_lookup_block(file, lock);\n\tif (block == NULL) {\n\t\tblock = nlmsvc_create_block(rqstp, host, file, lock, cookie);\n\t\tret = nlm_lck_denied_nolocks;\n\t\tif (block == NULL)\n\t\t\tgoto out;\n\t\tlock = &block->b_call->a_args.lock;\n\t} else\n\t\tlock->fl.fl_flags &= ~FL_SLEEP;\n\n\tif (block->b_flags & B_QUEUED) {\n\t\tdprintk(\"lockd: nlmsvc_lock deferred block %p flags %d\\n\",\n\t\t\t\t\t\t\tblock, block->b_flags);\n\t\tif (block->b_granted) {\n\t\t\tnlmsvc_unlink_block(block);\n\t\t\tret = nlm_granted;\n\t\t\tgoto out;\n\t\t}\n\t\tif (block->b_flags & B_TIMED_OUT) {\n\t\t\tnlmsvc_unlink_block(block);\n\t\t\tret = nlm_lck_denied;\n\t\t\tgoto out;\n\t\t}\n\t\tret = nlm_drop_reply;\n\t\tgoto out;\n\t}\n\n\tif (locks_in_grace(SVC_NET(rqstp)) && !reclaim) {\n\t\tret = nlm_lck_denied_grace_period;\n\t\tgoto out;\n\t}\n\tif (reclaim && !locks_in_grace(SVC_NET(rqstp))) {\n\t\tret = nlm_lck_denied_grace_period;\n\t\tgoto out;\n\t}\n\n\tif (!wait)\n\t\tlock->fl.fl_flags &= ~FL_SLEEP;\n\terror = vfs_lock_file(file->f_file, F_SETLK, &lock->fl, NULL);\n\tlock->fl.fl_flags &= ~FL_SLEEP;\n\n\tdprintk(\"lockd: vfs_lock_file returned %d\\n\", error);\n\tswitch (error) {\n\t\tcase 0:\n\t\t\tret = nlm_granted;\n\t\t\tgoto out;\n\t\tcase -EAGAIN:\n\t\t\t/*\n\t\t\t * If this is a blocking request for an\n\t\t\t * already pending lock request then we need\n\t\t\t * to put it back on lockd's block list\n\t\t\t */\n\t\t\tif (wait)\n\t\t\t\tbreak;\n\t\t\tret = nlm_lck_denied;\n\t\t\tgoto out;\n\t\tcase FILE_LOCK_DEFERRED:\n\t\t\tif (wait)\n\t\t\t\tbreak;\n\t\t\t/* Filesystem lock operation is in progress\n\t\t\t   Add it to the queue waiting for callback */\n\t\t\tret = nlmsvc_defer_lock_rqst(rqstp, block);\n\t\t\tgoto out;\n\t\tcase -EDEADLK:\n\t\t\tret = nlm_deadlock;\n\t\t\tgoto out;\n\t\tdefault:\t\t\t/* includes ENOLCK */\n\t\t\tret = nlm_lck_denied_nolocks;\n\t\t\tgoto out;\n\t}\n\n\tret = nlm_lck_blocked;\n\n\t/* Append to list of blocked */\n\tnlmsvc_insert_block(block, NLM_NEVER);\nout:\n\tmutex_unlock(&file->f_mutex);\n\tnlmsvc_release_block(block);\n\tdprintk(\"lockd: nlmsvc_lock returned %u\\n\", ret);\n\treturn ret;\n}\n\n/*\n * Test for presence of a conflicting lock.\n */\n__be32\nnlmsvc_testlock(struct svc_rqst *rqstp, struct nlm_file *file,\n\t\tstruct nlm_host *host, struct nlm_lock *lock,\n\t\tstruct nlm_lock *conflock, struct nlm_cookie *cookie)\n{\n\tint\t\t\terror;\n\t__be32\t\t\tret;\n\n\tdprintk(\"lockd: nlmsvc_testlock(%s/%ld, ty=%d, %Ld-%Ld)\\n\",\n\t\t\t\tfile_inode(file->f_file)->i_sb->s_id,\n\t\t\t\tfile_inode(file->f_file)->i_ino,\n\t\t\t\tlock->fl.fl_type,\n\t\t\t\t(long long)lock->fl.fl_start,\n\t\t\t\t(long long)lock->fl.fl_end);\n\n\tif (locks_in_grace(SVC_NET(rqstp))) {\n\t\tret = nlm_lck_denied_grace_period;\n\t\tgoto out;\n\t}\n\n\terror = vfs_test_lock(file->f_file, &lock->fl);\n\tif (error) {\n\t\t/* We can't currently deal with deferred test requests */\n\t\tif (error == FILE_LOCK_DEFERRED)\n\t\t\tWARN_ON_ONCE(1);\n\n\t\tret = nlm_lck_denied_nolocks;\n\t\tgoto out;\n\t}\n\n\tif (lock->fl.fl_type == F_UNLCK) {\n\t\tret = nlm_granted;\n\t\tgoto out;\n\t}\n\n\tdprintk(\"lockd: conflicting lock(ty=%d, %Ld-%Ld)\\n\",\n\t\tlock->fl.fl_type, (long long)lock->fl.fl_start,\n\t\t(long long)lock->fl.fl_end);\n\tconflock->caller = \"somehost\";\t/* FIXME */\n\tconflock->len = strlen(conflock->caller);\n\tconflock->oh.len = 0;\t\t/* don't return OH info */\n\tconflock->svid = lock->fl.fl_pid;\n\tconflock->fl.fl_type = lock->fl.fl_type;\n\tconflock->fl.fl_start = lock->fl.fl_start;\n\tconflock->fl.fl_end = lock->fl.fl_end;\n\tlocks_release_private(&lock->fl);\n\tret = nlm_lck_denied;\nout:\n\treturn ret;\n}\n\n/*\n * Remove a lock.\n * This implies a CANCEL call: We send a GRANT_MSG, the client replies\n * with a GRANT_RES call which gets lost, and calls UNLOCK immediately\n * afterwards. In this case the block will still be there, and hence\n * must be removed.\n */\n__be32\nnlmsvc_unlock(struct net *net, struct nlm_file *file, struct nlm_lock *lock)\n{\n\tint\terror;\n\n\tdprintk(\"lockd: nlmsvc_unlock(%s/%ld, pi=%d, %Ld-%Ld)\\n\",\n\t\t\t\tfile_inode(file->f_file)->i_sb->s_id,\n\t\t\t\tfile_inode(file->f_file)->i_ino,\n\t\t\t\tlock->fl.fl_pid,\n\t\t\t\t(long long)lock->fl.fl_start,\n\t\t\t\t(long long)lock->fl.fl_end);\n\n\t/* First, cancel any lock that might be there */\n\tnlmsvc_cancel_blocked(net, file, lock);\n\n\tlock->fl.fl_type = F_UNLCK;\n\terror = vfs_lock_file(file->f_file, F_SETLK, &lock->fl, NULL);\n\n\treturn (error < 0)? nlm_lck_denied_nolocks : nlm_granted;\n}\n\n/*\n * Cancel a previously blocked request.\n *\n * A cancel request always overrides any grant that may currently\n * be in progress.\n * The calling procedure must check whether the file can be closed.\n */\n__be32\nnlmsvc_cancel_blocked(struct net *net, struct nlm_file *file, struct nlm_lock *lock)\n{\n\tstruct nlm_block\t*block;\n\tint status = 0;\n\n\tdprintk(\"lockd: nlmsvc_cancel(%s/%ld, pi=%d, %Ld-%Ld)\\n\",\n\t\t\t\tfile_inode(file->f_file)->i_sb->s_id,\n\t\t\t\tfile_inode(file->f_file)->i_ino,\n\t\t\t\tlock->fl.fl_pid,\n\t\t\t\t(long long)lock->fl.fl_start,\n\t\t\t\t(long long)lock->fl.fl_end);\n\n\tif (locks_in_grace(net))\n\t\treturn nlm_lck_denied_grace_period;\n\n\tmutex_lock(&file->f_mutex);\n\tblock = nlmsvc_lookup_block(file, lock);\n\tmutex_unlock(&file->f_mutex);\n\tif (block != NULL) {\n\t\tvfs_cancel_lock(block->b_file->f_file,\n\t\t\t\t&block->b_call->a_args.lock.fl);\n\t\tstatus = nlmsvc_unlink_block(block);\n\t\tnlmsvc_release_block(block);\n\t}\n\treturn status ? nlm_lck_denied : nlm_granted;\n}\n\n/*\n * This is a callback from the filesystem for VFS file lock requests.\n * It will be used if lm_grant is defined and the filesystem can not\n * respond to the request immediately.\n * For SETLK or SETLKW request it will get the local posix lock.\n * In all cases it will move the block to the head of nlm_blocked q where\n * nlmsvc_retry_blocked() can send back a reply for SETLKW or revisit the\n * deferred rpc for GETLK and SETLK.\n */\nstatic void\nnlmsvc_update_deferred_block(struct nlm_block *block, int result)\n{\n\tblock->b_flags |= B_GOT_CALLBACK;\n\tif (result == 0)\n\t\tblock->b_granted = 1;\n\telse\n\t\tblock->b_flags |= B_TIMED_OUT;\n}\n\nstatic int nlmsvc_grant_deferred(struct file_lock *fl, int result)\n{\n\tstruct nlm_block *block;\n\tint rc = -ENOENT;\n\n\tspin_lock(&nlm_blocked_lock);\n\tlist_for_each_entry(block, &nlm_blocked, b_list) {\n\t\tif (nlm_compare_locks(&block->b_call->a_args.lock.fl, fl)) {\n\t\t\tdprintk(\"lockd: nlmsvc_notify_blocked block %p flags %d\\n\",\n\t\t\t\t\t\t\tblock, block->b_flags);\n\t\t\tif (block->b_flags & B_QUEUED) {\n\t\t\t\tif (block->b_flags & B_TIMED_OUT) {\n\t\t\t\t\trc = -ENOLCK;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tnlmsvc_update_deferred_block(block, result);\n\t\t\t} else if (result == 0)\n\t\t\t\tblock->b_granted = 1;\n\n\t\t\tnlmsvc_insert_block_locked(block, 0);\n\t\t\tsvc_wake_up(block->b_daemon);\n\t\t\trc = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&nlm_blocked_lock);\n\tif (rc == -ENOENT)\n\t\tprintk(KERN_WARNING \"lockd: grant for unknown block\\n\");\n\treturn rc;\n}\n\n/*\n * Unblock a blocked lock request. This is a callback invoked from the\n * VFS layer when a lock on which we blocked is removed.\n *\n * This function doesn't grant the blocked lock instantly, but rather moves\n * the block to the head of nlm_blocked where it can be picked up by lockd.\n */\nstatic void\nnlmsvc_notify_blocked(struct file_lock *fl)\n{\n\tstruct nlm_block\t*block;\n\n\tdprintk(\"lockd: VFS unblock notification for block %p\\n\", fl);\n\tspin_lock(&nlm_blocked_lock);\n\tlist_for_each_entry(block, &nlm_blocked, b_list) {\n\t\tif (nlm_compare_locks(&block->b_call->a_args.lock.fl, fl)) {\n\t\t\tnlmsvc_insert_block_locked(block, 0);\n\t\t\tspin_unlock(&nlm_blocked_lock);\n\t\t\tsvc_wake_up(block->b_daemon);\n\t\t\treturn;\n\t\t}\n\t}\n\tspin_unlock(&nlm_blocked_lock);\n\tprintk(KERN_WARNING \"lockd: notification for unknown block!\\n\");\n}\n\nstatic int nlmsvc_same_owner(struct file_lock *fl1, struct file_lock *fl2)\n{\n\treturn fl1->fl_owner == fl2->fl_owner && fl1->fl_pid == fl2->fl_pid;\n}\n\n/*\n * Since NLM uses two \"keys\" for tracking locks, we need to hash them down\n * to one for the blocked_hash. Here, we're just xor'ing the host address\n * with the pid in order to create a key value for picking a hash bucket.\n */\nstatic unsigned long\nnlmsvc_owner_key(struct file_lock *fl)\n{\n\treturn (unsigned long)fl->fl_owner ^ (unsigned long)fl->fl_pid;\n}\n\nconst struct lock_manager_operations nlmsvc_lock_operations = {\n\t.lm_compare_owner = nlmsvc_same_owner,\n\t.lm_owner_key = nlmsvc_owner_key,\n\t.lm_notify = nlmsvc_notify_blocked,\n\t.lm_grant = nlmsvc_grant_deferred,\n};\n\n/*\n * Try to claim a lock that was previously blocked.\n *\n * Note that we use both the RPC_GRANTED_MSG call _and_ an async\n * RPC thread when notifying the client. This seems like overkill...\n * Here's why:\n *  -\twe don't want to use a synchronous RPC thread, otherwise\n *\twe might find ourselves hanging on a dead portmapper.\n *  -\tSome lockd implementations (e.g. HP) don't react to\n *\tRPC_GRANTED calls; they seem to insist on RPC_GRANTED_MSG calls.\n */\nstatic void\nnlmsvc_grant_blocked(struct nlm_block *block)\n{\n\tstruct nlm_file\t\t*file = block->b_file;\n\tstruct nlm_lock\t\t*lock = &block->b_call->a_args.lock;\n\tint\t\t\terror;\n\tloff_t\t\t\tfl_start, fl_end;\n\n\tdprintk(\"lockd: grant blocked lock %p\\n\", block);\n\n\tkref_get(&block->b_count);\n\n\t/* Unlink block request from list */\n\tnlmsvc_unlink_block(block);\n\n\t/* If b_granted is true this means we've been here before.\n\t * Just retry the grant callback, possibly refreshing the RPC\n\t * binding */\n\tif (block->b_granted) {\n\t\tnlm_rebind_host(block->b_host);\n\t\tgoto callback;\n\t}\n\n\t/* Try the lock operation again */\n\t/* vfs_lock_file() can mangle fl_start and fl_end, but we need\n\t * them unchanged for the GRANT_MSG\n\t */\n\tlock->fl.fl_flags |= FL_SLEEP;\n\tfl_start = lock->fl.fl_start;\n\tfl_end = lock->fl.fl_end;\n\terror = vfs_lock_file(file->f_file, F_SETLK, &lock->fl, NULL);\n\tlock->fl.fl_flags &= ~FL_SLEEP;\n\tlock->fl.fl_start = fl_start;\n\tlock->fl.fl_end = fl_end;\n\n\tswitch (error) {\n\tcase 0:\n\t\tbreak;\n\tcase FILE_LOCK_DEFERRED:\n\t\tdprintk(\"lockd: lock still blocked error %d\\n\", error);\n\t\tnlmsvc_insert_block(block, NLM_NEVER);\n\t\tnlmsvc_release_block(block);\n\t\treturn;\n\tdefault:\n\t\tprintk(KERN_WARNING \"lockd: unexpected error %d in %s!\\n\",\n\t\t\t\t-error, __func__);\n\t\tnlmsvc_insert_block(block, 10 * HZ);\n\t\tnlmsvc_release_block(block);\n\t\treturn;\n\t}\n\ncallback:\n\t/* Lock was granted by VFS. */\n\tdprintk(\"lockd: GRANTing blocked lock.\\n\");\n\tblock->b_granted = 1;\n\n\t/* keep block on the list, but don't reattempt until the RPC\n\t * completes or the submission fails\n\t */\n\tnlmsvc_insert_block(block, NLM_NEVER);\n\n\t/* Call the client -- use a soft RPC task since nlmsvc_retry_blocked\n\t * will queue up a new one if this one times out\n\t */\n\terror = nlm_async_call(block->b_call, NLMPROC_GRANTED_MSG,\n\t\t\t\t&nlmsvc_grant_ops);\n\n\t/* RPC submission failed, wait a bit and retry */\n\tif (error < 0)\n\t\tnlmsvc_insert_block(block, 10 * HZ);\n}\n\n/*\n * This is the callback from the RPC layer when the NLM_GRANTED_MSG\n * RPC call has succeeded or timed out.\n * Like all RPC callbacks, it is invoked by the rpciod process, so it\n * better not sleep. Therefore, we put the blocked lock on the nlm_blocked\n * chain once more in order to have it removed by lockd itself (which can\n * then sleep on the file semaphore without disrupting e.g. the nfs client).\n */\nstatic void nlmsvc_grant_callback(struct rpc_task *task, void *data)\n{\n\tstruct nlm_rqst\t\t*call = data;\n\tstruct nlm_block\t*block = call->a_block;\n\tunsigned long\t\ttimeout;\n\n\tdprintk(\"lockd: GRANT_MSG RPC callback\\n\");\n\n\tspin_lock(&nlm_blocked_lock);\n\t/* if the block is not on a list at this point then it has\n\t * been invalidated. Don't try to requeue it.\n\t *\n\t * FIXME: it's possible that the block is removed from the list\n\t * after this check but before the nlmsvc_insert_block. In that\n\t * case it will be added back. Perhaps we need better locking\n\t * for nlm_blocked?\n\t */\n\tif (list_empty(&block->b_list))\n\t\tgoto out;\n\n\t/* Technically, we should down the file semaphore here. Since we\n\t * move the block towards the head of the queue only, no harm\n\t * can be done, though. */\n\tif (task->tk_status < 0) {\n\t\t/* RPC error: Re-insert for retransmission */\n\t\ttimeout = 10 * HZ;\n\t} else {\n\t\t/* Call was successful, now wait for client callback */\n\t\ttimeout = 60 * HZ;\n\t}\n\tnlmsvc_insert_block_locked(block, timeout);\n\tsvc_wake_up(block->b_daemon);\nout:\n\tspin_unlock(&nlm_blocked_lock);\n}\n\n/*\n * FIXME: nlmsvc_release_block() grabs a mutex.  This is not allowed for an\n * .rpc_release rpc_call_op\n */\nstatic void nlmsvc_grant_release(void *data)\n{\n\tstruct nlm_rqst\t\t*call = data;\n\tnlmsvc_release_block(call->a_block);\n}\n\nstatic const struct rpc_call_ops nlmsvc_grant_ops = {\n\t.rpc_call_done = nlmsvc_grant_callback,\n\t.rpc_release = nlmsvc_grant_release,\n};\n\n/*\n * We received a GRANT_RES callback. Try to find the corresponding\n * block.\n */\nvoid\nnlmsvc_grant_reply(struct nlm_cookie *cookie, __be32 status)\n{\n\tstruct nlm_block\t*block;\n\n\tdprintk(\"grant_reply: looking for cookie %x, s=%d \\n\",\n\t\t*(unsigned int *)(cookie->data), status);\n\tif (!(block = nlmsvc_find_block(cookie)))\n\t\treturn;\n\n\tif (status == nlm_lck_denied_grace_period) {\n\t\t/* Try again in a couple of seconds */\n\t\tnlmsvc_insert_block(block, 10 * HZ);\n\t} else {\n\t\t/*\n\t\t * Lock is now held by client, or has been rejected.\n\t\t * In both cases, the block should be removed.\n\t\t */\n\t\tnlmsvc_unlink_block(block);\n\t}\n\tnlmsvc_release_block(block);\n}\n\n/* Helper function to handle retry of a deferred block.\n * If it is a blocking lock, call grant_blocked.\n * For a non-blocking lock or test lock, revisit the request.\n */\nstatic void\nretry_deferred_block(struct nlm_block *block)\n{\n\tif (!(block->b_flags & B_GOT_CALLBACK))\n\t\tblock->b_flags |= B_TIMED_OUT;\n\tnlmsvc_insert_block(block, NLM_TIMEOUT);\n\tdprintk(\"revisit block %p flags %d\\n\",\tblock, block->b_flags);\n\tif (block->b_deferred_req) {\n\t\tblock->b_deferred_req->revisit(block->b_deferred_req, 0);\n\t\tblock->b_deferred_req = NULL;\n\t}\n}\n\n/*\n * Retry all blocked locks that have been notified. This is where lockd\n * picks up locks that can be granted, or grant notifications that must\n * be retransmitted.\n */\nunsigned long\nnlmsvc_retry_blocked(void)\n{\n\tunsigned long\ttimeout = MAX_SCHEDULE_TIMEOUT;\n\tstruct nlm_block *block;\n\n\tspin_lock(&nlm_blocked_lock);\n\twhile (!list_empty(&nlm_blocked) && !kthread_should_stop()) {\n\t\tblock = list_entry(nlm_blocked.next, struct nlm_block, b_list);\n\n\t\tif (block->b_when == NLM_NEVER)\n\t\t\tbreak;\n\t\tif (time_after(block->b_when, jiffies)) {\n\t\t\ttimeout = block->b_when - jiffies;\n\t\t\tbreak;\n\t\t}\n\t\tspin_unlock(&nlm_blocked_lock);\n\n\t\tdprintk(\"nlmsvc_retry_blocked(%p, when=%ld)\\n\",\n\t\t\tblock, block->b_when);\n\t\tif (block->b_flags & B_QUEUED) {\n\t\t\tdprintk(\"nlmsvc_retry_blocked delete block (%p, granted=%d, flags=%d)\\n\",\n\t\t\t\tblock, block->b_granted, block->b_flags);\n\t\t\tretry_deferred_block(block);\n\t\t} else\n\t\t\tnlmsvc_grant_blocked(block);\n\t\tspin_lock(&nlm_blocked_lock);\n\t}\n\tspin_unlock(&nlm_blocked_lock);\n\n\treturn timeout;\n}\n", "/*\n * linux/fs/nfs/callback.c\n *\n * Copyright (C) 2004 Trond Myklebust\n *\n * NFSv4 callback handling\n */\n\n#include <linux/completion.h>\n#include <linux/ip.h>\n#include <linux/module.h>\n#include <linux/sched/signal.h>\n#include <linux/sunrpc/svc.h>\n#include <linux/sunrpc/svcsock.h>\n#include <linux/nfs_fs.h>\n#include <linux/errno.h>\n#include <linux/mutex.h>\n#include <linux/freezer.h>\n#include <linux/kthread.h>\n#include <linux/sunrpc/svcauth_gss.h>\n#include <linux/sunrpc/bc_xprt.h>\n\n#include <net/inet_sock.h>\n\n#include \"nfs4_fs.h\"\n#include \"callback.h\"\n#include \"internal.h\"\n#include \"netns.h\"\n\n#define NFSDBG_FACILITY NFSDBG_CALLBACK\n\nstruct nfs_callback_data {\n\tunsigned int users;\n\tstruct svc_serv *serv;\n};\n\nstatic struct nfs_callback_data nfs_callback_info[NFS4_MAX_MINOR_VERSION + 1];\nstatic DEFINE_MUTEX(nfs_callback_mutex);\nstatic struct svc_program nfs4_callback_program;\n\nstatic int nfs4_callback_up_net(struct svc_serv *serv, struct net *net)\n{\n\tint ret;\n\tstruct nfs_net *nn = net_generic(net, nfs_net_id);\n\n\tret = svc_create_xprt(serv, \"tcp\", net, PF_INET,\n\t\t\t\tnfs_callback_set_tcpport, SVC_SOCK_ANONYMOUS);\n\tif (ret <= 0)\n\t\tgoto out_err;\n\tnn->nfs_callback_tcpport = ret;\n\tdprintk(\"NFS: Callback listener port = %u (af %u, net %p)\\n\",\n\t\t\tnn->nfs_callback_tcpport, PF_INET, net);\n\n\tret = svc_create_xprt(serv, \"tcp\", net, PF_INET6,\n\t\t\t\tnfs_callback_set_tcpport, SVC_SOCK_ANONYMOUS);\n\tif (ret > 0) {\n\t\tnn->nfs_callback_tcpport6 = ret;\n\t\tdprintk(\"NFS: Callback listener port = %u (af %u, net %p)\\n\",\n\t\t\t\tnn->nfs_callback_tcpport6, PF_INET6, net);\n\t} else if (ret != -EAFNOSUPPORT)\n\t\tgoto out_err;\n\treturn 0;\n\nout_err:\n\treturn (ret) ? ret : -ENOMEM;\n}\n\n/*\n * This is the NFSv4 callback kernel thread.\n */\nstatic int\nnfs4_callback_svc(void *vrqstp)\n{\n\tint err;\n\tstruct svc_rqst *rqstp = vrqstp;\n\n\tset_freezable();\n\n\twhile (!kthread_freezable_should_stop(NULL)) {\n\n\t\tif (signal_pending(current))\n\t\t\tflush_signals(current);\n\t\t/*\n\t\t * Listen for a request on the socket\n\t\t */\n\t\terr = svc_recv(rqstp, MAX_SCHEDULE_TIMEOUT);\n\t\tif (err == -EAGAIN || err == -EINTR)\n\t\t\tcontinue;\n\t\tsvc_process(rqstp);\n\t}\n\tsvc_exit_thread(rqstp);\n\tmodule_put_and_exit(0);\n\treturn 0;\n}\n\n#if defined(CONFIG_NFS_V4_1)\n/*\n * The callback service for NFSv4.1 callbacks\n */\nstatic int\nnfs41_callback_svc(void *vrqstp)\n{\n\tstruct svc_rqst *rqstp = vrqstp;\n\tstruct svc_serv *serv = rqstp->rq_server;\n\tstruct rpc_rqst *req;\n\tint error;\n\tDEFINE_WAIT(wq);\n\n\tset_freezable();\n\n\twhile (!kthread_freezable_should_stop(NULL)) {\n\n\t\tif (signal_pending(current))\n\t\t\tflush_signals(current);\n\n\t\tprepare_to_wait(&serv->sv_cb_waitq, &wq, TASK_INTERRUPTIBLE);\n\t\tspin_lock_bh(&serv->sv_cb_lock);\n\t\tif (!list_empty(&serv->sv_cb_list)) {\n\t\t\treq = list_first_entry(&serv->sv_cb_list,\n\t\t\t\t\tstruct rpc_rqst, rq_bc_list);\n\t\t\tlist_del(&req->rq_bc_list);\n\t\t\tspin_unlock_bh(&serv->sv_cb_lock);\n\t\t\tfinish_wait(&serv->sv_cb_waitq, &wq);\n\t\t\tdprintk(\"Invoking bc_svc_process()\\n\");\n\t\t\terror = bc_svc_process(serv, req, rqstp);\n\t\t\tdprintk(\"bc_svc_process() returned w/ error code= %d\\n\",\n\t\t\t\terror);\n\t\t} else {\n\t\t\tspin_unlock_bh(&serv->sv_cb_lock);\n\t\t\tif (!kthread_should_stop())\n\t\t\t\tschedule();\n\t\t\tfinish_wait(&serv->sv_cb_waitq, &wq);\n\t\t}\n\t}\n\tsvc_exit_thread(rqstp);\n\tmodule_put_and_exit(0);\n\treturn 0;\n}\n\nstatic inline void nfs_callback_bc_serv(u32 minorversion, struct rpc_xprt *xprt,\n\t\tstruct svc_serv *serv)\n{\n\tif (minorversion)\n\t\t/*\n\t\t * Save the svc_serv in the transport so that it can\n\t\t * be referenced when the session backchannel is initialized\n\t\t */\n\t\txprt->bc_serv = serv;\n}\n#else\nstatic inline void nfs_callback_bc_serv(u32 minorversion, struct rpc_xprt *xprt,\n\t\tstruct svc_serv *serv)\n{\n}\n#endif /* CONFIG_NFS_V4_1 */\n\nstatic int nfs_callback_start_svc(int minorversion, struct rpc_xprt *xprt,\n\t\t\t\t  struct svc_serv *serv)\n{\n\tint nrservs = nfs_callback_nr_threads;\n\tint ret;\n\n\tnfs_callback_bc_serv(minorversion, xprt, serv);\n\n\tif (nrservs < NFS4_MIN_NR_CALLBACK_THREADS)\n\t\tnrservs = NFS4_MIN_NR_CALLBACK_THREADS;\n\n\tif (serv->sv_nrthreads-1 == nrservs)\n\t\treturn 0;\n\n\tret = serv->sv_ops->svo_setup(serv, NULL, nrservs);\n\tif (ret) {\n\t\tserv->sv_ops->svo_setup(serv, NULL, 0);\n\t\treturn ret;\n\t}\n\tdprintk(\"nfs_callback_up: service started\\n\");\n\treturn 0;\n}\n\nstatic void nfs_callback_down_net(u32 minorversion, struct svc_serv *serv, struct net *net)\n{\n\tstruct nfs_net *nn = net_generic(net, nfs_net_id);\n\n\tif (--nn->cb_users[minorversion])\n\t\treturn;\n\n\tdprintk(\"NFS: destroy per-net callback data; net=%p\\n\", net);\n\tsvc_shutdown_net(serv, net);\n}\n\nstatic int nfs_callback_up_net(int minorversion, struct svc_serv *serv,\n\t\t\t       struct net *net, struct rpc_xprt *xprt)\n{\n\tstruct nfs_net *nn = net_generic(net, nfs_net_id);\n\tint ret;\n\n\tif (nn->cb_users[minorversion]++)\n\t\treturn 0;\n\n\tdprintk(\"NFS: create per-net callback data; net=%p\\n\", net);\n\n\tret = svc_bind(serv, net);\n\tif (ret < 0) {\n\t\tprintk(KERN_WARNING \"NFS: bind callback service failed\\n\");\n\t\tgoto err_bind;\n\t}\n\n\tret = -EPROTONOSUPPORT;\n\tif (!IS_ENABLED(CONFIG_NFS_V4_1) || minorversion == 0)\n\t\tret = nfs4_callback_up_net(serv, net);\n\telse if (xprt->ops->bc_up)\n\t\tret = xprt->ops->bc_up(serv, net);\n\n\tif (ret < 0) {\n\t\tprintk(KERN_ERR \"NFS: callback service start failed\\n\");\n\t\tgoto err_socks;\n\t}\n\treturn 0;\n\nerr_socks:\n\tsvc_rpcb_cleanup(serv, net);\nerr_bind:\n\tnn->cb_users[minorversion]--;\n\tdprintk(\"NFS: Couldn't create callback socket: err = %d; \"\n\t\t\t\"net = %p\\n\", ret, net);\n\treturn ret;\n}\n\nstatic struct svc_serv_ops nfs40_cb_sv_ops = {\n\t.svo_function\t\t= nfs4_callback_svc,\n\t.svo_enqueue_xprt\t= svc_xprt_do_enqueue,\n\t.svo_setup\t\t= svc_set_num_threads_sync,\n\t.svo_module\t\t= THIS_MODULE,\n};\n#if defined(CONFIG_NFS_V4_1)\nstatic struct svc_serv_ops nfs41_cb_sv_ops = {\n\t.svo_function\t\t= nfs41_callback_svc,\n\t.svo_enqueue_xprt\t= svc_xprt_do_enqueue,\n\t.svo_setup\t\t= svc_set_num_threads_sync,\n\t.svo_module\t\t= THIS_MODULE,\n};\n\nstatic struct svc_serv_ops *nfs4_cb_sv_ops[] = {\n\t[0] = &nfs40_cb_sv_ops,\n\t[1] = &nfs41_cb_sv_ops,\n};\n#else\nstatic struct svc_serv_ops *nfs4_cb_sv_ops[] = {\n\t[0] = &nfs40_cb_sv_ops,\n\t[1] = NULL,\n};\n#endif\n\nstatic struct svc_serv *nfs_callback_create_svc(int minorversion)\n{\n\tstruct nfs_callback_data *cb_info = &nfs_callback_info[minorversion];\n\tstruct svc_serv *serv;\n\tstruct svc_serv_ops *sv_ops;\n\n\t/*\n\t * Check whether we're already up and running.\n\t */\n\tif (cb_info->serv) {\n\t\t/*\n\t\t * Note: increase service usage, because later in case of error\n\t\t * svc_destroy() will be called.\n\t\t */\n\t\tsvc_get(cb_info->serv);\n\t\treturn cb_info->serv;\n\t}\n\n\tswitch (minorversion) {\n\tcase 0:\n\t\tsv_ops = nfs4_cb_sv_ops[0];\n\t\tbreak;\n\tdefault:\n\t\tsv_ops = nfs4_cb_sv_ops[1];\n\t}\n\n\tif (sv_ops == NULL)\n\t\treturn ERR_PTR(-ENOTSUPP);\n\n\t/*\n\t * Sanity check: if there's no task,\n\t * we should be the first user ...\n\t */\n\tif (cb_info->users)\n\t\tprintk(KERN_WARNING \"nfs_callback_create_svc: no kthread, %d users??\\n\",\n\t\t\tcb_info->users);\n\n\tserv = svc_create_pooled(&nfs4_callback_program, NFS4_CALLBACK_BUFSIZE, sv_ops);\n\tif (!serv) {\n\t\tprintk(KERN_ERR \"nfs_callback_create_svc: create service failed\\n\");\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\tcb_info->serv = serv;\n\t/* As there is only one thread we need to over-ride the\n\t * default maximum of 80 connections\n\t */\n\tserv->sv_maxconn = 1024;\n\tdprintk(\"nfs_callback_create_svc: service created\\n\");\n\treturn serv;\n}\n\n/*\n * Bring up the callback thread if it is not already up.\n */\nint nfs_callback_up(u32 minorversion, struct rpc_xprt *xprt)\n{\n\tstruct svc_serv *serv;\n\tstruct nfs_callback_data *cb_info = &nfs_callback_info[minorversion];\n\tint ret;\n\tstruct net *net = xprt->xprt_net;\n\n\tmutex_lock(&nfs_callback_mutex);\n\n\tserv = nfs_callback_create_svc(minorversion);\n\tif (IS_ERR(serv)) {\n\t\tret = PTR_ERR(serv);\n\t\tgoto err_create;\n\t}\n\n\tret = nfs_callback_up_net(minorversion, serv, net, xprt);\n\tif (ret < 0)\n\t\tgoto err_net;\n\n\tret = nfs_callback_start_svc(minorversion, xprt, serv);\n\tif (ret < 0)\n\t\tgoto err_start;\n\n\tcb_info->users++;\n\t/*\n\t * svc_create creates the svc_serv with sv_nrthreads == 1, and then\n\t * svc_prepare_thread increments that. So we need to call svc_destroy\n\t * on both success and failure so that the refcount is 1 when the\n\t * thread exits.\n\t */\nerr_net:\n\tif (!cb_info->users)\n\t\tcb_info->serv = NULL;\n\tsvc_destroy(serv);\nerr_create:\n\tmutex_unlock(&nfs_callback_mutex);\n\treturn ret;\n\nerr_start:\n\tnfs_callback_down_net(minorversion, serv, net);\n\tdprintk(\"NFS: Couldn't create server thread; err = %d\\n\", ret);\n\tgoto err_net;\n}\n\n/*\n * Kill the callback thread if it's no longer being used.\n */\nvoid nfs_callback_down(int minorversion, struct net *net)\n{\n\tstruct nfs_callback_data *cb_info = &nfs_callback_info[minorversion];\n\tstruct svc_serv *serv;\n\n\tmutex_lock(&nfs_callback_mutex);\n\tserv = cb_info->serv;\n\tnfs_callback_down_net(minorversion, serv, net);\n\tcb_info->users--;\n\tif (cb_info->users == 0) {\n\t\tsvc_get(serv);\n\t\tserv->sv_ops->svo_setup(serv, NULL, 0);\n\t\tsvc_destroy(serv);\n\t\tdprintk(\"nfs_callback_down: service destroyed\\n\");\n\t\tcb_info->serv = NULL;\n\t}\n\tmutex_unlock(&nfs_callback_mutex);\n}\n\n/* Boolean check of RPC_AUTH_GSS principal */\nint\ncheck_gss_callback_principal(struct nfs_client *clp, struct svc_rqst *rqstp)\n{\n\tchar *p = rqstp->rq_cred.cr_principal;\n\n\tif (rqstp->rq_authop->flavour != RPC_AUTH_GSS)\n\t\treturn 1;\n\n\t/* No RPC_AUTH_GSS on NFSv4.1 back channel yet */\n\tif (clp->cl_minorversion != 0)\n\t\treturn 0;\n\t/*\n\t * It might just be a normal user principal, in which case\n\t * userspace won't bother to tell us the name at all.\n\t */\n\tif (p == NULL)\n\t\treturn 0;\n\n\t/*\n\t * Did we get the acceptor from userland during the SETCLIENID\n\t * negotiation?\n\t */\n\tif (clp->cl_acceptor)\n\t\treturn !strcmp(p, clp->cl_acceptor);\n\n\t/*\n\t * Otherwise try to verify it using the cl_hostname. Note that this\n\t * doesn't work if a non-canonical hostname was used in the devname.\n\t */\n\n\t/* Expect a GSS_C_NT_HOSTBASED_NAME like \"nfs@serverhostname\" */\n\n\tif (memcmp(p, \"nfs@\", 4) != 0)\n\t\treturn 0;\n\tp += 4;\n\tif (strcmp(p, clp->cl_hostname) != 0)\n\t\treturn 0;\n\treturn 1;\n}\n\n/*\n * pg_authenticate method for nfsv4 callback threads.\n *\n * The authflavor has been negotiated, so an incorrect flavor is a server\n * bug. Deny packets with incorrect authflavor.\n *\n * All other checking done after NFS decoding where the nfs_client can be\n * found in nfs4_callback_compound\n */\nstatic int nfs_callback_authenticate(struct svc_rqst *rqstp)\n{\n\tswitch (rqstp->rq_authop->flavour) {\n\tcase RPC_AUTH_NULL:\n\t\tif (rqstp->rq_proc != CB_NULL)\n\t\t\treturn SVC_DENIED;\n\t\tbreak;\n\tcase RPC_AUTH_GSS:\n\t\t/* No RPC_AUTH_GSS support yet in NFSv4.1 */\n\t\t if (svc_is_backchannel(rqstp))\n\t\t\treturn SVC_DENIED;\n\t}\n\treturn SVC_OK;\n}\n\n/*\n * Define NFS4 callback program\n */\nstatic struct svc_version *nfs4_callback_version[] = {\n\t[1] = &nfs4_callback_version1,\n\t[4] = &nfs4_callback_version4,\n};\n\nstatic struct svc_stat nfs4_callback_stats;\n\nstatic struct svc_program nfs4_callback_program = {\n\t.pg_prog = NFS4_CALLBACK,\t\t\t/* RPC service number */\n\t.pg_nvers = ARRAY_SIZE(nfs4_callback_version),\t/* Number of entries */\n\t.pg_vers = nfs4_callback_version,\t\t/* version table */\n\t.pg_name = \"NFSv4 callback\",\t\t\t/* service name */\n\t.pg_class = \"nfs\",\t\t\t\t/* authentication class */\n\t.pg_stats = &nfs4_callback_stats,\n\t.pg_authenticate = nfs_callback_authenticate,\n};\n", "/*\n * XDR support for nfsd/protocol version 3.\n *\n * Copyright (C) 1995, 1996, 1997 Olaf Kirch <okir@monad.swb.de>\n *\n * 2003-08-09 Jamie Lokier: Use htonl() for nanoseconds, not htons()!\n */\n\n#include <linux/namei.h>\n#include <linux/sunrpc/svc_xprt.h>\n#include \"xdr3.h\"\n#include \"auth.h\"\n#include \"netns.h\"\n#include \"vfs.h\"\n\n#define NFSDDBG_FACILITY\t\tNFSDDBG_XDR\n\n\n/*\n * Mapping of S_IF* types to NFS file types\n */\nstatic u32\tnfs3_ftypes[] = {\n\tNF3NON,  NF3FIFO, NF3CHR, NF3BAD,\n\tNF3DIR,  NF3BAD,  NF3BLK, NF3BAD,\n\tNF3REG,  NF3BAD,  NF3LNK, NF3BAD,\n\tNF3SOCK, NF3BAD,  NF3LNK, NF3BAD,\n};\n\n/*\n * XDR functions for basic NFS types\n */\nstatic __be32 *\nencode_time3(__be32 *p, struct timespec *time)\n{\n\t*p++ = htonl((u32) time->tv_sec); *p++ = htonl(time->tv_nsec);\n\treturn p;\n}\n\nstatic __be32 *\ndecode_time3(__be32 *p, struct timespec *time)\n{\n\ttime->tv_sec = ntohl(*p++);\n\ttime->tv_nsec = ntohl(*p++);\n\treturn p;\n}\n\nstatic __be32 *\ndecode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\tunsigned int size;\n\tfh_init(fhp, NFS3_FHSIZE);\n\tsize = ntohl(*p++);\n\tif (size > NFS3_FHSIZE)\n\t\treturn NULL;\n\n\tmemcpy(&fhp->fh_handle.fh_base, p, size);\n\tfhp->fh_handle.fh_size = size;\n\treturn p + XDR_QUADLEN(size);\n}\n\n/* Helper function for NFSv3 ACL code */\n__be32 *nfs3svc_decode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\treturn decode_fh(p, fhp);\n}\n\nstatic __be32 *\nencode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\tunsigned int size = fhp->fh_handle.fh_size;\n\t*p++ = htonl(size);\n\tif (size) p[XDR_QUADLEN(size)-1]=0;\n\tmemcpy(p, &fhp->fh_handle.fh_base, size);\n\treturn p + XDR_QUADLEN(size);\n}\n\n/*\n * Decode a file name and make sure that the path contains\n * no slashes or null bytes.\n */\nstatic __be32 *\ndecode_filename(__be32 *p, char **namp, unsigned int *lenp)\n{\n\tchar\t\t*name;\n\tunsigned int\ti;\n\n\tif ((p = xdr_decode_string_inplace(p, namp, lenp, NFS3_MAXNAMLEN)) != NULL) {\n\t\tfor (i = 0, name = *namp; i < *lenp; i++, name++) {\n\t\t\tif (*name == '\\0' || *name == '/')\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn p;\n}\n\nstatic __be32 *\ndecode_sattr3(__be32 *p, struct iattr *iap)\n{\n\tu32\ttmp;\n\n\tiap->ia_valid = 0;\n\n\tif (*p++) {\n\t\tiap->ia_valid |= ATTR_MODE;\n\t\tiap->ia_mode = ntohl(*p++);\n\t}\n\tif (*p++) {\n\t\tiap->ia_uid = make_kuid(&init_user_ns, ntohl(*p++));\n\t\tif (uid_valid(iap->ia_uid))\n\t\t\tiap->ia_valid |= ATTR_UID;\n\t}\n\tif (*p++) {\n\t\tiap->ia_gid = make_kgid(&init_user_ns, ntohl(*p++));\n\t\tif (gid_valid(iap->ia_gid))\n\t\t\tiap->ia_valid |= ATTR_GID;\n\t}\n\tif (*p++) {\n\t\tu64\tnewsize;\n\n\t\tiap->ia_valid |= ATTR_SIZE;\n\t\tp = xdr_decode_hyper(p, &newsize);\n\t\tiap->ia_size = min_t(u64, newsize, NFS_OFFSET_MAX);\n\t}\n\tif ((tmp = ntohl(*p++)) == 1) {\t/* set to server time */\n\t\tiap->ia_valid |= ATTR_ATIME;\n\t} else if (tmp == 2) {\t\t/* set to client time */\n\t\tiap->ia_valid |= ATTR_ATIME | ATTR_ATIME_SET;\n\t\tiap->ia_atime.tv_sec = ntohl(*p++);\n\t\tiap->ia_atime.tv_nsec = ntohl(*p++);\n\t}\n\tif ((tmp = ntohl(*p++)) == 1) {\t/* set to server time */\n\t\tiap->ia_valid |= ATTR_MTIME;\n\t} else if (tmp == 2) {\t\t/* set to client time */\n\t\tiap->ia_valid |= ATTR_MTIME | ATTR_MTIME_SET;\n\t\tiap->ia_mtime.tv_sec = ntohl(*p++);\n\t\tiap->ia_mtime.tv_nsec = ntohl(*p++);\n\t}\n\treturn p;\n}\n\nstatic __be32 *encode_fsid(__be32 *p, struct svc_fh *fhp)\n{\n\tu64 f;\n\tswitch(fsid_source(fhp)) {\n\tdefault:\n\tcase FSIDSOURCE_DEV:\n\t\tp = xdr_encode_hyper(p, (u64)huge_encode_dev\n\t\t\t\t     (fhp->fh_dentry->d_sb->s_dev));\n\t\tbreak;\n\tcase FSIDSOURCE_FSID:\n\t\tp = xdr_encode_hyper(p, (u64) fhp->fh_export->ex_fsid);\n\t\tbreak;\n\tcase FSIDSOURCE_UUID:\n\t\tf = ((u64*)fhp->fh_export->ex_uuid)[0];\n\t\tf ^= ((u64*)fhp->fh_export->ex_uuid)[1];\n\t\tp = xdr_encode_hyper(p, f);\n\t\tbreak;\n\t}\n\treturn p;\n}\n\nstatic __be32 *\nencode_fattr3(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp,\n\t      struct kstat *stat)\n{\n\t*p++ = htonl(nfs3_ftypes[(stat->mode & S_IFMT) >> 12]);\n\t*p++ = htonl((u32) (stat->mode & S_IALLUGO));\n\t*p++ = htonl((u32) stat->nlink);\n\t*p++ = htonl((u32) from_kuid(&init_user_ns, stat->uid));\n\t*p++ = htonl((u32) from_kgid(&init_user_ns, stat->gid));\n\tif (S_ISLNK(stat->mode) && stat->size > NFS3_MAXPATHLEN) {\n\t\tp = xdr_encode_hyper(p, (u64) NFS3_MAXPATHLEN);\n\t} else {\n\t\tp = xdr_encode_hyper(p, (u64) stat->size);\n\t}\n\tp = xdr_encode_hyper(p, ((u64)stat->blocks) << 9);\n\t*p++ = htonl((u32) MAJOR(stat->rdev));\n\t*p++ = htonl((u32) MINOR(stat->rdev));\n\tp = encode_fsid(p, fhp);\n\tp = xdr_encode_hyper(p, stat->ino);\n\tp = encode_time3(p, &stat->atime);\n\tp = encode_time3(p, &stat->mtime);\n\tp = encode_time3(p, &stat->ctime);\n\n\treturn p;\n}\n\nstatic __be32 *\nencode_saved_post_attr(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp)\n{\n\t/* Attributes to follow */\n\t*p++ = xdr_one;\n\treturn encode_fattr3(rqstp, p, fhp, &fhp->fh_post_attr);\n}\n\n/*\n * Encode post-operation attributes.\n * The inode may be NULL if the call failed because of a stale file\n * handle. In this case, no attributes are returned.\n */\nstatic __be32 *\nencode_post_op_attr(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp)\n{\n\tstruct dentry *dentry = fhp->fh_dentry;\n\tif (dentry && d_really_is_positive(dentry)) {\n\t        __be32 err;\n\t\tstruct kstat stat;\n\n\t\terr = fh_getattr(fhp, &stat);\n\t\tif (!err) {\n\t\t\t*p++ = xdr_one;\t\t/* attributes follow */\n\t\t\tlease_get_mtime(d_inode(dentry), &stat.mtime);\n\t\t\treturn encode_fattr3(rqstp, p, fhp, &stat);\n\t\t}\n\t}\n\t*p++ = xdr_zero;\n\treturn p;\n}\n\n/* Helper for NFSv3 ACLs */\n__be32 *\nnfs3svc_encode_post_op_attr(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp)\n{\n\treturn encode_post_op_attr(rqstp, p, fhp);\n}\n\n/*\n * Enocde weak cache consistency data\n */\nstatic __be32 *\nencode_wcc_data(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp)\n{\n\tstruct dentry\t*dentry = fhp->fh_dentry;\n\n\tif (dentry && d_really_is_positive(dentry) && fhp->fh_post_saved) {\n\t\tif (fhp->fh_pre_saved) {\n\t\t\t*p++ = xdr_one;\n\t\t\tp = xdr_encode_hyper(p, (u64) fhp->fh_pre_size);\n\t\t\tp = encode_time3(p, &fhp->fh_pre_mtime);\n\t\t\tp = encode_time3(p, &fhp->fh_pre_ctime);\n\t\t} else {\n\t\t\t*p++ = xdr_zero;\n\t\t}\n\t\treturn encode_saved_post_attr(rqstp, p, fhp);\n\t}\n\t/* no pre- or post-attrs */\n\t*p++ = xdr_zero;\n\treturn encode_post_op_attr(rqstp, p, fhp);\n}\n\n/*\n * Fill in the post_op attr for the wcc data\n */\nvoid fill_post_wcc(struct svc_fh *fhp)\n{\n\t__be32 err;\n\n\tif (fhp->fh_post_saved)\n\t\tprintk(\"nfsd: inode locked twice during operation.\\n\");\n\n\terr = fh_getattr(fhp, &fhp->fh_post_attr);\n\tfhp->fh_post_change = d_inode(fhp->fh_dentry)->i_version;\n\tif (err) {\n\t\tfhp->fh_post_saved = false;\n\t\t/* Grab the ctime anyway - set_change_info might use it */\n\t\tfhp->fh_post_attr.ctime = d_inode(fhp->fh_dentry)->i_ctime;\n\t} else\n\t\tfhp->fh_post_saved = true;\n}\n\n/*\n * XDR decode functions\n */\nint\nnfs3svc_decode_fhandle(struct svc_rqst *rqstp, __be32 *p, struct nfsd_fhandle *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_sattrargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_sattrargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = decode_sattr3(p, &args->attrs);\n\n\tif ((args->check_guard = ntohl(*p++)) != 0) { \n\t\tstruct timespec time; \n\t\tp = decode_time3(p, &time);\n\t\targs->guardtime = time.tv_sec;\n\t}\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_diropargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_diropargs *args)\n{\n\tif (!(p = decode_fh(p, &args->fh))\n\t || !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_accessargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_accessargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\targs->access = ntohl(*p++);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_readargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readargs *args)\n{\n\tunsigned int len;\n\tint v;\n\tu32 max_blocksize = svc_max_payload(rqstp);\n\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = xdr_decode_hyper(p, &args->offset);\n\targs->count = ntohl(*p++);\n\n\tif (!xdr_argsize_check(rqstp, p))\n\t\treturn 0;\n\n\tlen = min(args->count, max_blocksize);\n\n\t/* set up the kvec */\n\tv=0;\n\twhile (len > 0) {\n\t\tstruct page *p = *(rqstp->rq_next_page++);\n\n\t\trqstp->rq_vec[v].iov_base = page_address(p);\n\t\trqstp->rq_vec[v].iov_len = min_t(unsigned int, len, PAGE_SIZE);\n\t\tlen -= rqstp->rq_vec[v].iov_len;\n\t\tv++;\n\t}\n\targs->vlen = v;\n\treturn 1;\n}\n\nint\nnfs3svc_decode_writeargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_writeargs *args)\n{\n\tunsigned int len, v, hdr, dlen;\n\tu32 max_blocksize = svc_max_payload(rqstp);\n\tstruct kvec *head = rqstp->rq_arg.head;\n\tstruct kvec *tail = rqstp->rq_arg.tail;\n\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = xdr_decode_hyper(p, &args->offset);\n\n\targs->count = ntohl(*p++);\n\targs->stable = ntohl(*p++);\n\tlen = args->len = ntohl(*p++);\n\tif ((void *)p > head->iov_base + head->iov_len)\n\t\treturn 0;\n\t/*\n\t * The count must equal the amount of data passed.\n\t */\n\tif (args->count != args->len)\n\t\treturn 0;\n\n\t/*\n\t * Check to make sure that we got the right number of\n\t * bytes.\n\t */\n\thdr = (void*)p - head->iov_base;\n\tdlen = head->iov_len + rqstp->rq_arg.page_len + tail->iov_len - hdr;\n\t/*\n\t * Round the length of the data which was specified up to\n\t * the next multiple of XDR units and then compare that\n\t * against the length which was actually received.\n\t * Note that when RPCSEC/GSS (for example) is used, the\n\t * data buffer can be padded so dlen might be larger\n\t * than required.  It must never be smaller.\n\t */\n\tif (dlen < XDR_QUADLEN(len)*4)\n\t\treturn 0;\n\n\tif (args->count > max_blocksize) {\n\t\targs->count = max_blocksize;\n\t\tlen = args->len = max_blocksize;\n\t}\n\trqstp->rq_vec[0].iov_base = (void*)p;\n\trqstp->rq_vec[0].iov_len = head->iov_len - hdr;\n\tv = 0;\n\twhile (len > rqstp->rq_vec[v].iov_len) {\n\t\tlen -= rqstp->rq_vec[v].iov_len;\n\t\tv++;\n\t\trqstp->rq_vec[v].iov_base = page_address(rqstp->rq_pages[v]);\n\t\trqstp->rq_vec[v].iov_len = PAGE_SIZE;\n\t}\n\trqstp->rq_vec[v].iov_len = len;\n\targs->vlen = v + 1;\n\treturn 1;\n}\n\nint\nnfs3svc_decode_createargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_createargs *args)\n{\n\tif (!(p = decode_fh(p, &args->fh))\n\t || !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\n\tswitch (args->createmode = ntohl(*p++)) {\n\tcase NFS3_CREATE_UNCHECKED:\n\tcase NFS3_CREATE_GUARDED:\n\t\tp = decode_sattr3(p, &args->attrs);\n\t\tbreak;\n\tcase NFS3_CREATE_EXCLUSIVE:\n\t\targs->verf = p;\n\t\tp += 2;\n\t\tbreak;\n\tdefault:\n\t\treturn 0;\n\t}\n\n\treturn xdr_argsize_check(rqstp, p);\n}\nint\nnfs3svc_decode_mkdirargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_createargs *args)\n{\n\tif (!(p = decode_fh(p, &args->fh)) ||\n\t    !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\tp = decode_sattr3(p, &args->attrs);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_symlinkargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_symlinkargs *args)\n{\n\tunsigned int len, avail;\n\tchar *old, *new;\n\tstruct kvec *vec;\n\n\tif (!(p = decode_fh(p, &args->ffh)) ||\n\t    !(p = decode_filename(p, &args->fname, &args->flen))\n\t\t)\n\t\treturn 0;\n\tp = decode_sattr3(p, &args->attrs);\n\n\t/* now decode the pathname, which might be larger than the first page.\n\t * As we have to check for nul's anyway, we copy it into a new page\n\t * This page appears in the rq_res.pages list, but as pages_len is always\n\t * 0, it won't get in the way\n\t */\n\tlen = ntohl(*p++);\n\tif (len == 0 || len > NFS3_MAXPATHLEN || len >= PAGE_SIZE)\n\t\treturn 0;\n\targs->tname = new = page_address(*(rqstp->rq_next_page++));\n\targs->tlen = len;\n\t/* first copy and check from the first page */\n\told = (char*)p;\n\tvec = &rqstp->rq_arg.head[0];\n\tif ((void *)old > vec->iov_base + vec->iov_len)\n\t\treturn 0;\n\tavail = vec->iov_len - (old - (char*)vec->iov_base);\n\twhile (len && avail && *old) {\n\t\t*new++ = *old++;\n\t\tlen--;\n\t\tavail--;\n\t}\n\t/* now copy next page if there is one */\n\tif (len && !avail && rqstp->rq_arg.page_len) {\n\t\tavail = min_t(unsigned int, rqstp->rq_arg.page_len, PAGE_SIZE);\n\t\told = page_address(rqstp->rq_arg.pages[0]);\n\t}\n\twhile (len && avail && *old) {\n\t\t*new++ = *old++;\n\t\tlen--;\n\t\tavail--;\n\t}\n\t*new = '\\0';\n\tif (len)\n\t\treturn 0;\n\n\treturn 1;\n}\n\nint\nnfs3svc_decode_mknodargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_mknodargs *args)\n{\n\tif (!(p = decode_fh(p, &args->fh))\n\t || !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\n\targs->ftype = ntohl(*p++);\n\n\tif (args->ftype == NF3BLK  || args->ftype == NF3CHR\n\t || args->ftype == NF3SOCK || args->ftype == NF3FIFO)\n\t\tp = decode_sattr3(p, &args->attrs);\n\n\tif (args->ftype == NF3BLK || args->ftype == NF3CHR) {\n\t\targs->major = ntohl(*p++);\n\t\targs->minor = ntohl(*p++);\n\t}\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_renameargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_renameargs *args)\n{\n\tif (!(p = decode_fh(p, &args->ffh))\n\t || !(p = decode_filename(p, &args->fname, &args->flen))\n\t || !(p = decode_fh(p, &args->tfh))\n\t || !(p = decode_filename(p, &args->tname, &args->tlen)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_readlinkargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readlinkargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tif (!xdr_argsize_check(rqstp, p))\n\t\treturn 0;\n\targs->buffer = page_address(*(rqstp->rq_next_page++));\n\n\treturn 1;\n}\n\nint\nnfs3svc_decode_linkargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_linkargs *args)\n{\n\tif (!(p = decode_fh(p, &args->ffh))\n\t || !(p = decode_fh(p, &args->tfh))\n\t || !(p = decode_filename(p, &args->tname, &args->tlen)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfs3svc_decode_readdirargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readdirargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = xdr_decode_hyper(p, &args->cookie);\n\targs->verf   = p; p += 2;\n\targs->dircount = ~0;\n\targs->count  = ntohl(*p++);\n\n\tif (!xdr_argsize_check(rqstp, p))\n\t\treturn 0;\n\n\targs->count  = min_t(u32, args->count, PAGE_SIZE);\n\targs->buffer = page_address(*(rqstp->rq_next_page++));\n\n\treturn 1;\n}\n\nint\nnfs3svc_decode_readdirplusargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readdirargs *args)\n{\n\tint len;\n\tu32 max_blocksize = svc_max_payload(rqstp);\n\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = xdr_decode_hyper(p, &args->cookie);\n\targs->verf     = p; p += 2;\n\targs->dircount = ntohl(*p++);\n\targs->count    = ntohl(*p++);\n\n\tif (!xdr_argsize_check(rqstp, p))\n\t\treturn 0;\n\n\tlen = args->count = min(args->count, max_blocksize);\n\twhile (len > 0) {\n\t\tstruct page *p = *(rqstp->rq_next_page++);\n\t\tif (!args->buffer)\n\t\t\targs->buffer = page_address(p);\n\t\tlen -= PAGE_SIZE;\n\t}\n\treturn 1;\n}\n\nint\nnfs3svc_decode_commitargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_commitargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = xdr_decode_hyper(p, &args->offset);\n\targs->count = ntohl(*p++);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\n/*\n * XDR encode functions\n */\n/*\n * There must be an encoding function for void results so svc_process\n * will work properly.\n */\nint\nnfs3svc_encode_voidres(struct svc_rqst *rqstp, __be32 *p, void *dummy)\n{\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* GETATTR */\nint\nnfs3svc_encode_attrstat(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_attrstat *resp)\n{\n\tif (resp->status == 0) {\n\t\tlease_get_mtime(d_inode(resp->fh.fh_dentry),\n\t\t\t\t&resp->stat.mtime);\n\t\tp = encode_fattr3(rqstp, p, &resp->fh, &resp->stat);\n\t}\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* SETATTR, REMOVE, RMDIR */\nint\nnfs3svc_encode_wccstat(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_attrstat *resp)\n{\n\tp = encode_wcc_data(rqstp, p, &resp->fh);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* LOOKUP */\nint\nnfs3svc_encode_diropres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_diropres *resp)\n{\n\tif (resp->status == 0) {\n\t\tp = encode_fh(p, &resp->fh);\n\t\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\t}\n\tp = encode_post_op_attr(rqstp, p, &resp->dirfh);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* ACCESS */\nint\nnfs3svc_encode_accessres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_accessres *resp)\n{\n\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\tif (resp->status == 0)\n\t\t*p++ = htonl(resp->access);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* READLINK */\nint\nnfs3svc_encode_readlinkres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readlinkres *resp)\n{\n\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(resp->len);\n\t\txdr_ressize_check(rqstp, p);\n\t\trqstp->rq_res.page_len = resp->len;\n\t\tif (resp->len & 3) {\n\t\t\t/* need to pad the tail */\n\t\t\trqstp->rq_res.tail[0].iov_base = p;\n\t\t\t*p = 0;\n\t\t\trqstp->rq_res.tail[0].iov_len = 4 - (resp->len&3);\n\t\t}\n\t\treturn 1;\n\t} else\n\t\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* READ */\nint\nnfs3svc_encode_readres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readres *resp)\n{\n\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(resp->count);\n\t\t*p++ = htonl(resp->eof);\n\t\t*p++ = htonl(resp->count);\t/* xdr opaque count */\n\t\txdr_ressize_check(rqstp, p);\n\t\t/* now update rqstp->rq_res to reflect data as well */\n\t\trqstp->rq_res.page_len = resp->count;\n\t\tif (resp->count & 3) {\n\t\t\t/* need to pad the tail */\n\t\t\trqstp->rq_res.tail[0].iov_base = p;\n\t\t\t*p = 0;\n\t\t\trqstp->rq_res.tail[0].iov_len = 4 - (resp->count & 3);\n\t\t}\n\t\treturn 1;\n\t} else\n\t\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* WRITE */\nint\nnfs3svc_encode_writeres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_writeres *resp)\n{\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tp = encode_wcc_data(rqstp, p, &resp->fh);\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(resp->count);\n\t\t*p++ = htonl(resp->committed);\n\t\t*p++ = htonl(nn->nfssvc_boot.tv_sec);\n\t\t*p++ = htonl(nn->nfssvc_boot.tv_usec);\n\t}\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* CREATE, MKDIR, SYMLINK, MKNOD */\nint\nnfs3svc_encode_createres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_diropres *resp)\n{\n\tif (resp->status == 0) {\n\t\t*p++ = xdr_one;\n\t\tp = encode_fh(p, &resp->fh);\n\t\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\t}\n\tp = encode_wcc_data(rqstp, p, &resp->dirfh);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* RENAME */\nint\nnfs3svc_encode_renameres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_renameres *resp)\n{\n\tp = encode_wcc_data(rqstp, p, &resp->ffh);\n\tp = encode_wcc_data(rqstp, p, &resp->tfh);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* LINK */\nint\nnfs3svc_encode_linkres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_linkres *resp)\n{\n\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\tp = encode_wcc_data(rqstp, p, &resp->tfh);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* READDIR */\nint\nnfs3svc_encode_readdirres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_readdirres *resp)\n{\n\tp = encode_post_op_attr(rqstp, p, &resp->fh);\n\n\tif (resp->status == 0) {\n\t\t/* stupid readdir cookie */\n\t\tmemcpy(p, resp->verf, 8); p += 2;\n\t\txdr_ressize_check(rqstp, p);\n\t\tif (rqstp->rq_res.head[0].iov_len + (2<<2) > PAGE_SIZE)\n\t\t\treturn 1; /*No room for trailer */\n\t\trqstp->rq_res.page_len = (resp->count) << 2;\n\n\t\t/* add the 'tail' to the end of the 'head' page - page 0. */\n\t\trqstp->rq_res.tail[0].iov_base = p;\n\t\t*p++ = 0;\t\t/* no more entries */\n\t\t*p++ = htonl(resp->common.err == nfserr_eof);\n\t\trqstp->rq_res.tail[0].iov_len = 2<<2;\n\t\treturn 1;\n\t} else\n\t\treturn xdr_ressize_check(rqstp, p);\n}\n\nstatic __be32 *\nencode_entry_baggage(struct nfsd3_readdirres *cd, __be32 *p, const char *name,\n\t     int namlen, u64 ino)\n{\n\t*p++ = xdr_one;\t\t\t\t /* mark entry present */\n\tp    = xdr_encode_hyper(p, ino);\t /* file id */\n\tp    = xdr_encode_array(p, name, namlen);/* name length & name */\n\n\tcd->offset = p;\t\t\t\t/* remember pointer */\n\tp = xdr_encode_hyper(p, NFS_OFFSET_MAX);/* offset of next entry */\n\n\treturn p;\n}\n\nstatic __be32\ncompose_entry_fh(struct nfsd3_readdirres *cd, struct svc_fh *fhp,\n\t\t const char *name, int namlen, u64 ino)\n{\n\tstruct svc_export\t*exp;\n\tstruct dentry\t\t*dparent, *dchild;\n\t__be32 rv = nfserr_noent;\n\n\tdparent = cd->fh.fh_dentry;\n\texp  = cd->fh.fh_export;\n\n\tif (isdotent(name, namlen)) {\n\t\tif (namlen == 2) {\n\t\t\tdchild = dget_parent(dparent);\n\t\t\t/* filesystem root - cannot return filehandle for \"..\" */\n\t\t\tif (dchild == dparent)\n\t\t\t\tgoto out;\n\t\t} else\n\t\t\tdchild = dget(dparent);\n\t} else\n\t\tdchild = lookup_one_len_unlocked(name, dparent, namlen);\n\tif (IS_ERR(dchild))\n\t\treturn rv;\n\tif (d_mountpoint(dchild))\n\t\tgoto out;\n\tif (d_really_is_negative(dchild))\n\t\tgoto out;\n\tif (dchild->d_inode->i_ino != ino)\n\t\tgoto out;\n\trv = fh_compose(fhp, exp, dchild, &cd->fh);\nout:\n\tdput(dchild);\n\treturn rv;\n}\n\nstatic __be32 *encode_entryplus_baggage(struct nfsd3_readdirres *cd, __be32 *p, const char *name, int namlen, u64 ino)\n{\n\tstruct svc_fh\t*fh = &cd->scratch;\n\t__be32 err;\n\n\tfh_init(fh, NFS3_FHSIZE);\n\terr = compose_entry_fh(cd, fh, name, namlen, ino);\n\tif (err) {\n\t\t*p++ = 0;\n\t\t*p++ = 0;\n\t\tgoto out;\n\t}\n\tp = encode_post_op_attr(cd->rqstp, p, fh);\n\t*p++ = xdr_one;\t\t\t/* yes, a file handle follows */\n\tp = encode_fh(p, fh);\nout:\n\tfh_put(fh);\n\treturn p;\n}\n\n/*\n * Encode a directory entry. This one works for both normal readdir\n * and readdirplus.\n * The normal readdir reply requires 2 (fileid) + 1 (stringlen)\n * + string + 2 (cookie) + 1 (next) words, i.e. 6 + strlen.\n * \n * The readdirplus baggage is 1+21 words for post_op_attr, plus the\n * file handle.\n */\n\n#define NFS3_ENTRY_BAGGAGE\t(2 + 1 + 2 + 1)\n#define NFS3_ENTRYPLUS_BAGGAGE\t(1 + 21 + 1 + (NFS3_FHSIZE >> 2))\nstatic int\nencode_entry(struct readdir_cd *ccd, const char *name, int namlen,\n\t     loff_t offset, u64 ino, unsigned int d_type, int plus)\n{\n\tstruct nfsd3_readdirres *cd = container_of(ccd, struct nfsd3_readdirres,\n\t\t       \t\t\t\t\tcommon);\n\t__be32\t\t*p = cd->buffer;\n\tcaddr_t\t\tcurr_page_addr = NULL;\n\tstruct page **\tpage;\n\tint\t\tslen;\t\t/* string (name) length */\n\tint\t\telen;\t\t/* estimated entry length in words */\n\tint\t\tnum_entry_words = 0;\t/* actual number of words */\n\n\tif (cd->offset) {\n\t\tu64 offset64 = offset;\n\n\t\tif (unlikely(cd->offset1)) {\n\t\t\t/* we ended up with offset on a page boundary */\n\t\t\t*cd->offset = htonl(offset64 >> 32);\n\t\t\t*cd->offset1 = htonl(offset64 & 0xffffffff);\n\t\t\tcd->offset1 = NULL;\n\t\t} else {\n\t\t\txdr_encode_hyper(cd->offset, offset64);\n\t\t}\n\t}\n\n\t/*\n\tdprintk(\"encode_entry(%.*s @%ld%s)\\n\",\n\t\tnamlen, name, (long) offset, plus? \" plus\" : \"\");\n\t */\n\n\t/* truncate filename if too long */\n\tnamlen = min(namlen, NFS3_MAXNAMLEN);\n\n\tslen = XDR_QUADLEN(namlen);\n\telen = slen + NFS3_ENTRY_BAGGAGE\n\t\t+ (plus? NFS3_ENTRYPLUS_BAGGAGE : 0);\n\n\tif (cd->buflen < elen) {\n\t\tcd->common.err = nfserr_toosmall;\n\t\treturn -EINVAL;\n\t}\n\n\t/* determine which page in rq_respages[] we are currently filling */\n\tfor (page = cd->rqstp->rq_respages + 1;\n\t\t\t\tpage < cd->rqstp->rq_next_page; page++) {\n\t\tcurr_page_addr = page_address(*page);\n\n\t\tif (((caddr_t)cd->buffer >= curr_page_addr) &&\n\t\t    ((caddr_t)cd->buffer <  curr_page_addr + PAGE_SIZE))\n\t\t\tbreak;\n\t}\n\n\tif ((caddr_t)(cd->buffer + elen) < (curr_page_addr + PAGE_SIZE)) {\n\t\t/* encode entry in current page */\n\n\t\tp = encode_entry_baggage(cd, p, name, namlen, ino);\n\n\t\tif (plus)\n\t\t\tp = encode_entryplus_baggage(cd, p, name, namlen, ino);\n\t\tnum_entry_words = p - cd->buffer;\n\t} else if (*(page+1) != NULL) {\n\t\t/* temporarily encode entry into next page, then move back to\n\t\t * current and next page in rq_respages[] */\n\t\t__be32 *p1, *tmp;\n\t\tint len1, len2;\n\n\t\t/* grab next page for temporary storage of entry */\n\t\tp1 = tmp = page_address(*(page+1));\n\n\t\tp1 = encode_entry_baggage(cd, p1, name, namlen, ino);\n\n\t\tif (plus)\n\t\t\tp1 = encode_entryplus_baggage(cd, p1, name, namlen, ino);\n\n\t\t/* determine entry word length and lengths to go in pages */\n\t\tnum_entry_words = p1 - tmp;\n\t\tlen1 = curr_page_addr + PAGE_SIZE - (caddr_t)cd->buffer;\n\t\tif ((num_entry_words << 2) < len1) {\n\t\t\t/* the actual number of words in the entry is less\n\t\t\t * than elen and can still fit in the current page\n\t\t\t */\n\t\t\tmemmove(p, tmp, num_entry_words << 2);\n\t\t\tp += num_entry_words;\n\n\t\t\t/* update offset */\n\t\t\tcd->offset = cd->buffer + (cd->offset - tmp);\n\t\t} else {\n\t\t\tunsigned int offset_r = (cd->offset - tmp) << 2;\n\n\t\t\t/* update pointer to offset location.\n\t\t\t * This is a 64bit quantity, so we need to\n\t\t\t * deal with 3 cases:\n\t\t\t *  -\tentirely in first page\n\t\t\t *  -\tentirely in second page\n\t\t\t *  -\t4 bytes in each page\n\t\t\t */\n\t\t\tif (offset_r + 8 <= len1) {\n\t\t\t\tcd->offset = p + (cd->offset - tmp);\n\t\t\t} else if (offset_r >= len1) {\n\t\t\t\tcd->offset -= len1 >> 2;\n\t\t\t} else {\n\t\t\t\t/* sitting on the fence */\n\t\t\t\tBUG_ON(offset_r != len1 - 4);\n\t\t\t\tcd->offset = p + (cd->offset - tmp);\n\t\t\t\tcd->offset1 = tmp;\n\t\t\t}\n\n\t\t\tlen2 = (num_entry_words << 2) - len1;\n\n\t\t\t/* move from temp page to current and next pages */\n\t\t\tmemmove(p, tmp, len1);\n\t\t\tmemmove(tmp, (caddr_t)tmp+len1, len2);\n\n\t\t\tp = tmp + (len2 >> 2);\n\t\t}\n\t}\n\telse {\n\t\tcd->common.err = nfserr_toosmall;\n\t\treturn -EINVAL;\n\t}\n\n\tcd->buflen -= num_entry_words;\n\tcd->buffer = p;\n\tcd->common.err = nfs_ok;\n\treturn 0;\n\n}\n\nint\nnfs3svc_encode_entry(void *cd, const char *name,\n\t\t     int namlen, loff_t offset, u64 ino, unsigned int d_type)\n{\n\treturn encode_entry(cd, name, namlen, offset, ino, d_type, 0);\n}\n\nint\nnfs3svc_encode_entry_plus(void *cd, const char *name,\n\t\t\t  int namlen, loff_t offset, u64 ino,\n\t\t\t  unsigned int d_type)\n{\n\treturn encode_entry(cd, name, namlen, offset, ino, d_type, 1);\n}\n\n/* FSSTAT */\nint\nnfs3svc_encode_fsstatres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_fsstatres *resp)\n{\n\tstruct kstatfs\t*s = &resp->stats;\n\tu64\t\tbs = s->f_bsize;\n\n\t*p++ = xdr_zero;\t/* no post_op_attr */\n\n\tif (resp->status == 0) {\n\t\tp = xdr_encode_hyper(p, bs * s->f_blocks);\t/* total bytes */\n\t\tp = xdr_encode_hyper(p, bs * s->f_bfree);\t/* free bytes */\n\t\tp = xdr_encode_hyper(p, bs * s->f_bavail);\t/* user available bytes */\n\t\tp = xdr_encode_hyper(p, s->f_files);\t/* total inodes */\n\t\tp = xdr_encode_hyper(p, s->f_ffree);\t/* free inodes */\n\t\tp = xdr_encode_hyper(p, s->f_ffree);\t/* user available inodes */\n\t\t*p++ = htonl(resp->invarsec);\t/* mean unchanged time */\n\t}\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* FSINFO */\nint\nnfs3svc_encode_fsinfores(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_fsinfores *resp)\n{\n\t*p++ = xdr_zero;\t/* no post_op_attr */\n\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(resp->f_rtmax);\n\t\t*p++ = htonl(resp->f_rtpref);\n\t\t*p++ = htonl(resp->f_rtmult);\n\t\t*p++ = htonl(resp->f_wtmax);\n\t\t*p++ = htonl(resp->f_wtpref);\n\t\t*p++ = htonl(resp->f_wtmult);\n\t\t*p++ = htonl(resp->f_dtpref);\n\t\tp = xdr_encode_hyper(p, resp->f_maxfilesize);\n\t\t*p++ = xdr_one;\n\t\t*p++ = xdr_zero;\n\t\t*p++ = htonl(resp->f_properties);\n\t}\n\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* PATHCONF */\nint\nnfs3svc_encode_pathconfres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_pathconfres *resp)\n{\n\t*p++ = xdr_zero;\t/* no post_op_attr */\n\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(resp->p_link_max);\n\t\t*p++ = htonl(resp->p_name_max);\n\t\t*p++ = htonl(resp->p_no_trunc);\n\t\t*p++ = htonl(resp->p_chown_restricted);\n\t\t*p++ = htonl(resp->p_case_insensitive);\n\t\t*p++ = htonl(resp->p_case_preserving);\n\t}\n\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/* COMMIT */\nint\nnfs3svc_encode_commitres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_commitres *resp)\n{\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tp = encode_wcc_data(rqstp, p, &resp->fh);\n\t/* Write verifier */\n\tif (resp->status == 0) {\n\t\t*p++ = htonl(nn->nfssvc_boot.tv_sec);\n\t\t*p++ = htonl(nn->nfssvc_boot.tv_usec);\n\t}\n\treturn xdr_ressize_check(rqstp, p);\n}\n\n/*\n * XDR release functions\n */\nint\nnfs3svc_release_fhandle(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_attrstat *resp)\n{\n\tfh_put(&resp->fh);\n\treturn 1;\n}\n\nint\nnfs3svc_release_fhandle2(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd3_fhandle_pair *resp)\n{\n\tfh_put(&resp->fh1);\n\tfh_put(&resp->fh2);\n\treturn 1;\n}\n", "/*\n *  Server-side procedures for NFSv4.\n *\n *  Copyright (c) 2002 The Regents of the University of Michigan.\n *  All rights reserved.\n *\n *  Kendrick Smith <kmsmith@umich.edu>\n *  Andy Adamson   <andros@umich.edu>\n *\n *  Redistribution and use in source and binary forms, with or without\n *  modification, are permitted provided that the following conditions\n *  are met:\n *\n *  1. Redistributions of source code must retain the above copyright\n *     notice, this list of conditions and the following disclaimer.\n *  2. Redistributions in binary form must reproduce the above copyright\n *     notice, this list of conditions and the following disclaimer in the\n *     documentation and/or other materials provided with the distribution.\n *  3. Neither the name of the University nor the names of its\n *     contributors may be used to endorse or promote products derived\n *     from this software without specific prior written permission.\n *\n *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED\n *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n *  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE\n *  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n *  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n *  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n *  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n *  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n *  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n *  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n#include <linux/file.h>\n#include <linux/falloc.h>\n#include <linux/slab.h>\n\n#include \"idmap.h\"\n#include \"cache.h\"\n#include \"xdr4.h\"\n#include \"vfs.h\"\n#include \"current_stateid.h\"\n#include \"netns.h\"\n#include \"acl.h\"\n#include \"pnfs.h\"\n#include \"trace.h\"\n\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n#include <linux/security.h>\n\nstatic inline void\nnfsd4_security_inode_setsecctx(struct svc_fh *resfh, struct xdr_netobj *label, u32 *bmval)\n{\n\tstruct inode *inode = d_inode(resfh->fh_dentry);\n\tint status;\n\n\tinode_lock(inode);\n\tstatus = security_inode_setsecctx(resfh->fh_dentry,\n\t\tlabel->data, label->len);\n\tinode_unlock(inode);\n\n\tif (status)\n\t\t/*\n\t\t * XXX: We should really fail the whole open, but we may\n\t\t * already have created a new file, so it may be too\n\t\t * late.  For now this seems the least of evils:\n\t\t */\n\t\tbmval[2] &= ~FATTR4_WORD2_SECURITY_LABEL;\n\n\treturn;\n}\n#else\nstatic inline void\nnfsd4_security_inode_setsecctx(struct svc_fh *resfh, struct xdr_netobj *label, u32 *bmval)\n{ }\n#endif\n\n#define NFSDDBG_FACILITY\t\tNFSDDBG_PROC\n\nstatic u32 nfsd_attrmask[] = {\n\tNFSD_WRITEABLE_ATTRS_WORD0,\n\tNFSD_WRITEABLE_ATTRS_WORD1,\n\tNFSD_WRITEABLE_ATTRS_WORD2\n};\n\nstatic u32 nfsd41_ex_attrmask[] = {\n\tNFSD_SUPPATTR_EXCLCREAT_WORD0,\n\tNFSD_SUPPATTR_EXCLCREAT_WORD1,\n\tNFSD_SUPPATTR_EXCLCREAT_WORD2\n};\n\nstatic __be32\ncheck_attr_support(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t   u32 *bmval, u32 *writable)\n{\n\tstruct dentry *dentry = cstate->current_fh.fh_dentry;\n\tstruct svc_export *exp = cstate->current_fh.fh_export;\n\n\tif (!nfsd_attrs_supported(cstate->minorversion, bmval))\n\t\treturn nfserr_attrnotsupp;\n\tif ((bmval[0] & FATTR4_WORD0_ACL) && !IS_POSIXACL(d_inode(dentry)))\n\t\treturn nfserr_attrnotsupp;\n\tif ((bmval[2] & FATTR4_WORD2_SECURITY_LABEL) &&\n\t\t\t!(exp->ex_flags & NFSEXP_SECURITY_LABEL))\n\t\treturn nfserr_attrnotsupp;\n\tif (writable && !bmval_is_subset(bmval, writable))\n\t\treturn nfserr_inval;\n\tif (writable && (bmval[2] & FATTR4_WORD2_MODE_UMASK) &&\n\t\t\t(bmval[1] & FATTR4_WORD1_MODE))\n\t\treturn nfserr_inval;\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_check_open_attributes(struct svc_rqst *rqstp,\n\tstruct nfsd4_compound_state *cstate, struct nfsd4_open *open)\n{\n\t__be32 status = nfs_ok;\n\n\tif (open->op_create == NFS4_OPEN_CREATE) {\n\t\tif (open->op_createmode == NFS4_CREATE_UNCHECKED\n\t\t    || open->op_createmode == NFS4_CREATE_GUARDED)\n\t\t\tstatus = check_attr_support(rqstp, cstate,\n\t\t\t\t\topen->op_bmval, nfsd_attrmask);\n\t\telse if (open->op_createmode == NFS4_CREATE_EXCLUSIVE4_1)\n\t\t\tstatus = check_attr_support(rqstp, cstate,\n\t\t\t\t\topen->op_bmval, nfsd41_ex_attrmask);\n\t}\n\n\treturn status;\n}\n\nstatic int\nis_create_with_attrs(struct nfsd4_open *open)\n{\n\treturn open->op_create == NFS4_OPEN_CREATE\n\t\t&& (open->op_createmode == NFS4_CREATE_UNCHECKED\n\t\t    || open->op_createmode == NFS4_CREATE_GUARDED\n\t\t    || open->op_createmode == NFS4_CREATE_EXCLUSIVE4_1);\n}\n\n/*\n * if error occurs when setting the acl, just clear the acl bit\n * in the returned attr bitmap.\n */\nstatic void\ndo_set_nfs4_acl(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tstruct nfs4_acl *acl, u32 *bmval)\n{\n\t__be32 status;\n\n\tstatus = nfsd4_set_nfs4_acl(rqstp, fhp, acl);\n\tif (status)\n\t\t/*\n\t\t * We should probably fail the whole open at this point,\n\t\t * but we've already created the file, so it's too late;\n\t\t * So this seems the least of evils:\n\t\t */\n\t\tbmval[0] &= ~FATTR4_WORD0_ACL;\n}\n\nstatic inline void\nfh_dup2(struct svc_fh *dst, struct svc_fh *src)\n{\n\tfh_put(dst);\n\tdget(src->fh_dentry);\n\tif (src->fh_export)\n\t\texp_get(src->fh_export);\n\t*dst = *src;\n}\n\nstatic __be32\ndo_open_permission(struct svc_rqst *rqstp, struct svc_fh *current_fh, struct nfsd4_open *open, int accmode)\n{\n\t__be32 status;\n\n\tif (open->op_truncate &&\n\t\t!(open->op_share_access & NFS4_SHARE_ACCESS_WRITE))\n\t\treturn nfserr_inval;\n\n\taccmode |= NFSD_MAY_READ_IF_EXEC;\n\n\tif (open->op_share_access & NFS4_SHARE_ACCESS_READ)\n\t\taccmode |= NFSD_MAY_READ;\n\tif (open->op_share_access & NFS4_SHARE_ACCESS_WRITE)\n\t\taccmode |= (NFSD_MAY_WRITE | NFSD_MAY_TRUNC);\n\tif (open->op_share_deny & NFS4_SHARE_DENY_READ)\n\t\taccmode |= NFSD_MAY_WRITE;\n\n\tstatus = fh_verify(rqstp, current_fh, S_IFREG, accmode);\n\n\treturn status;\n}\n\nstatic __be32 nfsd_check_obj_isreg(struct svc_fh *fh)\n{\n\tumode_t mode = d_inode(fh->fh_dentry)->i_mode;\n\n\tif (S_ISREG(mode))\n\t\treturn nfs_ok;\n\tif (S_ISDIR(mode))\n\t\treturn nfserr_isdir;\n\t/*\n\t * Using err_symlink as our catch-all case may look odd; but\n\t * there's no other obvious error for this case in 4.0, and we\n\t * happen to know that it will cause the linux v4 client to do\n\t * the right thing on attempts to open something other than a\n\t * regular file.\n\t */\n\treturn nfserr_symlink;\n}\n\nstatic void nfsd4_set_open_owner_reply_cache(struct nfsd4_compound_state *cstate, struct nfsd4_open *open, struct svc_fh *resfh)\n{\n\tif (nfsd4_has_session(cstate))\n\t\treturn;\n\tfh_copy_shallow(&open->op_openowner->oo_owner.so_replay.rp_openfh,\n\t\t\t&resfh->fh_handle);\n}\n\nstatic __be32\ndo_open_lookup(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_open *open, struct svc_fh **resfh)\n{\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\tint accmode;\n\t__be32 status;\n\n\t*resfh = kmalloc(sizeof(struct svc_fh), GFP_KERNEL);\n\tif (!*resfh)\n\t\treturn nfserr_jukebox;\n\tfh_init(*resfh, NFS4_FHSIZE);\n\topen->op_truncate = 0;\n\n\tif (open->op_create) {\n\t\t/* FIXME: check session persistence and pnfs flags.\n\t\t * The nfsv4.1 spec requires the following semantics:\n\t\t *\n\t\t * Persistent   | pNFS   | Server REQUIRED | Client Allowed\n\t\t * Reply Cache  | server |                 |\n\t\t * -------------+--------+-----------------+--------------------\n\t\t * no           | no     | EXCLUSIVE4_1    | EXCLUSIVE4_1\n\t\t *              |        |                 | (SHOULD)\n\t\t *              |        | and EXCLUSIVE4  | or EXCLUSIVE4\n\t\t *              |        |                 | (SHOULD NOT)\n\t\t * no           | yes    | EXCLUSIVE4_1    | EXCLUSIVE4_1\n\t\t * yes          | no     | GUARDED4        | GUARDED4\n\t\t * yes          | yes    | GUARDED4        | GUARDED4\n\t\t */\n\n\t\t/*\n\t\t * Note: create modes (UNCHECKED,GUARDED...) are the same\n\t\t * in NFSv4 as in v3 except EXCLUSIVE4_1.\n\t\t */\n\t\tstatus = do_nfsd_create(rqstp, current_fh, open->op_fname.data,\n\t\t\t\t\topen->op_fname.len, &open->op_iattr,\n\t\t\t\t\t*resfh, open->op_createmode,\n\t\t\t\t\t(u32 *)open->op_verf.data,\n\t\t\t\t\t&open->op_truncate, &open->op_created);\n\n\t\tif (!status && open->op_label.len)\n\t\t\tnfsd4_security_inode_setsecctx(*resfh, &open->op_label, open->op_bmval);\n\n\t\t/*\n\t\t * Following rfc 3530 14.2.16, and rfc 5661 18.16.4\n\t\t * use the returned bitmask to indicate which attributes\n\t\t * we used to store the verifier:\n\t\t */\n\t\tif (nfsd_create_is_exclusive(open->op_createmode) && status == 0)\n\t\t\topen->op_bmval[1] |= (FATTR4_WORD1_TIME_ACCESS |\n\t\t\t\t\t\tFATTR4_WORD1_TIME_MODIFY);\n\t} else\n\t\t/*\n\t\t * Note this may exit with the parent still locked.\n\t\t * We will hold the lock until nfsd4_open's final\n\t\t * lookup, to prevent renames or unlinks until we've had\n\t\t * a chance to an acquire a delegation if appropriate.\n\t\t */\n\t\tstatus = nfsd_lookup(rqstp, current_fh,\n\t\t\t\t     open->op_fname.data, open->op_fname.len, *resfh);\n\tif (status)\n\t\tgoto out;\n\tstatus = nfsd_check_obj_isreg(*resfh);\n\tif (status)\n\t\tgoto out;\n\n\tif (is_create_with_attrs(open) && open->op_acl != NULL)\n\t\tdo_set_nfs4_acl(rqstp, *resfh, open->op_acl, open->op_bmval);\n\n\tnfsd4_set_open_owner_reply_cache(cstate, open, *resfh);\n\taccmode = NFSD_MAY_NOP;\n\tif (open->op_created ||\n\t\t\topen->op_claim_type == NFS4_OPEN_CLAIM_DELEGATE_CUR)\n\t\taccmode |= NFSD_MAY_OWNER_OVERRIDE;\n\tstatus = do_open_permission(rqstp, *resfh, open, accmode);\n\tset_change_info(&open->op_cinfo, current_fh);\nout:\n\treturn status;\n}\n\nstatic __be32\ndo_open_fhandle(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_open *open)\n{\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\t__be32 status;\n\tint accmode = 0;\n\n\t/* We don't know the target directory, and therefore can not\n\t* set the change info\n\t*/\n\n\tmemset(&open->op_cinfo, 0, sizeof(struct nfsd4_change_info));\n\n\tnfsd4_set_open_owner_reply_cache(cstate, open, current_fh);\n\n\topen->op_truncate = (open->op_iattr.ia_valid & ATTR_SIZE) &&\n\t\t(open->op_iattr.ia_size == 0);\n\t/*\n\t * In the delegation case, the client is telling us about an\n\t * open that it *already* performed locally, some time ago.  We\n\t * should let it succeed now if possible.\n\t *\n\t * In the case of a CLAIM_FH open, on the other hand, the client\n\t * may be counting on us to enforce permissions (the Linux 4.1\n\t * client uses this for normal opens, for example).\n\t */\n\tif (open->op_claim_type == NFS4_OPEN_CLAIM_DELEG_CUR_FH)\n\t\taccmode = NFSD_MAY_OWNER_OVERRIDE;\n\n\tstatus = do_open_permission(rqstp, current_fh, open, accmode);\n\n\treturn status;\n}\n\nstatic void\ncopy_clientid(clientid_t *clid, struct nfsd4_session *session)\n{\n\tstruct nfsd4_sessionid *sid =\n\t\t\t(struct nfsd4_sessionid *)session->se_sessionid.data;\n\n\tclid->cl_boot = sid->clientid.cl_boot;\n\tclid->cl_id = sid->clientid.cl_id;\n}\n\nstatic __be32\nnfsd4_open(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t   struct nfsd4_open *open)\n{\n\t__be32 status;\n\tstruct svc_fh *resfh = NULL;\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_open filename %.*s op_openowner %p\\n\",\n\t\t(int)open->op_fname.len, open->op_fname.data,\n\t\topen->op_openowner);\n\n\t/* This check required by spec. */\n\tif (open->op_create && open->op_claim_type != NFS4_OPEN_CLAIM_NULL)\n\t\treturn nfserr_inval;\n\n\topen->op_created = 0;\n\t/*\n\t * RFC5661 18.51.3\n\t * Before RECLAIM_COMPLETE done, server should deny new lock\n\t */\n\tif (nfsd4_has_session(cstate) &&\n\t    !test_bit(NFSD4_CLIENT_RECLAIM_COMPLETE,\n\t\t      &cstate->session->se_client->cl_flags) &&\n\t    open->op_claim_type != NFS4_OPEN_CLAIM_PREVIOUS)\n\t\treturn nfserr_grace;\n\n\tif (nfsd4_has_session(cstate))\n\t\tcopy_clientid(&open->op_clientid, cstate->session);\n\n\t/* check seqid for replay. set nfs4_owner */\n\tstatus = nfsd4_process_open1(cstate, open, nn);\n\tif (status == nfserr_replay_me) {\n\t\tstruct nfs4_replay *rp = &open->op_openowner->oo_owner.so_replay;\n\t\tfh_put(&cstate->current_fh);\n\t\tfh_copy_shallow(&cstate->current_fh.fh_handle,\n\t\t\t\t&rp->rp_openfh);\n\t\tstatus = fh_verify(rqstp, &cstate->current_fh, 0, NFSD_MAY_NOP);\n\t\tif (status)\n\t\t\tdprintk(\"nfsd4_open: replay failed\"\n\t\t\t\t\" restoring previous filehandle\\n\");\n\t\telse\n\t\t\tstatus = nfserr_replay_me;\n\t}\n\tif (status)\n\t\tgoto out;\n\tif (open->op_xdr_error) {\n\t\tstatus = open->op_xdr_error;\n\t\tgoto out;\n\t}\n\n\tstatus = nfsd4_check_open_attributes(rqstp, cstate, open);\n\tif (status)\n\t\tgoto out;\n\n\t/* Openowner is now set, so sequence id will get bumped.  Now we need\n\t * these checks before we do any creates: */\n\tstatus = nfserr_grace;\n\tif (opens_in_grace(net) && open->op_claim_type != NFS4_OPEN_CLAIM_PREVIOUS)\n\t\tgoto out;\n\tstatus = nfserr_no_grace;\n\tif (!opens_in_grace(net) && open->op_claim_type == NFS4_OPEN_CLAIM_PREVIOUS)\n\t\tgoto out;\n\n\tswitch (open->op_claim_type) {\n\t\tcase NFS4_OPEN_CLAIM_DELEGATE_CUR:\n\t\tcase NFS4_OPEN_CLAIM_NULL:\n\t\t\tstatus = do_open_lookup(rqstp, cstate, open, &resfh);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase NFS4_OPEN_CLAIM_PREVIOUS:\n\t\t\tstatus = nfs4_check_open_reclaim(&open->op_clientid,\n\t\t\t\t\t\t\t cstate, nn);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t\topen->op_openowner->oo_flags |= NFS4_OO_CONFIRMED;\n\t\tcase NFS4_OPEN_CLAIM_FH:\n\t\tcase NFS4_OPEN_CLAIM_DELEG_CUR_FH:\n\t\t\tstatus = do_open_fhandle(rqstp, cstate, open);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t\tresfh = &cstate->current_fh;\n\t\t\tbreak;\n\t\tcase NFS4_OPEN_CLAIM_DELEG_PREV_FH:\n             \tcase NFS4_OPEN_CLAIM_DELEGATE_PREV:\n\t\t\tdprintk(\"NFSD: unsupported OPEN claim type %d\\n\",\n\t\t\t\topen->op_claim_type);\n\t\t\tstatus = nfserr_notsupp;\n\t\t\tgoto out;\n\t\tdefault:\n\t\t\tdprintk(\"NFSD: Invalid OPEN claim type %d\\n\",\n\t\t\t\topen->op_claim_type);\n\t\t\tstatus = nfserr_inval;\n\t\t\tgoto out;\n\t}\n\t/*\n\t * nfsd4_process_open2() does the actual opening of the file.  If\n\t * successful, it (1) truncates the file if open->op_truncate was\n\t * set, (2) sets open->op_stateid, (3) sets open->op_delegation.\n\t */\n\tstatus = nfsd4_process_open2(rqstp, resfh, open);\n\tWARN(status && open->op_created,\n\t     \"nfsd4_process_open2 failed to open newly-created file! status=%u\\n\",\n\t     be32_to_cpu(status));\nout:\n\tif (resfh && resfh != &cstate->current_fh) {\n\t\tfh_dup2(&cstate->current_fh, resfh);\n\t\tfh_put(resfh);\n\t\tkfree(resfh);\n\t}\n\tnfsd4_cleanup_open_state(cstate, open);\n\tnfsd4_bump_seqid(cstate, status);\n\treturn status;\n}\n\n/*\n * OPEN is the only seqid-mutating operation whose decoding can fail\n * with a seqid-mutating error (specifically, decoding of user names in\n * the attributes).  Therefore we have to do some processing to look up\n * the stateowner so that we can bump the seqid.\n */\nstatic __be32 nfsd4_open_omfg(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_op *op)\n{\n\tstruct nfsd4_open *open = (struct nfsd4_open *)&op->u;\n\n\tif (!seqid_mutating_err(ntohl(op->status)))\n\t\treturn op->status;\n\tif (nfsd4_has_session(cstate))\n\t\treturn op->status;\n\topen->op_xdr_error = op->status;\n\treturn nfsd4_open(rqstp, cstate, open);\n}\n\n/*\n * filehandle-manipulating ops.\n */\nstatic __be32\nnfsd4_getfh(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct svc_fh **getfh)\n{\n\tif (!cstate->current_fh.fh_dentry)\n\t\treturn nfserr_nofilehandle;\n\n\t*getfh = &cstate->current_fh;\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_putfh(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct nfsd4_putfh *putfh)\n{\n\tfh_put(&cstate->current_fh);\n\tcstate->current_fh.fh_handle.fh_size = putfh->pf_fhlen;\n\tmemcpy(&cstate->current_fh.fh_handle.fh_base, putfh->pf_fhval,\n\t       putfh->pf_fhlen);\n\treturn fh_verify(rqstp, &cstate->current_fh, 0, NFSD_MAY_BYPASS_GSS);\n}\n\nstatic __be32\nnfsd4_putrootfh(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tvoid *arg)\n{\n\t__be32 status;\n\n\tfh_put(&cstate->current_fh);\n\tstatus = exp_pseudoroot(rqstp, &cstate->current_fh);\n\treturn status;\n}\n\nstatic __be32\nnfsd4_restorefh(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tvoid *arg)\n{\n\tif (!cstate->save_fh.fh_dentry)\n\t\treturn nfserr_restorefh;\n\n\tfh_dup2(&cstate->current_fh, &cstate->save_fh);\n\tif (HAS_STATE_ID(cstate, SAVED_STATE_ID_FLAG)) {\n\t\tmemcpy(&cstate->current_stateid, &cstate->save_stateid, sizeof(stateid_t));\n\t\tSET_STATE_ID(cstate, CURRENT_STATE_ID_FLAG);\n\t}\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_savefh(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     void *arg)\n{\n\tif (!cstate->current_fh.fh_dentry)\n\t\treturn nfserr_nofilehandle;\n\n\tfh_dup2(&cstate->save_fh, &cstate->current_fh);\n\tif (HAS_STATE_ID(cstate, CURRENT_STATE_ID_FLAG)) {\n\t\tmemcpy(&cstate->save_stateid, &cstate->current_stateid, sizeof(stateid_t));\n\t\tSET_STATE_ID(cstate, SAVED_STATE_ID_FLAG);\n\t}\n\treturn nfs_ok;\n}\n\n/*\n * misc nfsv4 ops\n */\nstatic __be32\nnfsd4_access(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_access *access)\n{\n\tif (access->ac_req_access & ~NFS3_ACCESS_FULL)\n\t\treturn nfserr_inval;\n\n\taccess->ac_resp_access = access->ac_req_access;\n\treturn nfsd_access(rqstp, &cstate->current_fh, &access->ac_resp_access,\n\t\t\t   &access->ac_supported);\n}\n\nstatic void gen_boot_verifier(nfs4_verifier *verifier, struct net *net)\n{\n\t__be32 verf[2];\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\t/*\n\t * This is opaque to client, so no need to byte-swap. Use\n\t * __force to keep sparse happy\n\t */\n\tverf[0] = (__force __be32)nn->nfssvc_boot.tv_sec;\n\tverf[1] = (__force __be32)nn->nfssvc_boot.tv_usec;\n\tmemcpy(verifier->data, verf, sizeof(verifier->data));\n}\n\nstatic __be32\nnfsd4_commit(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_commit *commit)\n{\n\tgen_boot_verifier(&commit->co_verf, SVC_NET(rqstp));\n\treturn nfsd_commit(rqstp, &cstate->current_fh, commit->co_offset,\n\t\t\t     commit->co_count);\n}\n\nstatic __be32\nnfsd4_create(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_create *create)\n{\n\tstruct svc_fh resfh;\n\t__be32 status;\n\tdev_t rdev;\n\n\tfh_init(&resfh, NFS4_FHSIZE);\n\n\tstatus = fh_verify(rqstp, &cstate->current_fh, S_IFDIR, NFSD_MAY_NOP);\n\tif (status)\n\t\treturn status;\n\n\tstatus = check_attr_support(rqstp, cstate, create->cr_bmval,\n\t\t\t\t    nfsd_attrmask);\n\tif (status)\n\t\treturn status;\n\n\tswitch (create->cr_type) {\n\tcase NF4LNK:\n\t\tstatus = nfsd_symlink(rqstp, &cstate->current_fh,\n\t\t\t\t      create->cr_name, create->cr_namelen,\n\t\t\t\t      create->cr_data, &resfh);\n\t\tbreak;\n\n\tcase NF4BLK:\n\t\trdev = MKDEV(create->cr_specdata1, create->cr_specdata2);\n\t\tif (MAJOR(rdev) != create->cr_specdata1 ||\n\t\t    MINOR(rdev) != create->cr_specdata2)\n\t\t\treturn nfserr_inval;\n\t\tstatus = nfsd_create(rqstp, &cstate->current_fh,\n\t\t\t\t     create->cr_name, create->cr_namelen,\n\t\t\t\t     &create->cr_iattr, S_IFBLK, rdev, &resfh);\n\t\tbreak;\n\n\tcase NF4CHR:\n\t\trdev = MKDEV(create->cr_specdata1, create->cr_specdata2);\n\t\tif (MAJOR(rdev) != create->cr_specdata1 ||\n\t\t    MINOR(rdev) != create->cr_specdata2)\n\t\t\treturn nfserr_inval;\n\t\tstatus = nfsd_create(rqstp, &cstate->current_fh,\n\t\t\t\t     create->cr_name, create->cr_namelen,\n\t\t\t\t     &create->cr_iattr,S_IFCHR, rdev, &resfh);\n\t\tbreak;\n\n\tcase NF4SOCK:\n\t\tstatus = nfsd_create(rqstp, &cstate->current_fh,\n\t\t\t\t     create->cr_name, create->cr_namelen,\n\t\t\t\t     &create->cr_iattr, S_IFSOCK, 0, &resfh);\n\t\tbreak;\n\n\tcase NF4FIFO:\n\t\tstatus = nfsd_create(rqstp, &cstate->current_fh,\n\t\t\t\t     create->cr_name, create->cr_namelen,\n\t\t\t\t     &create->cr_iattr, S_IFIFO, 0, &resfh);\n\t\tbreak;\n\n\tcase NF4DIR:\n\t\tcreate->cr_iattr.ia_valid &= ~ATTR_SIZE;\n\t\tstatus = nfsd_create(rqstp, &cstate->current_fh,\n\t\t\t\t     create->cr_name, create->cr_namelen,\n\t\t\t\t     &create->cr_iattr, S_IFDIR, 0, &resfh);\n\t\tbreak;\n\n\tdefault:\n\t\tstatus = nfserr_badtype;\n\t}\n\n\tif (status)\n\t\tgoto out;\n\n\tif (create->cr_label.len)\n\t\tnfsd4_security_inode_setsecctx(&resfh, &create->cr_label, create->cr_bmval);\n\n\tif (create->cr_acl != NULL)\n\t\tdo_set_nfs4_acl(rqstp, &resfh, create->cr_acl,\n\t\t\t\tcreate->cr_bmval);\n\n\tfh_unlock(&cstate->current_fh);\n\tset_change_info(&create->cr_cinfo, &cstate->current_fh);\n\tfh_dup2(&cstate->current_fh, &resfh);\nout:\n\tfh_put(&resfh);\n\treturn status;\n}\n\nstatic __be32\nnfsd4_getattr(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_getattr *getattr)\n{\n\t__be32 status;\n\n\tstatus = fh_verify(rqstp, &cstate->current_fh, 0, NFSD_MAY_NOP);\n\tif (status)\n\t\treturn status;\n\n\tif (getattr->ga_bmval[1] & NFSD_WRITEONLY_ATTRS_WORD1)\n\t\treturn nfserr_inval;\n\n\tgetattr->ga_bmval[0] &= nfsd_suppattrs[cstate->minorversion][0];\n\tgetattr->ga_bmval[1] &= nfsd_suppattrs[cstate->minorversion][1];\n\tgetattr->ga_bmval[2] &= nfsd_suppattrs[cstate->minorversion][2];\n\n\tgetattr->ga_fhp = &cstate->current_fh;\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_link(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t   struct nfsd4_link *link)\n{\n\t__be32 status = nfserr_nofilehandle;\n\n\tif (!cstate->save_fh.fh_dentry)\n\t\treturn status;\n\tstatus = nfsd_link(rqstp, &cstate->current_fh,\n\t\t\t   link->li_name, link->li_namelen, &cstate->save_fh);\n\tif (!status)\n\t\tset_change_info(&link->li_cinfo, &cstate->current_fh);\n\treturn status;\n}\n\nstatic __be32 nfsd4_do_lookupp(struct svc_rqst *rqstp, struct svc_fh *fh)\n{\n\tstruct svc_fh tmp_fh;\n\t__be32 ret;\n\n\tfh_init(&tmp_fh, NFS4_FHSIZE);\n\tret = exp_pseudoroot(rqstp, &tmp_fh);\n\tif (ret)\n\t\treturn ret;\n\tif (tmp_fh.fh_dentry == fh->fh_dentry) {\n\t\tfh_put(&tmp_fh);\n\t\treturn nfserr_noent;\n\t}\n\tfh_put(&tmp_fh);\n\treturn nfsd_lookup(rqstp, fh, \"..\", 2, fh);\n}\n\nstatic __be32\nnfsd4_lookupp(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      void *arg)\n{\n\treturn nfsd4_do_lookupp(rqstp, &cstate->current_fh);\n}\n\nstatic __be32\nnfsd4_lookup(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_lookup *lookup)\n{\n\treturn nfsd_lookup(rqstp, &cstate->current_fh,\n\t\t\t   lookup->lo_name, lookup->lo_len,\n\t\t\t   &cstate->current_fh);\n}\n\nstatic __be32\nnfsd4_read(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t   struct nfsd4_read *read)\n{\n\t__be32 status;\n\n\tread->rd_filp = NULL;\n\tif (read->rd_offset >= OFFSET_MAX)\n\t\treturn nfserr_inval;\n\n\t/*\n\t * If we do a zero copy read, then a client will see read data\n\t * that reflects the state of the file *after* performing the\n\t * following compound.\n\t *\n\t * To ensure proper ordering, we therefore turn off zero copy if\n\t * the client wants us to do more in this compound:\n\t */\n\tif (!nfsd4_last_compound_op(rqstp))\n\t\tclear_bit(RQ_SPLICE_OK, &rqstp->rq_flags);\n\n\t/* check stateid */\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t&read->rd_stateid, RD_STATE,\n\t\t\t\t\t&read->rd_filp, &read->rd_tmp_file);\n\tif (status) {\n\t\tdprintk(\"NFSD: nfsd4_read: couldn't process stateid!\\n\");\n\t\tgoto out;\n\t}\n\tstatus = nfs_ok;\nout:\n\tread->rd_rqstp = rqstp;\n\tread->rd_fhp = &cstate->current_fh;\n\treturn status;\n}\n\nstatic __be32\nnfsd4_readdir(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_readdir *readdir)\n{\n\tu64 cookie = readdir->rd_cookie;\n\tstatic const nfs4_verifier zeroverf;\n\n\t/* no need to check permission - this will be done in nfsd_readdir() */\n\n\tif (readdir->rd_bmval[1] & NFSD_WRITEONLY_ATTRS_WORD1)\n\t\treturn nfserr_inval;\n\n\treaddir->rd_bmval[0] &= nfsd_suppattrs[cstate->minorversion][0];\n\treaddir->rd_bmval[1] &= nfsd_suppattrs[cstate->minorversion][1];\n\treaddir->rd_bmval[2] &= nfsd_suppattrs[cstate->minorversion][2];\n\n\tif ((cookie == 1) || (cookie == 2) ||\n\t    (cookie == 0 && memcmp(readdir->rd_verf.data, zeroverf.data, NFS4_VERIFIER_SIZE)))\n\t\treturn nfserr_bad_cookie;\n\n\treaddir->rd_rqstp = rqstp;\n\treaddir->rd_fhp = &cstate->current_fh;\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_readlink(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t       struct nfsd4_readlink *readlink)\n{\n\treadlink->rl_rqstp = rqstp;\n\treadlink->rl_fhp = &cstate->current_fh;\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_remove(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_remove *remove)\n{\n\t__be32 status;\n\n\tif (opens_in_grace(SVC_NET(rqstp)))\n\t\treturn nfserr_grace;\n\tstatus = nfsd_unlink(rqstp, &cstate->current_fh, 0,\n\t\t\t     remove->rm_name, remove->rm_namelen);\n\tif (!status) {\n\t\tfh_unlock(&cstate->current_fh);\n\t\tset_change_info(&remove->rm_cinfo, &cstate->current_fh);\n\t}\n\treturn status;\n}\n\nstatic __be32\nnfsd4_rename(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_rename *rename)\n{\n\t__be32 status = nfserr_nofilehandle;\n\n\tif (!cstate->save_fh.fh_dentry)\n\t\treturn status;\n\tif (opens_in_grace(SVC_NET(rqstp)) &&\n\t\t!(cstate->save_fh.fh_export->ex_flags & NFSEXP_NOSUBTREECHECK))\n\t\treturn nfserr_grace;\n\tstatus = nfsd_rename(rqstp, &cstate->save_fh, rename->rn_sname,\n\t\t\t     rename->rn_snamelen, &cstate->current_fh,\n\t\t\t     rename->rn_tname, rename->rn_tnamelen);\n\tif (status)\n\t\treturn status;\n\tset_change_info(&rename->rn_sinfo, &cstate->current_fh);\n\tset_change_info(&rename->rn_tinfo, &cstate->save_fh);\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_secinfo(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_secinfo *secinfo)\n{\n\tstruct svc_export *exp;\n\tstruct dentry *dentry;\n\t__be32 err;\n\n\terr = fh_verify(rqstp, &cstate->current_fh, S_IFDIR, NFSD_MAY_EXEC);\n\tif (err)\n\t\treturn err;\n\terr = nfsd_lookup_dentry(rqstp, &cstate->current_fh,\n\t\t\t\t    secinfo->si_name, secinfo->si_namelen,\n\t\t\t\t    &exp, &dentry);\n\tif (err)\n\t\treturn err;\n\tfh_unlock(&cstate->current_fh);\n\tif (d_really_is_negative(dentry)) {\n\t\texp_put(exp);\n\t\terr = nfserr_noent;\n\t} else\n\t\tsecinfo->si_exp = exp;\n\tdput(dentry);\n\tif (cstate->minorversion)\n\t\t/* See rfc 5661 section 2.6.3.1.1.8 */\n\t\tfh_put(&cstate->current_fh);\n\treturn err;\n}\n\nstatic __be32\nnfsd4_secinfo_no_name(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_secinfo_no_name *sin)\n{\n\t__be32 err;\n\n\tswitch (sin->sin_style) {\n\tcase NFS4_SECINFO_STYLE4_CURRENT_FH:\n\t\tbreak;\n\tcase NFS4_SECINFO_STYLE4_PARENT:\n\t\terr = nfsd4_do_lookupp(rqstp, &cstate->current_fh);\n\t\tif (err)\n\t\t\treturn err;\n\t\tbreak;\n\tdefault:\n\t\treturn nfserr_inval;\n\t}\n\n\tsin->sin_exp = exp_get(cstate->current_fh.fh_export);\n\tfh_put(&cstate->current_fh);\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_setattr(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_setattr *setattr)\n{\n\t__be32 status = nfs_ok;\n\tint err;\n\n\tif (setattr->sa_iattr.ia_valid & ATTR_SIZE) {\n\t\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate,\n\t\t\t\t&cstate->current_fh, &setattr->sa_stateid,\n\t\t\t\tWR_STATE, NULL, NULL);\n\t\tif (status) {\n\t\t\tdprintk(\"NFSD: nfsd4_setattr: couldn't process stateid!\\n\");\n\t\t\treturn status;\n\t\t}\n\t}\n\terr = fh_want_write(&cstate->current_fh);\n\tif (err)\n\t\treturn nfserrno(err);\n\tstatus = nfs_ok;\n\n\tstatus = check_attr_support(rqstp, cstate, setattr->sa_bmval,\n\t\t\t\t    nfsd_attrmask);\n\tif (status)\n\t\tgoto out;\n\n\tif (setattr->sa_acl != NULL)\n\t\tstatus = nfsd4_set_nfs4_acl(rqstp, &cstate->current_fh,\n\t\t\t\t\t    setattr->sa_acl);\n\tif (status)\n\t\tgoto out;\n\tif (setattr->sa_label.len)\n\t\tstatus = nfsd4_set_nfs4_label(rqstp, &cstate->current_fh,\n\t\t\t\t&setattr->sa_label);\n\tif (status)\n\t\tgoto out;\n\tstatus = nfsd_setattr(rqstp, &cstate->current_fh, &setattr->sa_iattr,\n\t\t\t\t0, (time_t)0);\nout:\n\tfh_drop_write(&cstate->current_fh);\n\treturn status;\n}\n\nstatic int fill_in_write_vector(struct kvec *vec, struct nfsd4_write *write)\n{\n        int i = 1;\n        int buflen = write->wr_buflen;\n\n        vec[0].iov_base = write->wr_head.iov_base;\n        vec[0].iov_len = min_t(int, buflen, write->wr_head.iov_len);\n        buflen -= vec[0].iov_len;\n\n        while (buflen) {\n                vec[i].iov_base = page_address(write->wr_pagelist[i - 1]);\n                vec[i].iov_len = min_t(int, PAGE_SIZE, buflen);\n                buflen -= vec[i].iov_len;\n                i++;\n        }\n        return i;\n}\n\nstatic __be32\nnfsd4_write(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct nfsd4_write *write)\n{\n\tstateid_t *stateid = &write->wr_stateid;\n\tstruct file *filp = NULL;\n\t__be32 status = nfs_ok;\n\tunsigned long cnt;\n\tint nvecs;\n\n\tif (write->wr_offset >= OFFSET_MAX)\n\t\treturn nfserr_inval;\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t\tstateid, WR_STATE, &filp, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: nfsd4_write: couldn't process stateid!\\n\");\n\t\treturn status;\n\t}\n\n\tcnt = write->wr_buflen;\n\twrite->wr_how_written = write->wr_stable_how;\n\tgen_boot_verifier(&write->wr_verifier, SVC_NET(rqstp));\n\n\tnvecs = fill_in_write_vector(rqstp->rq_vec, write);\n\tWARN_ON_ONCE(nvecs > ARRAY_SIZE(rqstp->rq_vec));\n\n\tstatus = nfsd_vfs_write(rqstp, &cstate->current_fh, filp,\n\t\t\t\twrite->wr_offset, rqstp->rq_vec, nvecs, &cnt,\n\t\t\t\twrite->wr_how_written);\n\tfput(filp);\n\n\twrite->wr_bytes_written = cnt;\n\n\treturn status;\n}\n\nstatic __be32\nnfsd4_verify_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t  stateid_t *src_stateid, struct file **src,\n\t\t  stateid_t *dst_stateid, struct file **dst)\n{\n\t__be32 status;\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->save_fh,\n\t\t\t\t\t    src_stateid, RD_STATE, src, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: %s: couldn't process src stateid!\\n\", __func__);\n\t\tgoto out;\n\t}\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t    dst_stateid, WR_STATE, dst, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: %s: couldn't process dst stateid!\\n\", __func__);\n\t\tgoto out_put_src;\n\t}\n\n\t/* fix up for NFS-specific error code */\n\tif (!S_ISREG(file_inode(*src)->i_mode) ||\n\t    !S_ISREG(file_inode(*dst)->i_mode)) {\n\t\tstatus = nfserr_wrong_type;\n\t\tgoto out_put_dst;\n\t}\n\nout:\n\treturn status;\nout_put_dst:\n\tfput(*dst);\nout_put_src:\n\tfput(*src);\n\tgoto out;\n}\n\nstatic __be32\nnfsd4_clone(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_clone *clone)\n{\n\tstruct file *src, *dst;\n\t__be32 status;\n\n\tstatus = nfsd4_verify_copy(rqstp, cstate, &clone->cl_src_stateid, &src,\n\t\t\t\t   &clone->cl_dst_stateid, &dst);\n\tif (status)\n\t\tgoto out;\n\n\tstatus = nfsd4_clone_file_range(src, clone->cl_src_pos,\n\t\t\tdst, clone->cl_dst_pos, clone->cl_count);\n\n\tfput(dst);\n\tfput(src);\nout:\n\treturn status;\n}\n\nstatic __be32\nnfsd4_copy(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_copy *copy)\n{\n\tstruct file *src, *dst;\n\t__be32 status;\n\tssize_t bytes;\n\n\tstatus = nfsd4_verify_copy(rqstp, cstate, &copy->cp_src_stateid, &src,\n\t\t\t\t   &copy->cp_dst_stateid, &dst);\n\tif (status)\n\t\tgoto out;\n\n\tbytes = nfsd_copy_file_range(src, copy->cp_src_pos,\n\t\t\tdst, copy->cp_dst_pos, copy->cp_count);\n\n\tif (bytes < 0)\n\t\tstatus = nfserrno(bytes);\n\telse {\n\t\tcopy->cp_res.wr_bytes_written = bytes;\n\t\tcopy->cp_res.wr_stable_how = NFS_UNSTABLE;\n\t\tcopy->cp_consecutive = 1;\n\t\tcopy->cp_synchronous = 1;\n\t\tgen_boot_verifier(&copy->cp_res.wr_verifier, SVC_NET(rqstp));\n\t\tstatus = nfs_ok;\n\t}\n\n\tfput(src);\n\tfput(dst);\nout:\n\treturn status;\n}\n\nstatic __be32\nnfsd4_fallocate(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_fallocate *fallocate, int flags)\n{\n\t__be32 status = nfserr_notsupp;\n\tstruct file *file;\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t    &fallocate->falloc_stateid,\n\t\t\t\t\t    WR_STATE, &file, NULL);\n\tif (status != nfs_ok) {\n\t\tdprintk(\"NFSD: nfsd4_fallocate: couldn't process stateid!\\n\");\n\t\treturn status;\n\t}\n\n\tstatus = nfsd4_vfs_fallocate(rqstp, &cstate->current_fh, file,\n\t\t\t\t     fallocate->falloc_offset,\n\t\t\t\t     fallocate->falloc_length,\n\t\t\t\t     flags);\n\tfput(file);\n\treturn status;\n}\n\nstatic __be32\nnfsd4_allocate(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t       struct nfsd4_fallocate *fallocate)\n{\n\treturn nfsd4_fallocate(rqstp, cstate, fallocate, 0);\n}\n\nstatic __be32\nnfsd4_deallocate(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t struct nfsd4_fallocate *fallocate)\n{\n\treturn nfsd4_fallocate(rqstp, cstate, fallocate,\n\t\t\t       FALLOC_FL_PUNCH_HOLE | FALLOC_FL_KEEP_SIZE);\n}\n\nstatic __be32\nnfsd4_seek(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_seek *seek)\n{\n\tint whence;\n\t__be32 status;\n\tstruct file *file;\n\n\tstatus = nfs4_preprocess_stateid_op(rqstp, cstate, &cstate->current_fh,\n\t\t\t\t\t    &seek->seek_stateid,\n\t\t\t\t\t    RD_STATE, &file, NULL);\n\tif (status) {\n\t\tdprintk(\"NFSD: nfsd4_seek: couldn't process stateid!\\n\");\n\t\treturn status;\n\t}\n\n\tswitch (seek->seek_whence) {\n\tcase NFS4_CONTENT_DATA:\n\t\twhence = SEEK_DATA;\n\t\tbreak;\n\tcase NFS4_CONTENT_HOLE:\n\t\twhence = SEEK_HOLE;\n\t\tbreak;\n\tdefault:\n\t\tstatus = nfserr_union_notsupp;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Note:  This call does change file->f_pos, but nothing in NFSD\n\t *        should ever file->f_pos.\n\t */\n\tseek->seek_pos = vfs_llseek(file, seek->seek_offset, whence);\n\tif (seek->seek_pos < 0)\n\t\tstatus = nfserrno(seek->seek_pos);\n\telse if (seek->seek_pos >= i_size_read(file_inode(file)))\n\t\tseek->seek_eof = true;\n\nout:\n\tfput(file);\n\treturn status;\n}\n\n/* This routine never returns NFS_OK!  If there are no other errors, it\n * will return NFSERR_SAME or NFSERR_NOT_SAME depending on whether the\n * attributes matched.  VERIFY is implemented by mapping NFSERR_SAME\n * to NFS_OK after the call; NVERIFY by mapping NFSERR_NOT_SAME to NFS_OK.\n */\nstatic __be32\n_nfsd4_verify(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_verify *verify)\n{\n\t__be32 *buf, *p;\n\tint count;\n\t__be32 status;\n\n\tstatus = fh_verify(rqstp, &cstate->current_fh, 0, NFSD_MAY_NOP);\n\tif (status)\n\t\treturn status;\n\n\tstatus = check_attr_support(rqstp, cstate, verify->ve_bmval, NULL);\n\tif (status)\n\t\treturn status;\n\n\tif ((verify->ve_bmval[0] & FATTR4_WORD0_RDATTR_ERROR)\n\t    || (verify->ve_bmval[1] & NFSD_WRITEONLY_ATTRS_WORD1))\n\t\treturn nfserr_inval;\n\tif (verify->ve_attrlen & 3)\n\t\treturn nfserr_inval;\n\n\t/* count in words:\n\t *   bitmap_len(1) + bitmap(2) + attr_len(1) = 4\n\t */\n\tcount = 4 + (verify->ve_attrlen >> 2);\n\tbuf = kmalloc(count << 2, GFP_KERNEL);\n\tif (!buf)\n\t\treturn nfserr_jukebox;\n\n\tp = buf;\n\tstatus = nfsd4_encode_fattr_to_buf(&p, count, &cstate->current_fh,\n\t\t\t\t    cstate->current_fh.fh_export,\n\t\t\t\t    cstate->current_fh.fh_dentry,\n\t\t\t\t    verify->ve_bmval,\n\t\t\t\t    rqstp, 0);\n\t/*\n\t * If nfsd4_encode_fattr() ran out of space, assume that's because\n\t * the attributes are longer (hence different) than those given:\n\t */\n\tif (status == nfserr_resource)\n\t\tstatus = nfserr_not_same;\n\tif (status)\n\t\tgoto out_kfree;\n\n\t/* skip bitmap */\n\tp = buf + 1 + ntohl(buf[0]);\n\tstatus = nfserr_not_same;\n\tif (ntohl(*p++) != verify->ve_attrlen)\n\t\tgoto out_kfree;\n\tif (!memcmp(p, verify->ve_attrval, verify->ve_attrlen))\n\t\tstatus = nfserr_same;\n\nout_kfree:\n\tkfree(buf);\n\treturn status;\n}\n\nstatic __be32\nnfsd4_nverify(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t      struct nfsd4_verify *verify)\n{\n\t__be32 status;\n\n\tstatus = _nfsd4_verify(rqstp, cstate, verify);\n\treturn status == nfserr_not_same ? nfs_ok : status;\n}\n\nstatic __be32\nnfsd4_verify(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t     struct nfsd4_verify *verify)\n{\n\t__be32 status;\n\n\tstatus = _nfsd4_verify(rqstp, cstate, verify);\n\treturn status == nfserr_same ? nfs_ok : status;\n}\n\n#ifdef CONFIG_NFSD_PNFS\nstatic const struct nfsd4_layout_ops *\nnfsd4_layout_verify(struct svc_export *exp, unsigned int layout_type)\n{\n\tif (!exp->ex_layout_types) {\n\t\tdprintk(\"%s: export does not support pNFS\\n\", __func__);\n\t\treturn NULL;\n\t}\n\n\tif (layout_type >= LAYOUT_TYPE_MAX ||\n\t    !(exp->ex_layout_types & (1 << layout_type))) {\n\t\tdprintk(\"%s: layout type %d not supported\\n\",\n\t\t\t__func__, layout_type);\n\t\treturn NULL;\n\t}\n\n\treturn nfsd4_layout_ops[layout_type];\n}\n\nstatic __be32\nnfsd4_getdeviceinfo(struct svc_rqst *rqstp,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_getdeviceinfo *gdp)\n{\n\tconst struct nfsd4_layout_ops *ops;\n\tstruct nfsd4_deviceid_map *map;\n\tstruct svc_export *exp;\n\t__be32 nfserr;\n\n\tdprintk(\"%s: layout_type %u dev_id [0x%llx:0x%x] maxcnt %u\\n\",\n\t       __func__,\n\t       gdp->gd_layout_type,\n\t       gdp->gd_devid.fsid_idx, gdp->gd_devid.generation,\n\t       gdp->gd_maxcount);\n\n\tmap = nfsd4_find_devid_map(gdp->gd_devid.fsid_idx);\n\tif (!map) {\n\t\tdprintk(\"%s: couldn't find device ID to export mapping!\\n\",\n\t\t\t__func__);\n\t\treturn nfserr_noent;\n\t}\n\n\texp = rqst_exp_find(rqstp, map->fsid_type, map->fsid);\n\tif (IS_ERR(exp)) {\n\t\tdprintk(\"%s: could not find device id\\n\", __func__);\n\t\treturn nfserr_noent;\n\t}\n\n\tnfserr = nfserr_layoutunavailable;\n\tops = nfsd4_layout_verify(exp, gdp->gd_layout_type);\n\tif (!ops)\n\t\tgoto out;\n\n\tnfserr = nfs_ok;\n\tif (gdp->gd_maxcount != 0) {\n\t\tnfserr = ops->proc_getdeviceinfo(exp->ex_path.mnt->mnt_sb,\n\t\t\t\trqstp, cstate->session->se_client, gdp);\n\t}\n\n\tgdp->gd_notify_types &= ops->notify_types;\nout:\n\texp_put(exp);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_layoutget(struct svc_rqst *rqstp,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_layoutget *lgp)\n{\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\tconst struct nfsd4_layout_ops *ops;\n\tstruct nfs4_layout_stateid *ls;\n\t__be32 nfserr;\n\tint accmode;\n\n\tswitch (lgp->lg_seg.iomode) {\n\tcase IOMODE_READ:\n\t\taccmode = NFSD_MAY_READ;\n\t\tbreak;\n\tcase IOMODE_RW:\n\t\taccmode = NFSD_MAY_READ | NFSD_MAY_WRITE;\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"%s: invalid iomode %d\\n\",\n\t\t\t__func__, lgp->lg_seg.iomode);\n\t\tnfserr = nfserr_badiomode;\n\t\tgoto out;\n\t}\n\n\tnfserr = fh_verify(rqstp, current_fh, 0, accmode);\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfserr_layoutunavailable;\n\tops = nfsd4_layout_verify(current_fh->fh_export, lgp->lg_layout_type);\n\tif (!ops)\n\t\tgoto out;\n\n\t/*\n\t * Verify minlength and range as per RFC5661:\n\t *  o  If loga_length is less than loga_minlength,\n\t *     the metadata server MUST return NFS4ERR_INVAL.\n\t *  o  If the sum of loga_offset and loga_minlength exceeds\n\t *     NFS4_UINT64_MAX, and loga_minlength is not\n\t *     NFS4_UINT64_MAX, the error NFS4ERR_INVAL MUST result.\n\t *  o  If the sum of loga_offset and loga_length exceeds\n\t *     NFS4_UINT64_MAX, and loga_length is not NFS4_UINT64_MAX,\n\t *     the error NFS4ERR_INVAL MUST result.\n\t */\n\tnfserr = nfserr_inval;\n\tif (lgp->lg_seg.length < lgp->lg_minlength ||\n\t    (lgp->lg_minlength != NFS4_MAX_UINT64 &&\n\t     lgp->lg_minlength > NFS4_MAX_UINT64 - lgp->lg_seg.offset) ||\n\t    (lgp->lg_seg.length != NFS4_MAX_UINT64 &&\n\t     lgp->lg_seg.length > NFS4_MAX_UINT64 - lgp->lg_seg.offset))\n\t\tgoto out;\n\tif (lgp->lg_seg.length == 0)\n\t\tgoto out;\n\n\tnfserr = nfsd4_preprocess_layout_stateid(rqstp, cstate, &lgp->lg_sid,\n\t\t\t\t\t\ttrue, lgp->lg_layout_type, &ls);\n\tif (nfserr) {\n\t\ttrace_layout_get_lookup_fail(&lgp->lg_sid);\n\t\tgoto out;\n\t}\n\n\tnfserr = nfserr_recallconflict;\n\tif (atomic_read(&ls->ls_stid.sc_file->fi_lo_recalls))\n\t\tgoto out_put_stid;\n\n\tnfserr = ops->proc_layoutget(d_inode(current_fh->fh_dentry),\n\t\t\t\t     current_fh, lgp);\n\tif (nfserr)\n\t\tgoto out_put_stid;\n\n\tnfserr = nfsd4_insert_layout(lgp, ls);\n\nout_put_stid:\n\tmutex_unlock(&ls->ls_mutex);\n\tnfs4_put_stid(&ls->ls_stid);\nout:\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_layoutcommit(struct svc_rqst *rqstp,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_layoutcommit *lcp)\n{\n\tconst struct nfsd4_layout_seg *seg = &lcp->lc_seg;\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\tconst struct nfsd4_layout_ops *ops;\n\tloff_t new_size = lcp->lc_last_wr + 1;\n\tstruct inode *inode;\n\tstruct nfs4_layout_stateid *ls;\n\t__be32 nfserr;\n\n\tnfserr = fh_verify(rqstp, current_fh, 0, NFSD_MAY_WRITE);\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfserr_layoutunavailable;\n\tops = nfsd4_layout_verify(current_fh->fh_export, lcp->lc_layout_type);\n\tif (!ops)\n\t\tgoto out;\n\tinode = d_inode(current_fh->fh_dentry);\n\n\tnfserr = nfserr_inval;\n\tif (new_size <= seg->offset) {\n\t\tdprintk(\"pnfsd: last write before layout segment\\n\");\n\t\tgoto out;\n\t}\n\tif (new_size > seg->offset + seg->length) {\n\t\tdprintk(\"pnfsd: last write beyond layout segment\\n\");\n\t\tgoto out;\n\t}\n\tif (!lcp->lc_newoffset && new_size > i_size_read(inode)) {\n\t\tdprintk(\"pnfsd: layoutcommit beyond EOF\\n\");\n\t\tgoto out;\n\t}\n\n\tnfserr = nfsd4_preprocess_layout_stateid(rqstp, cstate, &lcp->lc_sid,\n\t\t\t\t\t\tfalse, lcp->lc_layout_type,\n\t\t\t\t\t\t&ls);\n\tif (nfserr) {\n\t\ttrace_layout_commit_lookup_fail(&lcp->lc_sid);\n\t\t/* fixup error code as per RFC5661 */\n\t\tif (nfserr == nfserr_bad_stateid)\n\t\t\tnfserr = nfserr_badlayout;\n\t\tgoto out;\n\t}\n\n\t/* LAYOUTCOMMIT does not require any serialization */\n\tmutex_unlock(&ls->ls_mutex);\n\n\tif (new_size > i_size_read(inode)) {\n\t\tlcp->lc_size_chg = 1;\n\t\tlcp->lc_newsize = new_size;\n\t} else {\n\t\tlcp->lc_size_chg = 0;\n\t}\n\n\tnfserr = ops->proc_layoutcommit(inode, lcp);\n\tnfs4_put_stid(&ls->ls_stid);\nout:\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_layoutreturn(struct svc_rqst *rqstp,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd4_layoutreturn *lrp)\n{\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\t__be32 nfserr;\n\n\tnfserr = fh_verify(rqstp, current_fh, 0, NFSD_MAY_NOP);\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfserr_layoutunavailable;\n\tif (!nfsd4_layout_verify(current_fh->fh_export, lrp->lr_layout_type))\n\t\tgoto out;\n\n\tswitch (lrp->lr_seg.iomode) {\n\tcase IOMODE_READ:\n\tcase IOMODE_RW:\n\tcase IOMODE_ANY:\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"%s: invalid iomode %d\\n\", __func__,\n\t\t\tlrp->lr_seg.iomode);\n\t\tnfserr = nfserr_inval;\n\t\tgoto out;\n\t}\n\n\tswitch (lrp->lr_return_type) {\n\tcase RETURN_FILE:\n\t\tnfserr = nfsd4_return_file_layouts(rqstp, cstate, lrp);\n\t\tbreak;\n\tcase RETURN_FSID:\n\tcase RETURN_ALL:\n\t\tnfserr = nfsd4_return_client_layouts(rqstp, cstate, lrp);\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"%s: invalid return_type %d\\n\", __func__,\n\t\t\tlrp->lr_return_type);\n\t\tnfserr = nfserr_inval;\n\t\tbreak;\n\t}\nout:\n\treturn nfserr;\n}\n#endif /* CONFIG_NFSD_PNFS */\n\n/*\n * NULL call.\n */\nstatic __be32\nnfsd4_proc_null(struct svc_rqst *rqstp, void *argp, void *resp)\n{\n\treturn nfs_ok;\n}\n\nstatic inline void nfsd4_increment_op_stats(u32 opnum)\n{\n\tif (opnum >= FIRST_NFS4_OP && opnum <= LAST_NFS4_OP)\n\t\tnfsdstats.nfs4_opcount[opnum]++;\n}\n\ntypedef __be32(*nfsd4op_func)(struct svc_rqst *, struct nfsd4_compound_state *,\n\t\t\t      void *);\ntypedef u32(*nfsd4op_rsize)(struct svc_rqst *, struct nfsd4_op *op);\ntypedef void(*stateid_setter)(struct nfsd4_compound_state *, void *);\ntypedef void(*stateid_getter)(struct nfsd4_compound_state *, void *);\n\nenum nfsd4_op_flags {\n\tALLOWED_WITHOUT_FH = 1 << 0,\t/* No current filehandle required */\n\tALLOWED_ON_ABSENT_FS = 1 << 1,\t/* ops processed on absent fs */\n\tALLOWED_AS_FIRST_OP = 1 << 2,\t/* ops reqired first in compound */\n\t/* For rfc 5661 section 2.6.3.1.1: */\n\tOP_HANDLES_WRONGSEC = 1 << 3,\n\tOP_IS_PUTFH_LIKE = 1 << 4,\n\t/*\n\t * These are the ops whose result size we estimate before\n\t * encoding, to avoid performing an op then not being able to\n\t * respond or cache a response.  This includes writes and setattrs\n\t * as well as the operations usually called \"nonidempotent\":\n\t */\n\tOP_MODIFIES_SOMETHING = 1 << 5,\n\t/*\n\t * Cache compounds containing these ops in the xid-based drc:\n\t * We use the DRC for compounds containing non-idempotent\n\t * operations, *except* those that are 4.1-specific (since\n\t * sessions provide their own EOS), and except for stateful\n\t * operations other than setclientid and setclientid_confirm\n\t * (since sequence numbers provide EOS for open, lock, etc in\n\t * the v4.0 case).\n\t */\n\tOP_CACHEME = 1 << 6,\n\t/*\n\t * These are ops which clear current state id.\n\t */\n\tOP_CLEAR_STATEID = 1 << 7,\n};\n\nstruct nfsd4_operation {\n\tnfsd4op_func op_func;\n\tu32 op_flags;\n\tchar *op_name;\n\t/* Try to get response size before operation */\n\tnfsd4op_rsize op_rsize_bop;\n\tstateid_getter op_get_currentstateid;\n\tstateid_setter op_set_currentstateid;\n};\n\nstatic struct nfsd4_operation nfsd4_ops[];\n\nstatic const char *nfsd4_op_name(unsigned opnum);\n\n/*\n * Enforce NFSv4.1 COMPOUND ordering rules:\n *\n * Also note, enforced elsewhere:\n *\t- SEQUENCE other than as first op results in\n *\t  NFS4ERR_SEQUENCE_POS. (Enforced in nfsd4_sequence().)\n *\t- BIND_CONN_TO_SESSION must be the only op in its compound.\n *\t  (Enforced in nfsd4_bind_conn_to_session().)\n *\t- DESTROY_SESSION must be the final operation in a compound, if\n *\t  sessionid's in SEQUENCE and DESTROY_SESSION are the same.\n *\t  (Enforced in nfsd4_destroy_session().)\n */\nstatic __be32 nfs41_check_op_ordering(struct nfsd4_compoundargs *args)\n{\n\tstruct nfsd4_op *op = &args->ops[0];\n\n\t/* These ordering requirements don't apply to NFSv4.0: */\n\tif (args->minorversion == 0)\n\t\treturn nfs_ok;\n\t/* This is weird, but OK, not our problem: */\n\tif (args->opcnt == 0)\n\t\treturn nfs_ok;\n\tif (op->status == nfserr_op_illegal)\n\t\treturn nfs_ok;\n\tif (!(nfsd4_ops[op->opnum].op_flags & ALLOWED_AS_FIRST_OP))\n\t\treturn nfserr_op_not_in_session;\n\tif (op->opnum == OP_SEQUENCE)\n\t\treturn nfs_ok;\n\tif (args->opcnt != 1)\n\t\treturn nfserr_not_only_op;\n\treturn nfs_ok;\n}\n\nstatic inline struct nfsd4_operation *OPDESC(struct nfsd4_op *op)\n{\n\treturn &nfsd4_ops[op->opnum];\n}\n\nbool nfsd4_cache_this_op(struct nfsd4_op *op)\n{\n\tif (op->opnum == OP_ILLEGAL)\n\t\treturn false;\n\treturn OPDESC(op)->op_flags & OP_CACHEME;\n}\n\nstatic bool need_wrongsec_check(struct svc_rqst *rqstp)\n{\n\tstruct nfsd4_compoundres *resp = rqstp->rq_resp;\n\tstruct nfsd4_compoundargs *argp = rqstp->rq_argp;\n\tstruct nfsd4_op *this = &argp->ops[resp->opcnt - 1];\n\tstruct nfsd4_op *next = &argp->ops[resp->opcnt];\n\tstruct nfsd4_operation *thisd;\n\tstruct nfsd4_operation *nextd;\n\n\tthisd = OPDESC(this);\n\t/*\n\t * Most ops check wronsec on our own; only the putfh-like ops\n\t * have special rules.\n\t */\n\tif (!(thisd->op_flags & OP_IS_PUTFH_LIKE))\n\t\treturn false;\n\t/*\n\t * rfc 5661 2.6.3.1.1.6: don't bother erroring out a\n\t * put-filehandle operation if we're not going to use the\n\t * result:\n\t */\n\tif (argp->opcnt == resp->opcnt)\n\t\treturn false;\n\tif (next->opnum == OP_ILLEGAL)\n\t\treturn false;\n\tnextd = OPDESC(next);\n\t/*\n\t * Rest of 2.6.3.1.1: certain operations will return WRONGSEC\n\t * errors themselves as necessary; others should check for them\n\t * now:\n\t */\n\treturn !(nextd->op_flags & OP_HANDLES_WRONGSEC);\n}\n\nstatic void svcxdr_init_encode(struct svc_rqst *rqstp,\n\t\t\t       struct nfsd4_compoundres *resp)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct xdr_buf *buf = &rqstp->rq_res;\n\tstruct kvec *head = buf->head;\n\n\txdr->buf = buf;\n\txdr->iov = head;\n\txdr->p   = head->iov_base + head->iov_len;\n\txdr->end = head->iov_base + PAGE_SIZE - rqstp->rq_auth_slack;\n\t/* Tail and page_len should be zero at this point: */\n\tbuf->len = buf->head[0].iov_len;\n\txdr->scratch.iov_len = 0;\n\txdr->page_ptr = buf->pages - 1;\n\tbuf->buflen = PAGE_SIZE * (1 + rqstp->rq_page_end - buf->pages)\n\t\t- rqstp->rq_auth_slack;\n}\n\n/*\n * COMPOUND call.\n */\nstatic __be32\nnfsd4_proc_compound(struct svc_rqst *rqstp,\n\t\t    struct nfsd4_compoundargs *args,\n\t\t    struct nfsd4_compoundres *resp)\n{\n\tstruct nfsd4_op\t*op;\n\tstruct nfsd4_operation *opdesc;\n\tstruct nfsd4_compound_state *cstate = &resp->cstate;\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\tstruct svc_fh *save_fh = &cstate->save_fh;\n\t__be32\t\tstatus;\n\n\tsvcxdr_init_encode(rqstp, resp);\n\tresp->tagp = resp->xdr.p;\n\t/* reserve space for: taglen, tag, and opcnt */\n\txdr_reserve_space(&resp->xdr, 8 + args->taglen);\n\tresp->taglen = args->taglen;\n\tresp->tag = args->tag;\n\tresp->rqstp = rqstp;\n\tcstate->minorversion = args->minorversion;\n\tfh_init(current_fh, NFS4_FHSIZE);\n\tfh_init(save_fh, NFS4_FHSIZE);\n\t/*\n\t * Don't use the deferral mechanism for NFSv4; compounds make it\n\t * too hard to avoid non-idempotency problems.\n\t */\n\tclear_bit(RQ_USEDEFERRAL, &rqstp->rq_flags);\n\n\t/*\n\t * According to RFC3010, this takes precedence over all other errors.\n\t */\n\tstatus = nfserr_minor_vers_mismatch;\n\tif (nfsd_minorversion(args->minorversion, NFSD_TEST) <= 0)\n\t\tgoto out;\n\n\tstatus = nfs41_check_op_ordering(args);\n\tif (status) {\n\t\top = &args->ops[0];\n\t\top->status = status;\n\t\tgoto encode_op;\n\t}\n\n\twhile (!status && resp->opcnt < args->opcnt) {\n\t\top = &args->ops[resp->opcnt++];\n\n\t\tdprintk(\"nfsv4 compound op #%d/%d: %d (%s)\\n\",\n\t\t\tresp->opcnt, args->opcnt, op->opnum,\n\t\t\tnfsd4_op_name(op->opnum));\n\t\t/*\n\t\t * The XDR decode routines may have pre-set op->status;\n\t\t * for example, if there is a miscellaneous XDR error\n\t\t * it will be set to nfserr_bad_xdr.\n\t\t */\n\t\tif (op->status) {\n\t\t\tif (op->opnum == OP_OPEN)\n\t\t\t\top->status = nfsd4_open_omfg(rqstp, cstate, op);\n\t\t\tgoto encode_op;\n\t\t}\n\n\t\topdesc = OPDESC(op);\n\n\t\tif (!current_fh->fh_dentry) {\n\t\t\tif (!(opdesc->op_flags & ALLOWED_WITHOUT_FH)) {\n\t\t\t\top->status = nfserr_nofilehandle;\n\t\t\t\tgoto encode_op;\n\t\t\t}\n\t\t} else if (current_fh->fh_export->ex_fslocs.migrated &&\n\t\t\t  !(opdesc->op_flags & ALLOWED_ON_ABSENT_FS)) {\n\t\t\top->status = nfserr_moved;\n\t\t\tgoto encode_op;\n\t\t}\n\n\t\tfh_clear_wcc(current_fh);\n\n\t\t/* If op is non-idempotent */\n\t\tif (opdesc->op_flags & OP_MODIFIES_SOMETHING) {\n\t\t\t/*\n\t\t\t * Don't execute this op if we couldn't encode a\n\t\t\t * succesful reply:\n\t\t\t */\n\t\t\tu32 plen = opdesc->op_rsize_bop(rqstp, op);\n\t\t\t/*\n\t\t\t * Plus if there's another operation, make sure\n\t\t\t * we'll have space to at least encode an error:\n\t\t\t */\n\t\t\tif (resp->opcnt < args->opcnt)\n\t\t\t\tplen += COMPOUND_ERR_SLACK_SPACE;\n\t\t\top->status = nfsd4_check_resp_size(resp, plen);\n\t\t}\n\n\t\tif (op->status)\n\t\t\tgoto encode_op;\n\n\t\tif (opdesc->op_get_currentstateid)\n\t\t\topdesc->op_get_currentstateid(cstate, &op->u);\n\t\top->status = opdesc->op_func(rqstp, cstate, &op->u);\n\n\t\tif (!op->status) {\n\t\t\tif (opdesc->op_set_currentstateid)\n\t\t\t\topdesc->op_set_currentstateid(cstate, &op->u);\n\n\t\t\tif (opdesc->op_flags & OP_CLEAR_STATEID)\n\t\t\t\tclear_current_stateid(cstate);\n\n\t\t\tif (need_wrongsec_check(rqstp))\n\t\t\t\top->status = check_nfsd_access(current_fh->fh_export, rqstp);\n\t\t}\n\nencode_op:\n\t\t/* Only from SEQUENCE */\n\t\tif (cstate->status == nfserr_replay_cache) {\n\t\t\tdprintk(\"%s NFS4.1 replay from cache\\n\", __func__);\n\t\t\tstatus = op->status;\n\t\t\tgoto out;\n\t\t}\n\t\tif (op->status == nfserr_replay_me) {\n\t\t\top->replay = &cstate->replay_owner->so_replay;\n\t\t\tnfsd4_encode_replay(&resp->xdr, op);\n\t\t\tstatus = op->status = op->replay->rp_status;\n\t\t} else {\n\t\t\tnfsd4_encode_operation(resp, op);\n\t\t\tstatus = op->status;\n\t\t}\n\n\t\tdprintk(\"nfsv4 compound op %p opcnt %d #%d: %d: status %d\\n\",\n\t\t\targs->ops, args->opcnt, resp->opcnt, op->opnum,\n\t\t\tbe32_to_cpu(status));\n\n\t\tnfsd4_cstate_clear_replay(cstate);\n\t\tnfsd4_increment_op_stats(op->opnum);\n\t}\n\n\tcstate->status = status;\n\tfh_put(current_fh);\n\tfh_put(save_fh);\n\tBUG_ON(cstate->replay_owner);\nout:\n\t/* Reset deferral mechanism for RPC deferrals */\n\tset_bit(RQ_USEDEFERRAL, &rqstp->rq_flags);\n\tdprintk(\"nfsv4 compound returned %d\\n\", ntohl(status));\n\treturn status;\n}\n\n#define op_encode_hdr_size\t\t(2)\n#define op_encode_stateid_maxsz\t\t(XDR_QUADLEN(NFS4_STATEID_SIZE))\n#define op_encode_verifier_maxsz\t(XDR_QUADLEN(NFS4_VERIFIER_SIZE))\n#define op_encode_change_info_maxsz\t(5)\n#define nfs4_fattr_bitmap_maxsz\t\t(4)\n\n/* We'll fall back on returning no lockowner if run out of space: */\n#define op_encode_lockowner_maxsz\t(0)\n#define op_encode_lock_denied_maxsz\t(8 + op_encode_lockowner_maxsz)\n\n#define nfs4_owner_maxsz\t\t(1 + XDR_QUADLEN(IDMAP_NAMESZ))\n\n#define op_encode_ace_maxsz\t\t(3 + nfs4_owner_maxsz)\n#define op_encode_delegation_maxsz\t(1 + op_encode_stateid_maxsz + 1 + \\\n\t\t\t\t\t op_encode_ace_maxsz)\n\n#define op_encode_channel_attrs_maxsz\t(6 + 1 + 1)\n\nstatic inline u32 nfsd4_only_status_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_status_stateid_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_stateid_maxsz)* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_access_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\t/* ac_supported, ac_resp_access */\n\treturn (op_encode_hdr_size + 2)* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_commit_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_verifier_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_create_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_change_info_maxsz\n\t\t+ nfs4_fattr_bitmap_maxsz) * sizeof(__be32);\n}\n\n/*\n * Note since this is an idempotent operation we won't insist on failing\n * the op prematurely if the estimate is too large.  We may turn off splice\n * reads unnecessarily.\n */\nstatic inline u32 nfsd4_getattr_rsize(struct svc_rqst *rqstp,\n\t\t\t\t      struct nfsd4_op *op)\n{\n\tu32 *bmap = op->u.getattr.ga_bmval;\n\tu32 bmap0 = bmap[0], bmap1 = bmap[1], bmap2 = bmap[2];\n\tu32 ret = 0;\n\n\tif (bmap0 & FATTR4_WORD0_ACL)\n\t\treturn svc_max_payload(rqstp);\n\tif (bmap0 & FATTR4_WORD0_FS_LOCATIONS)\n\t\treturn svc_max_payload(rqstp);\n\n\tif (bmap1 & FATTR4_WORD1_OWNER) {\n\t\tret += IDMAP_NAMESZ + 4;\n\t\tbmap1 &= ~FATTR4_WORD1_OWNER;\n\t}\n\tif (bmap1 & FATTR4_WORD1_OWNER_GROUP) {\n\t\tret += IDMAP_NAMESZ + 4;\n\t\tbmap1 &= ~FATTR4_WORD1_OWNER_GROUP;\n\t}\n\tif (bmap0 & FATTR4_WORD0_FILEHANDLE) {\n\t\tret += NFS4_FHSIZE + 4;\n\t\tbmap0 &= ~FATTR4_WORD0_FILEHANDLE;\n\t}\n\tif (bmap2 & FATTR4_WORD2_SECURITY_LABEL) {\n\t\tret += NFS4_MAXLABELLEN + 12;\n\t\tbmap2 &= ~FATTR4_WORD2_SECURITY_LABEL;\n\t}\n\t/*\n\t * Largest of remaining attributes are 16 bytes (e.g.,\n\t * supported_attributes)\n\t */\n\tret += 16 * (hweight32(bmap0) + hweight32(bmap1) + hweight32(bmap2));\n\t/* bitmask, length */\n\tret += 20;\n\treturn ret;\n}\n\nstatic inline u32 nfsd4_getfh_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 1) * sizeof(__be32) + NFS4_FHSIZE;\n}\n\nstatic inline u32 nfsd4_link_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_change_info_maxsz)\n\t\t* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_lock_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_lock_denied_maxsz)\n\t\t* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_open_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_stateid_maxsz\n\t\t+ op_encode_change_info_maxsz + 1\n\t\t+ nfs4_fattr_bitmap_maxsz\n\t\t+ op_encode_delegation_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_read_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\tu32 maxcount = 0, rlen = 0;\n\n\tmaxcount = svc_max_payload(rqstp);\n\trlen = min(op->u.read.rd_length, maxcount);\n\n\treturn (op_encode_hdr_size + 2 + XDR_QUADLEN(rlen)) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_readdir_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\tu32 maxcount = 0, rlen = 0;\n\n\tmaxcount = svc_max_payload(rqstp);\n\trlen = min(op->u.readdir.rd_maxcount, maxcount);\n\n\treturn (op_encode_hdr_size + op_encode_verifier_maxsz +\n\t\tXDR_QUADLEN(rlen)) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_readlink_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 1) * sizeof(__be32) + PAGE_SIZE;\n}\n\nstatic inline u32 nfsd4_remove_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_change_info_maxsz)\n\t\t* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_rename_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + op_encode_change_info_maxsz\n\t\t+ op_encode_change_info_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_sequence_rsize(struct svc_rqst *rqstp,\n\t\t\t\t       struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size\n\t\t+ XDR_QUADLEN(NFS4_MAX_SESSIONID_LEN) + 5) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_test_stateid_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 1 + op->u.test_stateid.ts_num_ids)\n\t\t* sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_setattr_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + nfs4_fattr_bitmap_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_secinfo_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + RPC_AUTH_MAXFLAVOR *\n\t\t(4 + XDR_QUADLEN(GSS_OID_MAX_LEN))) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_setclientid_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 2 + XDR_QUADLEN(NFS4_VERIFIER_SIZE)) *\n\t\t\t\t\t\t\t\tsizeof(__be32);\n}\n\nstatic inline u32 nfsd4_write_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 2 + op_encode_verifier_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_exchange_id_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 2 + 1 + /* eir_clientid, eir_sequenceid */\\\n\t\t1 + 1 + /* eir_flags, spr_how */\\\n\t\t4 + /* spo_must_enforce & _allow with bitmap */\\\n\t\t2 + /*eir_server_owner.so_minor_id */\\\n\t\t/* eir_server_owner.so_major_id<> */\\\n\t\tXDR_QUADLEN(NFS4_OPAQUE_LIMIT) + 1 +\\\n\t\t/* eir_server_scope<> */\\\n\t\tXDR_QUADLEN(NFS4_OPAQUE_LIMIT) + 1 +\\\n\t\t1 + /* eir_server_impl_id array length */\\\n\t\t0 /* ignored eir_server_impl_id contents */) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_bind_conn_to_session_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + \\\n\t\tXDR_QUADLEN(NFS4_MAX_SESSIONID_LEN) + /* bctsr_sessid */\\\n\t\t2 /* bctsr_dir, use_conn_in_rdma_mode */) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_create_session_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + \\\n\t\tXDR_QUADLEN(NFS4_MAX_SESSIONID_LEN) + /* sessionid */\\\n\t\t2 + /* csr_sequence, csr_flags */\\\n\t\top_encode_channel_attrs_maxsz + \\\n\t\top_encode_channel_attrs_maxsz) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_copy_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size +\n\t\t1 /* wr_callback */ +\n\t\top_encode_stateid_maxsz /* wr_callback */ +\n\t\t2 /* wr_count */ +\n\t\t1 /* wr_committed */ +\n\t\top_encode_verifier_maxsz +\n\t\t1 /* cr_consecutive */ +\n\t\t1 /* cr_synchronous */) * sizeof(__be32);\n}\n\n#ifdef CONFIG_NFSD_PNFS\nstatic inline u32 nfsd4_getdeviceinfo_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\tu32 maxcount = 0, rlen = 0;\n\n\tmaxcount = svc_max_payload(rqstp);\n\trlen = min(op->u.getdeviceinfo.gd_maxcount, maxcount);\n\n\treturn (op_encode_hdr_size +\n\t\t1 /* gd_layout_type*/ +\n\t\tXDR_QUADLEN(rlen) +\n\t\t2 /* gd_notify_types */) * sizeof(__be32);\n}\n\n/*\n * At this stage we don't really know what layout driver will handle the request,\n * so we need to define an arbitrary upper bound here.\n */\n#define MAX_LAYOUT_SIZE\t\t128\nstatic inline u32 nfsd4_layoutget_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size +\n\t\t1 /* logr_return_on_close */ +\n\t\top_encode_stateid_maxsz +\n\t\t1 /* nr of layouts */ +\n\t\tMAX_LAYOUT_SIZE) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_layoutcommit_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size +\n\t\t1 /* locr_newsize */ +\n\t\t2 /* ns_size */) * sizeof(__be32);\n}\n\nstatic inline u32 nfsd4_layoutreturn_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size +\n\t\t1 /* lrs_stateid */ +\n\t\top_encode_stateid_maxsz) * sizeof(__be32);\n}\n#endif /* CONFIG_NFSD_PNFS */\n\n\nstatic inline u32 nfsd4_seek_rsize(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\treturn (op_encode_hdr_size + 3) * sizeof(__be32);\n}\n\nstatic struct nfsd4_operation nfsd4_ops[] = {\n\t[OP_ACCESS] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_access,\n\t\t.op_name = \"OP_ACCESS\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_access_rsize,\n\t},\n\t[OP_CLOSE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_close,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_CLOSE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_status_stateid_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_closestateid,\n\t\t.op_set_currentstateid = (stateid_setter)nfsd4_set_closestateid,\n\t},\n\t[OP_COMMIT] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_commit,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_COMMIT\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_commit_rsize,\n\t},\n\t[OP_CREATE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_create,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_CREATE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_create_rsize,\n\t},\n\t[OP_DELEGRETURN] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_delegreturn,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_DELEGRETURN\",\n\t\t.op_rsize_bop = nfsd4_only_status_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_delegreturnstateid,\n\t},\n\t[OP_GETATTR] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_getattr,\n\t\t.op_flags = ALLOWED_ON_ABSENT_FS,\n\t\t.op_rsize_bop = nfsd4_getattr_rsize,\n\t\t.op_name = \"OP_GETATTR\",\n\t},\n\t[OP_GETFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_getfh,\n\t\t.op_name = \"OP_GETFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_getfh_rsize,\n\t},\n\t[OP_LINK] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_link,\n\t\t.op_flags = ALLOWED_ON_ABSENT_FS | OP_MODIFIES_SOMETHING\n\t\t\t\t| OP_CACHEME,\n\t\t.op_name = \"OP_LINK\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_link_rsize,\n\t},\n\t[OP_LOCK] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_lock,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_LOCK\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_lock_rsize,\n\t\t.op_set_currentstateid = (stateid_setter)nfsd4_set_lockstateid,\n\t},\n\t[OP_LOCKT] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_lockt,\n\t\t.op_name = \"OP_LOCKT\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_lock_rsize,\n\t},\n\t[OP_LOCKU] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_locku,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_LOCKU\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_status_stateid_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_lockustateid,\n\t},\n\t[OP_LOOKUP] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_lookup,\n\t\t.op_flags = OP_HANDLES_WRONGSEC | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_LOOKUP\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_LOOKUPP] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_lookupp,\n\t\t.op_flags = OP_HANDLES_WRONGSEC | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_LOOKUPP\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_NVERIFY] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_nverify,\n\t\t.op_name = \"OP_NVERIFY\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_OPEN] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_open,\n\t\t.op_flags = OP_HANDLES_WRONGSEC | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_OPEN\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_open_rsize,\n\t\t.op_set_currentstateid = (stateid_setter)nfsd4_set_openstateid,\n\t},\n\t[OP_OPEN_CONFIRM] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_open_confirm,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_OPEN_CONFIRM\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_status_stateid_rsize,\n\t},\n\t[OP_OPEN_DOWNGRADE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_open_downgrade,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_OPEN_DOWNGRADE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_status_stateid_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_opendowngradestateid,\n\t\t.op_set_currentstateid = (stateid_setter)nfsd4_set_opendowngradestateid,\n\t},\n\t[OP_PUTFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_putfh,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_IS_PUTFH_LIKE | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_PUTFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_PUTPUBFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_putrootfh,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_IS_PUTFH_LIKE | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_PUTPUBFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_PUTROOTFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_putrootfh,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_IS_PUTFH_LIKE | OP_CLEAR_STATEID,\n\t\t.op_name = \"OP_PUTROOTFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_READ] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_read,\n\t\t.op_name = \"OP_READ\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_read_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_readstateid,\n\t},\n\t[OP_READDIR] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_readdir,\n\t\t.op_name = \"OP_READDIR\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_readdir_rsize,\n\t},\n\t[OP_READLINK] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_readlink,\n\t\t.op_name = \"OP_READLINK\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_readlink_rsize,\n\t},\n\t[OP_REMOVE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_remove,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_REMOVE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_remove_rsize,\n\t},\n\t[OP_RENAME] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_rename,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_RENAME\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_rename_rsize,\n\t},\n\t[OP_RENEW] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_renew,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_RENEW\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\n\t},\n\t[OP_RESTOREFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_restorefh,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_IS_PUTFH_LIKE | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_RESTOREFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_SAVEFH] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_savefh,\n\t\t.op_flags = OP_HANDLES_WRONGSEC | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_SAVEFH\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_SECINFO] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_secinfo,\n\t\t.op_flags = OP_HANDLES_WRONGSEC,\n\t\t.op_name = \"OP_SECINFO\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_secinfo_rsize,\n\t},\n\t[OP_SETATTR] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_setattr,\n\t\t.op_name = \"OP_SETATTR\",\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_setattr_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_setattrstateid,\n\t},\n\t[OP_SETCLIENTID] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_setclientid,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_SETCLIENTID\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_setclientid_rsize,\n\t},\n\t[OP_SETCLIENTID_CONFIRM] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_setclientid_confirm,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_SETCLIENTID_CONFIRM\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_VERIFY] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_verify,\n\t\t.op_name = \"OP_VERIFY\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_WRITE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_write,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_WRITE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_write_rsize,\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_writestateid,\n\t},\n\t[OP_RELEASE_LOCKOWNER] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_release_lockowner,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_ON_ABSENT_FS\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_RELEASE_LOCKOWNER\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\n\t/* NFSv4.1 operations */\n\t[OP_EXCHANGE_ID] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_exchange_id,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_EXCHANGE_ID\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_exchange_id_rsize,\n\t},\n\t[OP_BACKCHANNEL_CTL] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_backchannel_ctl,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_BACKCHANNEL_CTL\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_BIND_CONN_TO_SESSION] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_bind_conn_to_session,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_BIND_CONN_TO_SESSION\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_bind_conn_to_session_rsize,\n\t},\n\t[OP_CREATE_SESSION] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_create_session,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_CREATE_SESSION\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_create_session_rsize,\n\t},\n\t[OP_DESTROY_SESSION] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_destroy_session,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_DESTROY_SESSION\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_SEQUENCE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_sequence,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP,\n\t\t.op_name = \"OP_SEQUENCE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_sequence_rsize,\n\t},\n\t[OP_DESTROY_CLIENTID] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_destroy_clientid,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | ALLOWED_AS_FIRST_OP\n\t\t\t\t| OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_DESTROY_CLIENTID\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_RECLAIM_COMPLETE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_reclaim_complete,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_RECLAIM_COMPLETE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_SECINFO_NO_NAME] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_secinfo_no_name,\n\t\t.op_flags = OP_HANDLES_WRONGSEC,\n\t\t.op_name = \"OP_SECINFO_NO_NAME\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_secinfo_rsize,\n\t},\n\t[OP_TEST_STATEID] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_test_stateid,\n\t\t.op_flags = ALLOWED_WITHOUT_FH,\n\t\t.op_name = \"OP_TEST_STATEID\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_test_stateid_rsize,\n\t},\n\t[OP_FREE_STATEID] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_free_stateid,\n\t\t.op_flags = ALLOWED_WITHOUT_FH | OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_FREE_STATEID\",\n\t\t.op_get_currentstateid = (stateid_getter)nfsd4_get_freestateid,\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n#ifdef CONFIG_NFSD_PNFS\n\t[OP_GETDEVICEINFO] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_getdeviceinfo,\n\t\t.op_flags = ALLOWED_WITHOUT_FH,\n\t\t.op_name = \"OP_GETDEVICEINFO\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_getdeviceinfo_rsize,\n\t},\n\t[OP_LAYOUTGET] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_layoutget,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_LAYOUTGET\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_layoutget_rsize,\n\t},\n\t[OP_LAYOUTCOMMIT] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_layoutcommit,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_LAYOUTCOMMIT\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_layoutcommit_rsize,\n\t},\n\t[OP_LAYOUTRETURN] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_layoutreturn,\n\t\t.op_flags = OP_MODIFIES_SOMETHING,\n\t\t.op_name = \"OP_LAYOUTRETURN\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_layoutreturn_rsize,\n\t},\n#endif /* CONFIG_NFSD_PNFS */\n\n\t/* NFSv4.2 operations */\n\t[OP_ALLOCATE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_allocate,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_ALLOCATE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_DEALLOCATE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_deallocate,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_DEALLOCATE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_CLONE] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_clone,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_CLONE\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_only_status_rsize,\n\t},\n\t[OP_COPY] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_copy,\n\t\t.op_flags = OP_MODIFIES_SOMETHING | OP_CACHEME,\n\t\t.op_name = \"OP_COPY\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_copy_rsize,\n\t},\n\t[OP_SEEK] = {\n\t\t.op_func = (nfsd4op_func)nfsd4_seek,\n\t\t.op_name = \"OP_SEEK\",\n\t\t.op_rsize_bop = (nfsd4op_rsize)nfsd4_seek_rsize,\n\t},\n};\n\n/**\n * nfsd4_spo_must_allow - Determine if the compound op contains an\n * operation that is allowed to be sent with machine credentials\n *\n * @rqstp: a pointer to the struct svc_rqst\n *\n * Checks to see if the compound contains a spo_must_allow op\n * and confirms that it was sent with the proper machine creds.\n */\n\nbool nfsd4_spo_must_allow(struct svc_rqst *rqstp)\n{\n\tstruct nfsd4_compoundres *resp = rqstp->rq_resp;\n\tstruct nfsd4_compoundargs *argp = rqstp->rq_argp;\n\tstruct nfsd4_op *this = &argp->ops[resp->opcnt - 1];\n\tstruct nfsd4_compound_state *cstate = &resp->cstate;\n\tstruct nfs4_op_map *allow = &cstate->clp->cl_spo_must_allow;\n\tu32 opiter;\n\n\tif (!cstate->minorversion)\n\t\treturn false;\n\n\tif (cstate->spo_must_allowed == true)\n\t\treturn true;\n\n\topiter = resp->opcnt;\n\twhile (opiter < argp->opcnt) {\n\t\tthis = &argp->ops[opiter++];\n\t\tif (test_bit(this->opnum, allow->u.longs) &&\n\t\t\tcstate->clp->cl_mach_cred &&\n\t\t\tnfsd4_mach_creds_match(cstate->clp, rqstp)) {\n\t\t\tcstate->spo_must_allowed = true;\n\t\t\treturn true;\n\t\t}\n\t}\n\tcstate->spo_must_allowed = false;\n\treturn false;\n}\n\nint nfsd4_max_reply(struct svc_rqst *rqstp, struct nfsd4_op *op)\n{\n\tif (op->opnum == OP_ILLEGAL || op->status == nfserr_notsupp)\n\t\treturn op_encode_hdr_size * sizeof(__be32);\n\n\tBUG_ON(OPDESC(op)->op_rsize_bop == NULL);\n\treturn OPDESC(op)->op_rsize_bop(rqstp, op);\n}\n\nvoid warn_on_nonidempotent_op(struct nfsd4_op *op)\n{\n\tif (OPDESC(op)->op_flags & OP_MODIFIES_SOMETHING) {\n\t\tpr_err(\"unable to encode reply to nonidempotent op %d (%s)\\n\",\n\t\t\top->opnum, nfsd4_op_name(op->opnum));\n\t\tWARN_ON_ONCE(1);\n\t}\n}\n\nstatic const char *nfsd4_op_name(unsigned opnum)\n{\n\tif (opnum < ARRAY_SIZE(nfsd4_ops))\n\t\treturn nfsd4_ops[opnum].op_name;\n\treturn \"unknown_operation\";\n}\n\n#define nfsd4_voidres\t\t\tnfsd4_voidargs\nstruct nfsd4_voidargs { int dummy; };\n\nstatic struct svc_procedure\t\tnfsd_procedures4[2] = {\n\t[NFSPROC4_NULL] = {\n\t\t.pc_func = (svc_procfunc) nfsd4_proc_null,\n\t\t.pc_encode = (kxdrproc_t) nfs4svc_encode_voidres,\n\t\t.pc_argsize = sizeof(struct nfsd4_voidargs),\n\t\t.pc_ressize = sizeof(struct nfsd4_voidres),\n\t\t.pc_cachetype = RC_NOCACHE,\n\t\t.pc_xdrressize = 1,\n\t},\n\t[NFSPROC4_COMPOUND] = {\n\t\t.pc_func = (svc_procfunc) nfsd4_proc_compound,\n\t\t.pc_decode = (kxdrproc_t) nfs4svc_decode_compoundargs,\n\t\t.pc_encode = (kxdrproc_t) nfs4svc_encode_compoundres,\n\t\t.pc_argsize = sizeof(struct nfsd4_compoundargs),\n\t\t.pc_ressize = sizeof(struct nfsd4_compoundres),\n\t\t.pc_release = nfsd4_release_compoundargs,\n\t\t.pc_cachetype = RC_NOCACHE,\n\t\t.pc_xdrressize = NFSD_BUFSIZE/4,\n\t},\n};\n\nstruct svc_version\tnfsd_version4 = {\n\t.vs_vers\t\t= 4,\n\t.vs_nproc\t\t= 2,\n\t.vs_proc\t\t= nfsd_procedures4,\n\t.vs_dispatch\t\t= nfsd_dispatch,\n\t.vs_xdrsize\t\t= NFS4_SVC_XDRSIZE,\n\t.vs_rpcb_optnl\t\t= true,\n\t.vs_need_cong_ctrl\t= true,\n};\n\n/*\n * Local variables:\n *  c-basic-offset: 8\n * End:\n */\n", "/*\n*  Copyright (c) 2001 The Regents of the University of Michigan.\n*  All rights reserved.\n*\n*  Kendrick Smith <kmsmith@umich.edu>\n*  Andy Adamson <kandros@umich.edu>\n*\n*  Redistribution and use in source and binary forms, with or without\n*  modification, are permitted provided that the following conditions\n*  are met:\n*\n*  1. Redistributions of source code must retain the above copyright\n*     notice, this list of conditions and the following disclaimer.\n*  2. Redistributions in binary form must reproduce the above copyright\n*     notice, this list of conditions and the following disclaimer in the\n*     documentation and/or other materials provided with the distribution.\n*  3. Neither the name of the University nor the names of its\n*     contributors may be used to endorse or promote products derived\n*     from this software without specific prior written permission.\n*\n*  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED\n*  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n*  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n*  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE\n*  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n*  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n*  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n*  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n*  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n*  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n*  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n*\n*/\n\n#include <linux/file.h>\n#include <linux/fs.h>\n#include <linux/slab.h>\n#include <linux/namei.h>\n#include <linux/swap.h>\n#include <linux/pagemap.h>\n#include <linux/ratelimit.h>\n#include <linux/sunrpc/svcauth_gss.h>\n#include <linux/sunrpc/addr.h>\n#include <linux/jhash.h>\n#include \"xdr4.h\"\n#include \"xdr4cb.h\"\n#include \"vfs.h\"\n#include \"current_stateid.h\"\n\n#include \"netns.h\"\n#include \"pnfs.h\"\n\n#define NFSDDBG_FACILITY                NFSDDBG_PROC\n\n#define all_ones {{~0,~0},~0}\nstatic const stateid_t one_stateid = {\n\t.si_generation = ~0,\n\t.si_opaque = all_ones,\n};\nstatic const stateid_t zero_stateid = {\n\t/* all fields zero */\n};\nstatic const stateid_t currentstateid = {\n\t.si_generation = 1,\n};\n\nstatic u64 current_sessionid = 1;\n\n#define ZERO_STATEID(stateid) (!memcmp((stateid), &zero_stateid, sizeof(stateid_t)))\n#define ONE_STATEID(stateid)  (!memcmp((stateid), &one_stateid, sizeof(stateid_t)))\n#define CURRENT_STATEID(stateid) (!memcmp((stateid), &currentstateid, sizeof(stateid_t)))\n\n/* forward declarations */\nstatic bool check_for_locks(struct nfs4_file *fp, struct nfs4_lockowner *lowner);\nstatic void nfs4_free_ol_stateid(struct nfs4_stid *stid);\n\n/* Locking: */\n\n/*\n * Currently used for the del_recall_lru and file hash table.  In an\n * effort to decrease the scope of the client_mutex, this spinlock may\n * eventually cover more:\n */\nstatic DEFINE_SPINLOCK(state_lock);\n\n/*\n * A waitqueue for all in-progress 4.0 CLOSE operations that are waiting for\n * the refcount on the open stateid to drop.\n */\nstatic DECLARE_WAIT_QUEUE_HEAD(close_wq);\n\nstatic struct kmem_cache *openowner_slab;\nstatic struct kmem_cache *lockowner_slab;\nstatic struct kmem_cache *file_slab;\nstatic struct kmem_cache *stateid_slab;\nstatic struct kmem_cache *deleg_slab;\nstatic struct kmem_cache *odstate_slab;\n\nstatic void free_session(struct nfsd4_session *);\n\nstatic const struct nfsd4_callback_ops nfsd4_cb_recall_ops;\nstatic const struct nfsd4_callback_ops nfsd4_cb_notify_lock_ops;\n\nstatic bool is_session_dead(struct nfsd4_session *ses)\n{\n\treturn ses->se_flags & NFS4_SESSION_DEAD;\n}\n\nstatic __be32 mark_session_dead_locked(struct nfsd4_session *ses, int ref_held_by_me)\n{\n\tif (atomic_read(&ses->se_ref) > ref_held_by_me)\n\t\treturn nfserr_jukebox;\n\tses->se_flags |= NFS4_SESSION_DEAD;\n\treturn nfs_ok;\n}\n\nstatic bool is_client_expired(struct nfs4_client *clp)\n{\n\treturn clp->cl_time == 0;\n}\n\nstatic __be32 get_client_locked(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tif (is_client_expired(clp))\n\t\treturn nfserr_expired;\n\tatomic_inc(&clp->cl_refcount);\n\treturn nfs_ok;\n}\n\n/* must be called under the client_lock */\nstatic inline void\nrenew_client_locked(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tif (is_client_expired(clp)) {\n\t\tWARN_ON(1);\n\t\tprintk(\"%s: client (clientid %08x/%08x) already expired\\n\",\n\t\t\t__func__,\n\t\t\tclp->cl_clientid.cl_boot,\n\t\t\tclp->cl_clientid.cl_id);\n\t\treturn;\n\t}\n\n\tdprintk(\"renewing client (clientid %08x/%08x)\\n\",\n\t\t\tclp->cl_clientid.cl_boot,\n\t\t\tclp->cl_clientid.cl_id);\n\tlist_move_tail(&clp->cl_lru, &nn->client_lru);\n\tclp->cl_time = get_seconds();\n}\n\nstatic void put_client_renew_locked(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tif (!atomic_dec_and_test(&clp->cl_refcount))\n\t\treturn;\n\tif (!is_client_expired(clp))\n\t\trenew_client_locked(clp);\n}\n\nstatic void put_client_renew(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tif (!atomic_dec_and_lock(&clp->cl_refcount, &nn->client_lock))\n\t\treturn;\n\tif (!is_client_expired(clp))\n\t\trenew_client_locked(clp);\n\tspin_unlock(&nn->client_lock);\n}\n\nstatic __be32 nfsd4_get_session_locked(struct nfsd4_session *ses)\n{\n\t__be32 status;\n\n\tif (is_session_dead(ses))\n\t\treturn nfserr_badsession;\n\tstatus = get_client_locked(ses->se_client);\n\tif (status)\n\t\treturn status;\n\tatomic_inc(&ses->se_ref);\n\treturn nfs_ok;\n}\n\nstatic void nfsd4_put_session_locked(struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tif (atomic_dec_and_test(&ses->se_ref) && is_session_dead(ses))\n\t\tfree_session(ses);\n\tput_client_renew_locked(clp);\n}\n\nstatic void nfsd4_put_session(struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tspin_lock(&nn->client_lock);\n\tnfsd4_put_session_locked(ses);\n\tspin_unlock(&nn->client_lock);\n}\n\nstatic struct nfsd4_blocked_lock *\nfind_blocked_lock(struct nfs4_lockowner *lo, struct knfsd_fh *fh,\n\t\t\tstruct nfsd_net *nn)\n{\n\tstruct nfsd4_blocked_lock *cur, *found = NULL;\n\n\tspin_lock(&nn->blocked_locks_lock);\n\tlist_for_each_entry(cur, &lo->lo_blocked, nbl_list) {\n\t\tif (fh_match(fh, &cur->nbl_fh)) {\n\t\t\tlist_del_init(&cur->nbl_list);\n\t\t\tlist_del_init(&cur->nbl_lru);\n\t\t\tfound = cur;\n\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&nn->blocked_locks_lock);\n\tif (found)\n\t\tposix_unblock_lock(&found->nbl_lock);\n\treturn found;\n}\n\nstatic struct nfsd4_blocked_lock *\nfind_or_allocate_block(struct nfs4_lockowner *lo, struct knfsd_fh *fh,\n\t\t\tstruct nfsd_net *nn)\n{\n\tstruct nfsd4_blocked_lock *nbl;\n\n\tnbl = find_blocked_lock(lo, fh, nn);\n\tif (!nbl) {\n\t\tnbl= kmalloc(sizeof(*nbl), GFP_KERNEL);\n\t\tif (nbl) {\n\t\t\tfh_copy_shallow(&nbl->nbl_fh, fh);\n\t\t\tlocks_init_lock(&nbl->nbl_lock);\n\t\t\tnfsd4_init_cb(&nbl->nbl_cb, lo->lo_owner.so_client,\n\t\t\t\t\t&nfsd4_cb_notify_lock_ops,\n\t\t\t\t\tNFSPROC4_CLNT_CB_NOTIFY_LOCK);\n\t\t}\n\t}\n\treturn nbl;\n}\n\nstatic void\nfree_blocked_lock(struct nfsd4_blocked_lock *nbl)\n{\n\tlocks_release_private(&nbl->nbl_lock);\n\tkfree(nbl);\n}\n\nstatic int\nnfsd4_cb_notify_lock_done(struct nfsd4_callback *cb, struct rpc_task *task)\n{\n\t/*\n\t * Since this is just an optimization, we don't try very hard if it\n\t * turns out not to succeed. We'll requeue it on NFS4ERR_DELAY, and\n\t * just quit trying on anything else.\n\t */\n\tswitch (task->tk_status) {\n\tcase -NFS4ERR_DELAY:\n\t\trpc_delay(task, 1 * HZ);\n\t\treturn 0;\n\tdefault:\n\t\treturn 1;\n\t}\n}\n\nstatic void\nnfsd4_cb_notify_lock_release(struct nfsd4_callback *cb)\n{\n\tstruct nfsd4_blocked_lock\t*nbl = container_of(cb,\n\t\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_cb);\n\n\tfree_blocked_lock(nbl);\n}\n\nstatic const struct nfsd4_callback_ops nfsd4_cb_notify_lock_ops = {\n\t.done\t\t= nfsd4_cb_notify_lock_done,\n\t.release\t= nfsd4_cb_notify_lock_release,\n};\n\nstatic inline struct nfs4_stateowner *\nnfs4_get_stateowner(struct nfs4_stateowner *sop)\n{\n\tatomic_inc(&sop->so_count);\n\treturn sop;\n}\n\nstatic int\nsame_owner_str(struct nfs4_stateowner *sop, struct xdr_netobj *owner)\n{\n\treturn (sop->so_owner.len == owner->len) &&\n\t\t0 == memcmp(sop->so_owner.data, owner->data, owner->len);\n}\n\nstatic struct nfs4_openowner *\nfind_openstateowner_str_locked(unsigned int hashval, struct nfsd4_open *open,\n\t\t\tstruct nfs4_client *clp)\n{\n\tstruct nfs4_stateowner *so;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_for_each_entry(so, &clp->cl_ownerstr_hashtbl[hashval],\n\t\t\t    so_strhash) {\n\t\tif (!so->so_is_open_owner)\n\t\t\tcontinue;\n\t\tif (same_owner_str(so, &open->op_owner))\n\t\t\treturn openowner(nfs4_get_stateowner(so));\n\t}\n\treturn NULL;\n}\n\nstatic struct nfs4_openowner *\nfind_openstateowner_str(unsigned int hashval, struct nfsd4_open *open,\n\t\t\tstruct nfs4_client *clp)\n{\n\tstruct nfs4_openowner *oo;\n\n\tspin_lock(&clp->cl_lock);\n\too = find_openstateowner_str_locked(hashval, open, clp);\n\tspin_unlock(&clp->cl_lock);\n\treturn oo;\n}\n\nstatic inline u32\nopaque_hashval(const void *ptr, int nbytes)\n{\n\tunsigned char *cptr = (unsigned char *) ptr;\n\n\tu32 x = 0;\n\twhile (nbytes--) {\n\t\tx *= 37;\n\t\tx += *cptr++;\n\t}\n\treturn x;\n}\n\nstatic void nfsd4_free_file_rcu(struct rcu_head *rcu)\n{\n\tstruct nfs4_file *fp = container_of(rcu, struct nfs4_file, fi_rcu);\n\n\tkmem_cache_free(file_slab, fp);\n}\n\nvoid\nput_nfs4_file(struct nfs4_file *fi)\n{\n\tmight_lock(&state_lock);\n\n\tif (atomic_dec_and_lock(&fi->fi_ref, &state_lock)) {\n\t\thlist_del_rcu(&fi->fi_hash);\n\t\tspin_unlock(&state_lock);\n\t\tWARN_ON_ONCE(!list_empty(&fi->fi_clnt_odstate));\n\t\tWARN_ON_ONCE(!list_empty(&fi->fi_delegations));\n\t\tcall_rcu(&fi->fi_rcu, nfsd4_free_file_rcu);\n\t}\n}\n\nstatic struct file *\n__nfs4_get_fd(struct nfs4_file *f, int oflag)\n{\n\tif (f->fi_fds[oflag])\n\t\treturn get_file(f->fi_fds[oflag]);\n\treturn NULL;\n}\n\nstatic struct file *\nfind_writeable_file_locked(struct nfs4_file *f)\n{\n\tstruct file *ret;\n\n\tlockdep_assert_held(&f->fi_lock);\n\n\tret = __nfs4_get_fd(f, O_WRONLY);\n\tif (!ret)\n\t\tret = __nfs4_get_fd(f, O_RDWR);\n\treturn ret;\n}\n\nstatic struct file *\nfind_writeable_file(struct nfs4_file *f)\n{\n\tstruct file *ret;\n\n\tspin_lock(&f->fi_lock);\n\tret = find_writeable_file_locked(f);\n\tspin_unlock(&f->fi_lock);\n\n\treturn ret;\n}\n\nstatic struct file *find_readable_file_locked(struct nfs4_file *f)\n{\n\tstruct file *ret;\n\n\tlockdep_assert_held(&f->fi_lock);\n\n\tret = __nfs4_get_fd(f, O_RDONLY);\n\tif (!ret)\n\t\tret = __nfs4_get_fd(f, O_RDWR);\n\treturn ret;\n}\n\nstatic struct file *\nfind_readable_file(struct nfs4_file *f)\n{\n\tstruct file *ret;\n\n\tspin_lock(&f->fi_lock);\n\tret = find_readable_file_locked(f);\n\tspin_unlock(&f->fi_lock);\n\n\treturn ret;\n}\n\nstruct file *\nfind_any_file(struct nfs4_file *f)\n{\n\tstruct file *ret;\n\n\tspin_lock(&f->fi_lock);\n\tret = __nfs4_get_fd(f, O_RDWR);\n\tif (!ret) {\n\t\tret = __nfs4_get_fd(f, O_WRONLY);\n\t\tif (!ret)\n\t\t\tret = __nfs4_get_fd(f, O_RDONLY);\n\t}\n\tspin_unlock(&f->fi_lock);\n\treturn ret;\n}\n\nstatic atomic_long_t num_delegations;\nunsigned long max_delegations;\n\n/*\n * Open owner state (share locks)\n */\n\n/* hash tables for lock and open owners */\n#define OWNER_HASH_BITS              8\n#define OWNER_HASH_SIZE             (1 << OWNER_HASH_BITS)\n#define OWNER_HASH_MASK             (OWNER_HASH_SIZE - 1)\n\nstatic unsigned int ownerstr_hashval(struct xdr_netobj *ownername)\n{\n\tunsigned int ret;\n\n\tret = opaque_hashval(ownername->data, ownername->len);\n\treturn ret & OWNER_HASH_MASK;\n}\n\n/* hash table for nfs4_file */\n#define FILE_HASH_BITS                   8\n#define FILE_HASH_SIZE                  (1 << FILE_HASH_BITS)\n\nstatic unsigned int nfsd_fh_hashval(struct knfsd_fh *fh)\n{\n\treturn jhash2(fh->fh_base.fh_pad, XDR_QUADLEN(fh->fh_size), 0);\n}\n\nstatic unsigned int file_hashval(struct knfsd_fh *fh)\n{\n\treturn nfsd_fh_hashval(fh) & (FILE_HASH_SIZE - 1);\n}\n\nstatic struct hlist_head file_hashtbl[FILE_HASH_SIZE];\n\nstatic void\n__nfs4_file_get_access(struct nfs4_file *fp, u32 access)\n{\n\tlockdep_assert_held(&fp->fi_lock);\n\n\tif (access & NFS4_SHARE_ACCESS_WRITE)\n\t\tatomic_inc(&fp->fi_access[O_WRONLY]);\n\tif (access & NFS4_SHARE_ACCESS_READ)\n\t\tatomic_inc(&fp->fi_access[O_RDONLY]);\n}\n\nstatic __be32\nnfs4_file_get_access(struct nfs4_file *fp, u32 access)\n{\n\tlockdep_assert_held(&fp->fi_lock);\n\n\t/* Does this access mode make sense? */\n\tif (access & ~NFS4_SHARE_ACCESS_BOTH)\n\t\treturn nfserr_inval;\n\n\t/* Does it conflict with a deny mode already set? */\n\tif ((access & fp->fi_share_deny) != 0)\n\t\treturn nfserr_share_denied;\n\n\t__nfs4_file_get_access(fp, access);\n\treturn nfs_ok;\n}\n\nstatic __be32 nfs4_file_check_deny(struct nfs4_file *fp, u32 deny)\n{\n\t/* Common case is that there is no deny mode. */\n\tif (deny) {\n\t\t/* Does this deny mode make sense? */\n\t\tif (deny & ~NFS4_SHARE_DENY_BOTH)\n\t\t\treturn nfserr_inval;\n\n\t\tif ((deny & NFS4_SHARE_DENY_READ) &&\n\t\t    atomic_read(&fp->fi_access[O_RDONLY]))\n\t\t\treturn nfserr_share_denied;\n\n\t\tif ((deny & NFS4_SHARE_DENY_WRITE) &&\n\t\t    atomic_read(&fp->fi_access[O_WRONLY]))\n\t\t\treturn nfserr_share_denied;\n\t}\n\treturn nfs_ok;\n}\n\nstatic void __nfs4_file_put_access(struct nfs4_file *fp, int oflag)\n{\n\tmight_lock(&fp->fi_lock);\n\n\tif (atomic_dec_and_lock(&fp->fi_access[oflag], &fp->fi_lock)) {\n\t\tstruct file *f1 = NULL;\n\t\tstruct file *f2 = NULL;\n\n\t\tswap(f1, fp->fi_fds[oflag]);\n\t\tif (atomic_read(&fp->fi_access[1 - oflag]) == 0)\n\t\t\tswap(f2, fp->fi_fds[O_RDWR]);\n\t\tspin_unlock(&fp->fi_lock);\n\t\tif (f1)\n\t\t\tfput(f1);\n\t\tif (f2)\n\t\t\tfput(f2);\n\t}\n}\n\nstatic void nfs4_file_put_access(struct nfs4_file *fp, u32 access)\n{\n\tWARN_ON_ONCE(access & ~NFS4_SHARE_ACCESS_BOTH);\n\n\tif (access & NFS4_SHARE_ACCESS_WRITE)\n\t\t__nfs4_file_put_access(fp, O_WRONLY);\n\tif (access & NFS4_SHARE_ACCESS_READ)\n\t\t__nfs4_file_put_access(fp, O_RDONLY);\n}\n\n/*\n * Allocate a new open/delegation state counter. This is needed for\n * pNFS for proper return on close semantics.\n *\n * Note that we only allocate it for pNFS-enabled exports, otherwise\n * all pointers to struct nfs4_clnt_odstate are always NULL.\n */\nstatic struct nfs4_clnt_odstate *\nalloc_clnt_odstate(struct nfs4_client *clp)\n{\n\tstruct nfs4_clnt_odstate *co;\n\n\tco = kmem_cache_zalloc(odstate_slab, GFP_KERNEL);\n\tif (co) {\n\t\tco->co_client = clp;\n\t\tatomic_set(&co->co_odcount, 1);\n\t}\n\treturn co;\n}\n\nstatic void\nhash_clnt_odstate_locked(struct nfs4_clnt_odstate *co)\n{\n\tstruct nfs4_file *fp = co->co_file;\n\n\tlockdep_assert_held(&fp->fi_lock);\n\tlist_add(&co->co_perfile, &fp->fi_clnt_odstate);\n}\n\nstatic inline void\nget_clnt_odstate(struct nfs4_clnt_odstate *co)\n{\n\tif (co)\n\t\tatomic_inc(&co->co_odcount);\n}\n\nstatic void\nput_clnt_odstate(struct nfs4_clnt_odstate *co)\n{\n\tstruct nfs4_file *fp;\n\n\tif (!co)\n\t\treturn;\n\n\tfp = co->co_file;\n\tif (atomic_dec_and_lock(&co->co_odcount, &fp->fi_lock)) {\n\t\tlist_del(&co->co_perfile);\n\t\tspin_unlock(&fp->fi_lock);\n\n\t\tnfsd4_return_all_file_layouts(co->co_client, fp);\n\t\tkmem_cache_free(odstate_slab, co);\n\t}\n}\n\nstatic struct nfs4_clnt_odstate *\nfind_or_hash_clnt_odstate(struct nfs4_file *fp, struct nfs4_clnt_odstate *new)\n{\n\tstruct nfs4_clnt_odstate *co;\n\tstruct nfs4_client *cl;\n\n\tif (!new)\n\t\treturn NULL;\n\n\tcl = new->co_client;\n\n\tspin_lock(&fp->fi_lock);\n\tlist_for_each_entry(co, &fp->fi_clnt_odstate, co_perfile) {\n\t\tif (co->co_client == cl) {\n\t\t\tget_clnt_odstate(co);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tco = new;\n\tco->co_file = fp;\n\thash_clnt_odstate_locked(new);\nout:\n\tspin_unlock(&fp->fi_lock);\n\treturn co;\n}\n\nstruct nfs4_stid *nfs4_alloc_stid(struct nfs4_client *cl, struct kmem_cache *slab,\n\t\t\t\t  void (*sc_free)(struct nfs4_stid *))\n{\n\tstruct nfs4_stid *stid;\n\tint new_id;\n\n\tstid = kmem_cache_zalloc(slab, GFP_KERNEL);\n\tif (!stid)\n\t\treturn NULL;\n\n\tidr_preload(GFP_KERNEL);\n\tspin_lock(&cl->cl_lock);\n\tnew_id = idr_alloc_cyclic(&cl->cl_stateids, stid, 0, 0, GFP_NOWAIT);\n\tspin_unlock(&cl->cl_lock);\n\tidr_preload_end();\n\tif (new_id < 0)\n\t\tgoto out_free;\n\n\tstid->sc_free = sc_free;\n\tstid->sc_client = cl;\n\tstid->sc_stateid.si_opaque.so_id = new_id;\n\tstid->sc_stateid.si_opaque.so_clid = cl->cl_clientid;\n\t/* Will be incremented before return to client: */\n\tatomic_set(&stid->sc_count, 1);\n\tspin_lock_init(&stid->sc_lock);\n\n\t/*\n\t * It shouldn't be a problem to reuse an opaque stateid value.\n\t * I don't think it is for 4.1.  But with 4.0 I worry that, for\n\t * example, a stray write retransmission could be accepted by\n\t * the server when it should have been rejected.  Therefore,\n\t * adopt a trick from the sctp code to attempt to maximize the\n\t * amount of time until an id is reused, by ensuring they always\n\t * \"increase\" (mod INT_MAX):\n\t */\n\treturn stid;\nout_free:\n\tkmem_cache_free(slab, stid);\n\treturn NULL;\n}\n\nstatic struct nfs4_ol_stateid * nfs4_alloc_open_stateid(struct nfs4_client *clp)\n{\n\tstruct nfs4_stid *stid;\n\n\tstid = nfs4_alloc_stid(clp, stateid_slab, nfs4_free_ol_stateid);\n\tif (!stid)\n\t\treturn NULL;\n\n\treturn openlockstateid(stid);\n}\n\nstatic void nfs4_free_deleg(struct nfs4_stid *stid)\n{\n\tkmem_cache_free(deleg_slab, stid);\n\tatomic_long_dec(&num_delegations);\n}\n\n/*\n * When we recall a delegation, we should be careful not to hand it\n * out again straight away.\n * To ensure this we keep a pair of bloom filters ('new' and 'old')\n * in which the filehandles of recalled delegations are \"stored\".\n * If a filehandle appear in either filter, a delegation is blocked.\n * When a delegation is recalled, the filehandle is stored in the \"new\"\n * filter.\n * Every 30 seconds we swap the filters and clear the \"new\" one,\n * unless both are empty of course.\n *\n * Each filter is 256 bits.  We hash the filehandle to 32bit and use the\n * low 3 bytes as hash-table indices.\n *\n * 'blocked_delegations_lock', which is always taken in block_delegations(),\n * is used to manage concurrent access.  Testing does not need the lock\n * except when swapping the two filters.\n */\nstatic DEFINE_SPINLOCK(blocked_delegations_lock);\nstatic struct bloom_pair {\n\tint\tentries, old_entries;\n\ttime_t\tswap_time;\n\tint\tnew; /* index into 'set' */\n\tDECLARE_BITMAP(set[2], 256);\n} blocked_delegations;\n\nstatic int delegation_blocked(struct knfsd_fh *fh)\n{\n\tu32 hash;\n\tstruct bloom_pair *bd = &blocked_delegations;\n\n\tif (bd->entries == 0)\n\t\treturn 0;\n\tif (seconds_since_boot() - bd->swap_time > 30) {\n\t\tspin_lock(&blocked_delegations_lock);\n\t\tif (seconds_since_boot() - bd->swap_time > 30) {\n\t\t\tbd->entries -= bd->old_entries;\n\t\t\tbd->old_entries = bd->entries;\n\t\t\tmemset(bd->set[bd->new], 0,\n\t\t\t       sizeof(bd->set[0]));\n\t\t\tbd->new = 1-bd->new;\n\t\t\tbd->swap_time = seconds_since_boot();\n\t\t}\n\t\tspin_unlock(&blocked_delegations_lock);\n\t}\n\thash = jhash(&fh->fh_base, fh->fh_size, 0);\n\tif (test_bit(hash&255, bd->set[0]) &&\n\t    test_bit((hash>>8)&255, bd->set[0]) &&\n\t    test_bit((hash>>16)&255, bd->set[0]))\n\t\treturn 1;\n\n\tif (test_bit(hash&255, bd->set[1]) &&\n\t    test_bit((hash>>8)&255, bd->set[1]) &&\n\t    test_bit((hash>>16)&255, bd->set[1]))\n\t\treturn 1;\n\n\treturn 0;\n}\n\nstatic void block_delegations(struct knfsd_fh *fh)\n{\n\tu32 hash;\n\tstruct bloom_pair *bd = &blocked_delegations;\n\n\thash = jhash(&fh->fh_base, fh->fh_size, 0);\n\n\tspin_lock(&blocked_delegations_lock);\n\t__set_bit(hash&255, bd->set[bd->new]);\n\t__set_bit((hash>>8)&255, bd->set[bd->new]);\n\t__set_bit((hash>>16)&255, bd->set[bd->new]);\n\tif (bd->entries == 0)\n\t\tbd->swap_time = seconds_since_boot();\n\tbd->entries += 1;\n\tspin_unlock(&blocked_delegations_lock);\n}\n\nstatic struct nfs4_delegation *\nalloc_init_deleg(struct nfs4_client *clp, struct svc_fh *current_fh,\n\t\t struct nfs4_clnt_odstate *odstate)\n{\n\tstruct nfs4_delegation *dp;\n\tlong n;\n\n\tdprintk(\"NFSD alloc_init_deleg\\n\");\n\tn = atomic_long_inc_return(&num_delegations);\n\tif (n < 0 || n > max_delegations)\n\t\tgoto out_dec;\n\tif (delegation_blocked(&current_fh->fh_handle))\n\t\tgoto out_dec;\n\tdp = delegstateid(nfs4_alloc_stid(clp, deleg_slab, nfs4_free_deleg));\n\tif (dp == NULL)\n\t\tgoto out_dec;\n\n\t/*\n\t * delegation seqid's are never incremented.  The 4.1 special\n\t * meaning of seqid 0 isn't meaningful, really, but let's avoid\n\t * 0 anyway just for consistency and use 1:\n\t */\n\tdp->dl_stid.sc_stateid.si_generation = 1;\n\tINIT_LIST_HEAD(&dp->dl_perfile);\n\tINIT_LIST_HEAD(&dp->dl_perclnt);\n\tINIT_LIST_HEAD(&dp->dl_recall_lru);\n\tdp->dl_clnt_odstate = odstate;\n\tget_clnt_odstate(odstate);\n\tdp->dl_type = NFS4_OPEN_DELEGATE_READ;\n\tdp->dl_retries = 1;\n\tnfsd4_init_cb(&dp->dl_recall, dp->dl_stid.sc_client,\n\t\t      &nfsd4_cb_recall_ops, NFSPROC4_CLNT_CB_RECALL);\n\treturn dp;\nout_dec:\n\tatomic_long_dec(&num_delegations);\n\treturn NULL;\n}\n\nvoid\nnfs4_put_stid(struct nfs4_stid *s)\n{\n\tstruct nfs4_file *fp = s->sc_file;\n\tstruct nfs4_client *clp = s->sc_client;\n\n\tmight_lock(&clp->cl_lock);\n\n\tif (!atomic_dec_and_lock(&s->sc_count, &clp->cl_lock)) {\n\t\twake_up_all(&close_wq);\n\t\treturn;\n\t}\n\tidr_remove(&clp->cl_stateids, s->sc_stateid.si_opaque.so_id);\n\tspin_unlock(&clp->cl_lock);\n\ts->sc_free(s);\n\tif (fp)\n\t\tput_nfs4_file(fp);\n}\n\nvoid\nnfs4_inc_and_copy_stateid(stateid_t *dst, struct nfs4_stid *stid)\n{\n\tstateid_t *src = &stid->sc_stateid;\n\n\tspin_lock(&stid->sc_lock);\n\tif (unlikely(++src->si_generation == 0))\n\t\tsrc->si_generation = 1;\n\tmemcpy(dst, src, sizeof(*dst));\n\tspin_unlock(&stid->sc_lock);\n}\n\nstatic void nfs4_put_deleg_lease(struct nfs4_file *fp)\n{\n\tstruct file *filp = NULL;\n\n\tspin_lock(&fp->fi_lock);\n\tif (fp->fi_deleg_file && --fp->fi_delegees == 0)\n\t\tswap(filp, fp->fi_deleg_file);\n\tspin_unlock(&fp->fi_lock);\n\n\tif (filp) {\n\t\tvfs_setlease(filp, F_UNLCK, NULL, (void **)&fp);\n\t\tfput(filp);\n\t}\n}\n\nvoid nfs4_unhash_stid(struct nfs4_stid *s)\n{\n\ts->sc_type = 0;\n}\n\n/**\n * nfs4_get_existing_delegation - Discover if this delegation already exists\n * @clp:     a pointer to the nfs4_client we're granting a delegation to\n * @fp:      a pointer to the nfs4_file we're granting a delegation on\n *\n * Return:\n *      On success: NULL if an existing delegation was not found.\n *\n *      On error: -EAGAIN if one was previously granted to this nfs4_client\n *                 for this nfs4_file.\n *\n */\n\nstatic int\nnfs4_get_existing_delegation(struct nfs4_client *clp, struct nfs4_file *fp)\n{\n\tstruct nfs4_delegation *searchdp = NULL;\n\tstruct nfs4_client *searchclp = NULL;\n\n\tlockdep_assert_held(&state_lock);\n\tlockdep_assert_held(&fp->fi_lock);\n\n\tlist_for_each_entry(searchdp, &fp->fi_delegations, dl_perfile) {\n\t\tsearchclp = searchdp->dl_stid.sc_client;\n\t\tif (clp == searchclp) {\n\t\t\treturn -EAGAIN;\n\t\t}\n\t}\n\treturn 0;\n}\n\n/**\n * hash_delegation_locked - Add a delegation to the appropriate lists\n * @dp:     a pointer to the nfs4_delegation we are adding.\n * @fp:     a pointer to the nfs4_file we're granting a delegation on\n *\n * Return:\n *      On success: NULL if the delegation was successfully hashed.\n *\n *      On error: -EAGAIN if one was previously granted to this\n *                 nfs4_client for this nfs4_file. Delegation is not hashed.\n *\n */\n\nstatic int\nhash_delegation_locked(struct nfs4_delegation *dp, struct nfs4_file *fp)\n{\n\tint status;\n\tstruct nfs4_client *clp = dp->dl_stid.sc_client;\n\n\tlockdep_assert_held(&state_lock);\n\tlockdep_assert_held(&fp->fi_lock);\n\n\tstatus = nfs4_get_existing_delegation(clp, fp);\n\tif (status)\n\t\treturn status;\n\t++fp->fi_delegees;\n\tatomic_inc(&dp->dl_stid.sc_count);\n\tdp->dl_stid.sc_type = NFS4_DELEG_STID;\n\tlist_add(&dp->dl_perfile, &fp->fi_delegations);\n\tlist_add(&dp->dl_perclnt, &clp->cl_delegations);\n\treturn 0;\n}\n\nstatic bool\nunhash_delegation_locked(struct nfs4_delegation *dp)\n{\n\tstruct nfs4_file *fp = dp->dl_stid.sc_file;\n\n\tlockdep_assert_held(&state_lock);\n\n\tif (list_empty(&dp->dl_perfile))\n\t\treturn false;\n\n\tdp->dl_stid.sc_type = NFS4_CLOSED_DELEG_STID;\n\t/* Ensure that deleg break won't try to requeue it */\n\t++dp->dl_time;\n\tspin_lock(&fp->fi_lock);\n\tlist_del_init(&dp->dl_perclnt);\n\tlist_del_init(&dp->dl_recall_lru);\n\tlist_del_init(&dp->dl_perfile);\n\tspin_unlock(&fp->fi_lock);\n\treturn true;\n}\n\nstatic void destroy_delegation(struct nfs4_delegation *dp)\n{\n\tbool unhashed;\n\n\tspin_lock(&state_lock);\n\tunhashed = unhash_delegation_locked(dp);\n\tspin_unlock(&state_lock);\n\tif (unhashed) {\n\t\tput_clnt_odstate(dp->dl_clnt_odstate);\n\t\tnfs4_put_deleg_lease(dp->dl_stid.sc_file);\n\t\tnfs4_put_stid(&dp->dl_stid);\n\t}\n}\n\nstatic void revoke_delegation(struct nfs4_delegation *dp)\n{\n\tstruct nfs4_client *clp = dp->dl_stid.sc_client;\n\n\tWARN_ON(!list_empty(&dp->dl_recall_lru));\n\n\tput_clnt_odstate(dp->dl_clnt_odstate);\n\tnfs4_put_deleg_lease(dp->dl_stid.sc_file);\n\n\tif (clp->cl_minorversion == 0)\n\t\tnfs4_put_stid(&dp->dl_stid);\n\telse {\n\t\tdp->dl_stid.sc_type = NFS4_REVOKED_DELEG_STID;\n\t\tspin_lock(&clp->cl_lock);\n\t\tlist_add(&dp->dl_recall_lru, &clp->cl_revoked);\n\t\tspin_unlock(&clp->cl_lock);\n\t}\n}\n\n/* \n * SETCLIENTID state \n */\n\nstatic unsigned int clientid_hashval(u32 id)\n{\n\treturn id & CLIENT_HASH_MASK;\n}\n\nstatic unsigned int clientstr_hashval(const char *name)\n{\n\treturn opaque_hashval(name, 8) & CLIENT_HASH_MASK;\n}\n\n/*\n * We store the NONE, READ, WRITE, and BOTH bits separately in the\n * st_{access,deny}_bmap field of the stateid, in order to track not\n * only what share bits are currently in force, but also what\n * combinations of share bits previous opens have used.  This allows us\n * to enforce the recommendation of rfc 3530 14.2.19 that the server\n * return an error if the client attempt to downgrade to a combination\n * of share bits not explicable by closing some of its previous opens.\n *\n * XXX: This enforcement is actually incomplete, since we don't keep\n * track of access/deny bit combinations; so, e.g., we allow:\n *\n *\tOPEN allow read, deny write\n *\tOPEN allow both, deny none\n *\tDOWNGRADE allow read, deny none\n *\n * which we should reject.\n */\nstatic unsigned int\nbmap_to_share_mode(unsigned long bmap) {\n\tint i;\n\tunsigned int access = 0;\n\n\tfor (i = 1; i < 4; i++) {\n\t\tif (test_bit(i, &bmap))\n\t\t\taccess |= i;\n\t}\n\treturn access;\n}\n\n/* set share access for a given stateid */\nstatic inline void\nset_access(u32 access, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << access;\n\n\tWARN_ON_ONCE(access > NFS4_SHARE_ACCESS_BOTH);\n\tstp->st_access_bmap |= mask;\n}\n\n/* clear share access for a given stateid */\nstatic inline void\nclear_access(u32 access, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << access;\n\n\tWARN_ON_ONCE(access > NFS4_SHARE_ACCESS_BOTH);\n\tstp->st_access_bmap &= ~mask;\n}\n\n/* test whether a given stateid has access */\nstatic inline bool\ntest_access(u32 access, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << access;\n\n\treturn (bool)(stp->st_access_bmap & mask);\n}\n\n/* set share deny for a given stateid */\nstatic inline void\nset_deny(u32 deny, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << deny;\n\n\tWARN_ON_ONCE(deny > NFS4_SHARE_DENY_BOTH);\n\tstp->st_deny_bmap |= mask;\n}\n\n/* clear share deny for a given stateid */\nstatic inline void\nclear_deny(u32 deny, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << deny;\n\n\tWARN_ON_ONCE(deny > NFS4_SHARE_DENY_BOTH);\n\tstp->st_deny_bmap &= ~mask;\n}\n\n/* test whether a given stateid is denying specific access */\nstatic inline bool\ntest_deny(u32 deny, struct nfs4_ol_stateid *stp)\n{\n\tunsigned char mask = 1 << deny;\n\n\treturn (bool)(stp->st_deny_bmap & mask);\n}\n\nstatic int nfs4_access_to_omode(u32 access)\n{\n\tswitch (access & NFS4_SHARE_ACCESS_BOTH) {\n\tcase NFS4_SHARE_ACCESS_READ:\n\t\treturn O_RDONLY;\n\tcase NFS4_SHARE_ACCESS_WRITE:\n\t\treturn O_WRONLY;\n\tcase NFS4_SHARE_ACCESS_BOTH:\n\t\treturn O_RDWR;\n\t}\n\tWARN_ON_ONCE(1);\n\treturn O_RDONLY;\n}\n\n/*\n * A stateid that had a deny mode associated with it is being released\n * or downgraded. Recalculate the deny mode on the file.\n */\nstatic void\nrecalculate_deny_mode(struct nfs4_file *fp)\n{\n\tstruct nfs4_ol_stateid *stp;\n\n\tspin_lock(&fp->fi_lock);\n\tfp->fi_share_deny = 0;\n\tlist_for_each_entry(stp, &fp->fi_stateids, st_perfile)\n\t\tfp->fi_share_deny |= bmap_to_share_mode(stp->st_deny_bmap);\n\tspin_unlock(&fp->fi_lock);\n}\n\nstatic void\nreset_union_bmap_deny(u32 deny, struct nfs4_ol_stateid *stp)\n{\n\tint i;\n\tbool change = false;\n\n\tfor (i = 1; i < 4; i++) {\n\t\tif ((i & deny) != i) {\n\t\t\tchange = true;\n\t\t\tclear_deny(i, stp);\n\t\t}\n\t}\n\n\t/* Recalculate per-file deny mode if there was a change */\n\tif (change)\n\t\trecalculate_deny_mode(stp->st_stid.sc_file);\n}\n\n/* release all access and file references for a given stateid */\nstatic void\nrelease_all_access(struct nfs4_ol_stateid *stp)\n{\n\tint i;\n\tstruct nfs4_file *fp = stp->st_stid.sc_file;\n\n\tif (fp && stp->st_deny_bmap != 0)\n\t\trecalculate_deny_mode(fp);\n\n\tfor (i = 1; i < 4; i++) {\n\t\tif (test_access(i, stp))\n\t\t\tnfs4_file_put_access(stp->st_stid.sc_file, i);\n\t\tclear_access(i, stp);\n\t}\n}\n\nstatic inline void nfs4_free_stateowner(struct nfs4_stateowner *sop)\n{\n\tkfree(sop->so_owner.data);\n\tsop->so_ops->so_free(sop);\n}\n\nstatic void nfs4_put_stateowner(struct nfs4_stateowner *sop)\n{\n\tstruct nfs4_client *clp = sop->so_client;\n\n\tmight_lock(&clp->cl_lock);\n\n\tif (!atomic_dec_and_lock(&sop->so_count, &clp->cl_lock))\n\t\treturn;\n\tsop->so_ops->so_unhash(sop);\n\tspin_unlock(&clp->cl_lock);\n\tnfs4_free_stateowner(sop);\n}\n\nstatic bool unhash_ol_stateid(struct nfs4_ol_stateid *stp)\n{\n\tstruct nfs4_file *fp = stp->st_stid.sc_file;\n\n\tlockdep_assert_held(&stp->st_stateowner->so_client->cl_lock);\n\n\tif (list_empty(&stp->st_perfile))\n\t\treturn false;\n\n\tspin_lock(&fp->fi_lock);\n\tlist_del_init(&stp->st_perfile);\n\tspin_unlock(&fp->fi_lock);\n\tlist_del(&stp->st_perstateowner);\n\treturn true;\n}\n\nstatic void nfs4_free_ol_stateid(struct nfs4_stid *stid)\n{\n\tstruct nfs4_ol_stateid *stp = openlockstateid(stid);\n\n\tput_clnt_odstate(stp->st_clnt_odstate);\n\trelease_all_access(stp);\n\tif (stp->st_stateowner)\n\t\tnfs4_put_stateowner(stp->st_stateowner);\n\tkmem_cache_free(stateid_slab, stid);\n}\n\nstatic void nfs4_free_lock_stateid(struct nfs4_stid *stid)\n{\n\tstruct nfs4_ol_stateid *stp = openlockstateid(stid);\n\tstruct nfs4_lockowner *lo = lockowner(stp->st_stateowner);\n\tstruct file *file;\n\n\tfile = find_any_file(stp->st_stid.sc_file);\n\tif (file)\n\t\tfilp_close(file, (fl_owner_t)lo);\n\tnfs4_free_ol_stateid(stid);\n}\n\n/*\n * Put the persistent reference to an already unhashed generic stateid, while\n * holding the cl_lock. If it's the last reference, then put it onto the\n * reaplist for later destruction.\n */\nstatic void put_ol_stateid_locked(struct nfs4_ol_stateid *stp,\n\t\t\t\t       struct list_head *reaplist)\n{\n\tstruct nfs4_stid *s = &stp->st_stid;\n\tstruct nfs4_client *clp = s->sc_client;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tWARN_ON_ONCE(!list_empty(&stp->st_locks));\n\n\tif (!atomic_dec_and_test(&s->sc_count)) {\n\t\twake_up_all(&close_wq);\n\t\treturn;\n\t}\n\n\tidr_remove(&clp->cl_stateids, s->sc_stateid.si_opaque.so_id);\n\tlist_add(&stp->st_locks, reaplist);\n}\n\nstatic bool unhash_lock_stateid(struct nfs4_ol_stateid *stp)\n{\n\tlockdep_assert_held(&stp->st_stid.sc_client->cl_lock);\n\n\tlist_del_init(&stp->st_locks);\n\tnfs4_unhash_stid(&stp->st_stid);\n\treturn unhash_ol_stateid(stp);\n}\n\nstatic void release_lock_stateid(struct nfs4_ol_stateid *stp)\n{\n\tstruct nfs4_client *clp = stp->st_stid.sc_client;\n\tbool unhashed;\n\n\tspin_lock(&clp->cl_lock);\n\tunhashed = unhash_lock_stateid(stp);\n\tspin_unlock(&clp->cl_lock);\n\tif (unhashed)\n\t\tnfs4_put_stid(&stp->st_stid);\n}\n\nstatic void unhash_lockowner_locked(struct nfs4_lockowner *lo)\n{\n\tstruct nfs4_client *clp = lo->lo_owner.so_client;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_del_init(&lo->lo_owner.so_strhash);\n}\n\n/*\n * Free a list of generic stateids that were collected earlier after being\n * fully unhashed.\n */\nstatic void\nfree_ol_stateid_reaplist(struct list_head *reaplist)\n{\n\tstruct nfs4_ol_stateid *stp;\n\tstruct nfs4_file *fp;\n\n\tmight_sleep();\n\n\twhile (!list_empty(reaplist)) {\n\t\tstp = list_first_entry(reaplist, struct nfs4_ol_stateid,\n\t\t\t\t       st_locks);\n\t\tlist_del(&stp->st_locks);\n\t\tfp = stp->st_stid.sc_file;\n\t\tstp->st_stid.sc_free(&stp->st_stid);\n\t\tif (fp)\n\t\t\tput_nfs4_file(fp);\n\t}\n}\n\nstatic void release_open_stateid_locks(struct nfs4_ol_stateid *open_stp,\n\t\t\t\t       struct list_head *reaplist)\n{\n\tstruct nfs4_ol_stateid *stp;\n\n\tlockdep_assert_held(&open_stp->st_stid.sc_client->cl_lock);\n\n\twhile (!list_empty(&open_stp->st_locks)) {\n\t\tstp = list_entry(open_stp->st_locks.next,\n\t\t\t\tstruct nfs4_ol_stateid, st_locks);\n\t\tWARN_ON(!unhash_lock_stateid(stp));\n\t\tput_ol_stateid_locked(stp, reaplist);\n\t}\n}\n\nstatic bool unhash_open_stateid(struct nfs4_ol_stateid *stp,\n\t\t\t\tstruct list_head *reaplist)\n{\n\tbool unhashed;\n\n\tlockdep_assert_held(&stp->st_stid.sc_client->cl_lock);\n\n\tunhashed = unhash_ol_stateid(stp);\n\trelease_open_stateid_locks(stp, reaplist);\n\treturn unhashed;\n}\n\nstatic void release_open_stateid(struct nfs4_ol_stateid *stp)\n{\n\tLIST_HEAD(reaplist);\n\n\tspin_lock(&stp->st_stid.sc_client->cl_lock);\n\tif (unhash_open_stateid(stp, &reaplist))\n\t\tput_ol_stateid_locked(stp, &reaplist);\n\tspin_unlock(&stp->st_stid.sc_client->cl_lock);\n\tfree_ol_stateid_reaplist(&reaplist);\n}\n\nstatic void unhash_openowner_locked(struct nfs4_openowner *oo)\n{\n\tstruct nfs4_client *clp = oo->oo_owner.so_client;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_del_init(&oo->oo_owner.so_strhash);\n\tlist_del_init(&oo->oo_perclient);\n}\n\nstatic void release_last_closed_stateid(struct nfs4_openowner *oo)\n{\n\tstruct nfsd_net *nn = net_generic(oo->oo_owner.so_client->net,\n\t\t\t\t\t  nfsd_net_id);\n\tstruct nfs4_ol_stateid *s;\n\n\tspin_lock(&nn->client_lock);\n\ts = oo->oo_last_closed_stid;\n\tif (s) {\n\t\tlist_del_init(&oo->oo_close_lru);\n\t\too->oo_last_closed_stid = NULL;\n\t}\n\tspin_unlock(&nn->client_lock);\n\tif (s)\n\t\tnfs4_put_stid(&s->st_stid);\n}\n\nstatic void release_openowner(struct nfs4_openowner *oo)\n{\n\tstruct nfs4_ol_stateid *stp;\n\tstruct nfs4_client *clp = oo->oo_owner.so_client;\n\tstruct list_head reaplist;\n\n\tINIT_LIST_HEAD(&reaplist);\n\n\tspin_lock(&clp->cl_lock);\n\tunhash_openowner_locked(oo);\n\twhile (!list_empty(&oo->oo_owner.so_stateids)) {\n\t\tstp = list_first_entry(&oo->oo_owner.so_stateids,\n\t\t\t\tstruct nfs4_ol_stateid, st_perstateowner);\n\t\tif (unhash_open_stateid(stp, &reaplist))\n\t\t\tput_ol_stateid_locked(stp, &reaplist);\n\t}\n\tspin_unlock(&clp->cl_lock);\n\tfree_ol_stateid_reaplist(&reaplist);\n\trelease_last_closed_stateid(oo);\n\tnfs4_put_stateowner(&oo->oo_owner);\n}\n\nstatic inline int\nhash_sessionid(struct nfs4_sessionid *sessionid)\n{\n\tstruct nfsd4_sessionid *sid = (struct nfsd4_sessionid *)sessionid;\n\n\treturn sid->sequence % SESSION_HASH_SIZE;\n}\n\n#ifdef CONFIG_SUNRPC_DEBUG\nstatic inline void\ndump_sessionid(const char *fn, struct nfs4_sessionid *sessionid)\n{\n\tu32 *ptr = (u32 *)(&sessionid->data[0]);\n\tdprintk(\"%s: %u:%u:%u:%u\\n\", fn, ptr[0], ptr[1], ptr[2], ptr[3]);\n}\n#else\nstatic inline void\ndump_sessionid(const char *fn, struct nfs4_sessionid *sessionid)\n{\n}\n#endif\n\n/*\n * Bump the seqid on cstate->replay_owner, and clear replay_owner if it\n * won't be used for replay.\n */\nvoid nfsd4_bump_seqid(struct nfsd4_compound_state *cstate, __be32 nfserr)\n{\n\tstruct nfs4_stateowner *so = cstate->replay_owner;\n\n\tif (nfserr == nfserr_replay_me)\n\t\treturn;\n\n\tif (!seqid_mutating_err(ntohl(nfserr))) {\n\t\tnfsd4_cstate_clear_replay(cstate);\n\t\treturn;\n\t}\n\tif (!so)\n\t\treturn;\n\tif (so->so_is_open_owner)\n\t\trelease_last_closed_stateid(openowner(so));\n\tso->so_seqid++;\n\treturn;\n}\n\nstatic void\ngen_sessionid(struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\tstruct nfsd4_sessionid *sid;\n\n\tsid = (struct nfsd4_sessionid *)ses->se_sessionid.data;\n\tsid->clientid = clp->cl_clientid;\n\tsid->sequence = current_sessionid++;\n\tsid->reserved = 0;\n}\n\n/*\n * The protocol defines ca_maxresponssize_cached to include the size of\n * the rpc header, but all we need to cache is the data starting after\n * the end of the initial SEQUENCE operation--the rest we regenerate\n * each time.  Therefore we can advertise a ca_maxresponssize_cached\n * value that is the number of bytes in our cache plus a few additional\n * bytes.  In order to stay on the safe side, and not promise more than\n * we can cache, those additional bytes must be the minimum possible: 24\n * bytes of rpc header (xid through accept state, with AUTH_NULL\n * verifier), 12 for the compound header (with zero-length tag), and 44\n * for the SEQUENCE op response:\n */\n#define NFSD_MIN_HDR_SEQ_SZ  (24 + 12 + 44)\n\nstatic void\nfree_session_slots(struct nfsd4_session *ses)\n{\n\tint i;\n\n\tfor (i = 0; i < ses->se_fchannel.maxreqs; i++)\n\t\tkfree(ses->se_slots[i]);\n}\n\n/*\n * We don't actually need to cache the rpc and session headers, so we\n * can allocate a little less for each slot:\n */\nstatic inline u32 slot_bytes(struct nfsd4_channel_attrs *ca)\n{\n\tu32 size;\n\n\tif (ca->maxresp_cached < NFSD_MIN_HDR_SEQ_SZ)\n\t\tsize = 0;\n\telse\n\t\tsize = ca->maxresp_cached - NFSD_MIN_HDR_SEQ_SZ;\n\treturn size + sizeof(struct nfsd4_slot);\n}\n\n/*\n * XXX: If we run out of reserved DRC memory we could (up to a point)\n * re-negotiate active sessions and reduce their slot usage to make\n * room for new connections. For now we just fail the create session.\n */\nstatic u32 nfsd4_get_drc_mem(struct nfsd4_channel_attrs *ca)\n{\n\tu32 slotsize = slot_bytes(ca);\n\tu32 num = ca->maxreqs;\n\tint avail;\n\n\tspin_lock(&nfsd_drc_lock);\n\tavail = min((unsigned long)NFSD_MAX_MEM_PER_SESSION,\n\t\t    nfsd_drc_max_mem - nfsd_drc_mem_used);\n\tnum = min_t(int, num, avail / slotsize);\n\tnfsd_drc_mem_used += num * slotsize;\n\tspin_unlock(&nfsd_drc_lock);\n\n\treturn num;\n}\n\nstatic void nfsd4_put_drc_mem(struct nfsd4_channel_attrs *ca)\n{\n\tint slotsize = slot_bytes(ca);\n\n\tspin_lock(&nfsd_drc_lock);\n\tnfsd_drc_mem_used -= slotsize * ca->maxreqs;\n\tspin_unlock(&nfsd_drc_lock);\n}\n\nstatic struct nfsd4_session *alloc_session(struct nfsd4_channel_attrs *fattrs,\n\t\t\t\t\t   struct nfsd4_channel_attrs *battrs)\n{\n\tint numslots = fattrs->maxreqs;\n\tint slotsize = slot_bytes(fattrs);\n\tstruct nfsd4_session *new;\n\tint mem, i;\n\n\tBUILD_BUG_ON(NFSD_MAX_SLOTS_PER_SESSION * sizeof(struct nfsd4_slot *)\n\t\t\t+ sizeof(struct nfsd4_session) > PAGE_SIZE);\n\tmem = numslots * sizeof(struct nfsd4_slot *);\n\n\tnew = kzalloc(sizeof(*new) + mem, GFP_KERNEL);\n\tif (!new)\n\t\treturn NULL;\n\t/* allocate each struct nfsd4_slot and data cache in one piece */\n\tfor (i = 0; i < numslots; i++) {\n\t\tnew->se_slots[i] = kzalloc(slotsize, GFP_KERNEL);\n\t\tif (!new->se_slots[i])\n\t\t\tgoto out_free;\n\t}\n\n\tmemcpy(&new->se_fchannel, fattrs, sizeof(struct nfsd4_channel_attrs));\n\tmemcpy(&new->se_bchannel, battrs, sizeof(struct nfsd4_channel_attrs));\n\n\treturn new;\nout_free:\n\twhile (i--)\n\t\tkfree(new->se_slots[i]);\n\tkfree(new);\n\treturn NULL;\n}\n\nstatic void free_conn(struct nfsd4_conn *c)\n{\n\tsvc_xprt_put(c->cn_xprt);\n\tkfree(c);\n}\n\nstatic void nfsd4_conn_lost(struct svc_xpt_user *u)\n{\n\tstruct nfsd4_conn *c = container_of(u, struct nfsd4_conn, cn_xpt_user);\n\tstruct nfs4_client *clp = c->cn_session->se_client;\n\n\tspin_lock(&clp->cl_lock);\n\tif (!list_empty(&c->cn_persession)) {\n\t\tlist_del(&c->cn_persession);\n\t\tfree_conn(c);\n\t}\n\tnfsd4_probe_callback(clp);\n\tspin_unlock(&clp->cl_lock);\n}\n\nstatic struct nfsd4_conn *alloc_conn(struct svc_rqst *rqstp, u32 flags)\n{\n\tstruct nfsd4_conn *conn;\n\n\tconn = kmalloc(sizeof(struct nfsd4_conn), GFP_KERNEL);\n\tif (!conn)\n\t\treturn NULL;\n\tsvc_xprt_get(rqstp->rq_xprt);\n\tconn->cn_xprt = rqstp->rq_xprt;\n\tconn->cn_flags = flags;\n\tINIT_LIST_HEAD(&conn->cn_xpt_user.list);\n\treturn conn;\n}\n\nstatic void __nfsd4_hash_conn(struct nfsd4_conn *conn, struct nfsd4_session *ses)\n{\n\tconn->cn_session = ses;\n\tlist_add(&conn->cn_persession, &ses->se_conns);\n}\n\nstatic void nfsd4_hash_conn(struct nfsd4_conn *conn, struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\n\tspin_lock(&clp->cl_lock);\n\t__nfsd4_hash_conn(conn, ses);\n\tspin_unlock(&clp->cl_lock);\n}\n\nstatic int nfsd4_register_conn(struct nfsd4_conn *conn)\n{\n\tconn->cn_xpt_user.callback = nfsd4_conn_lost;\n\treturn register_xpt_user(conn->cn_xprt, &conn->cn_xpt_user);\n}\n\nstatic void nfsd4_init_conn(struct svc_rqst *rqstp, struct nfsd4_conn *conn, struct nfsd4_session *ses)\n{\n\tint ret;\n\n\tnfsd4_hash_conn(conn, ses);\n\tret = nfsd4_register_conn(conn);\n\tif (ret)\n\t\t/* oops; xprt is already down: */\n\t\tnfsd4_conn_lost(&conn->cn_xpt_user);\n\t/* We may have gained or lost a callback channel: */\n\tnfsd4_probe_callback_sync(ses->se_client);\n}\n\nstatic struct nfsd4_conn *alloc_conn_from_crses(struct svc_rqst *rqstp, struct nfsd4_create_session *cses)\n{\n\tu32 dir = NFS4_CDFC4_FORE;\n\n\tif (cses->flags & SESSION4_BACK_CHAN)\n\t\tdir |= NFS4_CDFC4_BACK;\n\treturn alloc_conn(rqstp, dir);\n}\n\n/* must be called under client_lock */\nstatic void nfsd4_del_conns(struct nfsd4_session *s)\n{\n\tstruct nfs4_client *clp = s->se_client;\n\tstruct nfsd4_conn *c;\n\n\tspin_lock(&clp->cl_lock);\n\twhile (!list_empty(&s->se_conns)) {\n\t\tc = list_first_entry(&s->se_conns, struct nfsd4_conn, cn_persession);\n\t\tlist_del_init(&c->cn_persession);\n\t\tspin_unlock(&clp->cl_lock);\n\n\t\tunregister_xpt_user(c->cn_xprt, &c->cn_xpt_user);\n\t\tfree_conn(c);\n\n\t\tspin_lock(&clp->cl_lock);\n\t}\n\tspin_unlock(&clp->cl_lock);\n}\n\nstatic void __free_session(struct nfsd4_session *ses)\n{\n\tfree_session_slots(ses);\n\tkfree(ses);\n}\n\nstatic void free_session(struct nfsd4_session *ses)\n{\n\tnfsd4_del_conns(ses);\n\tnfsd4_put_drc_mem(&ses->se_fchannel);\n\t__free_session(ses);\n}\n\nstatic void init_session(struct svc_rqst *rqstp, struct nfsd4_session *new, struct nfs4_client *clp, struct nfsd4_create_session *cses)\n{\n\tint idx;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tnew->se_client = clp;\n\tgen_sessionid(new);\n\n\tINIT_LIST_HEAD(&new->se_conns);\n\n\tnew->se_cb_seq_nr = 1;\n\tnew->se_flags = cses->flags;\n\tnew->se_cb_prog = cses->callback_prog;\n\tnew->se_cb_sec = cses->cb_sec;\n\tatomic_set(&new->se_ref, 0);\n\tidx = hash_sessionid(&new->se_sessionid);\n\tlist_add(&new->se_hash, &nn->sessionid_hashtbl[idx]);\n\tspin_lock(&clp->cl_lock);\n\tlist_add(&new->se_perclnt, &clp->cl_sessions);\n\tspin_unlock(&clp->cl_lock);\n\n\t{\n\t\tstruct sockaddr *sa = svc_addr(rqstp);\n\t\t/*\n\t\t * This is a little silly; with sessions there's no real\n\t\t * use for the callback address.  Use the peer address\n\t\t * as a reasonable default for now, but consider fixing\n\t\t * the rpc client not to require an address in the\n\t\t * future:\n\t\t */\n\t\trpc_copy_addr((struct sockaddr *)&clp->cl_cb_conn.cb_addr, sa);\n\t\tclp->cl_cb_conn.cb_addrlen = svc_addr_len(sa);\n\t}\n}\n\n/* caller must hold client_lock */\nstatic struct nfsd4_session *\n__find_in_sessionid_hashtbl(struct nfs4_sessionid *sessionid, struct net *net)\n{\n\tstruct nfsd4_session *elem;\n\tint idx;\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tdump_sessionid(__func__, sessionid);\n\tidx = hash_sessionid(sessionid);\n\t/* Search in the appropriate list */\n\tlist_for_each_entry(elem, &nn->sessionid_hashtbl[idx], se_hash) {\n\t\tif (!memcmp(elem->se_sessionid.data, sessionid->data,\n\t\t\t    NFS4_MAX_SESSIONID_LEN)) {\n\t\t\treturn elem;\n\t\t}\n\t}\n\n\tdprintk(\"%s: session not found\\n\", __func__);\n\treturn NULL;\n}\n\nstatic struct nfsd4_session *\nfind_in_sessionid_hashtbl(struct nfs4_sessionid *sessionid, struct net *net,\n\t\t__be32 *ret)\n{\n\tstruct nfsd4_session *session;\n\t__be32 status = nfserr_badsession;\n\n\tsession = __find_in_sessionid_hashtbl(sessionid, net);\n\tif (!session)\n\t\tgoto out;\n\tstatus = nfsd4_get_session_locked(session);\n\tif (status)\n\t\tsession = NULL;\nout:\n\t*ret = status;\n\treturn session;\n}\n\n/* caller must hold client_lock */\nstatic void\nunhash_session(struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tlist_del(&ses->se_hash);\n\tspin_lock(&ses->se_client->cl_lock);\n\tlist_del(&ses->se_perclnt);\n\tspin_unlock(&ses->se_client->cl_lock);\n}\n\n/* SETCLIENTID and SETCLIENTID_CONFIRM Helper functions */\nstatic int\nSTALE_CLIENTID(clientid_t *clid, struct nfsd_net *nn)\n{\n\t/*\n\t * We're assuming the clid was not given out from a boot\n\t * precisely 2^32 (about 136 years) before this one.  That seems\n\t * a safe assumption:\n\t */\n\tif (clid->cl_boot == (u32)nn->boot_time)\n\t\treturn 0;\n\tdprintk(\"NFSD stale clientid (%08x/%08x) boot_time %08lx\\n\",\n\t\tclid->cl_boot, clid->cl_id, nn->boot_time);\n\treturn 1;\n}\n\n/* \n * XXX Should we use a slab cache ?\n * This type of memory management is somewhat inefficient, but we use it\n * anyway since SETCLIENTID is not a common operation.\n */\nstatic struct nfs4_client *alloc_client(struct xdr_netobj name)\n{\n\tstruct nfs4_client *clp;\n\tint i;\n\n\tclp = kzalloc(sizeof(struct nfs4_client), GFP_KERNEL);\n\tif (clp == NULL)\n\t\treturn NULL;\n\tclp->cl_name.data = kmemdup(name.data, name.len, GFP_KERNEL);\n\tif (clp->cl_name.data == NULL)\n\t\tgoto err_no_name;\n\tclp->cl_ownerstr_hashtbl = kmalloc(sizeof(struct list_head) *\n\t\t\tOWNER_HASH_SIZE, GFP_KERNEL);\n\tif (!clp->cl_ownerstr_hashtbl)\n\t\tgoto err_no_hashtbl;\n\tfor (i = 0; i < OWNER_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&clp->cl_ownerstr_hashtbl[i]);\n\tclp->cl_name.len = name.len;\n\tINIT_LIST_HEAD(&clp->cl_sessions);\n\tidr_init(&clp->cl_stateids);\n\tatomic_set(&clp->cl_refcount, 0);\n\tclp->cl_cb_state = NFSD4_CB_UNKNOWN;\n\tINIT_LIST_HEAD(&clp->cl_idhash);\n\tINIT_LIST_HEAD(&clp->cl_openowners);\n\tINIT_LIST_HEAD(&clp->cl_delegations);\n\tINIT_LIST_HEAD(&clp->cl_lru);\n\tINIT_LIST_HEAD(&clp->cl_revoked);\n#ifdef CONFIG_NFSD_PNFS\n\tINIT_LIST_HEAD(&clp->cl_lo_states);\n#endif\n\tspin_lock_init(&clp->cl_lock);\n\trpc_init_wait_queue(&clp->cl_cb_waitq, \"Backchannel slot table\");\n\treturn clp;\nerr_no_hashtbl:\n\tkfree(clp->cl_name.data);\nerr_no_name:\n\tkfree(clp);\n\treturn NULL;\n}\n\nstatic void\nfree_client(struct nfs4_client *clp)\n{\n\twhile (!list_empty(&clp->cl_sessions)) {\n\t\tstruct nfsd4_session *ses;\n\t\tses = list_entry(clp->cl_sessions.next, struct nfsd4_session,\n\t\t\t\tse_perclnt);\n\t\tlist_del(&ses->se_perclnt);\n\t\tWARN_ON_ONCE(atomic_read(&ses->se_ref));\n\t\tfree_session(ses);\n\t}\n\trpc_destroy_wait_queue(&clp->cl_cb_waitq);\n\tfree_svc_cred(&clp->cl_cred);\n\tkfree(clp->cl_ownerstr_hashtbl);\n\tkfree(clp->cl_name.data);\n\tidr_destroy(&clp->cl_stateids);\n\tkfree(clp);\n}\n\n/* must be called under the client_lock */\nstatic void\nunhash_client_locked(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\tstruct nfsd4_session *ses;\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\t/* Mark the client as expired! */\n\tclp->cl_time = 0;\n\t/* Make it invisible */\n\tif (!list_empty(&clp->cl_idhash)) {\n\t\tlist_del_init(&clp->cl_idhash);\n\t\tif (test_bit(NFSD4_CLIENT_CONFIRMED, &clp->cl_flags))\n\t\t\trb_erase(&clp->cl_namenode, &nn->conf_name_tree);\n\t\telse\n\t\t\trb_erase(&clp->cl_namenode, &nn->unconf_name_tree);\n\t}\n\tlist_del_init(&clp->cl_lru);\n\tspin_lock(&clp->cl_lock);\n\tlist_for_each_entry(ses, &clp->cl_sessions, se_perclnt)\n\t\tlist_del_init(&ses->se_hash);\n\tspin_unlock(&clp->cl_lock);\n}\n\nstatic void\nunhash_client(struct nfs4_client *clp)\n{\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tspin_lock(&nn->client_lock);\n\tunhash_client_locked(clp);\n\tspin_unlock(&nn->client_lock);\n}\n\nstatic __be32 mark_client_expired_locked(struct nfs4_client *clp)\n{\n\tif (atomic_read(&clp->cl_refcount))\n\t\treturn nfserr_jukebox;\n\tunhash_client_locked(clp);\n\treturn nfs_ok;\n}\n\nstatic void\n__destroy_client(struct nfs4_client *clp)\n{\n\tstruct nfs4_openowner *oo;\n\tstruct nfs4_delegation *dp;\n\tstruct list_head reaplist;\n\n\tINIT_LIST_HEAD(&reaplist);\n\tspin_lock(&state_lock);\n\twhile (!list_empty(&clp->cl_delegations)) {\n\t\tdp = list_entry(clp->cl_delegations.next, struct nfs4_delegation, dl_perclnt);\n\t\tWARN_ON(!unhash_delegation_locked(dp));\n\t\tlist_add(&dp->dl_recall_lru, &reaplist);\n\t}\n\tspin_unlock(&state_lock);\n\twhile (!list_empty(&reaplist)) {\n\t\tdp = list_entry(reaplist.next, struct nfs4_delegation, dl_recall_lru);\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tput_clnt_odstate(dp->dl_clnt_odstate);\n\t\tnfs4_put_deleg_lease(dp->dl_stid.sc_file);\n\t\tnfs4_put_stid(&dp->dl_stid);\n\t}\n\twhile (!list_empty(&clp->cl_revoked)) {\n\t\tdp = list_entry(clp->cl_revoked.next, struct nfs4_delegation, dl_recall_lru);\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tnfs4_put_stid(&dp->dl_stid);\n\t}\n\twhile (!list_empty(&clp->cl_openowners)) {\n\t\too = list_entry(clp->cl_openowners.next, struct nfs4_openowner, oo_perclient);\n\t\tnfs4_get_stateowner(&oo->oo_owner);\n\t\trelease_openowner(oo);\n\t}\n\tnfsd4_return_all_client_layouts(clp);\n\tnfsd4_shutdown_callback(clp);\n\tif (clp->cl_cb_conn.cb_xprt)\n\t\tsvc_xprt_put(clp->cl_cb_conn.cb_xprt);\n\tfree_client(clp);\n}\n\nstatic void\ndestroy_client(struct nfs4_client *clp)\n{\n\tunhash_client(clp);\n\t__destroy_client(clp);\n}\n\nstatic void expire_client(struct nfs4_client *clp)\n{\n\tunhash_client(clp);\n\tnfsd4_client_record_remove(clp);\n\t__destroy_client(clp);\n}\n\nstatic void copy_verf(struct nfs4_client *target, nfs4_verifier *source)\n{\n\tmemcpy(target->cl_verifier.data, source->data,\n\t\t\tsizeof(target->cl_verifier.data));\n}\n\nstatic void copy_clid(struct nfs4_client *target, struct nfs4_client *source)\n{\n\ttarget->cl_clientid.cl_boot = source->cl_clientid.cl_boot; \n\ttarget->cl_clientid.cl_id = source->cl_clientid.cl_id; \n}\n\nstatic int copy_cred(struct svc_cred *target, struct svc_cred *source)\n{\n\ttarget->cr_principal = kstrdup(source->cr_principal, GFP_KERNEL);\n\ttarget->cr_raw_principal = kstrdup(source->cr_raw_principal,\n\t\t\t\t\t\t\t\tGFP_KERNEL);\n\tif ((source->cr_principal && ! target->cr_principal) ||\n\t    (source->cr_raw_principal && ! target->cr_raw_principal))\n\t\treturn -ENOMEM;\n\n\ttarget->cr_flavor = source->cr_flavor;\n\ttarget->cr_uid = source->cr_uid;\n\ttarget->cr_gid = source->cr_gid;\n\ttarget->cr_group_info = source->cr_group_info;\n\tget_group_info(target->cr_group_info);\n\ttarget->cr_gss_mech = source->cr_gss_mech;\n\tif (source->cr_gss_mech)\n\t\tgss_mech_get(source->cr_gss_mech);\n\treturn 0;\n}\n\nstatic int\ncompare_blob(const struct xdr_netobj *o1, const struct xdr_netobj *o2)\n{\n\tif (o1->len < o2->len)\n\t\treturn -1;\n\tif (o1->len > o2->len)\n\t\treturn 1;\n\treturn memcmp(o1->data, o2->data, o1->len);\n}\n\nstatic int same_name(const char *n1, const char *n2)\n{\n\treturn 0 == memcmp(n1, n2, HEXDIR_LEN);\n}\n\nstatic int\nsame_verf(nfs4_verifier *v1, nfs4_verifier *v2)\n{\n\treturn 0 == memcmp(v1->data, v2->data, sizeof(v1->data));\n}\n\nstatic int\nsame_clid(clientid_t *cl1, clientid_t *cl2)\n{\n\treturn (cl1->cl_boot == cl2->cl_boot) && (cl1->cl_id == cl2->cl_id);\n}\n\nstatic bool groups_equal(struct group_info *g1, struct group_info *g2)\n{\n\tint i;\n\n\tif (g1->ngroups != g2->ngroups)\n\t\treturn false;\n\tfor (i=0; i<g1->ngroups; i++)\n\t\tif (!gid_eq(g1->gid[i], g2->gid[i]))\n\t\t\treturn false;\n\treturn true;\n}\n\n/*\n * RFC 3530 language requires clid_inuse be returned when the\n * \"principal\" associated with a requests differs from that previously\n * used.  We use uid, gid's, and gss principal string as our best\n * approximation.  We also don't want to allow non-gss use of a client\n * established using gss: in theory cr_principal should catch that\n * change, but in practice cr_principal can be null even in the gss case\n * since gssd doesn't always pass down a principal string.\n */\nstatic bool is_gss_cred(struct svc_cred *cr)\n{\n\t/* Is cr_flavor one of the gss \"pseudoflavors\"?: */\n\treturn (cr->cr_flavor > RPC_AUTH_MAXFLAVOR);\n}\n\n\nstatic bool\nsame_creds(struct svc_cred *cr1, struct svc_cred *cr2)\n{\n\tif ((is_gss_cred(cr1) != is_gss_cred(cr2))\n\t\t|| (!uid_eq(cr1->cr_uid, cr2->cr_uid))\n\t\t|| (!gid_eq(cr1->cr_gid, cr2->cr_gid))\n\t\t|| !groups_equal(cr1->cr_group_info, cr2->cr_group_info))\n\t\treturn false;\n\tif (cr1->cr_principal == cr2->cr_principal)\n\t\treturn true;\n\tif (!cr1->cr_principal || !cr2->cr_principal)\n\t\treturn false;\n\treturn 0 == strcmp(cr1->cr_principal, cr2->cr_principal);\n}\n\nstatic bool svc_rqst_integrity_protected(struct svc_rqst *rqstp)\n{\n\tstruct svc_cred *cr = &rqstp->rq_cred;\n\tu32 service;\n\n\tif (!cr->cr_gss_mech)\n\t\treturn false;\n\tservice = gss_pseudoflavor_to_service(cr->cr_gss_mech, cr->cr_flavor);\n\treturn service == RPC_GSS_SVC_INTEGRITY ||\n\t       service == RPC_GSS_SVC_PRIVACY;\n}\n\nbool nfsd4_mach_creds_match(struct nfs4_client *cl, struct svc_rqst *rqstp)\n{\n\tstruct svc_cred *cr = &rqstp->rq_cred;\n\n\tif (!cl->cl_mach_cred)\n\t\treturn true;\n\tif (cl->cl_cred.cr_gss_mech != cr->cr_gss_mech)\n\t\treturn false;\n\tif (!svc_rqst_integrity_protected(rqstp))\n\t\treturn false;\n\tif (cl->cl_cred.cr_raw_principal)\n\t\treturn 0 == strcmp(cl->cl_cred.cr_raw_principal,\n\t\t\t\t\t\tcr->cr_raw_principal);\n\tif (!cr->cr_principal)\n\t\treturn false;\n\treturn 0 == strcmp(cl->cl_cred.cr_principal, cr->cr_principal);\n}\n\nstatic void gen_confirm(struct nfs4_client *clp, struct nfsd_net *nn)\n{\n\t__be32 verf[2];\n\n\t/*\n\t * This is opaque to client, so no need to byte-swap. Use\n\t * __force to keep sparse happy\n\t */\n\tverf[0] = (__force __be32)get_seconds();\n\tverf[1] = (__force __be32)nn->clverifier_counter++;\n\tmemcpy(clp->cl_confirm.data, verf, sizeof(clp->cl_confirm.data));\n}\n\nstatic void gen_clid(struct nfs4_client *clp, struct nfsd_net *nn)\n{\n\tclp->cl_clientid.cl_boot = nn->boot_time;\n\tclp->cl_clientid.cl_id = nn->clientid_counter++;\n\tgen_confirm(clp, nn);\n}\n\nstatic struct nfs4_stid *\nfind_stateid_locked(struct nfs4_client *cl, stateid_t *t)\n{\n\tstruct nfs4_stid *ret;\n\n\tret = idr_find(&cl->cl_stateids, t->si_opaque.so_id);\n\tif (!ret || !ret->sc_type)\n\t\treturn NULL;\n\treturn ret;\n}\n\nstatic struct nfs4_stid *\nfind_stateid_by_type(struct nfs4_client *cl, stateid_t *t, char typemask)\n{\n\tstruct nfs4_stid *s;\n\n\tspin_lock(&cl->cl_lock);\n\ts = find_stateid_locked(cl, t);\n\tif (s != NULL) {\n\t\tif (typemask & s->sc_type)\n\t\t\tatomic_inc(&s->sc_count);\n\t\telse\n\t\t\ts = NULL;\n\t}\n\tspin_unlock(&cl->cl_lock);\n\treturn s;\n}\n\nstatic struct nfs4_client *create_client(struct xdr_netobj name,\n\t\tstruct svc_rqst *rqstp, nfs4_verifier *verf)\n{\n\tstruct nfs4_client *clp;\n\tstruct sockaddr *sa = svc_addr(rqstp);\n\tint ret;\n\tstruct net *net = SVC_NET(rqstp);\n\n\tclp = alloc_client(name);\n\tif (clp == NULL)\n\t\treturn NULL;\n\n\tret = copy_cred(&clp->cl_cred, &rqstp->rq_cred);\n\tif (ret) {\n\t\tfree_client(clp);\n\t\treturn NULL;\n\t}\n\tnfsd4_init_cb(&clp->cl_cb_null, clp, NULL, NFSPROC4_CLNT_CB_NULL);\n\tclp->cl_time = get_seconds();\n\tclear_bit(0, &clp->cl_cb_slot_busy);\n\tcopy_verf(clp, verf);\n\trpc_copy_addr((struct sockaddr *) &clp->cl_addr, sa);\n\tclp->cl_cb_session = NULL;\n\tclp->net = net;\n\treturn clp;\n}\n\nstatic void\nadd_clp_to_name_tree(struct nfs4_client *new_clp, struct rb_root *root)\n{\n\tstruct rb_node **new = &(root->rb_node), *parent = NULL;\n\tstruct nfs4_client *clp;\n\n\twhile (*new) {\n\t\tclp = rb_entry(*new, struct nfs4_client, cl_namenode);\n\t\tparent = *new;\n\n\t\tif (compare_blob(&clp->cl_name, &new_clp->cl_name) > 0)\n\t\t\tnew = &((*new)->rb_left);\n\t\telse\n\t\t\tnew = &((*new)->rb_right);\n\t}\n\n\trb_link_node(&new_clp->cl_namenode, parent, new);\n\trb_insert_color(&new_clp->cl_namenode, root);\n}\n\nstatic struct nfs4_client *\nfind_clp_in_name_tree(struct xdr_netobj *name, struct rb_root *root)\n{\n\tint cmp;\n\tstruct rb_node *node = root->rb_node;\n\tstruct nfs4_client *clp;\n\n\twhile (node) {\n\t\tclp = rb_entry(node, struct nfs4_client, cl_namenode);\n\t\tcmp = compare_blob(&clp->cl_name, name);\n\t\tif (cmp > 0)\n\t\t\tnode = node->rb_left;\n\t\telse if (cmp < 0)\n\t\t\tnode = node->rb_right;\n\t\telse\n\t\t\treturn clp;\n\t}\n\treturn NULL;\n}\n\nstatic void\nadd_to_unconfirmed(struct nfs4_client *clp)\n{\n\tunsigned int idhashval;\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tclear_bit(NFSD4_CLIENT_CONFIRMED, &clp->cl_flags);\n\tadd_clp_to_name_tree(clp, &nn->unconf_name_tree);\n\tidhashval = clientid_hashval(clp->cl_clientid.cl_id);\n\tlist_add(&clp->cl_idhash, &nn->unconf_id_hashtbl[idhashval]);\n\trenew_client_locked(clp);\n}\n\nstatic void\nmove_to_confirmed(struct nfs4_client *clp)\n{\n\tunsigned int idhashval = clientid_hashval(clp->cl_clientid.cl_id);\n\tstruct nfsd_net *nn = net_generic(clp->net, nfsd_net_id);\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tdprintk(\"NFSD: move_to_confirm nfs4_client %p\\n\", clp);\n\tlist_move(&clp->cl_idhash, &nn->conf_id_hashtbl[idhashval]);\n\trb_erase(&clp->cl_namenode, &nn->unconf_name_tree);\n\tadd_clp_to_name_tree(clp, &nn->conf_name_tree);\n\tset_bit(NFSD4_CLIENT_CONFIRMED, &clp->cl_flags);\n\trenew_client_locked(clp);\n}\n\nstatic struct nfs4_client *\nfind_client_in_id_table(struct list_head *tbl, clientid_t *clid, bool sessions)\n{\n\tstruct nfs4_client *clp;\n\tunsigned int idhashval = clientid_hashval(clid->cl_id);\n\n\tlist_for_each_entry(clp, &tbl[idhashval], cl_idhash) {\n\t\tif (same_clid(&clp->cl_clientid, clid)) {\n\t\t\tif ((bool)clp->cl_minorversion != sessions)\n\t\t\t\treturn NULL;\n\t\t\trenew_client_locked(clp);\n\t\t\treturn clp;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct nfs4_client *\nfind_confirmed_client(clientid_t *clid, bool sessions, struct nfsd_net *nn)\n{\n\tstruct list_head *tbl = nn->conf_id_hashtbl;\n\n\tlockdep_assert_held(&nn->client_lock);\n\treturn find_client_in_id_table(tbl, clid, sessions);\n}\n\nstatic struct nfs4_client *\nfind_unconfirmed_client(clientid_t *clid, bool sessions, struct nfsd_net *nn)\n{\n\tstruct list_head *tbl = nn->unconf_id_hashtbl;\n\n\tlockdep_assert_held(&nn->client_lock);\n\treturn find_client_in_id_table(tbl, clid, sessions);\n}\n\nstatic bool clp_used_exchangeid(struct nfs4_client *clp)\n{\n\treturn clp->cl_exchange_flags != 0;\n} \n\nstatic struct nfs4_client *\nfind_confirmed_client_by_name(struct xdr_netobj *name, struct nfsd_net *nn)\n{\n\tlockdep_assert_held(&nn->client_lock);\n\treturn find_clp_in_name_tree(name, &nn->conf_name_tree);\n}\n\nstatic struct nfs4_client *\nfind_unconfirmed_client_by_name(struct xdr_netobj *name, struct nfsd_net *nn)\n{\n\tlockdep_assert_held(&nn->client_lock);\n\treturn find_clp_in_name_tree(name, &nn->unconf_name_tree);\n}\n\nstatic void\ngen_callback(struct nfs4_client *clp, struct nfsd4_setclientid *se, struct svc_rqst *rqstp)\n{\n\tstruct nfs4_cb_conn *conn = &clp->cl_cb_conn;\n\tstruct sockaddr\t*sa = svc_addr(rqstp);\n\tu32 scopeid = rpc_get_scope_id(sa);\n\tunsigned short expected_family;\n\n\t/* Currently, we only support tcp and tcp6 for the callback channel */\n\tif (se->se_callback_netid_len == 3 &&\n\t    !memcmp(se->se_callback_netid_val, \"tcp\", 3))\n\t\texpected_family = AF_INET;\n\telse if (se->se_callback_netid_len == 4 &&\n\t\t !memcmp(se->se_callback_netid_val, \"tcp6\", 4))\n\t\texpected_family = AF_INET6;\n\telse\n\t\tgoto out_err;\n\n\tconn->cb_addrlen = rpc_uaddr2sockaddr(clp->net, se->se_callback_addr_val,\n\t\t\t\t\t    se->se_callback_addr_len,\n\t\t\t\t\t    (struct sockaddr *)&conn->cb_addr,\n\t\t\t\t\t    sizeof(conn->cb_addr));\n\n\tif (!conn->cb_addrlen || conn->cb_addr.ss_family != expected_family)\n\t\tgoto out_err;\n\n\tif (conn->cb_addr.ss_family == AF_INET6)\n\t\t((struct sockaddr_in6 *)&conn->cb_addr)->sin6_scope_id = scopeid;\n\n\tconn->cb_prog = se->se_callback_prog;\n\tconn->cb_ident = se->se_callback_ident;\n\tmemcpy(&conn->cb_saddr, &rqstp->rq_daddr, rqstp->rq_daddrlen);\n\treturn;\nout_err:\n\tconn->cb_addr.ss_family = AF_UNSPEC;\n\tconn->cb_addrlen = 0;\n\tdprintk(\"NFSD: this client (clientid %08x/%08x) \"\n\t\t\"will not receive delegations\\n\",\n\t\tclp->cl_clientid.cl_boot, clp->cl_clientid.cl_id);\n\n\treturn;\n}\n\n/*\n * Cache a reply. nfsd4_check_resp_size() has bounded the cache size.\n */\nstatic void\nnfsd4_store_cache_entry(struct nfsd4_compoundres *resp)\n{\n\tstruct xdr_buf *buf = resp->xdr.buf;\n\tstruct nfsd4_slot *slot = resp->cstate.slot;\n\tunsigned int base;\n\n\tdprintk(\"--> %s slot %p\\n\", __func__, slot);\n\n\tslot->sl_opcnt = resp->opcnt;\n\tslot->sl_status = resp->cstate.status;\n\n\tslot->sl_flags |= NFSD4_SLOT_INITIALIZED;\n\tif (nfsd4_not_cached(resp)) {\n\t\tslot->sl_datalen = 0;\n\t\treturn;\n\t}\n\tbase = resp->cstate.data_offset;\n\tslot->sl_datalen = buf->len - base;\n\tif (read_bytes_from_xdr_buf(buf, base, slot->sl_data, slot->sl_datalen))\n\t\tWARN(1, \"%s: sessions DRC could not cache compound\\n\",\n\t\t     __func__);\n\treturn;\n}\n\n/*\n * Encode the replay sequence operation from the slot values.\n * If cachethis is FALSE encode the uncached rep error on the next\n * operation which sets resp->p and increments resp->opcnt for\n * nfs4svc_encode_compoundres.\n *\n */\nstatic __be32\nnfsd4_enc_sequence_replay(struct nfsd4_compoundargs *args,\n\t\t\t  struct nfsd4_compoundres *resp)\n{\n\tstruct nfsd4_op *op;\n\tstruct nfsd4_slot *slot = resp->cstate.slot;\n\n\t/* Encode the replayed sequence operation */\n\top = &args->ops[resp->opcnt - 1];\n\tnfsd4_encode_operation(resp, op);\n\n\t/* Return nfserr_retry_uncached_rep in next operation. */\n\tif (args->opcnt > 1 && !(slot->sl_flags & NFSD4_SLOT_CACHETHIS)) {\n\t\top = &args->ops[resp->opcnt++];\n\t\top->status = nfserr_retry_uncached_rep;\n\t\tnfsd4_encode_operation(resp, op);\n\t}\n\treturn op->status;\n}\n\n/*\n * The sequence operation is not cached because we can use the slot and\n * session values.\n */\nstatic __be32\nnfsd4_replay_cache_entry(struct nfsd4_compoundres *resp,\n\t\t\t struct nfsd4_sequence *seq)\n{\n\tstruct nfsd4_slot *slot = resp->cstate.slot;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\t__be32 status;\n\n\tdprintk(\"--> %s slot %p\\n\", __func__, slot);\n\n\tstatus = nfsd4_enc_sequence_replay(resp->rqstp->rq_argp, resp);\n\tif (status)\n\t\treturn status;\n\n\tp = xdr_reserve_space(xdr, slot->sl_datalen);\n\tif (!p) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn nfserr_serverfault;\n\t}\n\txdr_encode_opaque_fixed(p, slot->sl_data, slot->sl_datalen);\n\txdr_commit_encode(xdr);\n\n\tresp->opcnt = slot->sl_opcnt;\n\treturn slot->sl_status;\n}\n\n/*\n * Set the exchange_id flags returned by the server.\n */\nstatic void\nnfsd4_set_ex_flags(struct nfs4_client *new, struct nfsd4_exchange_id *clid)\n{\n#ifdef CONFIG_NFSD_PNFS\n\tnew->cl_exchange_flags |= EXCHGID4_FLAG_USE_PNFS_MDS;\n#else\n\tnew->cl_exchange_flags |= EXCHGID4_FLAG_USE_NON_PNFS;\n#endif\n\n\t/* Referrals are supported, Migration is not. */\n\tnew->cl_exchange_flags |= EXCHGID4_FLAG_SUPP_MOVED_REFER;\n\n\t/* set the wire flags to return to client. */\n\tclid->flags = new->cl_exchange_flags;\n}\n\nstatic bool client_has_openowners(struct nfs4_client *clp)\n{\n\tstruct nfs4_openowner *oo;\n\n\tlist_for_each_entry(oo, &clp->cl_openowners, oo_perclient) {\n\t\tif (!list_empty(&oo->oo_owner.so_stateids))\n\t\t\treturn true;\n\t}\n\treturn false;\n}\n\nstatic bool client_has_state(struct nfs4_client *clp)\n{\n\treturn client_has_openowners(clp)\n#ifdef CONFIG_NFSD_PNFS\n\t\t|| !list_empty(&clp->cl_lo_states)\n#endif\n\t\t|| !list_empty(&clp->cl_delegations)\n\t\t|| !list_empty(&clp->cl_sessions);\n}\n\n__be32\nnfsd4_exchange_id(struct svc_rqst *rqstp,\n\t\t  struct nfsd4_compound_state *cstate,\n\t\t  struct nfsd4_exchange_id *exid)\n{\n\tstruct nfs4_client *conf, *new;\n\tstruct nfs4_client *unconf = NULL;\n\t__be32 status;\n\tchar\t\t\taddr_str[INET6_ADDRSTRLEN];\n\tnfs4_verifier\t\tverf = exid->verifier;\n\tstruct sockaddr\t\t*sa = svc_addr(rqstp);\n\tbool\tupdate = exid->flags & EXCHGID4_FLAG_UPD_CONFIRMED_REC_A;\n\tstruct nfsd_net\t\t*nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\trpc_ntop(sa, addr_str, sizeof(addr_str));\n\tdprintk(\"%s rqstp=%p exid=%p clname.len=%u clname.data=%p \"\n\t\t\"ip_addr=%s flags %x, spa_how %d\\n\",\n\t\t__func__, rqstp, exid, exid->clname.len, exid->clname.data,\n\t\taddr_str, exid->flags, exid->spa_how);\n\n\tif (exid->flags & ~EXCHGID4_FLAG_MASK_A)\n\t\treturn nfserr_inval;\n\n\tnew = create_client(exid->clname, rqstp, &verf);\n\tif (new == NULL)\n\t\treturn nfserr_jukebox;\n\n\tswitch (exid->spa_how) {\n\tcase SP4_MACH_CRED:\n\t\texid->spo_must_enforce[0] = 0;\n\t\texid->spo_must_enforce[1] = (\n\t\t\t1 << (OP_BIND_CONN_TO_SESSION - 32) |\n\t\t\t1 << (OP_EXCHANGE_ID - 32) |\n\t\t\t1 << (OP_CREATE_SESSION - 32) |\n\t\t\t1 << (OP_DESTROY_SESSION - 32) |\n\t\t\t1 << (OP_DESTROY_CLIENTID - 32));\n\n\t\texid->spo_must_allow[0] &= (1 << (OP_CLOSE) |\n\t\t\t\t\t1 << (OP_OPEN_DOWNGRADE) |\n\t\t\t\t\t1 << (OP_LOCKU) |\n\t\t\t\t\t1 << (OP_DELEGRETURN));\n\n\t\texid->spo_must_allow[1] &= (\n\t\t\t\t\t1 << (OP_TEST_STATEID - 32) |\n\t\t\t\t\t1 << (OP_FREE_STATEID - 32));\n\t\tif (!svc_rqst_integrity_protected(rqstp)) {\n\t\t\tstatus = nfserr_inval;\n\t\t\tgoto out_nolock;\n\t\t}\n\t\t/*\n\t\t * Sometimes userspace doesn't give us a principal.\n\t\t * Which is a bug, really.  Anyway, we can't enforce\n\t\t * MACH_CRED in that case, better to give up now:\n\t\t */\n\t\tif (!new->cl_cred.cr_principal &&\n\t\t\t\t\t!new->cl_cred.cr_raw_principal) {\n\t\t\tstatus = nfserr_serverfault;\n\t\t\tgoto out_nolock;\n\t\t}\n\t\tnew->cl_mach_cred = true;\n\tcase SP4_NONE:\n\t\tbreak;\n\tdefault:\t\t\t\t/* checked by xdr code */\n\t\tWARN_ON_ONCE(1);\n\tcase SP4_SSV:\n\t\tstatus = nfserr_encr_alg_unsupp;\n\t\tgoto out_nolock;\n\t}\n\n\t/* Cases below refer to rfc 5661 section 18.35.4: */\n\tspin_lock(&nn->client_lock);\n\tconf = find_confirmed_client_by_name(&exid->clname, nn);\n\tif (conf) {\n\t\tbool creds_match = same_creds(&conf->cl_cred, &rqstp->rq_cred);\n\t\tbool verfs_match = same_verf(&verf, &conf->cl_verifier);\n\n\t\tif (update) {\n\t\t\tif (!clp_used_exchangeid(conf)) { /* buggy client */\n\t\t\t\tstatus = nfserr_inval;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (!nfsd4_mach_creds_match(conf, rqstp)) {\n\t\t\t\tstatus = nfserr_wrong_cred;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (!creds_match) { /* case 9 */\n\t\t\t\tstatus = nfserr_perm;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tif (!verfs_match) { /* case 8 */\n\t\t\t\tstatus = nfserr_not_same;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\t/* case 6 */\n\t\t\texid->flags |= EXCHGID4_FLAG_CONFIRMED_R;\n\t\t\tgoto out_copy;\n\t\t}\n\t\tif (!creds_match) { /* case 3 */\n\t\t\tif (client_has_state(conf)) {\n\t\t\t\tstatus = nfserr_clid_inuse;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tgoto out_new;\n\t\t}\n\t\tif (verfs_match) { /* case 2 */\n\t\t\tconf->cl_exchange_flags |= EXCHGID4_FLAG_CONFIRMED_R;\n\t\t\tgoto out_copy;\n\t\t}\n\t\t/* case 5, client reboot */\n\t\tconf = NULL;\n\t\tgoto out_new;\n\t}\n\n\tif (update) { /* case 7 */\n\t\tstatus = nfserr_noent;\n\t\tgoto out;\n\t}\n\n\tunconf  = find_unconfirmed_client_by_name(&exid->clname, nn);\n\tif (unconf) /* case 4, possible retry or client restart */\n\t\tunhash_client_locked(unconf);\n\n\t/* case 1 (normal case) */\nout_new:\n\tif (conf) {\n\t\tstatus = mark_client_expired_locked(conf);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\tnew->cl_minorversion = cstate->minorversion;\n\tnew->cl_spo_must_allow.u.words[0] = exid->spo_must_allow[0];\n\tnew->cl_spo_must_allow.u.words[1] = exid->spo_must_allow[1];\n\n\tgen_clid(new, nn);\n\tadd_to_unconfirmed(new);\n\tswap(new, conf);\nout_copy:\n\texid->clientid.cl_boot = conf->cl_clientid.cl_boot;\n\texid->clientid.cl_id = conf->cl_clientid.cl_id;\n\n\texid->seqid = conf->cl_cs_slot.sl_seqid + 1;\n\tnfsd4_set_ex_flags(conf, exid);\n\n\tdprintk(\"nfsd4_exchange_id seqid %d flags %x\\n\",\n\t\tconf->cl_cs_slot.sl_seqid, conf->cl_exchange_flags);\n\tstatus = nfs_ok;\n\nout:\n\tspin_unlock(&nn->client_lock);\nout_nolock:\n\tif (new)\n\t\texpire_client(new);\n\tif (unconf)\n\t\texpire_client(unconf);\n\treturn status;\n}\n\nstatic __be32\ncheck_slot_seqid(u32 seqid, u32 slot_seqid, int slot_inuse)\n{\n\tdprintk(\"%s enter. seqid %d slot_seqid %d\\n\", __func__, seqid,\n\t\tslot_seqid);\n\n\t/* The slot is in use, and no response has been sent. */\n\tif (slot_inuse) {\n\t\tif (seqid == slot_seqid)\n\t\t\treturn nfserr_jukebox;\n\t\telse\n\t\t\treturn nfserr_seq_misordered;\n\t}\n\t/* Note unsigned 32-bit arithmetic handles wraparound: */\n\tif (likely(seqid == slot_seqid + 1))\n\t\treturn nfs_ok;\n\tif (seqid == slot_seqid)\n\t\treturn nfserr_replay_cache;\n\treturn nfserr_seq_misordered;\n}\n\n/*\n * Cache the create session result into the create session single DRC\n * slot cache by saving the xdr structure. sl_seqid has been set.\n * Do this for solo or embedded create session operations.\n */\nstatic void\nnfsd4_cache_create_session(struct nfsd4_create_session *cr_ses,\n\t\t\t   struct nfsd4_clid_slot *slot, __be32 nfserr)\n{\n\tslot->sl_status = nfserr;\n\tmemcpy(&slot->sl_cr_ses, cr_ses, sizeof(*cr_ses));\n}\n\nstatic __be32\nnfsd4_replay_create_session(struct nfsd4_create_session *cr_ses,\n\t\t\t    struct nfsd4_clid_slot *slot)\n{\n\tmemcpy(cr_ses, &slot->sl_cr_ses, sizeof(*cr_ses));\n\treturn slot->sl_status;\n}\n\n#define NFSD_MIN_REQ_HDR_SEQ_SZ\t((\\\n\t\t\t2 * 2 + /* credential,verifier: AUTH_NULL, length 0 */ \\\n\t\t\t1 +\t/* MIN tag is length with zero, only length */ \\\n\t\t\t3 +\t/* version, opcount, opcode */ \\\n\t\t\tXDR_QUADLEN(NFS4_MAX_SESSIONID_LEN) + \\\n\t\t\t\t/* seqid, slotID, slotID, cache */ \\\n\t\t\t4 ) * sizeof(__be32))\n\n#define NFSD_MIN_RESP_HDR_SEQ_SZ ((\\\n\t\t\t2 +\t/* verifier: AUTH_NULL, length 0 */\\\n\t\t\t1 +\t/* status */ \\\n\t\t\t1 +\t/* MIN tag is length with zero, only length */ \\\n\t\t\t3 +\t/* opcount, opcode, opstatus*/ \\\n\t\t\tXDR_QUADLEN(NFS4_MAX_SESSIONID_LEN) + \\\n\t\t\t\t/* seqid, slotID, slotID, slotID, status */ \\\n\t\t\t5 ) * sizeof(__be32))\n\nstatic __be32 check_forechannel_attrs(struct nfsd4_channel_attrs *ca, struct nfsd_net *nn)\n{\n\tu32 maxrpc = nn->nfsd_serv->sv_max_mesg;\n\n\tif (ca->maxreq_sz < NFSD_MIN_REQ_HDR_SEQ_SZ)\n\t\treturn nfserr_toosmall;\n\tif (ca->maxresp_sz < NFSD_MIN_RESP_HDR_SEQ_SZ)\n\t\treturn nfserr_toosmall;\n\tca->headerpadsz = 0;\n\tca->maxreq_sz = min_t(u32, ca->maxreq_sz, maxrpc);\n\tca->maxresp_sz = min_t(u32, ca->maxresp_sz, maxrpc);\n\tca->maxops = min_t(u32, ca->maxops, NFSD_MAX_OPS_PER_COMPOUND);\n\tca->maxresp_cached = min_t(u32, ca->maxresp_cached,\n\t\t\tNFSD_SLOT_CACHE_SIZE + NFSD_MIN_HDR_SEQ_SZ);\n\tca->maxreqs = min_t(u32, ca->maxreqs, NFSD_MAX_SLOTS_PER_SESSION);\n\t/*\n\t * Note decreasing slot size below client's request may make it\n\t * difficult for client to function correctly, whereas\n\t * decreasing the number of slots will (just?) affect\n\t * performance.  When short on memory we therefore prefer to\n\t * decrease number of slots instead of their size.  Clients that\n\t * request larger slots than they need will get poor results:\n\t */\n\tca->maxreqs = nfsd4_get_drc_mem(ca);\n\tif (!ca->maxreqs)\n\t\treturn nfserr_jukebox;\n\n\treturn nfs_ok;\n}\n\n/*\n * Server's NFSv4.1 backchannel support is AUTH_SYS-only for now.\n * These are based on similar macros in linux/sunrpc/msg_prot.h .\n */\n#define RPC_MAX_HEADER_WITH_AUTH_SYS \\\n\t(RPC_CALLHDRSIZE + 2 * (2 + UNX_CALLSLACK))\n\n#define RPC_MAX_REPHEADER_WITH_AUTH_SYS \\\n\t(RPC_REPHDRSIZE + (2 + NUL_REPLYSLACK))\n\n#define NFSD_CB_MAX_REQ_SZ\t((NFS4_enc_cb_recall_sz + \\\n\t\t\t\t RPC_MAX_HEADER_WITH_AUTH_SYS) * sizeof(__be32))\n#define NFSD_CB_MAX_RESP_SZ\t((NFS4_dec_cb_recall_sz + \\\n\t\t\t\t RPC_MAX_REPHEADER_WITH_AUTH_SYS) * \\\n\t\t\t\t sizeof(__be32))\n\nstatic __be32 check_backchannel_attrs(struct nfsd4_channel_attrs *ca)\n{\n\tca->headerpadsz = 0;\n\n\tif (ca->maxreq_sz < NFSD_CB_MAX_REQ_SZ)\n\t\treturn nfserr_toosmall;\n\tif (ca->maxresp_sz < NFSD_CB_MAX_RESP_SZ)\n\t\treturn nfserr_toosmall;\n\tca->maxresp_cached = 0;\n\tif (ca->maxops < 2)\n\t\treturn nfserr_toosmall;\n\n\treturn nfs_ok;\n}\n\nstatic __be32 nfsd4_check_cb_sec(struct nfsd4_cb_sec *cbs)\n{\n\tswitch (cbs->flavor) {\n\tcase RPC_AUTH_NULL:\n\tcase RPC_AUTH_UNIX:\n\t\treturn nfs_ok;\n\tdefault:\n\t\t/*\n\t\t * GSS case: the spec doesn't allow us to return this\n\t\t * error.  But it also doesn't allow us not to support\n\t\t * GSS.\n\t\t * I'd rather this fail hard than return some error the\n\t\t * client might think it can already handle:\n\t\t */\n\t\treturn nfserr_encr_alg_unsupp;\n\t}\n}\n\n__be32\nnfsd4_create_session(struct svc_rqst *rqstp,\n\t\t     struct nfsd4_compound_state *cstate,\n\t\t     struct nfsd4_create_session *cr_ses)\n{\n\tstruct sockaddr *sa = svc_addr(rqstp);\n\tstruct nfs4_client *conf, *unconf;\n\tstruct nfs4_client *old = NULL;\n\tstruct nfsd4_session *new;\n\tstruct nfsd4_conn *conn;\n\tstruct nfsd4_clid_slot *cs_slot = NULL;\n\t__be32 status = 0;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tif (cr_ses->flags & ~SESSION4_FLAG_MASK_A)\n\t\treturn nfserr_inval;\n\tstatus = nfsd4_check_cb_sec(&cr_ses->cb_sec);\n\tif (status)\n\t\treturn status;\n\tstatus = check_forechannel_attrs(&cr_ses->fore_channel, nn);\n\tif (status)\n\t\treturn status;\n\tstatus = check_backchannel_attrs(&cr_ses->back_channel);\n\tif (status)\n\t\tgoto out_release_drc_mem;\n\tstatus = nfserr_jukebox;\n\tnew = alloc_session(&cr_ses->fore_channel, &cr_ses->back_channel);\n\tif (!new)\n\t\tgoto out_release_drc_mem;\n\tconn = alloc_conn_from_crses(rqstp, cr_ses);\n\tif (!conn)\n\t\tgoto out_free_session;\n\n\tspin_lock(&nn->client_lock);\n\tunconf = find_unconfirmed_client(&cr_ses->clientid, true, nn);\n\tconf = find_confirmed_client(&cr_ses->clientid, true, nn);\n\tWARN_ON_ONCE(conf && unconf);\n\n\tif (conf) {\n\t\tstatus = nfserr_wrong_cred;\n\t\tif (!nfsd4_mach_creds_match(conf, rqstp))\n\t\t\tgoto out_free_conn;\n\t\tcs_slot = &conf->cl_cs_slot;\n\t\tstatus = check_slot_seqid(cr_ses->seqid, cs_slot->sl_seqid, 0);\n\t\tif (status) {\n\t\t\tif (status == nfserr_replay_cache)\n\t\t\t\tstatus = nfsd4_replay_create_session(cr_ses, cs_slot);\n\t\t\tgoto out_free_conn;\n\t\t}\n\t} else if (unconf) {\n\t\tif (!same_creds(&unconf->cl_cred, &rqstp->rq_cred) ||\n\t\t    !rpc_cmp_addr(sa, (struct sockaddr *) &unconf->cl_addr)) {\n\t\t\tstatus = nfserr_clid_inuse;\n\t\t\tgoto out_free_conn;\n\t\t}\n\t\tstatus = nfserr_wrong_cred;\n\t\tif (!nfsd4_mach_creds_match(unconf, rqstp))\n\t\t\tgoto out_free_conn;\n\t\tcs_slot = &unconf->cl_cs_slot;\n\t\tstatus = check_slot_seqid(cr_ses->seqid, cs_slot->sl_seqid, 0);\n\t\tif (status) {\n\t\t\t/* an unconfirmed replay returns misordered */\n\t\t\tstatus = nfserr_seq_misordered;\n\t\t\tgoto out_free_conn;\n\t\t}\n\t\told = find_confirmed_client_by_name(&unconf->cl_name, nn);\n\t\tif (old) {\n\t\t\tstatus = mark_client_expired_locked(old);\n\t\t\tif (status) {\n\t\t\t\told = NULL;\n\t\t\t\tgoto out_free_conn;\n\t\t\t}\n\t\t}\n\t\tmove_to_confirmed(unconf);\n\t\tconf = unconf;\n\t} else {\n\t\tstatus = nfserr_stale_clientid;\n\t\tgoto out_free_conn;\n\t}\n\tstatus = nfs_ok;\n\t/* Persistent sessions are not supported */\n\tcr_ses->flags &= ~SESSION4_PERSIST;\n\t/* Upshifting from TCP to RDMA is not supported */\n\tcr_ses->flags &= ~SESSION4_RDMA;\n\n\tinit_session(rqstp, new, conf, cr_ses);\n\tnfsd4_get_session_locked(new);\n\n\tmemcpy(cr_ses->sessionid.data, new->se_sessionid.data,\n\t       NFS4_MAX_SESSIONID_LEN);\n\tcs_slot->sl_seqid++;\n\tcr_ses->seqid = cs_slot->sl_seqid;\n\n\t/* cache solo and embedded create sessions under the client_lock */\n\tnfsd4_cache_create_session(cr_ses, cs_slot, status);\n\tspin_unlock(&nn->client_lock);\n\t/* init connection and backchannel */\n\tnfsd4_init_conn(rqstp, conn, new);\n\tnfsd4_put_session(new);\n\tif (old)\n\t\texpire_client(old);\n\treturn status;\nout_free_conn:\n\tspin_unlock(&nn->client_lock);\n\tfree_conn(conn);\n\tif (old)\n\t\texpire_client(old);\nout_free_session:\n\t__free_session(new);\nout_release_drc_mem:\n\tnfsd4_put_drc_mem(&cr_ses->fore_channel);\n\treturn status;\n}\n\nstatic __be32 nfsd4_map_bcts_dir(u32 *dir)\n{\n\tswitch (*dir) {\n\tcase NFS4_CDFC4_FORE:\n\tcase NFS4_CDFC4_BACK:\n\t\treturn nfs_ok;\n\tcase NFS4_CDFC4_FORE_OR_BOTH:\n\tcase NFS4_CDFC4_BACK_OR_BOTH:\n\t\t*dir = NFS4_CDFC4_BOTH;\n\t\treturn nfs_ok;\n\t};\n\treturn nfserr_inval;\n}\n\n__be32 nfsd4_backchannel_ctl(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_backchannel_ctl *bc)\n{\n\tstruct nfsd4_session *session = cstate->session;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\t__be32 status;\n\n\tstatus = nfsd4_check_cb_sec(&bc->bc_cb_sec);\n\tif (status)\n\t\treturn status;\n\tspin_lock(&nn->client_lock);\n\tsession->se_cb_prog = bc->bc_cb_program;\n\tsession->se_cb_sec = bc->bc_cb_sec;\n\tspin_unlock(&nn->client_lock);\n\n\tnfsd4_probe_callback(session->se_client);\n\n\treturn nfs_ok;\n}\n\n__be32 nfsd4_bind_conn_to_session(struct svc_rqst *rqstp,\n\t\t     struct nfsd4_compound_state *cstate,\n\t\t     struct nfsd4_bind_conn_to_session *bcts)\n{\n\t__be32 status;\n\tstruct nfsd4_conn *conn;\n\tstruct nfsd4_session *session;\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tif (!nfsd4_last_compound_op(rqstp))\n\t\treturn nfserr_not_only_op;\n\tspin_lock(&nn->client_lock);\n\tsession = find_in_sessionid_hashtbl(&bcts->sessionid, net, &status);\n\tspin_unlock(&nn->client_lock);\n\tif (!session)\n\t\tgoto out_no_session;\n\tstatus = nfserr_wrong_cred;\n\tif (!nfsd4_mach_creds_match(session->se_client, rqstp))\n\t\tgoto out;\n\tstatus = nfsd4_map_bcts_dir(&bcts->dir);\n\tif (status)\n\t\tgoto out;\n\tconn = alloc_conn(rqstp, bcts->dir);\n\tstatus = nfserr_jukebox;\n\tif (!conn)\n\t\tgoto out;\n\tnfsd4_init_conn(rqstp, conn, session);\n\tstatus = nfs_ok;\nout:\n\tnfsd4_put_session(session);\nout_no_session:\n\treturn status;\n}\n\nstatic bool nfsd4_compound_in_session(struct nfsd4_session *session, struct nfs4_sessionid *sid)\n{\n\tif (!session)\n\t\treturn 0;\n\treturn !memcmp(sid, &session->se_sessionid, sizeof(*sid));\n}\n\n__be32\nnfsd4_destroy_session(struct svc_rqst *r,\n\t\t      struct nfsd4_compound_state *cstate,\n\t\t      struct nfsd4_destroy_session *sessionid)\n{\n\tstruct nfsd4_session *ses;\n\t__be32 status;\n\tint ref_held_by_me = 0;\n\tstruct net *net = SVC_NET(r);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tstatus = nfserr_not_only_op;\n\tif (nfsd4_compound_in_session(cstate->session, &sessionid->sessionid)) {\n\t\tif (!nfsd4_last_compound_op(r))\n\t\t\tgoto out;\n\t\tref_held_by_me++;\n\t}\n\tdump_sessionid(__func__, &sessionid->sessionid);\n\tspin_lock(&nn->client_lock);\n\tses = find_in_sessionid_hashtbl(&sessionid->sessionid, net, &status);\n\tif (!ses)\n\t\tgoto out_client_lock;\n\tstatus = nfserr_wrong_cred;\n\tif (!nfsd4_mach_creds_match(ses->se_client, r))\n\t\tgoto out_put_session;\n\tstatus = mark_session_dead_locked(ses, 1 + ref_held_by_me);\n\tif (status)\n\t\tgoto out_put_session;\n\tunhash_session(ses);\n\tspin_unlock(&nn->client_lock);\n\n\tnfsd4_probe_callback_sync(ses->se_client);\n\n\tspin_lock(&nn->client_lock);\n\tstatus = nfs_ok;\nout_put_session:\n\tnfsd4_put_session_locked(ses);\nout_client_lock:\n\tspin_unlock(&nn->client_lock);\nout:\n\treturn status;\n}\n\nstatic struct nfsd4_conn *__nfsd4_find_conn(struct svc_xprt *xpt, struct nfsd4_session *s)\n{\n\tstruct nfsd4_conn *c;\n\n\tlist_for_each_entry(c, &s->se_conns, cn_persession) {\n\t\tif (c->cn_xprt == xpt) {\n\t\t\treturn c;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic __be32 nfsd4_sequence_check_conn(struct nfsd4_conn *new, struct nfsd4_session *ses)\n{\n\tstruct nfs4_client *clp = ses->se_client;\n\tstruct nfsd4_conn *c;\n\t__be32 status = nfs_ok;\n\tint ret;\n\n\tspin_lock(&clp->cl_lock);\n\tc = __nfsd4_find_conn(new->cn_xprt, ses);\n\tif (c)\n\t\tgoto out_free;\n\tstatus = nfserr_conn_not_bound_to_session;\n\tif (clp->cl_mach_cred)\n\t\tgoto out_free;\n\t__nfsd4_hash_conn(new, ses);\n\tspin_unlock(&clp->cl_lock);\n\tret = nfsd4_register_conn(new);\n\tif (ret)\n\t\t/* oops; xprt is already down: */\n\t\tnfsd4_conn_lost(&new->cn_xpt_user);\n\treturn nfs_ok;\nout_free:\n\tspin_unlock(&clp->cl_lock);\n\tfree_conn(new);\n\treturn status;\n}\n\nstatic bool nfsd4_session_too_many_ops(struct svc_rqst *rqstp, struct nfsd4_session *session)\n{\n\tstruct nfsd4_compoundargs *args = rqstp->rq_argp;\n\n\treturn args->opcnt > session->se_fchannel.maxops;\n}\n\nstatic bool nfsd4_request_too_big(struct svc_rqst *rqstp,\n\t\t\t\t  struct nfsd4_session *session)\n{\n\tstruct xdr_buf *xb = &rqstp->rq_arg;\n\n\treturn xb->len > session->se_fchannel.maxreq_sz;\n}\n\n__be32\nnfsd4_sequence(struct svc_rqst *rqstp,\n\t       struct nfsd4_compound_state *cstate,\n\t       struct nfsd4_sequence *seq)\n{\n\tstruct nfsd4_compoundres *resp = rqstp->rq_resp;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct nfsd4_session *session;\n\tstruct nfs4_client *clp;\n\tstruct nfsd4_slot *slot;\n\tstruct nfsd4_conn *conn;\n\t__be32 status;\n\tint buflen;\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tif (resp->opcnt != 1)\n\t\treturn nfserr_sequence_pos;\n\n\t/*\n\t * Will be either used or freed by nfsd4_sequence_check_conn\n\t * below.\n\t */\n\tconn = alloc_conn(rqstp, NFS4_CDFC4_FORE);\n\tif (!conn)\n\t\treturn nfserr_jukebox;\n\n\tspin_lock(&nn->client_lock);\n\tsession = find_in_sessionid_hashtbl(&seq->sessionid, net, &status);\n\tif (!session)\n\t\tgoto out_no_session;\n\tclp = session->se_client;\n\n\tstatus = nfserr_too_many_ops;\n\tif (nfsd4_session_too_many_ops(rqstp, session))\n\t\tgoto out_put_session;\n\n\tstatus = nfserr_req_too_big;\n\tif (nfsd4_request_too_big(rqstp, session))\n\t\tgoto out_put_session;\n\n\tstatus = nfserr_badslot;\n\tif (seq->slotid >= session->se_fchannel.maxreqs)\n\t\tgoto out_put_session;\n\n\tslot = session->se_slots[seq->slotid];\n\tdprintk(\"%s: slotid %d\\n\", __func__, seq->slotid);\n\n\t/* We do not negotiate the number of slots yet, so set the\n\t * maxslots to the session maxreqs which is used to encode\n\t * sr_highest_slotid and the sr_target_slot id to maxslots */\n\tseq->maxslots = session->se_fchannel.maxreqs;\n\n\tstatus = check_slot_seqid(seq->seqid, slot->sl_seqid,\n\t\t\t\t\tslot->sl_flags & NFSD4_SLOT_INUSE);\n\tif (status == nfserr_replay_cache) {\n\t\tstatus = nfserr_seq_misordered;\n\t\tif (!(slot->sl_flags & NFSD4_SLOT_INITIALIZED))\n\t\t\tgoto out_put_session;\n\t\tcstate->slot = slot;\n\t\tcstate->session = session;\n\t\tcstate->clp = clp;\n\t\t/* Return the cached reply status and set cstate->status\n\t\t * for nfsd4_proc_compound processing */\n\t\tstatus = nfsd4_replay_cache_entry(resp, seq);\n\t\tcstate->status = nfserr_replay_cache;\n\t\tgoto out;\n\t}\n\tif (status)\n\t\tgoto out_put_session;\n\n\tstatus = nfsd4_sequence_check_conn(conn, session);\n\tconn = NULL;\n\tif (status)\n\t\tgoto out_put_session;\n\n\tbuflen = (seq->cachethis) ?\n\t\t\tsession->se_fchannel.maxresp_cached :\n\t\t\tsession->se_fchannel.maxresp_sz;\n\tstatus = (seq->cachethis) ? nfserr_rep_too_big_to_cache :\n\t\t\t\t    nfserr_rep_too_big;\n\tif (xdr_restrict_buflen(xdr, buflen - rqstp->rq_auth_slack))\n\t\tgoto out_put_session;\n\tsvc_reserve(rqstp, buflen);\n\n\tstatus = nfs_ok;\n\t/* Success! bump slot seqid */\n\tslot->sl_seqid = seq->seqid;\n\tslot->sl_flags |= NFSD4_SLOT_INUSE;\n\tif (seq->cachethis)\n\t\tslot->sl_flags |= NFSD4_SLOT_CACHETHIS;\n\telse\n\t\tslot->sl_flags &= ~NFSD4_SLOT_CACHETHIS;\n\n\tcstate->slot = slot;\n\tcstate->session = session;\n\tcstate->clp = clp;\n\nout:\n\tswitch (clp->cl_cb_state) {\n\tcase NFSD4_CB_DOWN:\n\t\tseq->status_flags = SEQ4_STATUS_CB_PATH_DOWN;\n\t\tbreak;\n\tcase NFSD4_CB_FAULT:\n\t\tseq->status_flags = SEQ4_STATUS_BACKCHANNEL_FAULT;\n\t\tbreak;\n\tdefault:\n\t\tseq->status_flags = 0;\n\t}\n\tif (!list_empty(&clp->cl_revoked))\n\t\tseq->status_flags |= SEQ4_STATUS_RECALLABLE_STATE_REVOKED;\nout_no_session:\n\tif (conn)\n\t\tfree_conn(conn);\n\tspin_unlock(&nn->client_lock);\n\treturn status;\nout_put_session:\n\tnfsd4_put_session_locked(session);\n\tgoto out_no_session;\n}\n\nvoid\nnfsd4_sequence_done(struct nfsd4_compoundres *resp)\n{\n\tstruct nfsd4_compound_state *cs = &resp->cstate;\n\n\tif (nfsd4_has_session(cs)) {\n\t\tif (cs->status != nfserr_replay_cache) {\n\t\t\tnfsd4_store_cache_entry(resp);\n\t\t\tcs->slot->sl_flags &= ~NFSD4_SLOT_INUSE;\n\t\t}\n\t\t/* Drop session reference that was taken in nfsd4_sequence() */\n\t\tnfsd4_put_session(cs->session);\n\t} else if (cs->clp)\n\t\tput_client_renew(cs->clp);\n}\n\n__be32\nnfsd4_destroy_clientid(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_destroy_clientid *dc)\n{\n\tstruct nfs4_client *conf, *unconf;\n\tstruct nfs4_client *clp = NULL;\n\t__be32 status = 0;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tspin_lock(&nn->client_lock);\n\tunconf = find_unconfirmed_client(&dc->clientid, true, nn);\n\tconf = find_confirmed_client(&dc->clientid, true, nn);\n\tWARN_ON_ONCE(conf && unconf);\n\n\tif (conf) {\n\t\tif (client_has_state(conf)) {\n\t\t\tstatus = nfserr_clientid_busy;\n\t\t\tgoto out;\n\t\t}\n\t\tstatus = mark_client_expired_locked(conf);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tclp = conf;\n\t} else if (unconf)\n\t\tclp = unconf;\n\telse {\n\t\tstatus = nfserr_stale_clientid;\n\t\tgoto out;\n\t}\n\tif (!nfsd4_mach_creds_match(clp, rqstp)) {\n\t\tclp = NULL;\n\t\tstatus = nfserr_wrong_cred;\n\t\tgoto out;\n\t}\n\tunhash_client_locked(clp);\nout:\n\tspin_unlock(&nn->client_lock);\n\tif (clp)\n\t\texpire_client(clp);\n\treturn status;\n}\n\n__be32\nnfsd4_reclaim_complete(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate, struct nfsd4_reclaim_complete *rc)\n{\n\t__be32 status = 0;\n\n\tif (rc->rca_one_fs) {\n\t\tif (!cstate->current_fh.fh_dentry)\n\t\t\treturn nfserr_nofilehandle;\n\t\t/*\n\t\t * We don't take advantage of the rca_one_fs case.\n\t\t * That's OK, it's optional, we can safely ignore it.\n\t\t */\n\t\treturn nfs_ok;\n\t}\n\n\tstatus = nfserr_complete_already;\n\tif (test_and_set_bit(NFSD4_CLIENT_RECLAIM_COMPLETE,\n\t\t\t     &cstate->session->se_client->cl_flags))\n\t\tgoto out;\n\n\tstatus = nfserr_stale_clientid;\n\tif (is_client_expired(cstate->session->se_client))\n\t\t/*\n\t\t * The following error isn't really legal.\n\t\t * But we only get here if the client just explicitly\n\t\t * destroyed the client.  Surely it no longer cares what\n\t\t * error it gets back on an operation for the dead\n\t\t * client.\n\t\t */\n\t\tgoto out;\n\n\tstatus = nfs_ok;\n\tnfsd4_client_record_create(cstate->session->se_client);\nout:\n\treturn status;\n}\n\n__be32\nnfsd4_setclientid(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t  struct nfsd4_setclientid *setclid)\n{\n\tstruct xdr_netobj \tclname = setclid->se_name;\n\tnfs4_verifier\t\tclverifier = setclid->se_verf;\n\tstruct nfs4_client\t*conf, *new;\n\tstruct nfs4_client\t*unconf = NULL;\n\t__be32 \t\t\tstatus;\n\tstruct nfsd_net\t\t*nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tnew = create_client(clname, rqstp, &clverifier);\n\tif (new == NULL)\n\t\treturn nfserr_jukebox;\n\t/* Cases below refer to rfc 3530 section 14.2.33: */\n\tspin_lock(&nn->client_lock);\n\tconf = find_confirmed_client_by_name(&clname, nn);\n\tif (conf && client_has_state(conf)) {\n\t\t/* case 0: */\n\t\tstatus = nfserr_clid_inuse;\n\t\tif (clp_used_exchangeid(conf))\n\t\t\tgoto out;\n\t\tif (!same_creds(&conf->cl_cred, &rqstp->rq_cred)) {\n\t\t\tchar addr_str[INET6_ADDRSTRLEN];\n\t\t\trpc_ntop((struct sockaddr *) &conf->cl_addr, addr_str,\n\t\t\t\t sizeof(addr_str));\n\t\t\tdprintk(\"NFSD: setclientid: string in use by client \"\n\t\t\t\t\"at %s\\n\", addr_str);\n\t\t\tgoto out;\n\t\t}\n\t}\n\tunconf = find_unconfirmed_client_by_name(&clname, nn);\n\tif (unconf)\n\t\tunhash_client_locked(unconf);\n\tif (conf && same_verf(&conf->cl_verifier, &clverifier)) {\n\t\t/* case 1: probable callback update */\n\t\tcopy_clid(new, conf);\n\t\tgen_confirm(new, nn);\n\t} else /* case 4 (new client) or cases 2, 3 (client reboot): */\n\t\tgen_clid(new, nn);\n\tnew->cl_minorversion = 0;\n\tgen_callback(new, setclid, rqstp);\n\tadd_to_unconfirmed(new);\n\tsetclid->se_clientid.cl_boot = new->cl_clientid.cl_boot;\n\tsetclid->se_clientid.cl_id = new->cl_clientid.cl_id;\n\tmemcpy(setclid->se_confirm.data, new->cl_confirm.data, sizeof(setclid->se_confirm.data));\n\tnew = NULL;\n\tstatus = nfs_ok;\nout:\n\tspin_unlock(&nn->client_lock);\n\tif (new)\n\t\tfree_client(new);\n\tif (unconf)\n\t\texpire_client(unconf);\n\treturn status;\n}\n\n\n__be32\nnfsd4_setclientid_confirm(struct svc_rqst *rqstp,\n\t\t\t struct nfsd4_compound_state *cstate,\n\t\t\t struct nfsd4_setclientid_confirm *setclientid_confirm)\n{\n\tstruct nfs4_client *conf, *unconf;\n\tstruct nfs4_client *old = NULL;\n\tnfs4_verifier confirm = setclientid_confirm->sc_confirm; \n\tclientid_t * clid = &setclientid_confirm->sc_clientid;\n\t__be32 status;\n\tstruct nfsd_net\t*nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tif (STALE_CLIENTID(clid, nn))\n\t\treturn nfserr_stale_clientid;\n\n\tspin_lock(&nn->client_lock);\n\tconf = find_confirmed_client(clid, false, nn);\n\tunconf = find_unconfirmed_client(clid, false, nn);\n\t/*\n\t * We try hard to give out unique clientid's, so if we get an\n\t * attempt to confirm the same clientid with a different cred,\n\t * the client may be buggy; this should never happen.\n\t *\n\t * Nevertheless, RFC 7530 recommends INUSE for this case:\n\t */\n\tstatus = nfserr_clid_inuse;\n\tif (unconf && !same_creds(&unconf->cl_cred, &rqstp->rq_cred))\n\t\tgoto out;\n\tif (conf && !same_creds(&conf->cl_cred, &rqstp->rq_cred))\n\t\tgoto out;\n\t/* cases below refer to rfc 3530 section 14.2.34: */\n\tif (!unconf || !same_verf(&confirm, &unconf->cl_confirm)) {\n\t\tif (conf && same_verf(&confirm, &conf->cl_confirm)) {\n\t\t\t/* case 2: probable retransmit */\n\t\t\tstatus = nfs_ok;\n\t\t} else /* case 4: client hasn't noticed we rebooted yet? */\n\t\t\tstatus = nfserr_stale_clientid;\n\t\tgoto out;\n\t}\n\tstatus = nfs_ok;\n\tif (conf) { /* case 1: callback update */\n\t\told = unconf;\n\t\tunhash_client_locked(old);\n\t\tnfsd4_change_callback(conf, &unconf->cl_cb_conn);\n\t} else { /* case 3: normal case; new or rebooted client */\n\t\told = find_confirmed_client_by_name(&unconf->cl_name, nn);\n\t\tif (old) {\n\t\t\tstatus = nfserr_clid_inuse;\n\t\t\tif (client_has_state(old)\n\t\t\t\t\t&& !same_creds(&unconf->cl_cred,\n\t\t\t\t\t\t\t&old->cl_cred))\n\t\t\t\tgoto out;\n\t\t\tstatus = mark_client_expired_locked(old);\n\t\t\tif (status) {\n\t\t\t\told = NULL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t\tmove_to_confirmed(unconf);\n\t\tconf = unconf;\n\t}\n\tget_client_locked(conf);\n\tspin_unlock(&nn->client_lock);\n\tnfsd4_probe_callback(conf);\n\tspin_lock(&nn->client_lock);\n\tput_client_renew_locked(conf);\nout:\n\tspin_unlock(&nn->client_lock);\n\tif (old)\n\t\texpire_client(old);\n\treturn status;\n}\n\nstatic struct nfs4_file *nfsd4_alloc_file(void)\n{\n\treturn kmem_cache_alloc(file_slab, GFP_KERNEL);\n}\n\n/* OPEN Share state helper functions */\nstatic void nfsd4_init_file(struct knfsd_fh *fh, unsigned int hashval,\n\t\t\t\tstruct nfs4_file *fp)\n{\n\tlockdep_assert_held(&state_lock);\n\n\tatomic_set(&fp->fi_ref, 1);\n\tspin_lock_init(&fp->fi_lock);\n\tINIT_LIST_HEAD(&fp->fi_stateids);\n\tINIT_LIST_HEAD(&fp->fi_delegations);\n\tINIT_LIST_HEAD(&fp->fi_clnt_odstate);\n\tfh_copy_shallow(&fp->fi_fhandle, fh);\n\tfp->fi_deleg_file = NULL;\n\tfp->fi_had_conflict = false;\n\tfp->fi_share_deny = 0;\n\tmemset(fp->fi_fds, 0, sizeof(fp->fi_fds));\n\tmemset(fp->fi_access, 0, sizeof(fp->fi_access));\n#ifdef CONFIG_NFSD_PNFS\n\tINIT_LIST_HEAD(&fp->fi_lo_states);\n\tatomic_set(&fp->fi_lo_recalls, 0);\n#endif\n\thlist_add_head_rcu(&fp->fi_hash, &file_hashtbl[hashval]);\n}\n\nvoid\nnfsd4_free_slabs(void)\n{\n\tkmem_cache_destroy(odstate_slab);\n\tkmem_cache_destroy(openowner_slab);\n\tkmem_cache_destroy(lockowner_slab);\n\tkmem_cache_destroy(file_slab);\n\tkmem_cache_destroy(stateid_slab);\n\tkmem_cache_destroy(deleg_slab);\n}\n\nint\nnfsd4_init_slabs(void)\n{\n\topenowner_slab = kmem_cache_create(\"nfsd4_openowners\",\n\t\t\tsizeof(struct nfs4_openowner), 0, 0, NULL);\n\tif (openowner_slab == NULL)\n\t\tgoto out;\n\tlockowner_slab = kmem_cache_create(\"nfsd4_lockowners\",\n\t\t\tsizeof(struct nfs4_lockowner), 0, 0, NULL);\n\tif (lockowner_slab == NULL)\n\t\tgoto out_free_openowner_slab;\n\tfile_slab = kmem_cache_create(\"nfsd4_files\",\n\t\t\tsizeof(struct nfs4_file), 0, 0, NULL);\n\tif (file_slab == NULL)\n\t\tgoto out_free_lockowner_slab;\n\tstateid_slab = kmem_cache_create(\"nfsd4_stateids\",\n\t\t\tsizeof(struct nfs4_ol_stateid), 0, 0, NULL);\n\tif (stateid_slab == NULL)\n\t\tgoto out_free_file_slab;\n\tdeleg_slab = kmem_cache_create(\"nfsd4_delegations\",\n\t\t\tsizeof(struct nfs4_delegation), 0, 0, NULL);\n\tif (deleg_slab == NULL)\n\t\tgoto out_free_stateid_slab;\n\todstate_slab = kmem_cache_create(\"nfsd4_odstate\",\n\t\t\tsizeof(struct nfs4_clnt_odstate), 0, 0, NULL);\n\tif (odstate_slab == NULL)\n\t\tgoto out_free_deleg_slab;\n\treturn 0;\n\nout_free_deleg_slab:\n\tkmem_cache_destroy(deleg_slab);\nout_free_stateid_slab:\n\tkmem_cache_destroy(stateid_slab);\nout_free_file_slab:\n\tkmem_cache_destroy(file_slab);\nout_free_lockowner_slab:\n\tkmem_cache_destroy(lockowner_slab);\nout_free_openowner_slab:\n\tkmem_cache_destroy(openowner_slab);\nout:\n\tdprintk(\"nfsd4: out of memory while initializing nfsv4\\n\");\n\treturn -ENOMEM;\n}\n\nstatic void init_nfs4_replay(struct nfs4_replay *rp)\n{\n\trp->rp_status = nfserr_serverfault;\n\trp->rp_buflen = 0;\n\trp->rp_buf = rp->rp_ibuf;\n\tmutex_init(&rp->rp_mutex);\n}\n\nstatic void nfsd4_cstate_assign_replay(struct nfsd4_compound_state *cstate,\n\t\tstruct nfs4_stateowner *so)\n{\n\tif (!nfsd4_has_session(cstate)) {\n\t\tmutex_lock(&so->so_replay.rp_mutex);\n\t\tcstate->replay_owner = nfs4_get_stateowner(so);\n\t}\n}\n\nvoid nfsd4_cstate_clear_replay(struct nfsd4_compound_state *cstate)\n{\n\tstruct nfs4_stateowner *so = cstate->replay_owner;\n\n\tif (so != NULL) {\n\t\tcstate->replay_owner = NULL;\n\t\tmutex_unlock(&so->so_replay.rp_mutex);\n\t\tnfs4_put_stateowner(so);\n\t}\n}\n\nstatic inline void *alloc_stateowner(struct kmem_cache *slab, struct xdr_netobj *owner, struct nfs4_client *clp)\n{\n\tstruct nfs4_stateowner *sop;\n\n\tsop = kmem_cache_alloc(slab, GFP_KERNEL);\n\tif (!sop)\n\t\treturn NULL;\n\n\tsop->so_owner.data = kmemdup(owner->data, owner->len, GFP_KERNEL);\n\tif (!sop->so_owner.data) {\n\t\tkmem_cache_free(slab, sop);\n\t\treturn NULL;\n\t}\n\tsop->so_owner.len = owner->len;\n\n\tINIT_LIST_HEAD(&sop->so_stateids);\n\tsop->so_client = clp;\n\tinit_nfs4_replay(&sop->so_replay);\n\tatomic_set(&sop->so_count, 1);\n\treturn sop;\n}\n\nstatic void hash_openowner(struct nfs4_openowner *oo, struct nfs4_client *clp, unsigned int strhashval)\n{\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_add(&oo->oo_owner.so_strhash,\n\t\t &clp->cl_ownerstr_hashtbl[strhashval]);\n\tlist_add(&oo->oo_perclient, &clp->cl_openowners);\n}\n\nstatic void nfs4_unhash_openowner(struct nfs4_stateowner *so)\n{\n\tunhash_openowner_locked(openowner(so));\n}\n\nstatic void nfs4_free_openowner(struct nfs4_stateowner *so)\n{\n\tstruct nfs4_openowner *oo = openowner(so);\n\n\tkmem_cache_free(openowner_slab, oo);\n}\n\nstatic const struct nfs4_stateowner_operations openowner_ops = {\n\t.so_unhash =\tnfs4_unhash_openowner,\n\t.so_free =\tnfs4_free_openowner,\n};\n\nstatic struct nfs4_ol_stateid *\nnfsd4_find_existing_open(struct nfs4_file *fp, struct nfsd4_open *open)\n{\n\tstruct nfs4_ol_stateid *local, *ret = NULL;\n\tstruct nfs4_openowner *oo = open->op_openowner;\n\n\tlockdep_assert_held(&fp->fi_lock);\n\n\tlist_for_each_entry(local, &fp->fi_stateids, st_perfile) {\n\t\t/* ignore lock owners */\n\t\tif (local->st_stateowner->so_is_open_owner == 0)\n\t\t\tcontinue;\n\t\tif (local->st_stateowner == &oo->oo_owner) {\n\t\t\tret = local;\n\t\t\tatomic_inc(&ret->st_stid.sc_count);\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn ret;\n}\n\nstatic struct nfs4_openowner *\nalloc_init_open_stateowner(unsigned int strhashval, struct nfsd4_open *open,\n\t\t\t   struct nfsd4_compound_state *cstate)\n{\n\tstruct nfs4_client *clp = cstate->clp;\n\tstruct nfs4_openowner *oo, *ret;\n\n\too = alloc_stateowner(openowner_slab, &open->op_owner, clp);\n\tif (!oo)\n\t\treturn NULL;\n\too->oo_owner.so_ops = &openowner_ops;\n\too->oo_owner.so_is_open_owner = 1;\n\too->oo_owner.so_seqid = open->op_seqid;\n\too->oo_flags = 0;\n\tif (nfsd4_has_session(cstate))\n\t\too->oo_flags |= NFS4_OO_CONFIRMED;\n\too->oo_time = 0;\n\too->oo_last_closed_stid = NULL;\n\tINIT_LIST_HEAD(&oo->oo_close_lru);\n\tspin_lock(&clp->cl_lock);\n\tret = find_openstateowner_str_locked(strhashval, open, clp);\n\tif (ret == NULL) {\n\t\thash_openowner(oo, clp, strhashval);\n\t\tret = oo;\n\t} else\n\t\tnfs4_free_stateowner(&oo->oo_owner);\n\n\tspin_unlock(&clp->cl_lock);\n\treturn ret;\n}\n\nstatic struct nfs4_ol_stateid *\ninit_open_stateid(struct nfs4_file *fp, struct nfsd4_open *open)\n{\n\n\tstruct nfs4_openowner *oo = open->op_openowner;\n\tstruct nfs4_ol_stateid *retstp = NULL;\n\tstruct nfs4_ol_stateid *stp;\n\n\tstp = open->op_stp;\n\t/* We are moving these outside of the spinlocks to avoid the warnings */\n\tmutex_init(&stp->st_mutex);\n\tmutex_lock(&stp->st_mutex);\n\n\tspin_lock(&oo->oo_owner.so_client->cl_lock);\n\tspin_lock(&fp->fi_lock);\n\n\tretstp = nfsd4_find_existing_open(fp, open);\n\tif (retstp)\n\t\tgoto out_unlock;\n\n\topen->op_stp = NULL;\n\tatomic_inc(&stp->st_stid.sc_count);\n\tstp->st_stid.sc_type = NFS4_OPEN_STID;\n\tINIT_LIST_HEAD(&stp->st_locks);\n\tstp->st_stateowner = nfs4_get_stateowner(&oo->oo_owner);\n\tget_nfs4_file(fp);\n\tstp->st_stid.sc_file = fp;\n\tstp->st_access_bmap = 0;\n\tstp->st_deny_bmap = 0;\n\tstp->st_openstp = NULL;\n\tlist_add(&stp->st_perstateowner, &oo->oo_owner.so_stateids);\n\tlist_add(&stp->st_perfile, &fp->fi_stateids);\n\nout_unlock:\n\tspin_unlock(&fp->fi_lock);\n\tspin_unlock(&oo->oo_owner.so_client->cl_lock);\n\tif (retstp) {\n\t\tmutex_lock(&retstp->st_mutex);\n\t\t/* To keep mutex tracking happy */\n\t\tmutex_unlock(&stp->st_mutex);\n\t\tstp = retstp;\n\t}\n\treturn stp;\n}\n\n/*\n * In the 4.0 case we need to keep the owners around a little while to handle\n * CLOSE replay. We still do need to release any file access that is held by\n * them before returning however.\n */\nstatic void\nmove_to_close_lru(struct nfs4_ol_stateid *s, struct net *net)\n{\n\tstruct nfs4_ol_stateid *last;\n\tstruct nfs4_openowner *oo = openowner(s->st_stateowner);\n\tstruct nfsd_net *nn = net_generic(s->st_stid.sc_client->net,\n\t\t\t\t\t\tnfsd_net_id);\n\n\tdprintk(\"NFSD: move_to_close_lru nfs4_openowner %p\\n\", oo);\n\n\t/*\n\t * We know that we hold one reference via nfsd4_close, and another\n\t * \"persistent\" reference for the client. If the refcount is higher\n\t * than 2, then there are still calls in progress that are using this\n\t * stateid. We can't put the sc_file reference until they are finished.\n\t * Wait for the refcount to drop to 2. Since it has been unhashed,\n\t * there should be no danger of the refcount going back up again at\n\t * this point.\n\t */\n\twait_event(close_wq, atomic_read(&s->st_stid.sc_count) == 2);\n\n\trelease_all_access(s);\n\tif (s->st_stid.sc_file) {\n\t\tput_nfs4_file(s->st_stid.sc_file);\n\t\ts->st_stid.sc_file = NULL;\n\t}\n\n\tspin_lock(&nn->client_lock);\n\tlast = oo->oo_last_closed_stid;\n\too->oo_last_closed_stid = s;\n\tlist_move_tail(&oo->oo_close_lru, &nn->close_lru);\n\too->oo_time = get_seconds();\n\tspin_unlock(&nn->client_lock);\n\tif (last)\n\t\tnfs4_put_stid(&last->st_stid);\n}\n\n/* search file_hashtbl[] for file */\nstatic struct nfs4_file *\nfind_file_locked(struct knfsd_fh *fh, unsigned int hashval)\n{\n\tstruct nfs4_file *fp;\n\n\thlist_for_each_entry_rcu(fp, &file_hashtbl[hashval], fi_hash) {\n\t\tif (fh_match(&fp->fi_fhandle, fh)) {\n\t\t\tif (atomic_inc_not_zero(&fp->fi_ref))\n\t\t\t\treturn fp;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstruct nfs4_file *\nfind_file(struct knfsd_fh *fh)\n{\n\tstruct nfs4_file *fp;\n\tunsigned int hashval = file_hashval(fh);\n\n\trcu_read_lock();\n\tfp = find_file_locked(fh, hashval);\n\trcu_read_unlock();\n\treturn fp;\n}\n\nstatic struct nfs4_file *\nfind_or_add_file(struct nfs4_file *new, struct knfsd_fh *fh)\n{\n\tstruct nfs4_file *fp;\n\tunsigned int hashval = file_hashval(fh);\n\n\trcu_read_lock();\n\tfp = find_file_locked(fh, hashval);\n\trcu_read_unlock();\n\tif (fp)\n\t\treturn fp;\n\n\tspin_lock(&state_lock);\n\tfp = find_file_locked(fh, hashval);\n\tif (likely(fp == NULL)) {\n\t\tnfsd4_init_file(fh, hashval, new);\n\t\tfp = new;\n\t}\n\tspin_unlock(&state_lock);\n\n\treturn fp;\n}\n\n/*\n * Called to check deny when READ with all zero stateid or\n * WRITE with all zero or all one stateid\n */\nstatic __be32\nnfs4_share_conflict(struct svc_fh *current_fh, unsigned int deny_type)\n{\n\tstruct nfs4_file *fp;\n\t__be32 ret = nfs_ok;\n\n\tfp = find_file(&current_fh->fh_handle);\n\tif (!fp)\n\t\treturn ret;\n\t/* Check for conflicting share reservations */\n\tspin_lock(&fp->fi_lock);\n\tif (fp->fi_share_deny & deny_type)\n\t\tret = nfserr_locked;\n\tspin_unlock(&fp->fi_lock);\n\tput_nfs4_file(fp);\n\treturn ret;\n}\n\nstatic void nfsd4_cb_recall_prepare(struct nfsd4_callback *cb)\n{\n\tstruct nfs4_delegation *dp = cb_to_delegation(cb);\n\tstruct nfsd_net *nn = net_generic(dp->dl_stid.sc_client->net,\n\t\t\t\t\t  nfsd_net_id);\n\n\tblock_delegations(&dp->dl_stid.sc_file->fi_fhandle);\n\n\t/*\n\t * We can't do this in nfsd_break_deleg_cb because it is\n\t * already holding inode->i_lock.\n\t *\n\t * If the dl_time != 0, then we know that it has already been\n\t * queued for a lease break. Don't queue it again.\n\t */\n\tspin_lock(&state_lock);\n\tif (dp->dl_time == 0) {\n\t\tdp->dl_time = get_seconds();\n\t\tlist_add_tail(&dp->dl_recall_lru, &nn->del_recall_lru);\n\t}\n\tspin_unlock(&state_lock);\n}\n\nstatic int nfsd4_cb_recall_done(struct nfsd4_callback *cb,\n\t\tstruct rpc_task *task)\n{\n\tstruct nfs4_delegation *dp = cb_to_delegation(cb);\n\n\tif (dp->dl_stid.sc_type == NFS4_CLOSED_DELEG_STID)\n\t        return 1;\n\n\tswitch (task->tk_status) {\n\tcase 0:\n\t\treturn 1;\n\tcase -EBADHANDLE:\n\tcase -NFS4ERR_BAD_STATEID:\n\t\t/*\n\t\t * Race: client probably got cb_recall before open reply\n\t\t * granting delegation.\n\t\t */\n\t\tif (dp->dl_retries--) {\n\t\t\trpc_delay(task, 2 * HZ);\n\t\t\treturn 0;\n\t\t}\n\t\t/*FALLTHRU*/\n\tdefault:\n\t\treturn -1;\n\t}\n}\n\nstatic void nfsd4_cb_recall_release(struct nfsd4_callback *cb)\n{\n\tstruct nfs4_delegation *dp = cb_to_delegation(cb);\n\n\tnfs4_put_stid(&dp->dl_stid);\n}\n\nstatic const struct nfsd4_callback_ops nfsd4_cb_recall_ops = {\n\t.prepare\t= nfsd4_cb_recall_prepare,\n\t.done\t\t= nfsd4_cb_recall_done,\n\t.release\t= nfsd4_cb_recall_release,\n};\n\nstatic void nfsd_break_one_deleg(struct nfs4_delegation *dp)\n{\n\t/*\n\t * We're assuming the state code never drops its reference\n\t * without first removing the lease.  Since we're in this lease\n\t * callback (and since the lease code is serialized by the kernel\n\t * lock) we know the server hasn't removed the lease yet, we know\n\t * it's safe to take a reference.\n\t */\n\tatomic_inc(&dp->dl_stid.sc_count);\n\tnfsd4_run_cb(&dp->dl_recall);\n}\n\n/* Called from break_lease() with i_lock held. */\nstatic bool\nnfsd_break_deleg_cb(struct file_lock *fl)\n{\n\tbool ret = false;\n\tstruct nfs4_file *fp = (struct nfs4_file *)fl->fl_owner;\n\tstruct nfs4_delegation *dp;\n\n\tif (!fp) {\n\t\tWARN(1, \"(%p)->fl_owner NULL\\n\", fl);\n\t\treturn ret;\n\t}\n\tif (fp->fi_had_conflict) {\n\t\tWARN(1, \"duplicate break on %p\\n\", fp);\n\t\treturn ret;\n\t}\n\t/*\n\t * We don't want the locks code to timeout the lease for us;\n\t * we'll remove it ourself if a delegation isn't returned\n\t * in time:\n\t */\n\tfl->fl_break_time = 0;\n\n\tspin_lock(&fp->fi_lock);\n\tfp->fi_had_conflict = true;\n\t/*\n\t * If there are no delegations on the list, then return true\n\t * so that the lease code will go ahead and delete it.\n\t */\n\tif (list_empty(&fp->fi_delegations))\n\t\tret = true;\n\telse\n\t\tlist_for_each_entry(dp, &fp->fi_delegations, dl_perfile)\n\t\t\tnfsd_break_one_deleg(dp);\n\tspin_unlock(&fp->fi_lock);\n\treturn ret;\n}\n\nstatic int\nnfsd_change_deleg_cb(struct file_lock *onlist, int arg,\n\t\t     struct list_head *dispose)\n{\n\tif (arg & F_UNLCK)\n\t\treturn lease_modify(onlist, arg, dispose);\n\telse\n\t\treturn -EAGAIN;\n}\n\nstatic const struct lock_manager_operations nfsd_lease_mng_ops = {\n\t.lm_break = nfsd_break_deleg_cb,\n\t.lm_change = nfsd_change_deleg_cb,\n};\n\nstatic __be32 nfsd4_check_seqid(struct nfsd4_compound_state *cstate, struct nfs4_stateowner *so, u32 seqid)\n{\n\tif (nfsd4_has_session(cstate))\n\t\treturn nfs_ok;\n\tif (seqid == so->so_seqid - 1)\n\t\treturn nfserr_replay_me;\n\tif (seqid == so->so_seqid)\n\t\treturn nfs_ok;\n\treturn nfserr_bad_seqid;\n}\n\nstatic __be32 lookup_clientid(clientid_t *clid,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd_net *nn)\n{\n\tstruct nfs4_client *found;\n\n\tif (cstate->clp) {\n\t\tfound = cstate->clp;\n\t\tif (!same_clid(&found->cl_clientid, clid))\n\t\t\treturn nfserr_stale_clientid;\n\t\treturn nfs_ok;\n\t}\n\n\tif (STALE_CLIENTID(clid, nn))\n\t\treturn nfserr_stale_clientid;\n\n\t/*\n\t * For v4.1+ we get the client in the SEQUENCE op. If we don't have one\n\t * cached already then we know this is for is for v4.0 and \"sessions\"\n\t * will be false.\n\t */\n\tWARN_ON_ONCE(cstate->session);\n\tspin_lock(&nn->client_lock);\n\tfound = find_confirmed_client(clid, false, nn);\n\tif (!found) {\n\t\tspin_unlock(&nn->client_lock);\n\t\treturn nfserr_expired;\n\t}\n\tatomic_inc(&found->cl_refcount);\n\tspin_unlock(&nn->client_lock);\n\n\t/* Cache the nfs4_client in cstate! */\n\tcstate->clp = found;\n\treturn nfs_ok;\n}\n\n__be32\nnfsd4_process_open1(struct nfsd4_compound_state *cstate,\n\t\t    struct nfsd4_open *open, struct nfsd_net *nn)\n{\n\tclientid_t *clientid = &open->op_clientid;\n\tstruct nfs4_client *clp = NULL;\n\tunsigned int strhashval;\n\tstruct nfs4_openowner *oo = NULL;\n\t__be32 status;\n\n\tif (STALE_CLIENTID(&open->op_clientid, nn))\n\t\treturn nfserr_stale_clientid;\n\t/*\n\t * In case we need it later, after we've already created the\n\t * file and don't want to risk a further failure:\n\t */\n\topen->op_file = nfsd4_alloc_file();\n\tif (open->op_file == NULL)\n\t\treturn nfserr_jukebox;\n\n\tstatus = lookup_clientid(clientid, cstate, nn);\n\tif (status)\n\t\treturn status;\n\tclp = cstate->clp;\n\n\tstrhashval = ownerstr_hashval(&open->op_owner);\n\too = find_openstateowner_str(strhashval, open, clp);\n\topen->op_openowner = oo;\n\tif (!oo) {\n\t\tgoto new_owner;\n\t}\n\tif (!(oo->oo_flags & NFS4_OO_CONFIRMED)) {\n\t\t/* Replace unconfirmed owners without checking for replay. */\n\t\trelease_openowner(oo);\n\t\topen->op_openowner = NULL;\n\t\tgoto new_owner;\n\t}\n\tstatus = nfsd4_check_seqid(cstate, &oo->oo_owner, open->op_seqid);\n\tif (status)\n\t\treturn status;\n\tgoto alloc_stateid;\nnew_owner:\n\too = alloc_init_open_stateowner(strhashval, open, cstate);\n\tif (oo == NULL)\n\t\treturn nfserr_jukebox;\n\topen->op_openowner = oo;\nalloc_stateid:\n\topen->op_stp = nfs4_alloc_open_stateid(clp);\n\tif (!open->op_stp)\n\t\treturn nfserr_jukebox;\n\n\tif (nfsd4_has_session(cstate) &&\n\t    (cstate->current_fh.fh_export->ex_flags & NFSEXP_PNFS)) {\n\t\topen->op_odstate = alloc_clnt_odstate(clp);\n\t\tif (!open->op_odstate)\n\t\t\treturn nfserr_jukebox;\n\t}\n\n\treturn nfs_ok;\n}\n\nstatic inline __be32\nnfs4_check_delegmode(struct nfs4_delegation *dp, int flags)\n{\n\tif ((flags & WR_STATE) && (dp->dl_type == NFS4_OPEN_DELEGATE_READ))\n\t\treturn nfserr_openmode;\n\telse\n\t\treturn nfs_ok;\n}\n\nstatic int share_access_to_flags(u32 share_access)\n{\n\treturn share_access == NFS4_SHARE_ACCESS_READ ? RD_STATE : WR_STATE;\n}\n\nstatic struct nfs4_delegation *find_deleg_stateid(struct nfs4_client *cl, stateid_t *s)\n{\n\tstruct nfs4_stid *ret;\n\n\tret = find_stateid_by_type(cl, s, NFS4_DELEG_STID);\n\tif (!ret)\n\t\treturn NULL;\n\treturn delegstateid(ret);\n}\n\nstatic bool nfsd4_is_deleg_cur(struct nfsd4_open *open)\n{\n\treturn open->op_claim_type == NFS4_OPEN_CLAIM_DELEGATE_CUR ||\n\t       open->op_claim_type == NFS4_OPEN_CLAIM_DELEG_CUR_FH;\n}\n\nstatic __be32\nnfs4_check_deleg(struct nfs4_client *cl, struct nfsd4_open *open,\n\t\tstruct nfs4_delegation **dp)\n{\n\tint flags;\n\t__be32 status = nfserr_bad_stateid;\n\tstruct nfs4_delegation *deleg;\n\n\tdeleg = find_deleg_stateid(cl, &open->op_delegate_stateid);\n\tif (deleg == NULL)\n\t\tgoto out;\n\tflags = share_access_to_flags(open->op_share_access);\n\tstatus = nfs4_check_delegmode(deleg, flags);\n\tif (status) {\n\t\tnfs4_put_stid(&deleg->dl_stid);\n\t\tgoto out;\n\t}\n\t*dp = deleg;\nout:\n\tif (!nfsd4_is_deleg_cur(open))\n\t\treturn nfs_ok;\n\tif (status)\n\t\treturn status;\n\topen->op_openowner->oo_flags |= NFS4_OO_CONFIRMED;\n\treturn nfs_ok;\n}\n\nstatic inline int nfs4_access_to_access(u32 nfs4_access)\n{\n\tint flags = 0;\n\n\tif (nfs4_access & NFS4_SHARE_ACCESS_READ)\n\t\tflags |= NFSD_MAY_READ;\n\tif (nfs4_access & NFS4_SHARE_ACCESS_WRITE)\n\t\tflags |= NFSD_MAY_WRITE;\n\treturn flags;\n}\n\nstatic inline __be32\nnfsd4_truncate(struct svc_rqst *rqstp, struct svc_fh *fh,\n\t\tstruct nfsd4_open *open)\n{\n\tstruct iattr iattr = {\n\t\t.ia_valid = ATTR_SIZE,\n\t\t.ia_size = 0,\n\t};\n\tif (!open->op_truncate)\n\t\treturn 0;\n\tif (!(open->op_share_access & NFS4_SHARE_ACCESS_WRITE))\n\t\treturn nfserr_inval;\n\treturn nfsd_setattr(rqstp, fh, &iattr, 0, (time_t)0);\n}\n\nstatic __be32 nfs4_get_vfs_file(struct svc_rqst *rqstp, struct nfs4_file *fp,\n\t\tstruct svc_fh *cur_fh, struct nfs4_ol_stateid *stp,\n\t\tstruct nfsd4_open *open)\n{\n\tstruct file *filp = NULL;\n\t__be32 status;\n\tint oflag = nfs4_access_to_omode(open->op_share_access);\n\tint access = nfs4_access_to_access(open->op_share_access);\n\tunsigned char old_access_bmap, old_deny_bmap;\n\n\tspin_lock(&fp->fi_lock);\n\n\t/*\n\t * Are we trying to set a deny mode that would conflict with\n\t * current access?\n\t */\n\tstatus = nfs4_file_check_deny(fp, open->op_share_deny);\n\tif (status != nfs_ok) {\n\t\tspin_unlock(&fp->fi_lock);\n\t\tgoto out;\n\t}\n\n\t/* set access to the file */\n\tstatus = nfs4_file_get_access(fp, open->op_share_access);\n\tif (status != nfs_ok) {\n\t\tspin_unlock(&fp->fi_lock);\n\t\tgoto out;\n\t}\n\n\t/* Set access bits in stateid */\n\told_access_bmap = stp->st_access_bmap;\n\tset_access(open->op_share_access, stp);\n\n\t/* Set new deny mask */\n\told_deny_bmap = stp->st_deny_bmap;\n\tset_deny(open->op_share_deny, stp);\n\tfp->fi_share_deny |= (open->op_share_deny & NFS4_SHARE_DENY_BOTH);\n\n\tif (!fp->fi_fds[oflag]) {\n\t\tspin_unlock(&fp->fi_lock);\n\t\tstatus = nfsd_open(rqstp, cur_fh, S_IFREG, access, &filp);\n\t\tif (status)\n\t\t\tgoto out_put_access;\n\t\tspin_lock(&fp->fi_lock);\n\t\tif (!fp->fi_fds[oflag]) {\n\t\t\tfp->fi_fds[oflag] = filp;\n\t\t\tfilp = NULL;\n\t\t}\n\t}\n\tspin_unlock(&fp->fi_lock);\n\tif (filp)\n\t\tfput(filp);\n\n\tstatus = nfsd4_truncate(rqstp, cur_fh, open);\n\tif (status)\n\t\tgoto out_put_access;\nout:\n\treturn status;\nout_put_access:\n\tstp->st_access_bmap = old_access_bmap;\n\tnfs4_file_put_access(fp, open->op_share_access);\n\treset_union_bmap_deny(bmap_to_share_mode(old_deny_bmap), stp);\n\tgoto out;\n}\n\nstatic __be32\nnfs4_upgrade_open(struct svc_rqst *rqstp, struct nfs4_file *fp, struct svc_fh *cur_fh, struct nfs4_ol_stateid *stp, struct nfsd4_open *open)\n{\n\t__be32 status;\n\tunsigned char old_deny_bmap = stp->st_deny_bmap;\n\n\tif (!test_access(open->op_share_access, stp))\n\t\treturn nfs4_get_vfs_file(rqstp, fp, cur_fh, stp, open);\n\n\t/* test and set deny mode */\n\tspin_lock(&fp->fi_lock);\n\tstatus = nfs4_file_check_deny(fp, open->op_share_deny);\n\tif (status == nfs_ok) {\n\t\tset_deny(open->op_share_deny, stp);\n\t\tfp->fi_share_deny |=\n\t\t\t\t(open->op_share_deny & NFS4_SHARE_DENY_BOTH);\n\t}\n\tspin_unlock(&fp->fi_lock);\n\n\tif (status != nfs_ok)\n\t\treturn status;\n\n\tstatus = nfsd4_truncate(rqstp, cur_fh, open);\n\tif (status != nfs_ok)\n\t\treset_union_bmap_deny(old_deny_bmap, stp);\n\treturn status;\n}\n\n/* Should we give out recallable state?: */\nstatic bool nfsd4_cb_channel_good(struct nfs4_client *clp)\n{\n\tif (clp->cl_cb_state == NFSD4_CB_UP)\n\t\treturn true;\n\t/*\n\t * In the sessions case, since we don't have to establish a\n\t * separate connection for callbacks, we assume it's OK\n\t * until we hear otherwise:\n\t */\n\treturn clp->cl_minorversion && clp->cl_cb_state == NFSD4_CB_UNKNOWN;\n}\n\nstatic struct file_lock *nfs4_alloc_init_lease(struct nfs4_file *fp, int flag)\n{\n\tstruct file_lock *fl;\n\n\tfl = locks_alloc_lock();\n\tif (!fl)\n\t\treturn NULL;\n\tfl->fl_lmops = &nfsd_lease_mng_ops;\n\tfl->fl_flags = FL_DELEG;\n\tfl->fl_type = flag == NFS4_OPEN_DELEGATE_READ? F_RDLCK: F_WRLCK;\n\tfl->fl_end = OFFSET_MAX;\n\tfl->fl_owner = (fl_owner_t)fp;\n\tfl->fl_pid = current->tgid;\n\treturn fl;\n}\n\n/**\n * nfs4_setlease - Obtain a delegation by requesting lease from vfs layer\n * @dp:   a pointer to the nfs4_delegation we're adding.\n *\n * Return:\n *      On success: Return code will be 0 on success.\n *\n *      On error: -EAGAIN if there was an existing delegation.\n *                 nonzero if there is an error in other cases.\n *\n */\n\nstatic int nfs4_setlease(struct nfs4_delegation *dp)\n{\n\tstruct nfs4_file *fp = dp->dl_stid.sc_file;\n\tstruct file_lock *fl;\n\tstruct file *filp;\n\tint status = 0;\n\n\tfl = nfs4_alloc_init_lease(fp, NFS4_OPEN_DELEGATE_READ);\n\tif (!fl)\n\t\treturn -ENOMEM;\n\tfilp = find_readable_file(fp);\n\tif (!filp) {\n\t\t/* We should always have a readable file here */\n\t\tWARN_ON_ONCE(1);\n\t\tlocks_free_lock(fl);\n\t\treturn -EBADF;\n\t}\n\tfl->fl_file = filp;\n\tstatus = vfs_setlease(filp, fl->fl_type, &fl, NULL);\n\tif (fl)\n\t\tlocks_free_lock(fl);\n\tif (status)\n\t\tgoto out_fput;\n\tspin_lock(&state_lock);\n\tspin_lock(&fp->fi_lock);\n\t/* Did the lease get broken before we took the lock? */\n\tstatus = -EAGAIN;\n\tif (fp->fi_had_conflict)\n\t\tgoto out_unlock;\n\t/* Race breaker */\n\tif (fp->fi_deleg_file) {\n\t\tstatus = hash_delegation_locked(dp, fp);\n\t\tgoto out_unlock;\n\t}\n\tfp->fi_deleg_file = filp;\n\tfp->fi_delegees = 0;\n\tstatus = hash_delegation_locked(dp, fp);\n\tspin_unlock(&fp->fi_lock);\n\tspin_unlock(&state_lock);\n\tif (status) {\n\t\t/* Should never happen, this is a new fi_deleg_file  */\n\t\tWARN_ON_ONCE(1);\n\t\tgoto out_fput;\n\t}\n\treturn 0;\nout_unlock:\n\tspin_unlock(&fp->fi_lock);\n\tspin_unlock(&state_lock);\nout_fput:\n\tfput(filp);\n\treturn status;\n}\n\nstatic struct nfs4_delegation *\nnfs4_set_delegation(struct nfs4_client *clp, struct svc_fh *fh,\n\t\t    struct nfs4_file *fp, struct nfs4_clnt_odstate *odstate)\n{\n\tint status;\n\tstruct nfs4_delegation *dp;\n\n\tif (fp->fi_had_conflict)\n\t\treturn ERR_PTR(-EAGAIN);\n\n\tspin_lock(&state_lock);\n\tspin_lock(&fp->fi_lock);\n\tstatus = nfs4_get_existing_delegation(clp, fp);\n\tspin_unlock(&fp->fi_lock);\n\tspin_unlock(&state_lock);\n\n\tif (status)\n\t\treturn ERR_PTR(status);\n\n\tdp = alloc_init_deleg(clp, fh, odstate);\n\tif (!dp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tget_nfs4_file(fp);\n\tspin_lock(&state_lock);\n\tspin_lock(&fp->fi_lock);\n\tdp->dl_stid.sc_file = fp;\n\tif (!fp->fi_deleg_file) {\n\t\tspin_unlock(&fp->fi_lock);\n\t\tspin_unlock(&state_lock);\n\t\tstatus = nfs4_setlease(dp);\n\t\tgoto out;\n\t}\n\tif (fp->fi_had_conflict) {\n\t\tstatus = -EAGAIN;\n\t\tgoto out_unlock;\n\t}\n\tstatus = hash_delegation_locked(dp, fp);\nout_unlock:\n\tspin_unlock(&fp->fi_lock);\n\tspin_unlock(&state_lock);\nout:\n\tif (status) {\n\t\tput_clnt_odstate(dp->dl_clnt_odstate);\n\t\tnfs4_put_stid(&dp->dl_stid);\n\t\treturn ERR_PTR(status);\n\t}\n\treturn dp;\n}\n\nstatic void nfsd4_open_deleg_none_ext(struct nfsd4_open *open, int status)\n{\n\topen->op_delegate_type = NFS4_OPEN_DELEGATE_NONE_EXT;\n\tif (status == -EAGAIN)\n\t\topen->op_why_no_deleg = WND4_CONTENTION;\n\telse {\n\t\topen->op_why_no_deleg = WND4_RESOURCE;\n\t\tswitch (open->op_deleg_want) {\n\t\tcase NFS4_SHARE_WANT_READ_DELEG:\n\t\tcase NFS4_SHARE_WANT_WRITE_DELEG:\n\t\tcase NFS4_SHARE_WANT_ANY_DELEG:\n\t\t\tbreak;\n\t\tcase NFS4_SHARE_WANT_CANCEL:\n\t\t\topen->op_why_no_deleg = WND4_CANCELLED;\n\t\t\tbreak;\n\t\tcase NFS4_SHARE_WANT_NO_DELEG:\n\t\t\tWARN_ON_ONCE(1);\n\t\t}\n\t}\n}\n\n/*\n * Attempt to hand out a delegation.\n *\n * Note we don't support write delegations, and won't until the vfs has\n * proper support for them.\n */\nstatic void\nnfs4_open_delegation(struct svc_fh *fh, struct nfsd4_open *open,\n\t\t\tstruct nfs4_ol_stateid *stp)\n{\n\tstruct nfs4_delegation *dp;\n\tstruct nfs4_openowner *oo = openowner(stp->st_stateowner);\n\tstruct nfs4_client *clp = stp->st_stid.sc_client;\n\tint cb_up;\n\tint status = 0;\n\n\tcb_up = nfsd4_cb_channel_good(oo->oo_owner.so_client);\n\topen->op_recall = 0;\n\tswitch (open->op_claim_type) {\n\t\tcase NFS4_OPEN_CLAIM_PREVIOUS:\n\t\t\tif (!cb_up)\n\t\t\t\topen->op_recall = 1;\n\t\t\tif (open->op_delegate_type != NFS4_OPEN_DELEGATE_READ)\n\t\t\t\tgoto out_no_deleg;\n\t\t\tbreak;\n\t\tcase NFS4_OPEN_CLAIM_NULL:\n\t\tcase NFS4_OPEN_CLAIM_FH:\n\t\t\t/*\n\t\t\t * Let's not give out any delegations till everyone's\n\t\t\t * had the chance to reclaim theirs, *and* until\n\t\t\t * NLM locks have all been reclaimed:\n\t\t\t */\n\t\t\tif (locks_in_grace(clp->net))\n\t\t\t\tgoto out_no_deleg;\n\t\t\tif (!cb_up || !(oo->oo_flags & NFS4_OO_CONFIRMED))\n\t\t\t\tgoto out_no_deleg;\n\t\t\t/*\n\t\t\t * Also, if the file was opened for write or\n\t\t\t * create, there's a good chance the client's\n\t\t\t * about to write to it, resulting in an\n\t\t\t * immediate recall (since we don't support\n\t\t\t * write delegations):\n\t\t\t */\n\t\t\tif (open->op_share_access & NFS4_SHARE_ACCESS_WRITE)\n\t\t\t\tgoto out_no_deleg;\n\t\t\tif (open->op_create == NFS4_OPEN_CREATE)\n\t\t\t\tgoto out_no_deleg;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto out_no_deleg;\n\t}\n\tdp = nfs4_set_delegation(clp, fh, stp->st_stid.sc_file, stp->st_clnt_odstate);\n\tif (IS_ERR(dp))\n\t\tgoto out_no_deleg;\n\n\tmemcpy(&open->op_delegate_stateid, &dp->dl_stid.sc_stateid, sizeof(dp->dl_stid.sc_stateid));\n\n\tdprintk(\"NFSD: delegation stateid=\" STATEID_FMT \"\\n\",\n\t\tSTATEID_VAL(&dp->dl_stid.sc_stateid));\n\topen->op_delegate_type = NFS4_OPEN_DELEGATE_READ;\n\tnfs4_put_stid(&dp->dl_stid);\n\treturn;\nout_no_deleg:\n\topen->op_delegate_type = NFS4_OPEN_DELEGATE_NONE;\n\tif (open->op_claim_type == NFS4_OPEN_CLAIM_PREVIOUS &&\n\t    open->op_delegate_type != NFS4_OPEN_DELEGATE_NONE) {\n\t\tdprintk(\"NFSD: WARNING: refusing delegation reclaim\\n\");\n\t\topen->op_recall = 1;\n\t}\n\n\t/* 4.1 client asking for a delegation? */\n\tif (open->op_deleg_want)\n\t\tnfsd4_open_deleg_none_ext(open, status);\n\treturn;\n}\n\nstatic void nfsd4_deleg_xgrade_none_ext(struct nfsd4_open *open,\n\t\t\t\t\tstruct nfs4_delegation *dp)\n{\n\tif (open->op_deleg_want == NFS4_SHARE_WANT_READ_DELEG &&\n\t    dp->dl_type == NFS4_OPEN_DELEGATE_WRITE) {\n\t\topen->op_delegate_type = NFS4_OPEN_DELEGATE_NONE_EXT;\n\t\topen->op_why_no_deleg = WND4_NOT_SUPP_DOWNGRADE;\n\t} else if (open->op_deleg_want == NFS4_SHARE_WANT_WRITE_DELEG &&\n\t\t   dp->dl_type == NFS4_OPEN_DELEGATE_WRITE) {\n\t\topen->op_delegate_type = NFS4_OPEN_DELEGATE_NONE_EXT;\n\t\topen->op_why_no_deleg = WND4_NOT_SUPP_UPGRADE;\n\t}\n\t/* Otherwise the client must be confused wanting a delegation\n\t * it already has, therefore we don't return\n\t * NFS4_OPEN_DELEGATE_NONE_EXT and reason.\n\t */\n}\n\n__be32\nnfsd4_process_open2(struct svc_rqst *rqstp, struct svc_fh *current_fh, struct nfsd4_open *open)\n{\n\tstruct nfsd4_compoundres *resp = rqstp->rq_resp;\n\tstruct nfs4_client *cl = open->op_openowner->oo_owner.so_client;\n\tstruct nfs4_file *fp = NULL;\n\tstruct nfs4_ol_stateid *stp = NULL;\n\tstruct nfs4_delegation *dp = NULL;\n\t__be32 status;\n\n\t/*\n\t * Lookup file; if found, lookup stateid and check open request,\n\t * and check for delegations in the process of being recalled.\n\t * If not found, create the nfs4_file struct\n\t */\n\tfp = find_or_add_file(open->op_file, &current_fh->fh_handle);\n\tif (fp != open->op_file) {\n\t\tstatus = nfs4_check_deleg(cl, open, &dp);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tspin_lock(&fp->fi_lock);\n\t\tstp = nfsd4_find_existing_open(fp, open);\n\t\tspin_unlock(&fp->fi_lock);\n\t} else {\n\t\topen->op_file = NULL;\n\t\tstatus = nfserr_bad_stateid;\n\t\tif (nfsd4_is_deleg_cur(open))\n\t\t\tgoto out;\n\t}\n\n\t/*\n\t * OPEN the file, or upgrade an existing OPEN.\n\t * If truncate fails, the OPEN fails.\n\t */\n\tif (stp) {\n\t\t/* Stateid was found, this is an OPEN upgrade */\n\t\tmutex_lock(&stp->st_mutex);\n\t\tstatus = nfs4_upgrade_open(rqstp, fp, current_fh, stp, open);\n\t\tif (status) {\n\t\t\tmutex_unlock(&stp->st_mutex);\n\t\t\tgoto out;\n\t\t}\n\t} else {\n\t\t/* stp is returned locked. */\n\t\tstp = init_open_stateid(fp, open);\n\t\t/* See if we lost the race to some other thread */\n\t\tif (stp->st_access_bmap != 0) {\n\t\t\tstatus = nfs4_upgrade_open(rqstp, fp, current_fh,\n\t\t\t\t\t\tstp, open);\n\t\t\tif (status) {\n\t\t\t\tmutex_unlock(&stp->st_mutex);\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tgoto upgrade_out;\n\t\t}\n\t\tstatus = nfs4_get_vfs_file(rqstp, fp, current_fh, stp, open);\n\t\tif (status) {\n\t\t\tmutex_unlock(&stp->st_mutex);\n\t\t\trelease_open_stateid(stp);\n\t\t\tgoto out;\n\t\t}\n\n\t\tstp->st_clnt_odstate = find_or_hash_clnt_odstate(fp,\n\t\t\t\t\t\t\topen->op_odstate);\n\t\tif (stp->st_clnt_odstate == open->op_odstate)\n\t\t\topen->op_odstate = NULL;\n\t}\nupgrade_out:\n\tnfs4_inc_and_copy_stateid(&open->op_stateid, &stp->st_stid);\n\tmutex_unlock(&stp->st_mutex);\n\n\tif (nfsd4_has_session(&resp->cstate)) {\n\t\tif (open->op_deleg_want & NFS4_SHARE_WANT_NO_DELEG) {\n\t\t\topen->op_delegate_type = NFS4_OPEN_DELEGATE_NONE_EXT;\n\t\t\topen->op_why_no_deleg = WND4_NOT_WANTED;\n\t\t\tgoto nodeleg;\n\t\t}\n\t}\n\n\t/*\n\t* Attempt to hand out a delegation. No error return, because the\n\t* OPEN succeeds even if we fail.\n\t*/\n\tnfs4_open_delegation(current_fh, open, stp);\nnodeleg:\n\tstatus = nfs_ok;\n\n\tdprintk(\"%s: stateid=\" STATEID_FMT \"\\n\", __func__,\n\t\tSTATEID_VAL(&stp->st_stid.sc_stateid));\nout:\n\t/* 4.1 client trying to upgrade/downgrade delegation? */\n\tif (open->op_delegate_type == NFS4_OPEN_DELEGATE_NONE && dp &&\n\t    open->op_deleg_want)\n\t\tnfsd4_deleg_xgrade_none_ext(open, dp);\n\n\tif (fp)\n\t\tput_nfs4_file(fp);\n\tif (status == 0 && open->op_claim_type == NFS4_OPEN_CLAIM_PREVIOUS)\n\t\topen->op_openowner->oo_flags |= NFS4_OO_CONFIRMED;\n\t/*\n\t* To finish the open response, we just need to set the rflags.\n\t*/\n\topen->op_rflags = NFS4_OPEN_RESULT_LOCKTYPE_POSIX;\n\tif (nfsd4_has_session(&resp->cstate))\n\t\topen->op_rflags |= NFS4_OPEN_RESULT_MAY_NOTIFY_LOCK;\n\telse if (!(open->op_openowner->oo_flags & NFS4_OO_CONFIRMED))\n\t\topen->op_rflags |= NFS4_OPEN_RESULT_CONFIRM;\n\n\tif (dp)\n\t\tnfs4_put_stid(&dp->dl_stid);\n\tif (stp)\n\t\tnfs4_put_stid(&stp->st_stid);\n\n\treturn status;\n}\n\nvoid nfsd4_cleanup_open_state(struct nfsd4_compound_state *cstate,\n\t\t\t      struct nfsd4_open *open)\n{\n\tif (open->op_openowner) {\n\t\tstruct nfs4_stateowner *so = &open->op_openowner->oo_owner;\n\n\t\tnfsd4_cstate_assign_replay(cstate, so);\n\t\tnfs4_put_stateowner(so);\n\t}\n\tif (open->op_file)\n\t\tkmem_cache_free(file_slab, open->op_file);\n\tif (open->op_stp)\n\t\tnfs4_put_stid(&open->op_stp->st_stid);\n\tif (open->op_odstate)\n\t\tkmem_cache_free(odstate_slab, open->op_odstate);\n}\n\n__be32\nnfsd4_renew(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    clientid_t *clid)\n{\n\tstruct nfs4_client *clp;\n\t__be32 status;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tdprintk(\"process_renew(%08x/%08x): starting\\n\", \n\t\t\tclid->cl_boot, clid->cl_id);\n\tstatus = lookup_clientid(clid, cstate, nn);\n\tif (status)\n\t\tgoto out;\n\tclp = cstate->clp;\n\tstatus = nfserr_cb_path_down;\n\tif (!list_empty(&clp->cl_delegations)\n\t\t\t&& clp->cl_cb_state != NFSD4_CB_UP)\n\t\tgoto out;\n\tstatus = nfs_ok;\nout:\n\treturn status;\n}\n\nvoid\nnfsd4_end_grace(struct nfsd_net *nn)\n{\n\t/* do nothing if grace period already ended */\n\tif (nn->grace_ended)\n\t\treturn;\n\n\tdprintk(\"NFSD: end of grace period\\n\");\n\tnn->grace_ended = true;\n\t/*\n\t * If the server goes down again right now, an NFSv4\n\t * client will still be allowed to reclaim after it comes back up,\n\t * even if it hasn't yet had a chance to reclaim state this time.\n\t *\n\t */\n\tnfsd4_record_grace_done(nn);\n\t/*\n\t * At this point, NFSv4 clients can still reclaim.  But if the\n\t * server crashes, any that have not yet reclaimed will be out\n\t * of luck on the next boot.\n\t *\n\t * (NFSv4.1+ clients are considered to have reclaimed once they\n\t * call RECLAIM_COMPLETE.  NFSv4.0 clients are considered to\n\t * have reclaimed after their first OPEN.)\n\t */\n\tlocks_end_grace(&nn->nfsd4_manager);\n\t/*\n\t * At this point, and once lockd and/or any other containers\n\t * exit their grace period, further reclaims will fail and\n\t * regular locking can resume.\n\t */\n}\n\nstatic time_t\nnfs4_laundromat(struct nfsd_net *nn)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfs4_openowner *oo;\n\tstruct nfs4_delegation *dp;\n\tstruct nfs4_ol_stateid *stp;\n\tstruct nfsd4_blocked_lock *nbl;\n\tstruct list_head *pos, *next, reaplist;\n\ttime_t cutoff = get_seconds() - nn->nfsd4_lease;\n\ttime_t t, new_timeo = nn->nfsd4_lease;\n\n\tdprintk(\"NFSD: laundromat service - starting\\n\");\n\tnfsd4_end_grace(nn);\n\tINIT_LIST_HEAD(&reaplist);\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_safe(pos, next, &nn->client_lru) {\n\t\tclp = list_entry(pos, struct nfs4_client, cl_lru);\n\t\tif (time_after((unsigned long)clp->cl_time, (unsigned long)cutoff)) {\n\t\t\tt = clp->cl_time - cutoff;\n\t\t\tnew_timeo = min(new_timeo, t);\n\t\t\tbreak;\n\t\t}\n\t\tif (mark_client_expired_locked(clp)) {\n\t\t\tdprintk(\"NFSD: client in use (clientid %08x)\\n\",\n\t\t\t\tclp->cl_clientid.cl_id);\n\t\t\tcontinue;\n\t\t}\n\t\tlist_add(&clp->cl_lru, &reaplist);\n\t}\n\tspin_unlock(&nn->client_lock);\n\tlist_for_each_safe(pos, next, &reaplist) {\n\t\tclp = list_entry(pos, struct nfs4_client, cl_lru);\n\t\tdprintk(\"NFSD: purging unused client (clientid %08x)\\n\",\n\t\t\tclp->cl_clientid.cl_id);\n\t\tlist_del_init(&clp->cl_lru);\n\t\texpire_client(clp);\n\t}\n\tspin_lock(&state_lock);\n\tlist_for_each_safe(pos, next, &nn->del_recall_lru) {\n\t\tdp = list_entry (pos, struct nfs4_delegation, dl_recall_lru);\n\t\tif (time_after((unsigned long)dp->dl_time, (unsigned long)cutoff)) {\n\t\t\tt = dp->dl_time - cutoff;\n\t\t\tnew_timeo = min(new_timeo, t);\n\t\t\tbreak;\n\t\t}\n\t\tWARN_ON(!unhash_delegation_locked(dp));\n\t\tlist_add(&dp->dl_recall_lru, &reaplist);\n\t}\n\tspin_unlock(&state_lock);\n\twhile (!list_empty(&reaplist)) {\n\t\tdp = list_first_entry(&reaplist, struct nfs4_delegation,\n\t\t\t\t\tdl_recall_lru);\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\trevoke_delegation(dp);\n\t}\n\n\tspin_lock(&nn->client_lock);\n\twhile (!list_empty(&nn->close_lru)) {\n\t\too = list_first_entry(&nn->close_lru, struct nfs4_openowner,\n\t\t\t\t\too_close_lru);\n\t\tif (time_after((unsigned long)oo->oo_time,\n\t\t\t       (unsigned long)cutoff)) {\n\t\t\tt = oo->oo_time - cutoff;\n\t\t\tnew_timeo = min(new_timeo, t);\n\t\t\tbreak;\n\t\t}\n\t\tlist_del_init(&oo->oo_close_lru);\n\t\tstp = oo->oo_last_closed_stid;\n\t\too->oo_last_closed_stid = NULL;\n\t\tspin_unlock(&nn->client_lock);\n\t\tnfs4_put_stid(&stp->st_stid);\n\t\tspin_lock(&nn->client_lock);\n\t}\n\tspin_unlock(&nn->client_lock);\n\n\t/*\n\t * It's possible for a client to try and acquire an already held lock\n\t * that is being held for a long time, and then lose interest in it.\n\t * So, we clean out any un-revisited request after a lease period\n\t * under the assumption that the client is no longer interested.\n\t *\n\t * RFC5661, sec. 9.6 states that the client must not rely on getting\n\t * notifications and must continue to poll for locks, even when the\n\t * server supports them. Thus this shouldn't lead to clients blocking\n\t * indefinitely once the lock does become free.\n\t */\n\tBUG_ON(!list_empty(&reaplist));\n\tspin_lock(&nn->blocked_locks_lock);\n\twhile (!list_empty(&nn->blocked_locks_lru)) {\n\t\tnbl = list_first_entry(&nn->blocked_locks_lru,\n\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_lru);\n\t\tif (time_after((unsigned long)nbl->nbl_time,\n\t\t\t       (unsigned long)cutoff)) {\n\t\t\tt = nbl->nbl_time - cutoff;\n\t\t\tnew_timeo = min(new_timeo, t);\n\t\t\tbreak;\n\t\t}\n\t\tlist_move(&nbl->nbl_lru, &reaplist);\n\t\tlist_del_init(&nbl->nbl_list);\n\t}\n\tspin_unlock(&nn->blocked_locks_lock);\n\n\twhile (!list_empty(&reaplist)) {\n\t\tnbl = list_first_entry(&nn->blocked_locks_lru,\n\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_lru);\n\t\tlist_del_init(&nbl->nbl_lru);\n\t\tposix_unblock_lock(&nbl->nbl_lock);\n\t\tfree_blocked_lock(nbl);\n\t}\n\n\tnew_timeo = max_t(time_t, new_timeo, NFSD_LAUNDROMAT_MINTIMEOUT);\n\treturn new_timeo;\n}\n\nstatic struct workqueue_struct *laundry_wq;\nstatic void laundromat_main(struct work_struct *);\n\nstatic void\nlaundromat_main(struct work_struct *laundry)\n{\n\ttime_t t;\n\tstruct delayed_work *dwork = to_delayed_work(laundry);\n\tstruct nfsd_net *nn = container_of(dwork, struct nfsd_net,\n\t\t\t\t\t   laundromat_work);\n\n\tt = nfs4_laundromat(nn);\n\tdprintk(\"NFSD: laundromat_main - sleeping for %ld seconds\\n\", t);\n\tqueue_delayed_work(laundry_wq, &nn->laundromat_work, t*HZ);\n}\n\nstatic inline __be32 nfs4_check_fh(struct svc_fh *fhp, struct nfs4_stid *stp)\n{\n\tif (!fh_match(&fhp->fh_handle, &stp->sc_file->fi_fhandle))\n\t\treturn nfserr_bad_stateid;\n\treturn nfs_ok;\n}\n\nstatic inline int\naccess_permit_read(struct nfs4_ol_stateid *stp)\n{\n\treturn test_access(NFS4_SHARE_ACCESS_READ, stp) ||\n\t\ttest_access(NFS4_SHARE_ACCESS_BOTH, stp) ||\n\t\ttest_access(NFS4_SHARE_ACCESS_WRITE, stp);\n}\n\nstatic inline int\naccess_permit_write(struct nfs4_ol_stateid *stp)\n{\n\treturn test_access(NFS4_SHARE_ACCESS_WRITE, stp) ||\n\t\ttest_access(NFS4_SHARE_ACCESS_BOTH, stp);\n}\n\nstatic\n__be32 nfs4_check_openmode(struct nfs4_ol_stateid *stp, int flags)\n{\n        __be32 status = nfserr_openmode;\n\n\t/* For lock stateid's, we test the parent open, not the lock: */\n\tif (stp->st_openstp)\n\t\tstp = stp->st_openstp;\n\tif ((flags & WR_STATE) && !access_permit_write(stp))\n                goto out;\n\tif ((flags & RD_STATE) && !access_permit_read(stp))\n                goto out;\n\tstatus = nfs_ok;\nout:\n\treturn status;\n}\n\nstatic inline __be32\ncheck_special_stateids(struct net *net, svc_fh *current_fh, stateid_t *stateid, int flags)\n{\n\tif (ONE_STATEID(stateid) && (flags & RD_STATE))\n\t\treturn nfs_ok;\n\telse if (opens_in_grace(net)) {\n\t\t/* Answer in remaining cases depends on existence of\n\t\t * conflicting state; so we must wait out the grace period. */\n\t\treturn nfserr_grace;\n\t} else if (flags & WR_STATE)\n\t\treturn nfs4_share_conflict(current_fh,\n\t\t\t\tNFS4_SHARE_DENY_WRITE);\n\telse /* (flags & RD_STATE) && ZERO_STATEID(stateid) */\n\t\treturn nfs4_share_conflict(current_fh,\n\t\t\t\tNFS4_SHARE_DENY_READ);\n}\n\n/*\n * Allow READ/WRITE during grace period on recovered state only for files\n * that are not able to provide mandatory locking.\n */\nstatic inline int\ngrace_disallows_io(struct net *net, struct inode *inode)\n{\n\treturn opens_in_grace(net) && mandatory_lock(inode);\n}\n\nstatic __be32 check_stateid_generation(stateid_t *in, stateid_t *ref, bool has_session)\n{\n\t/*\n\t * When sessions are used the stateid generation number is ignored\n\t * when it is zero.\n\t */\n\tif (has_session && in->si_generation == 0)\n\t\treturn nfs_ok;\n\n\tif (in->si_generation == ref->si_generation)\n\t\treturn nfs_ok;\n\n\t/* If the client sends us a stateid from the future, it's buggy: */\n\tif (nfsd4_stateid_generation_after(in, ref))\n\t\treturn nfserr_bad_stateid;\n\t/*\n\t * However, we could see a stateid from the past, even from a\n\t * non-buggy client.  For example, if the client sends a lock\n\t * while some IO is outstanding, the lock may bump si_generation\n\t * while the IO is still in flight.  The client could avoid that\n\t * situation by waiting for responses on all the IO requests,\n\t * but better performance may result in retrying IO that\n\t * receives an old_stateid error if requests are rarely\n\t * reordered in flight:\n\t */\n\treturn nfserr_old_stateid;\n}\n\nstatic __be32 nfsd4_check_openowner_confirmed(struct nfs4_ol_stateid *ols)\n{\n\tif (ols->st_stateowner->so_is_open_owner &&\n\t    !(openowner(ols->st_stateowner)->oo_flags & NFS4_OO_CONFIRMED))\n\t\treturn nfserr_bad_stateid;\n\treturn nfs_ok;\n}\n\nstatic __be32 nfsd4_validate_stateid(struct nfs4_client *cl, stateid_t *stateid)\n{\n\tstruct nfs4_stid *s;\n\t__be32 status = nfserr_bad_stateid;\n\n\tif (ZERO_STATEID(stateid) || ONE_STATEID(stateid))\n\t\treturn status;\n\t/* Client debugging aid. */\n\tif (!same_clid(&stateid->si_opaque.so_clid, &cl->cl_clientid)) {\n\t\tchar addr_str[INET6_ADDRSTRLEN];\n\t\trpc_ntop((struct sockaddr *)&cl->cl_addr, addr_str,\n\t\t\t\t sizeof(addr_str));\n\t\tpr_warn_ratelimited(\"NFSD: client %s testing state ID \"\n\t\t\t\t\t\"with incorrect client ID\\n\", addr_str);\n\t\treturn status;\n\t}\n\tspin_lock(&cl->cl_lock);\n\ts = find_stateid_locked(cl, stateid);\n\tif (!s)\n\t\tgoto out_unlock;\n\tstatus = check_stateid_generation(stateid, &s->sc_stateid, 1);\n\tif (status)\n\t\tgoto out_unlock;\n\tswitch (s->sc_type) {\n\tcase NFS4_DELEG_STID:\n\t\tstatus = nfs_ok;\n\t\tbreak;\n\tcase NFS4_REVOKED_DELEG_STID:\n\t\tstatus = nfserr_deleg_revoked;\n\t\tbreak;\n\tcase NFS4_OPEN_STID:\n\tcase NFS4_LOCK_STID:\n\t\tstatus = nfsd4_check_openowner_confirmed(openlockstateid(s));\n\t\tbreak;\n\tdefault:\n\t\tprintk(\"unknown stateid type %x\\n\", s->sc_type);\n\t\t/* Fallthrough */\n\tcase NFS4_CLOSED_STID:\n\tcase NFS4_CLOSED_DELEG_STID:\n\t\tstatus = nfserr_bad_stateid;\n\t}\nout_unlock:\n\tspin_unlock(&cl->cl_lock);\n\treturn status;\n}\n\n__be32\nnfsd4_lookup_stateid(struct nfsd4_compound_state *cstate,\n\t\t     stateid_t *stateid, unsigned char typemask,\n\t\t     struct nfs4_stid **s, struct nfsd_net *nn)\n{\n\t__be32 status;\n\n\tif (ZERO_STATEID(stateid) || ONE_STATEID(stateid))\n\t\treturn nfserr_bad_stateid;\n\tstatus = lookup_clientid(&stateid->si_opaque.so_clid, cstate, nn);\n\tif (status == nfserr_stale_clientid) {\n\t\tif (cstate->session)\n\t\t\treturn nfserr_bad_stateid;\n\t\treturn nfserr_stale_stateid;\n\t}\n\tif (status)\n\t\treturn status;\n\t*s = find_stateid_by_type(cstate->clp, stateid, typemask);\n\tif (!*s)\n\t\treturn nfserr_bad_stateid;\n\treturn nfs_ok;\n}\n\nstatic struct file *\nnfs4_find_file(struct nfs4_stid *s, int flags)\n{\n\tif (!s)\n\t\treturn NULL;\n\n\tswitch (s->sc_type) {\n\tcase NFS4_DELEG_STID:\n\t\tif (WARN_ON_ONCE(!s->sc_file->fi_deleg_file))\n\t\t\treturn NULL;\n\t\treturn get_file(s->sc_file->fi_deleg_file);\n\tcase NFS4_OPEN_STID:\n\tcase NFS4_LOCK_STID:\n\t\tif (flags & RD_STATE)\n\t\t\treturn find_readable_file(s->sc_file);\n\t\telse\n\t\t\treturn find_writeable_file(s->sc_file);\n\t\tbreak;\n\t}\n\n\treturn NULL;\n}\n\nstatic __be32\nnfs4_check_olstateid(struct svc_fh *fhp, struct nfs4_ol_stateid *ols, int flags)\n{\n\t__be32 status;\n\n\tstatus = nfsd4_check_openowner_confirmed(ols);\n\tif (status)\n\t\treturn status;\n\treturn nfs4_check_openmode(ols, flags);\n}\n\nstatic __be32\nnfs4_check_file(struct svc_rqst *rqstp, struct svc_fh *fhp, struct nfs4_stid *s,\n\t\tstruct file **filpp, bool *tmp_file, int flags)\n{\n\tint acc = (flags & RD_STATE) ? NFSD_MAY_READ : NFSD_MAY_WRITE;\n\tstruct file *file;\n\t__be32 status;\n\n\tfile = nfs4_find_file(s, flags);\n\tif (file) {\n\t\tstatus = nfsd_permission(rqstp, fhp->fh_export, fhp->fh_dentry,\n\t\t\t\tacc | NFSD_MAY_OWNER_OVERRIDE);\n\t\tif (status) {\n\t\t\tfput(file);\n\t\t\treturn status;\n\t\t}\n\n\t\t*filpp = file;\n\t} else {\n\t\tstatus = nfsd_open(rqstp, fhp, S_IFREG, acc, filpp);\n\t\tif (status)\n\t\t\treturn status;\n\n\t\tif (tmp_file)\n\t\t\t*tmp_file = true;\n\t}\n\n\treturn 0;\n}\n\n/*\n * Checks for stateid operations\n */\n__be32\nnfs4_preprocess_stateid_op(struct svc_rqst *rqstp,\n\t\tstruct nfsd4_compound_state *cstate, struct svc_fh *fhp,\n\t\tstateid_t *stateid, int flags, struct file **filpp, bool *tmp_file)\n{\n\tstruct inode *ino = d_inode(fhp->fh_dentry);\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\tstruct nfs4_stid *s = NULL;\n\t__be32 status;\n\n\tif (filpp)\n\t\t*filpp = NULL;\n\tif (tmp_file)\n\t\t*tmp_file = false;\n\n\tif (grace_disallows_io(net, ino))\n\t\treturn nfserr_grace;\n\n\tif (ZERO_STATEID(stateid) || ONE_STATEID(stateid)) {\n\t\tstatus = check_special_stateids(net, fhp, stateid, flags);\n\t\tgoto done;\n\t}\n\n\tstatus = nfsd4_lookup_stateid(cstate, stateid,\n\t\t\t\tNFS4_DELEG_STID|NFS4_OPEN_STID|NFS4_LOCK_STID,\n\t\t\t\t&s, nn);\n\tif (status)\n\t\treturn status;\n\tstatus = check_stateid_generation(stateid, &s->sc_stateid,\n\t\t\tnfsd4_has_session(cstate));\n\tif (status)\n\t\tgoto out;\n\n\tswitch (s->sc_type) {\n\tcase NFS4_DELEG_STID:\n\t\tstatus = nfs4_check_delegmode(delegstateid(s), flags);\n\t\tbreak;\n\tcase NFS4_OPEN_STID:\n\tcase NFS4_LOCK_STID:\n\t\tstatus = nfs4_check_olstateid(fhp, openlockstateid(s), flags);\n\t\tbreak;\n\tdefault:\n\t\tstatus = nfserr_bad_stateid;\n\t\tbreak;\n\t}\n\tif (status)\n\t\tgoto out;\n\tstatus = nfs4_check_fh(fhp, s);\n\ndone:\n\tif (!status && filpp)\n\t\tstatus = nfs4_check_file(rqstp, fhp, s, filpp, tmp_file, flags);\nout:\n\tif (s)\n\t\tnfs4_put_stid(s);\n\treturn status;\n}\n\n/*\n * Test if the stateid is valid\n */\n__be32\nnfsd4_test_stateid(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t   struct nfsd4_test_stateid *test_stateid)\n{\n\tstruct nfsd4_test_stateid_id *stateid;\n\tstruct nfs4_client *cl = cstate->session->se_client;\n\n\tlist_for_each_entry(stateid, &test_stateid->ts_stateid_list, ts_id_list)\n\t\tstateid->ts_id_status =\n\t\t\tnfsd4_validate_stateid(cl, &stateid->ts_id_stateid);\n\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_free_lock_stateid(stateid_t *stateid, struct nfs4_stid *s)\n{\n\tstruct nfs4_ol_stateid *stp = openlockstateid(s);\n\t__be32 ret;\n\n\tmutex_lock(&stp->st_mutex);\n\n\tret = check_stateid_generation(stateid, &s->sc_stateid, 1);\n\tif (ret)\n\t\tgoto out;\n\n\tret = nfserr_locks_held;\n\tif (check_for_locks(stp->st_stid.sc_file,\n\t\t\t    lockowner(stp->st_stateowner)))\n\t\tgoto out;\n\n\trelease_lock_stateid(stp);\n\tret = nfs_ok;\n\nout:\n\tmutex_unlock(&stp->st_mutex);\n\tnfs4_put_stid(s);\n\treturn ret;\n}\n\n__be32\nnfsd4_free_stateid(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t   struct nfsd4_free_stateid *free_stateid)\n{\n\tstateid_t *stateid = &free_stateid->fr_stateid;\n\tstruct nfs4_stid *s;\n\tstruct nfs4_delegation *dp;\n\tstruct nfs4_client *cl = cstate->session->se_client;\n\t__be32 ret = nfserr_bad_stateid;\n\n\tspin_lock(&cl->cl_lock);\n\ts = find_stateid_locked(cl, stateid);\n\tif (!s)\n\t\tgoto out_unlock;\n\tswitch (s->sc_type) {\n\tcase NFS4_DELEG_STID:\n\t\tret = nfserr_locks_held;\n\t\tbreak;\n\tcase NFS4_OPEN_STID:\n\t\tret = check_stateid_generation(stateid, &s->sc_stateid, 1);\n\t\tif (ret)\n\t\t\tbreak;\n\t\tret = nfserr_locks_held;\n\t\tbreak;\n\tcase NFS4_LOCK_STID:\n\t\tatomic_inc(&s->sc_count);\n\t\tspin_unlock(&cl->cl_lock);\n\t\tret = nfsd4_free_lock_stateid(stateid, s);\n\t\tgoto out;\n\tcase NFS4_REVOKED_DELEG_STID:\n\t\tdp = delegstateid(s);\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tspin_unlock(&cl->cl_lock);\n\t\tnfs4_put_stid(s);\n\t\tret = nfs_ok;\n\t\tgoto out;\n\t/* Default falls through and returns nfserr_bad_stateid */\n\t}\nout_unlock:\n\tspin_unlock(&cl->cl_lock);\nout:\n\treturn ret;\n}\n\nstatic inline int\nsetlkflg (int type)\n{\n\treturn (type == NFS4_READW_LT || type == NFS4_READ_LT) ?\n\t\tRD_STATE : WR_STATE;\n}\n\nstatic __be32 nfs4_seqid_op_checks(struct nfsd4_compound_state *cstate, stateid_t *stateid, u32 seqid, struct nfs4_ol_stateid *stp)\n{\n\tstruct svc_fh *current_fh = &cstate->current_fh;\n\tstruct nfs4_stateowner *sop = stp->st_stateowner;\n\t__be32 status;\n\n\tstatus = nfsd4_check_seqid(cstate, sop, seqid);\n\tif (status)\n\t\treturn status;\n\tif (stp->st_stid.sc_type == NFS4_CLOSED_STID\n\t\t|| stp->st_stid.sc_type == NFS4_REVOKED_DELEG_STID)\n\t\t/*\n\t\t * \"Closed\" stateid's exist *only* to return\n\t\t * nfserr_replay_me from the previous step, and\n\t\t * revoked delegations are kept only for free_stateid.\n\t\t */\n\t\treturn nfserr_bad_stateid;\n\tmutex_lock(&stp->st_mutex);\n\tstatus = check_stateid_generation(stateid, &stp->st_stid.sc_stateid, nfsd4_has_session(cstate));\n\tif (status == nfs_ok)\n\t\tstatus = nfs4_check_fh(current_fh, &stp->st_stid);\n\tif (status != nfs_ok)\n\t\tmutex_unlock(&stp->st_mutex);\n\treturn status;\n}\n\n/* \n * Checks for sequence id mutating operations. \n */\nstatic __be32\nnfs4_preprocess_seqid_op(struct nfsd4_compound_state *cstate, u32 seqid,\n\t\t\t stateid_t *stateid, char typemask,\n\t\t\t struct nfs4_ol_stateid **stpp,\n\t\t\t struct nfsd_net *nn)\n{\n\t__be32 status;\n\tstruct nfs4_stid *s;\n\tstruct nfs4_ol_stateid *stp = NULL;\n\n\tdprintk(\"NFSD: %s: seqid=%d stateid = \" STATEID_FMT \"\\n\", __func__,\n\t\tseqid, STATEID_VAL(stateid));\n\n\t*stpp = NULL;\n\tstatus = nfsd4_lookup_stateid(cstate, stateid, typemask, &s, nn);\n\tif (status)\n\t\treturn status;\n\tstp = openlockstateid(s);\n\tnfsd4_cstate_assign_replay(cstate, stp->st_stateowner);\n\n\tstatus = nfs4_seqid_op_checks(cstate, stateid, seqid, stp);\n\tif (!status)\n\t\t*stpp = stp;\n\telse\n\t\tnfs4_put_stid(&stp->st_stid);\n\treturn status;\n}\n\nstatic __be32 nfs4_preprocess_confirmed_seqid_op(struct nfsd4_compound_state *cstate, u32 seqid,\n\t\t\t\t\t\t stateid_t *stateid, struct nfs4_ol_stateid **stpp, struct nfsd_net *nn)\n{\n\t__be32 status;\n\tstruct nfs4_openowner *oo;\n\tstruct nfs4_ol_stateid *stp;\n\n\tstatus = nfs4_preprocess_seqid_op(cstate, seqid, stateid,\n\t\t\t\t\t\tNFS4_OPEN_STID, &stp, nn);\n\tif (status)\n\t\treturn status;\n\too = openowner(stp->st_stateowner);\n\tif (!(oo->oo_flags & NFS4_OO_CONFIRMED)) {\n\t\tmutex_unlock(&stp->st_mutex);\n\t\tnfs4_put_stid(&stp->st_stid);\n\t\treturn nfserr_bad_stateid;\n\t}\n\t*stpp = stp;\n\treturn nfs_ok;\n}\n\n__be32\nnfsd4_open_confirm(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t   struct nfsd4_open_confirm *oc)\n{\n\t__be32 status;\n\tstruct nfs4_openowner *oo;\n\tstruct nfs4_ol_stateid *stp;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_open_confirm on file %pd\\n\",\n\t\t\tcstate->current_fh.fh_dentry);\n\n\tstatus = fh_verify(rqstp, &cstate->current_fh, S_IFREG, 0);\n\tif (status)\n\t\treturn status;\n\n\tstatus = nfs4_preprocess_seqid_op(cstate,\n\t\t\t\t\toc->oc_seqid, &oc->oc_req_stateid,\n\t\t\t\t\tNFS4_OPEN_STID, &stp, nn);\n\tif (status)\n\t\tgoto out;\n\too = openowner(stp->st_stateowner);\n\tstatus = nfserr_bad_stateid;\n\tif (oo->oo_flags & NFS4_OO_CONFIRMED) {\n\t\tmutex_unlock(&stp->st_mutex);\n\t\tgoto put_stateid;\n\t}\n\too->oo_flags |= NFS4_OO_CONFIRMED;\n\tnfs4_inc_and_copy_stateid(&oc->oc_resp_stateid, &stp->st_stid);\n\tmutex_unlock(&stp->st_mutex);\n\tdprintk(\"NFSD: %s: success, seqid=%d stateid=\" STATEID_FMT \"\\n\",\n\t\t__func__, oc->oc_seqid, STATEID_VAL(&stp->st_stid.sc_stateid));\n\n\tnfsd4_client_record_create(oo->oo_owner.so_client);\n\tstatus = nfs_ok;\nput_stateid:\n\tnfs4_put_stid(&stp->st_stid);\nout:\n\tnfsd4_bump_seqid(cstate, status);\n\treturn status;\n}\n\nstatic inline void nfs4_stateid_downgrade_bit(struct nfs4_ol_stateid *stp, u32 access)\n{\n\tif (!test_access(access, stp))\n\t\treturn;\n\tnfs4_file_put_access(stp->st_stid.sc_file, access);\n\tclear_access(access, stp);\n}\n\nstatic inline void nfs4_stateid_downgrade(struct nfs4_ol_stateid *stp, u32 to_access)\n{\n\tswitch (to_access) {\n\tcase NFS4_SHARE_ACCESS_READ:\n\t\tnfs4_stateid_downgrade_bit(stp, NFS4_SHARE_ACCESS_WRITE);\n\t\tnfs4_stateid_downgrade_bit(stp, NFS4_SHARE_ACCESS_BOTH);\n\t\tbreak;\n\tcase NFS4_SHARE_ACCESS_WRITE:\n\t\tnfs4_stateid_downgrade_bit(stp, NFS4_SHARE_ACCESS_READ);\n\t\tnfs4_stateid_downgrade_bit(stp, NFS4_SHARE_ACCESS_BOTH);\n\t\tbreak;\n\tcase NFS4_SHARE_ACCESS_BOTH:\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n}\n\n__be32\nnfsd4_open_downgrade(struct svc_rqst *rqstp,\n\t\t     struct nfsd4_compound_state *cstate,\n\t\t     struct nfsd4_open_downgrade *od)\n{\n\t__be32 status;\n\tstruct nfs4_ol_stateid *stp;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_open_downgrade on file %pd\\n\", \n\t\t\tcstate->current_fh.fh_dentry);\n\n\t/* We don't yet support WANT bits: */\n\tif (od->od_deleg_want)\n\t\tdprintk(\"NFSD: %s: od_deleg_want=0x%x ignored\\n\", __func__,\n\t\t\tod->od_deleg_want);\n\n\tstatus = nfs4_preprocess_confirmed_seqid_op(cstate, od->od_seqid,\n\t\t\t\t\t&od->od_stateid, &stp, nn);\n\tif (status)\n\t\tgoto out; \n\tstatus = nfserr_inval;\n\tif (!test_access(od->od_share_access, stp)) {\n\t\tdprintk(\"NFSD: access not a subset of current bitmap: 0x%hhx, input access=%08x\\n\",\n\t\t\tstp->st_access_bmap, od->od_share_access);\n\t\tgoto put_stateid;\n\t}\n\tif (!test_deny(od->od_share_deny, stp)) {\n\t\tdprintk(\"NFSD: deny not a subset of current bitmap: 0x%hhx, input deny=%08x\\n\",\n\t\t\tstp->st_deny_bmap, od->od_share_deny);\n\t\tgoto put_stateid;\n\t}\n\tnfs4_stateid_downgrade(stp, od->od_share_access);\n\treset_union_bmap_deny(od->od_share_deny, stp);\n\tnfs4_inc_and_copy_stateid(&od->od_stateid, &stp->st_stid);\n\tstatus = nfs_ok;\nput_stateid:\n\tmutex_unlock(&stp->st_mutex);\n\tnfs4_put_stid(&stp->st_stid);\nout:\n\tnfsd4_bump_seqid(cstate, status);\n\treturn status;\n}\n\nstatic void nfsd4_close_open_stateid(struct nfs4_ol_stateid *s)\n{\n\tstruct nfs4_client *clp = s->st_stid.sc_client;\n\tbool unhashed;\n\tLIST_HEAD(reaplist);\n\n\ts->st_stid.sc_type = NFS4_CLOSED_STID;\n\tspin_lock(&clp->cl_lock);\n\tunhashed = unhash_open_stateid(s, &reaplist);\n\n\tif (clp->cl_minorversion) {\n\t\tif (unhashed)\n\t\t\tput_ol_stateid_locked(s, &reaplist);\n\t\tspin_unlock(&clp->cl_lock);\n\t\tfree_ol_stateid_reaplist(&reaplist);\n\t} else {\n\t\tspin_unlock(&clp->cl_lock);\n\t\tfree_ol_stateid_reaplist(&reaplist);\n\t\tif (unhashed)\n\t\t\tmove_to_close_lru(s, clp->net);\n\t}\n}\n\n/*\n * nfs4_unlock_state() called after encode\n */\n__be32\nnfsd4_close(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct nfsd4_close *close)\n{\n\t__be32 status;\n\tstruct nfs4_ol_stateid *stp;\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_close on file %pd\\n\", \n\t\t\tcstate->current_fh.fh_dentry);\n\n\tstatus = nfs4_preprocess_seqid_op(cstate, close->cl_seqid,\n\t\t\t\t\t&close->cl_stateid,\n\t\t\t\t\tNFS4_OPEN_STID|NFS4_CLOSED_STID,\n\t\t\t\t\t&stp, nn);\n\tnfsd4_bump_seqid(cstate, status);\n\tif (status)\n\t\tgoto out; \n\tnfs4_inc_and_copy_stateid(&close->cl_stateid, &stp->st_stid);\n\tmutex_unlock(&stp->st_mutex);\n\n\tnfsd4_close_open_stateid(stp);\n\n\t/* put reference from nfs4_preprocess_seqid_op */\n\tnfs4_put_stid(&stp->st_stid);\nout:\n\treturn status;\n}\n\n__be32\nnfsd4_delegreturn(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t\t  struct nfsd4_delegreturn *dr)\n{\n\tstruct nfs4_delegation *dp;\n\tstateid_t *stateid = &dr->dr_stateid;\n\tstruct nfs4_stid *s;\n\t__be32 status;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tif ((status = fh_verify(rqstp, &cstate->current_fh, S_IFREG, 0)))\n\t\treturn status;\n\n\tstatus = nfsd4_lookup_stateid(cstate, stateid, NFS4_DELEG_STID, &s, nn);\n\tif (status)\n\t\tgoto out;\n\tdp = delegstateid(s);\n\tstatus = check_stateid_generation(stateid, &dp->dl_stid.sc_stateid, nfsd4_has_session(cstate));\n\tif (status)\n\t\tgoto put_stateid;\n\n\tdestroy_delegation(dp);\nput_stateid:\n\tnfs4_put_stid(&dp->dl_stid);\nout:\n\treturn status;\n}\n\nstatic inline u64\nend_offset(u64 start, u64 len)\n{\n\tu64 end;\n\n\tend = start + len;\n\treturn end >= start ? end: NFS4_MAX_UINT64;\n}\n\n/* last octet in a range */\nstatic inline u64\nlast_byte_offset(u64 start, u64 len)\n{\n\tu64 end;\n\n\tWARN_ON_ONCE(!len);\n\tend = start + len;\n\treturn end > start ? end - 1: NFS4_MAX_UINT64;\n}\n\n/*\n * TODO: Linux file offsets are _signed_ 64-bit quantities, which means that\n * we can't properly handle lock requests that go beyond the (2^63 - 1)-th\n * byte, because of sign extension problems.  Since NFSv4 calls for 64-bit\n * locking, this prevents us from being completely protocol-compliant.  The\n * real solution to this problem is to start using unsigned file offsets in\n * the VFS, but this is a very deep change!\n */\nstatic inline void\nnfs4_transform_lock_offset(struct file_lock *lock)\n{\n\tif (lock->fl_start < 0)\n\t\tlock->fl_start = OFFSET_MAX;\n\tif (lock->fl_end < 0)\n\t\tlock->fl_end = OFFSET_MAX;\n}\n\nstatic fl_owner_t\nnfsd4_fl_get_owner(fl_owner_t owner)\n{\n\tstruct nfs4_lockowner *lo = (struct nfs4_lockowner *)owner;\n\n\tnfs4_get_stateowner(&lo->lo_owner);\n\treturn owner;\n}\n\nstatic void\nnfsd4_fl_put_owner(fl_owner_t owner)\n{\n\tstruct nfs4_lockowner *lo = (struct nfs4_lockowner *)owner;\n\n\tif (lo)\n\t\tnfs4_put_stateowner(&lo->lo_owner);\n}\n\nstatic void\nnfsd4_lm_notify(struct file_lock *fl)\n{\n\tstruct nfs4_lockowner\t\t*lo = (struct nfs4_lockowner *)fl->fl_owner;\n\tstruct net\t\t\t*net = lo->lo_owner.so_client->net;\n\tstruct nfsd_net\t\t\t*nn = net_generic(net, nfsd_net_id);\n\tstruct nfsd4_blocked_lock\t*nbl = container_of(fl,\n\t\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_lock);\n\tbool queue = false;\n\n\t/* An empty list means that something else is going to be using it */\n\tspin_lock(&nn->blocked_locks_lock);\n\tif (!list_empty(&nbl->nbl_list)) {\n\t\tlist_del_init(&nbl->nbl_list);\n\t\tlist_del_init(&nbl->nbl_lru);\n\t\tqueue = true;\n\t}\n\tspin_unlock(&nn->blocked_locks_lock);\n\n\tif (queue)\n\t\tnfsd4_run_cb(&nbl->nbl_cb);\n}\n\nstatic const struct lock_manager_operations nfsd_posix_mng_ops  = {\n\t.lm_notify = nfsd4_lm_notify,\n\t.lm_get_owner = nfsd4_fl_get_owner,\n\t.lm_put_owner = nfsd4_fl_put_owner,\n};\n\nstatic inline void\nnfs4_set_lock_denied(struct file_lock *fl, struct nfsd4_lock_denied *deny)\n{\n\tstruct nfs4_lockowner *lo;\n\n\tif (fl->fl_lmops == &nfsd_posix_mng_ops) {\n\t\tlo = (struct nfs4_lockowner *) fl->fl_owner;\n\t\tdeny->ld_owner.data = kmemdup(lo->lo_owner.so_owner.data,\n\t\t\t\t\tlo->lo_owner.so_owner.len, GFP_KERNEL);\n\t\tif (!deny->ld_owner.data)\n\t\t\t/* We just don't care that much */\n\t\t\tgoto nevermind;\n\t\tdeny->ld_owner.len = lo->lo_owner.so_owner.len;\n\t\tdeny->ld_clientid = lo->lo_owner.so_client->cl_clientid;\n\t} else {\nnevermind:\n\t\tdeny->ld_owner.len = 0;\n\t\tdeny->ld_owner.data = NULL;\n\t\tdeny->ld_clientid.cl_boot = 0;\n\t\tdeny->ld_clientid.cl_id = 0;\n\t}\n\tdeny->ld_start = fl->fl_start;\n\tdeny->ld_length = NFS4_MAX_UINT64;\n\tif (fl->fl_end != NFS4_MAX_UINT64)\n\t\tdeny->ld_length = fl->fl_end - fl->fl_start + 1;        \n\tdeny->ld_type = NFS4_READ_LT;\n\tif (fl->fl_type != F_RDLCK)\n\t\tdeny->ld_type = NFS4_WRITE_LT;\n}\n\nstatic struct nfs4_lockowner *\nfind_lockowner_str_locked(struct nfs4_client *clp, struct xdr_netobj *owner)\n{\n\tunsigned int strhashval = ownerstr_hashval(owner);\n\tstruct nfs4_stateowner *so;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_for_each_entry(so, &clp->cl_ownerstr_hashtbl[strhashval],\n\t\t\t    so_strhash) {\n\t\tif (so->so_is_open_owner)\n\t\t\tcontinue;\n\t\tif (same_owner_str(so, owner))\n\t\t\treturn lockowner(nfs4_get_stateowner(so));\n\t}\n\treturn NULL;\n}\n\nstatic struct nfs4_lockowner *\nfind_lockowner_str(struct nfs4_client *clp, struct xdr_netobj *owner)\n{\n\tstruct nfs4_lockowner *lo;\n\n\tspin_lock(&clp->cl_lock);\n\tlo = find_lockowner_str_locked(clp, owner);\n\tspin_unlock(&clp->cl_lock);\n\treturn lo;\n}\n\nstatic void nfs4_unhash_lockowner(struct nfs4_stateowner *sop)\n{\n\tunhash_lockowner_locked(lockowner(sop));\n}\n\nstatic void nfs4_free_lockowner(struct nfs4_stateowner *sop)\n{\n\tstruct nfs4_lockowner *lo = lockowner(sop);\n\n\tkmem_cache_free(lockowner_slab, lo);\n}\n\nstatic const struct nfs4_stateowner_operations lockowner_ops = {\n\t.so_unhash =\tnfs4_unhash_lockowner,\n\t.so_free =\tnfs4_free_lockowner,\n};\n\n/*\n * Alloc a lock owner structure.\n * Called in nfsd4_lock - therefore, OPEN and OPEN_CONFIRM (if needed) has \n * occurred. \n *\n * strhashval = ownerstr_hashval\n */\nstatic struct nfs4_lockowner *\nalloc_init_lock_stateowner(unsigned int strhashval, struct nfs4_client *clp,\n\t\t\t   struct nfs4_ol_stateid *open_stp,\n\t\t\t   struct nfsd4_lock *lock)\n{\n\tstruct nfs4_lockowner *lo, *ret;\n\n\tlo = alloc_stateowner(lockowner_slab, &lock->lk_new_owner, clp);\n\tif (!lo)\n\t\treturn NULL;\n\tINIT_LIST_HEAD(&lo->lo_blocked);\n\tINIT_LIST_HEAD(&lo->lo_owner.so_stateids);\n\tlo->lo_owner.so_is_open_owner = 0;\n\tlo->lo_owner.so_seqid = lock->lk_new_lock_seqid;\n\tlo->lo_owner.so_ops = &lockowner_ops;\n\tspin_lock(&clp->cl_lock);\n\tret = find_lockowner_str_locked(clp, &lock->lk_new_owner);\n\tif (ret == NULL) {\n\t\tlist_add(&lo->lo_owner.so_strhash,\n\t\t\t &clp->cl_ownerstr_hashtbl[strhashval]);\n\t\tret = lo;\n\t} else\n\t\tnfs4_free_stateowner(&lo->lo_owner);\n\n\tspin_unlock(&clp->cl_lock);\n\treturn ret;\n}\n\nstatic void\ninit_lock_stateid(struct nfs4_ol_stateid *stp, struct nfs4_lockowner *lo,\n\t\t  struct nfs4_file *fp, struct inode *inode,\n\t\t  struct nfs4_ol_stateid *open_stp)\n{\n\tstruct nfs4_client *clp = lo->lo_owner.so_client;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tatomic_inc(&stp->st_stid.sc_count);\n\tstp->st_stid.sc_type = NFS4_LOCK_STID;\n\tstp->st_stateowner = nfs4_get_stateowner(&lo->lo_owner);\n\tget_nfs4_file(fp);\n\tstp->st_stid.sc_file = fp;\n\tstp->st_access_bmap = 0;\n\tstp->st_deny_bmap = open_stp->st_deny_bmap;\n\tstp->st_openstp = open_stp;\n\tmutex_init(&stp->st_mutex);\n\tlist_add(&stp->st_locks, &open_stp->st_locks);\n\tlist_add(&stp->st_perstateowner, &lo->lo_owner.so_stateids);\n\tspin_lock(&fp->fi_lock);\n\tlist_add(&stp->st_perfile, &fp->fi_stateids);\n\tspin_unlock(&fp->fi_lock);\n}\n\nstatic struct nfs4_ol_stateid *\nfind_lock_stateid(struct nfs4_lockowner *lo, struct nfs4_file *fp)\n{\n\tstruct nfs4_ol_stateid *lst;\n\tstruct nfs4_client *clp = lo->lo_owner.so_client;\n\n\tlockdep_assert_held(&clp->cl_lock);\n\n\tlist_for_each_entry(lst, &lo->lo_owner.so_stateids, st_perstateowner) {\n\t\tif (lst->st_stid.sc_file == fp) {\n\t\t\tatomic_inc(&lst->st_stid.sc_count);\n\t\t\treturn lst;\n\t\t}\n\t}\n\treturn NULL;\n}\n\nstatic struct nfs4_ol_stateid *\nfind_or_create_lock_stateid(struct nfs4_lockowner *lo, struct nfs4_file *fi,\n\t\t\t    struct inode *inode, struct nfs4_ol_stateid *ost,\n\t\t\t    bool *new)\n{\n\tstruct nfs4_stid *ns = NULL;\n\tstruct nfs4_ol_stateid *lst;\n\tstruct nfs4_openowner *oo = openowner(ost->st_stateowner);\n\tstruct nfs4_client *clp = oo->oo_owner.so_client;\n\n\tspin_lock(&clp->cl_lock);\n\tlst = find_lock_stateid(lo, fi);\n\tif (lst == NULL) {\n\t\tspin_unlock(&clp->cl_lock);\n\t\tns = nfs4_alloc_stid(clp, stateid_slab, nfs4_free_lock_stateid);\n\t\tif (ns == NULL)\n\t\t\treturn NULL;\n\n\t\tspin_lock(&clp->cl_lock);\n\t\tlst = find_lock_stateid(lo, fi);\n\t\tif (likely(!lst)) {\n\t\t\tlst = openlockstateid(ns);\n\t\t\tinit_lock_stateid(lst, lo, fi, inode, ost);\n\t\t\tns = NULL;\n\t\t\t*new = true;\n\t\t}\n\t}\n\tspin_unlock(&clp->cl_lock);\n\tif (ns)\n\t\tnfs4_put_stid(ns);\n\treturn lst;\n}\n\nstatic int\ncheck_lock_length(u64 offset, u64 length)\n{\n\treturn ((length == 0) || ((length != NFS4_MAX_UINT64) &&\n\t\t(length > ~offset)));\n}\n\nstatic void get_lock_access(struct nfs4_ol_stateid *lock_stp, u32 access)\n{\n\tstruct nfs4_file *fp = lock_stp->st_stid.sc_file;\n\n\tlockdep_assert_held(&fp->fi_lock);\n\n\tif (test_access(access, lock_stp))\n\t\treturn;\n\t__nfs4_file_get_access(fp, access);\n\tset_access(access, lock_stp);\n}\n\nstatic __be32\nlookup_or_create_lock_state(struct nfsd4_compound_state *cstate,\n\t\t\t    struct nfs4_ol_stateid *ost,\n\t\t\t    struct nfsd4_lock *lock,\n\t\t\t    struct nfs4_ol_stateid **plst, bool *new)\n{\n\t__be32 status;\n\tstruct nfs4_file *fi = ost->st_stid.sc_file;\n\tstruct nfs4_openowner *oo = openowner(ost->st_stateowner);\n\tstruct nfs4_client *cl = oo->oo_owner.so_client;\n\tstruct inode *inode = d_inode(cstate->current_fh.fh_dentry);\n\tstruct nfs4_lockowner *lo;\n\tstruct nfs4_ol_stateid *lst;\n\tunsigned int strhashval;\n\tbool hashed;\n\n\tlo = find_lockowner_str(cl, &lock->lk_new_owner);\n\tif (!lo) {\n\t\tstrhashval = ownerstr_hashval(&lock->lk_new_owner);\n\t\tlo = alloc_init_lock_stateowner(strhashval, cl, ost, lock);\n\t\tif (lo == NULL)\n\t\t\treturn nfserr_jukebox;\n\t} else {\n\t\t/* with an existing lockowner, seqids must be the same */\n\t\tstatus = nfserr_bad_seqid;\n\t\tif (!cstate->minorversion &&\n\t\t    lock->lk_new_lock_seqid != lo->lo_owner.so_seqid)\n\t\t\tgoto out;\n\t}\n\nretry:\n\tlst = find_or_create_lock_stateid(lo, fi, inode, ost, new);\n\tif (lst == NULL) {\n\t\tstatus = nfserr_jukebox;\n\t\tgoto out;\n\t}\n\n\tmutex_lock(&lst->st_mutex);\n\n\t/* See if it's still hashed to avoid race with FREE_STATEID */\n\tspin_lock(&cl->cl_lock);\n\thashed = !list_empty(&lst->st_perfile);\n\tspin_unlock(&cl->cl_lock);\n\n\tif (!hashed) {\n\t\tmutex_unlock(&lst->st_mutex);\n\t\tnfs4_put_stid(&lst->st_stid);\n\t\tgoto retry;\n\t}\n\tstatus = nfs_ok;\n\t*plst = lst;\nout:\n\tnfs4_put_stateowner(&lo->lo_owner);\n\treturn status;\n}\n\n/*\n *  LOCK operation \n */\n__be32\nnfsd4_lock(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t   struct nfsd4_lock *lock)\n{\n\tstruct nfs4_openowner *open_sop = NULL;\n\tstruct nfs4_lockowner *lock_sop = NULL;\n\tstruct nfs4_ol_stateid *lock_stp = NULL;\n\tstruct nfs4_ol_stateid *open_stp = NULL;\n\tstruct nfs4_file *fp;\n\tstruct file *filp = NULL;\n\tstruct nfsd4_blocked_lock *nbl = NULL;\n\tstruct file_lock *file_lock = NULL;\n\tstruct file_lock *conflock = NULL;\n\t__be32 status = 0;\n\tint lkflg;\n\tint err;\n\tbool new = false;\n\tunsigned char fl_type;\n\tunsigned int fl_flags = FL_POSIX;\n\tstruct net *net = SVC_NET(rqstp);\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_lock: start=%Ld length=%Ld\\n\",\n\t\t(long long) lock->lk_offset,\n\t\t(long long) lock->lk_length);\n\n\tif (check_lock_length(lock->lk_offset, lock->lk_length))\n\t\t return nfserr_inval;\n\n\tif ((status = fh_verify(rqstp, &cstate->current_fh,\n\t\t\t\tS_IFREG, NFSD_MAY_LOCK))) {\n\t\tdprintk(\"NFSD: nfsd4_lock: permission denied!\\n\");\n\t\treturn status;\n\t}\n\n\tif (lock->lk_is_new) {\n\t\tif (nfsd4_has_session(cstate))\n\t\t\t/* See rfc 5661 18.10.3: given clientid is ignored: */\n\t\t\tmemcpy(&lock->lk_new_clientid,\n\t\t\t\t&cstate->session->se_client->cl_clientid,\n\t\t\t\tsizeof(clientid_t));\n\n\t\tstatus = nfserr_stale_clientid;\n\t\tif (STALE_CLIENTID(&lock->lk_new_clientid, nn))\n\t\t\tgoto out;\n\n\t\t/* validate and update open stateid and open seqid */\n\t\tstatus = nfs4_preprocess_confirmed_seqid_op(cstate,\n\t\t\t\t        lock->lk_new_open_seqid,\n\t\t                        &lock->lk_new_open_stateid,\n\t\t\t\t\t&open_stp, nn);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tmutex_unlock(&open_stp->st_mutex);\n\t\topen_sop = openowner(open_stp->st_stateowner);\n\t\tstatus = nfserr_bad_stateid;\n\t\tif (!same_clid(&open_sop->oo_owner.so_client->cl_clientid,\n\t\t\t\t\t\t&lock->lk_new_clientid))\n\t\t\tgoto out;\n\t\tstatus = lookup_or_create_lock_state(cstate, open_stp, lock,\n\t\t\t\t\t\t\t&lock_stp, &new);\n\t} else {\n\t\tstatus = nfs4_preprocess_seqid_op(cstate,\n\t\t\t\t       lock->lk_old_lock_seqid,\n\t\t\t\t       &lock->lk_old_lock_stateid,\n\t\t\t\t       NFS4_LOCK_STID, &lock_stp, nn);\n\t}\n\tif (status)\n\t\tgoto out;\n\tlock_sop = lockowner(lock_stp->st_stateowner);\n\n\tlkflg = setlkflg(lock->lk_type);\n\tstatus = nfs4_check_openmode(lock_stp, lkflg);\n\tif (status)\n\t\tgoto out;\n\n\tstatus = nfserr_grace;\n\tif (locks_in_grace(net) && !lock->lk_reclaim)\n\t\tgoto out;\n\tstatus = nfserr_no_grace;\n\tif (!locks_in_grace(net) && lock->lk_reclaim)\n\t\tgoto out;\n\n\tfp = lock_stp->st_stid.sc_file;\n\tswitch (lock->lk_type) {\n\t\tcase NFS4_READW_LT:\n\t\t\tif (nfsd4_has_session(cstate))\n\t\t\t\tfl_flags |= FL_SLEEP;\n\t\t\t/* Fallthrough */\n\t\tcase NFS4_READ_LT:\n\t\t\tspin_lock(&fp->fi_lock);\n\t\t\tfilp = find_readable_file_locked(fp);\n\t\t\tif (filp)\n\t\t\t\tget_lock_access(lock_stp, NFS4_SHARE_ACCESS_READ);\n\t\t\tspin_unlock(&fp->fi_lock);\n\t\t\tfl_type = F_RDLCK;\n\t\t\tbreak;\n\t\tcase NFS4_WRITEW_LT:\n\t\t\tif (nfsd4_has_session(cstate))\n\t\t\t\tfl_flags |= FL_SLEEP;\n\t\t\t/* Fallthrough */\n\t\tcase NFS4_WRITE_LT:\n\t\t\tspin_lock(&fp->fi_lock);\n\t\t\tfilp = find_writeable_file_locked(fp);\n\t\t\tif (filp)\n\t\t\t\tget_lock_access(lock_stp, NFS4_SHARE_ACCESS_WRITE);\n\t\t\tspin_unlock(&fp->fi_lock);\n\t\t\tfl_type = F_WRLCK;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tstatus = nfserr_inval;\n\t\tgoto out;\n\t}\n\n\tif (!filp) {\n\t\tstatus = nfserr_openmode;\n\t\tgoto out;\n\t}\n\n\tnbl = find_or_allocate_block(lock_sop, &fp->fi_fhandle, nn);\n\tif (!nbl) {\n\t\tdprintk(\"NFSD: %s: unable to allocate block!\\n\", __func__);\n\t\tstatus = nfserr_jukebox;\n\t\tgoto out;\n\t}\n\n\tfile_lock = &nbl->nbl_lock;\n\tfile_lock->fl_type = fl_type;\n\tfile_lock->fl_owner = (fl_owner_t)lockowner(nfs4_get_stateowner(&lock_sop->lo_owner));\n\tfile_lock->fl_pid = current->tgid;\n\tfile_lock->fl_file = filp;\n\tfile_lock->fl_flags = fl_flags;\n\tfile_lock->fl_lmops = &nfsd_posix_mng_ops;\n\tfile_lock->fl_start = lock->lk_offset;\n\tfile_lock->fl_end = last_byte_offset(lock->lk_offset, lock->lk_length);\n\tnfs4_transform_lock_offset(file_lock);\n\n\tconflock = locks_alloc_lock();\n\tif (!conflock) {\n\t\tdprintk(\"NFSD: %s: unable to allocate lock!\\n\", __func__);\n\t\tstatus = nfserr_jukebox;\n\t\tgoto out;\n\t}\n\n\tif (fl_flags & FL_SLEEP) {\n\t\tnbl->nbl_time = jiffies;\n\t\tspin_lock(&nn->blocked_locks_lock);\n\t\tlist_add_tail(&nbl->nbl_list, &lock_sop->lo_blocked);\n\t\tlist_add_tail(&nbl->nbl_lru, &nn->blocked_locks_lru);\n\t\tspin_unlock(&nn->blocked_locks_lock);\n\t}\n\n\terr = vfs_lock_file(filp, F_SETLK, file_lock, conflock);\n\tswitch (err) {\n\tcase 0: /* success! */\n\t\tnfs4_inc_and_copy_stateid(&lock->lk_resp_stateid, &lock_stp->st_stid);\n\t\tstatus = 0;\n\t\tbreak;\n\tcase FILE_LOCK_DEFERRED:\n\t\tnbl = NULL;\n\t\t/* Fallthrough */\n\tcase -EAGAIN:\t\t/* conflock holds conflicting lock */\n\t\tstatus = nfserr_denied;\n\t\tdprintk(\"NFSD: nfsd4_lock: conflicting lock found!\\n\");\n\t\tnfs4_set_lock_denied(conflock, &lock->lk_denied);\n\t\tbreak;\n\tcase -EDEADLK:\n\t\tstatus = nfserr_deadlock;\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"NFSD: nfsd4_lock: vfs_lock_file() failed! status %d\\n\",err);\n\t\tstatus = nfserrno(err);\n\t\tbreak;\n\t}\nout:\n\tif (nbl) {\n\t\t/* dequeue it if we queued it before */\n\t\tif (fl_flags & FL_SLEEP) {\n\t\t\tspin_lock(&nn->blocked_locks_lock);\n\t\t\tlist_del_init(&nbl->nbl_list);\n\t\t\tlist_del_init(&nbl->nbl_lru);\n\t\t\tspin_unlock(&nn->blocked_locks_lock);\n\t\t}\n\t\tfree_blocked_lock(nbl);\n\t}\n\tif (filp)\n\t\tfput(filp);\n\tif (lock_stp) {\n\t\t/* Bump seqid manually if the 4.0 replay owner is openowner */\n\t\tif (cstate->replay_owner &&\n\t\t    cstate->replay_owner != &lock_sop->lo_owner &&\n\t\t    seqid_mutating_err(ntohl(status)))\n\t\t\tlock_sop->lo_owner.so_seqid++;\n\n\t\tmutex_unlock(&lock_stp->st_mutex);\n\n\t\t/*\n\t\t * If this is a new, never-before-used stateid, and we are\n\t\t * returning an error, then just go ahead and release it.\n\t\t */\n\t\tif (status && new)\n\t\t\trelease_lock_stateid(lock_stp);\n\n\t\tnfs4_put_stid(&lock_stp->st_stid);\n\t}\n\tif (open_stp)\n\t\tnfs4_put_stid(&open_stp->st_stid);\n\tnfsd4_bump_seqid(cstate, status);\n\tif (conflock)\n\t\tlocks_free_lock(conflock);\n\treturn status;\n}\n\n/*\n * The NFSv4 spec allows a client to do a LOCKT without holding an OPEN,\n * so we do a temporary open here just to get an open file to pass to\n * vfs_test_lock.  (Arguably perhaps test_lock should be done with an\n * inode operation.)\n */\nstatic __be32 nfsd_test_lock(struct svc_rqst *rqstp, struct svc_fh *fhp, struct file_lock *lock)\n{\n\tstruct file *file;\n\t__be32 err = nfsd_open(rqstp, fhp, S_IFREG, NFSD_MAY_READ, &file);\n\tif (!err) {\n\t\terr = nfserrno(vfs_test_lock(file, lock));\n\t\tfput(file);\n\t}\n\treturn err;\n}\n\n/*\n * LOCKT operation\n */\n__be32\nnfsd4_lockt(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct nfsd4_lockt *lockt)\n{\n\tstruct file_lock *file_lock = NULL;\n\tstruct nfs4_lockowner *lo = NULL;\n\t__be32 status;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tif (locks_in_grace(SVC_NET(rqstp)))\n\t\treturn nfserr_grace;\n\n\tif (check_lock_length(lockt->lt_offset, lockt->lt_length))\n\t\t return nfserr_inval;\n\n\tif (!nfsd4_has_session(cstate)) {\n\t\tstatus = lookup_clientid(&lockt->lt_clientid, cstate, nn);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tif ((status = fh_verify(rqstp, &cstate->current_fh, S_IFREG, 0)))\n\t\tgoto out;\n\n\tfile_lock = locks_alloc_lock();\n\tif (!file_lock) {\n\t\tdprintk(\"NFSD: %s: unable to allocate lock!\\n\", __func__);\n\t\tstatus = nfserr_jukebox;\n\t\tgoto out;\n\t}\n\n\tswitch (lockt->lt_type) {\n\t\tcase NFS4_READ_LT:\n\t\tcase NFS4_READW_LT:\n\t\t\tfile_lock->fl_type = F_RDLCK;\n\t\tbreak;\n\t\tcase NFS4_WRITE_LT:\n\t\tcase NFS4_WRITEW_LT:\n\t\t\tfile_lock->fl_type = F_WRLCK;\n\t\tbreak;\n\t\tdefault:\n\t\t\tdprintk(\"NFSD: nfs4_lockt: bad lock type!\\n\");\n\t\t\tstatus = nfserr_inval;\n\t\tgoto out;\n\t}\n\n\tlo = find_lockowner_str(cstate->clp, &lockt->lt_owner);\n\tif (lo)\n\t\tfile_lock->fl_owner = (fl_owner_t)lo;\n\tfile_lock->fl_pid = current->tgid;\n\tfile_lock->fl_flags = FL_POSIX;\n\n\tfile_lock->fl_start = lockt->lt_offset;\n\tfile_lock->fl_end = last_byte_offset(lockt->lt_offset, lockt->lt_length);\n\n\tnfs4_transform_lock_offset(file_lock);\n\n\tstatus = nfsd_test_lock(rqstp, &cstate->current_fh, file_lock);\n\tif (status)\n\t\tgoto out;\n\n\tif (file_lock->fl_type != F_UNLCK) {\n\t\tstatus = nfserr_denied;\n\t\tnfs4_set_lock_denied(file_lock, &lockt->lt_denied);\n\t}\nout:\n\tif (lo)\n\t\tnfs4_put_stateowner(&lo->lo_owner);\n\tif (file_lock)\n\t\tlocks_free_lock(file_lock);\n\treturn status;\n}\n\n__be32\nnfsd4_locku(struct svc_rqst *rqstp, struct nfsd4_compound_state *cstate,\n\t    struct nfsd4_locku *locku)\n{\n\tstruct nfs4_ol_stateid *stp;\n\tstruct file *filp = NULL;\n\tstruct file_lock *file_lock = NULL;\n\t__be32 status;\n\tint err;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tdprintk(\"NFSD: nfsd4_locku: start=%Ld length=%Ld\\n\",\n\t\t(long long) locku->lu_offset,\n\t\t(long long) locku->lu_length);\n\n\tif (check_lock_length(locku->lu_offset, locku->lu_length))\n\t\t return nfserr_inval;\n\n\tstatus = nfs4_preprocess_seqid_op(cstate, locku->lu_seqid,\n\t\t\t\t\t&locku->lu_stateid, NFS4_LOCK_STID,\n\t\t\t\t\t&stp, nn);\n\tif (status)\n\t\tgoto out;\n\tfilp = find_any_file(stp->st_stid.sc_file);\n\tif (!filp) {\n\t\tstatus = nfserr_lock_range;\n\t\tgoto put_stateid;\n\t}\n\tfile_lock = locks_alloc_lock();\n\tif (!file_lock) {\n\t\tdprintk(\"NFSD: %s: unable to allocate lock!\\n\", __func__);\n\t\tstatus = nfserr_jukebox;\n\t\tgoto fput;\n\t}\n\n\tfile_lock->fl_type = F_UNLCK;\n\tfile_lock->fl_owner = (fl_owner_t)lockowner(nfs4_get_stateowner(stp->st_stateowner));\n\tfile_lock->fl_pid = current->tgid;\n\tfile_lock->fl_file = filp;\n\tfile_lock->fl_flags = FL_POSIX;\n\tfile_lock->fl_lmops = &nfsd_posix_mng_ops;\n\tfile_lock->fl_start = locku->lu_offset;\n\n\tfile_lock->fl_end = last_byte_offset(locku->lu_offset,\n\t\t\t\t\t\tlocku->lu_length);\n\tnfs4_transform_lock_offset(file_lock);\n\n\terr = vfs_lock_file(filp, F_SETLK, file_lock, NULL);\n\tif (err) {\n\t\tdprintk(\"NFSD: nfs4_locku: vfs_lock_file failed!\\n\");\n\t\tgoto out_nfserr;\n\t}\n\tnfs4_inc_and_copy_stateid(&locku->lu_stateid, &stp->st_stid);\nfput:\n\tfput(filp);\nput_stateid:\n\tmutex_unlock(&stp->st_mutex);\n\tnfs4_put_stid(&stp->st_stid);\nout:\n\tnfsd4_bump_seqid(cstate, status);\n\tif (file_lock)\n\t\tlocks_free_lock(file_lock);\n\treturn status;\n\nout_nfserr:\n\tstatus = nfserrno(err);\n\tgoto fput;\n}\n\n/*\n * returns\n * \ttrue:  locks held by lockowner\n * \tfalse: no locks held by lockowner\n */\nstatic bool\ncheck_for_locks(struct nfs4_file *fp, struct nfs4_lockowner *lowner)\n{\n\tstruct file_lock *fl;\n\tint status = false;\n\tstruct file *filp = find_any_file(fp);\n\tstruct inode *inode;\n\tstruct file_lock_context *flctx;\n\n\tif (!filp) {\n\t\t/* Any valid lock stateid should have some sort of access */\n\t\tWARN_ON_ONCE(1);\n\t\treturn status;\n\t}\n\n\tinode = file_inode(filp);\n\tflctx = inode->i_flctx;\n\n\tif (flctx && !list_empty_careful(&flctx->flc_posix)) {\n\t\tspin_lock(&flctx->flc_lock);\n\t\tlist_for_each_entry(fl, &flctx->flc_posix, fl_list) {\n\t\t\tif (fl->fl_owner == (fl_owner_t)lowner) {\n\t\t\t\tstatus = true;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tspin_unlock(&flctx->flc_lock);\n\t}\n\tfput(filp);\n\treturn status;\n}\n\n__be32\nnfsd4_release_lockowner(struct svc_rqst *rqstp,\n\t\t\tstruct nfsd4_compound_state *cstate,\n\t\t\tstruct nfsd4_release_lockowner *rlockowner)\n{\n\tclientid_t *clid = &rlockowner->rl_clientid;\n\tstruct nfs4_stateowner *sop;\n\tstruct nfs4_lockowner *lo = NULL;\n\tstruct nfs4_ol_stateid *stp;\n\tstruct xdr_netobj *owner = &rlockowner->rl_owner;\n\tunsigned int hashval = ownerstr_hashval(owner);\n\t__be32 status;\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\tstruct nfs4_client *clp;\n\tLIST_HEAD (reaplist);\n\n\tdprintk(\"nfsd4_release_lockowner clientid: (%08x/%08x):\\n\",\n\t\tclid->cl_boot, clid->cl_id);\n\n\tstatus = lookup_clientid(clid, cstate, nn);\n\tif (status)\n\t\treturn status;\n\n\tclp = cstate->clp;\n\t/* Find the matching lock stateowner */\n\tspin_lock(&clp->cl_lock);\n\tlist_for_each_entry(sop, &clp->cl_ownerstr_hashtbl[hashval],\n\t\t\t    so_strhash) {\n\n\t\tif (sop->so_is_open_owner || !same_owner_str(sop, owner))\n\t\t\tcontinue;\n\n\t\t/* see if there are still any locks associated with it */\n\t\tlo = lockowner(sop);\n\t\tlist_for_each_entry(stp, &sop->so_stateids, st_perstateowner) {\n\t\t\tif (check_for_locks(stp->st_stid.sc_file, lo)) {\n\t\t\t\tstatus = nfserr_locks_held;\n\t\t\t\tspin_unlock(&clp->cl_lock);\n\t\t\t\treturn status;\n\t\t\t}\n\t\t}\n\n\t\tnfs4_get_stateowner(sop);\n\t\tbreak;\n\t}\n\tif (!lo) {\n\t\tspin_unlock(&clp->cl_lock);\n\t\treturn status;\n\t}\n\n\tunhash_lockowner_locked(lo);\n\twhile (!list_empty(&lo->lo_owner.so_stateids)) {\n\t\tstp = list_first_entry(&lo->lo_owner.so_stateids,\n\t\t\t\t       struct nfs4_ol_stateid,\n\t\t\t\t       st_perstateowner);\n\t\tWARN_ON(!unhash_lock_stateid(stp));\n\t\tput_ol_stateid_locked(stp, &reaplist);\n\t}\n\tspin_unlock(&clp->cl_lock);\n\tfree_ol_stateid_reaplist(&reaplist);\n\tnfs4_put_stateowner(&lo->lo_owner);\n\n\treturn status;\n}\n\nstatic inline struct nfs4_client_reclaim *\nalloc_reclaim(void)\n{\n\treturn kmalloc(sizeof(struct nfs4_client_reclaim), GFP_KERNEL);\n}\n\nbool\nnfs4_has_reclaimed_state(const char *name, struct nfsd_net *nn)\n{\n\tstruct nfs4_client_reclaim *crp;\n\n\tcrp = nfsd4_find_reclaim_client(name, nn);\n\treturn (crp && crp->cr_clp);\n}\n\n/*\n * failure => all reset bets are off, nfserr_no_grace...\n */\nstruct nfs4_client_reclaim *\nnfs4_client_to_reclaim(const char *name, struct nfsd_net *nn)\n{\n\tunsigned int strhashval;\n\tstruct nfs4_client_reclaim *crp;\n\n\tdprintk(\"NFSD nfs4_client_to_reclaim NAME: %.*s\\n\", HEXDIR_LEN, name);\n\tcrp = alloc_reclaim();\n\tif (crp) {\n\t\tstrhashval = clientstr_hashval(name);\n\t\tINIT_LIST_HEAD(&crp->cr_strhash);\n\t\tlist_add(&crp->cr_strhash, &nn->reclaim_str_hashtbl[strhashval]);\n\t\tmemcpy(crp->cr_recdir, name, HEXDIR_LEN);\n\t\tcrp->cr_clp = NULL;\n\t\tnn->reclaim_str_hashtbl_size++;\n\t}\n\treturn crp;\n}\n\nvoid\nnfs4_remove_reclaim_record(struct nfs4_client_reclaim *crp, struct nfsd_net *nn)\n{\n\tlist_del(&crp->cr_strhash);\n\tkfree(crp);\n\tnn->reclaim_str_hashtbl_size--;\n}\n\nvoid\nnfs4_release_reclaim(struct nfsd_net *nn)\n{\n\tstruct nfs4_client_reclaim *crp = NULL;\n\tint i;\n\n\tfor (i = 0; i < CLIENT_HASH_SIZE; i++) {\n\t\twhile (!list_empty(&nn->reclaim_str_hashtbl[i])) {\n\t\t\tcrp = list_entry(nn->reclaim_str_hashtbl[i].next,\n\t\t\t                struct nfs4_client_reclaim, cr_strhash);\n\t\t\tnfs4_remove_reclaim_record(crp, nn);\n\t\t}\n\t}\n\tWARN_ON_ONCE(nn->reclaim_str_hashtbl_size);\n}\n\n/*\n * called from OPEN, CLAIM_PREVIOUS with a new clientid. */\nstruct nfs4_client_reclaim *\nnfsd4_find_reclaim_client(const char *recdir, struct nfsd_net *nn)\n{\n\tunsigned int strhashval;\n\tstruct nfs4_client_reclaim *crp = NULL;\n\n\tdprintk(\"NFSD: nfs4_find_reclaim_client for recdir %s\\n\", recdir);\n\n\tstrhashval = clientstr_hashval(recdir);\n\tlist_for_each_entry(crp, &nn->reclaim_str_hashtbl[strhashval], cr_strhash) {\n\t\tif (same_name(crp->cr_recdir, recdir)) {\n\t\t\treturn crp;\n\t\t}\n\t}\n\treturn NULL;\n}\n\n/*\n* Called from OPEN. Look for clientid in reclaim list.\n*/\n__be32\nnfs4_check_open_reclaim(clientid_t *clid,\n\t\tstruct nfsd4_compound_state *cstate,\n\t\tstruct nfsd_net *nn)\n{\n\t__be32 status;\n\n\t/* find clientid in conf_id_hashtbl */\n\tstatus = lookup_clientid(clid, cstate, nn);\n\tif (status)\n\t\treturn nfserr_reclaim_bad;\n\n\tif (test_bit(NFSD4_CLIENT_RECLAIM_COMPLETE, &cstate->clp->cl_flags))\n\t\treturn nfserr_no_grace;\n\n\tif (nfsd4_client_record_check(cstate->clp))\n\t\treturn nfserr_reclaim_bad;\n\n\treturn nfs_ok;\n}\n\n#ifdef CONFIG_NFSD_FAULT_INJECTION\nstatic inline void\nput_client(struct nfs4_client *clp)\n{\n\tatomic_dec(&clp->cl_refcount);\n}\n\nstatic struct nfs4_client *\nnfsd_find_client(struct sockaddr_storage *addr, size_t addr_size)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t  nfsd_net_id);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn NULL;\n\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru) {\n\t\tif (memcmp(&clp->cl_addr, addr, addr_size) == 0)\n\t\t\treturn clp;\n\t}\n\treturn NULL;\n}\n\nu64\nnfsd_inject_print_clients(void)\n{\n\tstruct nfs4_client *clp;\n\tu64 count = 0;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t  nfsd_net_id);\n\tchar buf[INET6_ADDRSTRLEN];\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn 0;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru) {\n\t\trpc_ntop((struct sockaddr *)&clp->cl_addr, buf, sizeof(buf));\n\t\tpr_info(\"NFS Client: %s\\n\", buf);\n\t\t++count;\n\t}\n\tspin_unlock(&nn->client_lock);\n\n\treturn count;\n}\n\nu64\nnfsd_inject_forget_client(struct sockaddr_storage *addr, size_t addr_size)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t  nfsd_net_id);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tclp = nfsd_find_client(addr, addr_size);\n\tif (clp) {\n\t\tif (mark_client_expired_locked(clp) == nfs_ok)\n\t\t\t++count;\n\t\telse\n\t\t\tclp = NULL;\n\t}\n\tspin_unlock(&nn->client_lock);\n\n\tif (clp)\n\t\texpire_client(clp);\n\n\treturn count;\n}\n\nu64\nnfsd_inject_forget_clients(u64 max)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp, *next;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry_safe(clp, next, &nn->client_lru, cl_lru) {\n\t\tif (mark_client_expired_locked(clp) == nfs_ok) {\n\t\t\tlist_add(&clp->cl_lru, &reaplist);\n\t\t\tif (max != 0 && ++count >= max)\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tspin_unlock(&nn->client_lock);\n\n\tlist_for_each_entry_safe(clp, next, &reaplist, cl_lru)\n\t\texpire_client(clp);\n\n\treturn count;\n}\n\nstatic void nfsd_print_count(struct nfs4_client *clp, unsigned int count,\n\t\t\t     const char *type)\n{\n\tchar buf[INET6_ADDRSTRLEN];\n\trpc_ntop((struct sockaddr *)&clp->cl_addr, buf, sizeof(buf));\n\tprintk(KERN_INFO \"NFS Client: %s has %u %s\\n\", buf, count, type);\n}\n\nstatic void\nnfsd_inject_add_lock_to_list(struct nfs4_ol_stateid *lst,\n\t\t\t     struct list_head *collect)\n{\n\tstruct nfs4_client *clp = lst->st_stid.sc_client;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t  nfsd_net_id);\n\n\tif (!collect)\n\t\treturn;\n\n\tlockdep_assert_held(&nn->client_lock);\n\tatomic_inc(&clp->cl_refcount);\n\tlist_add(&lst->st_locks, collect);\n}\n\nstatic u64 nfsd_foreach_client_lock(struct nfs4_client *clp, u64 max,\n\t\t\t\t    struct list_head *collect,\n\t\t\t\t    bool (*func)(struct nfs4_ol_stateid *))\n{\n\tstruct nfs4_openowner *oop;\n\tstruct nfs4_ol_stateid *stp, *st_next;\n\tstruct nfs4_ol_stateid *lst, *lst_next;\n\tu64 count = 0;\n\n\tspin_lock(&clp->cl_lock);\n\tlist_for_each_entry(oop, &clp->cl_openowners, oo_perclient) {\n\t\tlist_for_each_entry_safe(stp, st_next,\n\t\t\t\t&oop->oo_owner.so_stateids, st_perstateowner) {\n\t\t\tlist_for_each_entry_safe(lst, lst_next,\n\t\t\t\t\t&stp->st_locks, st_locks) {\n\t\t\t\tif (func) {\n\t\t\t\t\tif (func(lst))\n\t\t\t\t\t\tnfsd_inject_add_lock_to_list(lst,\n\t\t\t\t\t\t\t\t\tcollect);\n\t\t\t\t}\n\t\t\t\t++count;\n\t\t\t\t/*\n\t\t\t\t * Despite the fact that these functions deal\n\t\t\t\t * with 64-bit integers for \"count\", we must\n\t\t\t\t * ensure that it doesn't blow up the\n\t\t\t\t * clp->cl_refcount. Throw a warning if we\n\t\t\t\t * start to approach INT_MAX here.\n\t\t\t\t */\n\t\t\t\tWARN_ON_ONCE(count == (INT_MAX / 2));\n\t\t\t\tif (count == max)\n\t\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\nout:\n\tspin_unlock(&clp->cl_lock);\n\n\treturn count;\n}\n\nstatic u64\nnfsd_collect_client_locks(struct nfs4_client *clp, struct list_head *collect,\n\t\t\t  u64 max)\n{\n\treturn nfsd_foreach_client_lock(clp, max, collect, unhash_lock_stateid);\n}\n\nstatic u64\nnfsd_print_client_locks(struct nfs4_client *clp)\n{\n\tu64 count = nfsd_foreach_client_lock(clp, 0, NULL, NULL);\n\tnfsd_print_count(clp, count, \"locked files\");\n\treturn count;\n}\n\nu64\nnfsd_inject_print_locks(void)\n{\n\tstruct nfs4_client *clp;\n\tu64 count = 0;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn 0;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru)\n\t\tcount += nfsd_print_client_locks(clp);\n\tspin_unlock(&nn->client_lock);\n\n\treturn count;\n}\n\nstatic void\nnfsd_reap_locks(struct list_head *reaplist)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfs4_ol_stateid *stp, *next;\n\n\tlist_for_each_entry_safe(stp, next, reaplist, st_locks) {\n\t\tlist_del_init(&stp->st_locks);\n\t\tclp = stp->st_stid.sc_client;\n\t\tnfs4_put_stid(&stp->st_stid);\n\t\tput_client(clp);\n\t}\n}\n\nu64\nnfsd_inject_forget_client_locks(struct sockaddr_storage *addr, size_t addr_size)\n{\n\tunsigned int count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tclp = nfsd_find_client(addr, addr_size);\n\tif (clp)\n\t\tcount = nfsd_collect_client_locks(clp, &reaplist, 0);\n\tspin_unlock(&nn->client_lock);\n\tnfsd_reap_locks(&reaplist);\n\treturn count;\n}\n\nu64\nnfsd_inject_forget_locks(u64 max)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru) {\n\t\tcount += nfsd_collect_client_locks(clp, &reaplist, max - count);\n\t\tif (max != 0 && count >= max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&nn->client_lock);\n\tnfsd_reap_locks(&reaplist);\n\treturn count;\n}\n\nstatic u64\nnfsd_foreach_client_openowner(struct nfs4_client *clp, u64 max,\n\t\t\t      struct list_head *collect,\n\t\t\t      void (*func)(struct nfs4_openowner *))\n{\n\tstruct nfs4_openowner *oop, *next;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tu64 count = 0;\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tspin_lock(&clp->cl_lock);\n\tlist_for_each_entry_safe(oop, next, &clp->cl_openowners, oo_perclient) {\n\t\tif (func) {\n\t\t\tfunc(oop);\n\t\t\tif (collect) {\n\t\t\t\tatomic_inc(&clp->cl_refcount);\n\t\t\t\tlist_add(&oop->oo_perclient, collect);\n\t\t\t}\n\t\t}\n\t\t++count;\n\t\t/*\n\t\t * Despite the fact that these functions deal with\n\t\t * 64-bit integers for \"count\", we must ensure that\n\t\t * it doesn't blow up the clp->cl_refcount. Throw a\n\t\t * warning if we start to approach INT_MAX here.\n\t\t */\n\t\tWARN_ON_ONCE(count == (INT_MAX / 2));\n\t\tif (count == max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&clp->cl_lock);\n\n\treturn count;\n}\n\nstatic u64\nnfsd_print_client_openowners(struct nfs4_client *clp)\n{\n\tu64 count = nfsd_foreach_client_openowner(clp, 0, NULL, NULL);\n\n\tnfsd_print_count(clp, count, \"openowners\");\n\treturn count;\n}\n\nstatic u64\nnfsd_collect_client_openowners(struct nfs4_client *clp,\n\t\t\t       struct list_head *collect, u64 max)\n{\n\treturn nfsd_foreach_client_openowner(clp, max, collect,\n\t\t\t\t\t\tunhash_openowner_locked);\n}\n\nu64\nnfsd_inject_print_openowners(void)\n{\n\tstruct nfs4_client *clp;\n\tu64 count = 0;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn 0;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru)\n\t\tcount += nfsd_print_client_openowners(clp);\n\tspin_unlock(&nn->client_lock);\n\n\treturn count;\n}\n\nstatic void\nnfsd_reap_openowners(struct list_head *reaplist)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfs4_openowner *oop, *next;\n\n\tlist_for_each_entry_safe(oop, next, reaplist, oo_perclient) {\n\t\tlist_del_init(&oop->oo_perclient);\n\t\tclp = oop->oo_owner.so_client;\n\t\trelease_openowner(oop);\n\t\tput_client(clp);\n\t}\n}\n\nu64\nnfsd_inject_forget_client_openowners(struct sockaddr_storage *addr,\n\t\t\t\t     size_t addr_size)\n{\n\tunsigned int count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tclp = nfsd_find_client(addr, addr_size);\n\tif (clp)\n\t\tcount = nfsd_collect_client_openowners(clp, &reaplist, 0);\n\tspin_unlock(&nn->client_lock);\n\tnfsd_reap_openowners(&reaplist);\n\treturn count;\n}\n\nu64\nnfsd_inject_forget_openowners(u64 max)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru) {\n\t\tcount += nfsd_collect_client_openowners(clp, &reaplist,\n\t\t\t\t\t\t\tmax - count);\n\t\tif (max != 0 && count >= max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&nn->client_lock);\n\tnfsd_reap_openowners(&reaplist);\n\treturn count;\n}\n\nstatic u64 nfsd_find_all_delegations(struct nfs4_client *clp, u64 max,\n\t\t\t\t     struct list_head *victims)\n{\n\tstruct nfs4_delegation *dp, *next;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tu64 count = 0;\n\n\tlockdep_assert_held(&nn->client_lock);\n\n\tspin_lock(&state_lock);\n\tlist_for_each_entry_safe(dp, next, &clp->cl_delegations, dl_perclnt) {\n\t\tif (victims) {\n\t\t\t/*\n\t\t\t * It's not safe to mess with delegations that have a\n\t\t\t * non-zero dl_time. They might have already been broken\n\t\t\t * and could be processed by the laundromat outside of\n\t\t\t * the state_lock. Just leave them be.\n\t\t\t */\n\t\t\tif (dp->dl_time != 0)\n\t\t\t\tcontinue;\n\n\t\t\tatomic_inc(&clp->cl_refcount);\n\t\t\tWARN_ON(!unhash_delegation_locked(dp));\n\t\t\tlist_add(&dp->dl_recall_lru, victims);\n\t\t}\n\t\t++count;\n\t\t/*\n\t\t * Despite the fact that these functions deal with\n\t\t * 64-bit integers for \"count\", we must ensure that\n\t\t * it doesn't blow up the clp->cl_refcount. Throw a\n\t\t * warning if we start to approach INT_MAX here.\n\t\t */\n\t\tWARN_ON_ONCE(count == (INT_MAX / 2));\n\t\tif (count == max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&state_lock);\n\treturn count;\n}\n\nstatic u64\nnfsd_print_client_delegations(struct nfs4_client *clp)\n{\n\tu64 count = nfsd_find_all_delegations(clp, 0, NULL);\n\n\tnfsd_print_count(clp, count, \"delegations\");\n\treturn count;\n}\n\nu64\nnfsd_inject_print_delegations(void)\n{\n\tstruct nfs4_client *clp;\n\tu64 count = 0;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn 0;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru)\n\t\tcount += nfsd_print_client_delegations(clp);\n\tspin_unlock(&nn->client_lock);\n\n\treturn count;\n}\n\nstatic void\nnfsd_forget_delegations(struct list_head *reaplist)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfs4_delegation *dp, *next;\n\n\tlist_for_each_entry_safe(dp, next, reaplist, dl_recall_lru) {\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tclp = dp->dl_stid.sc_client;\n\t\trevoke_delegation(dp);\n\t\tput_client(clp);\n\t}\n}\n\nu64\nnfsd_inject_forget_client_delegations(struct sockaddr_storage *addr,\n\t\t\t\t      size_t addr_size)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tclp = nfsd_find_client(addr, addr_size);\n\tif (clp)\n\t\tcount = nfsd_find_all_delegations(clp, 0, &reaplist);\n\tspin_unlock(&nn->client_lock);\n\n\tnfsd_forget_delegations(&reaplist);\n\treturn count;\n}\n\nu64\nnfsd_inject_forget_delegations(u64 max)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry(clp, &nn->client_lru, cl_lru) {\n\t\tcount += nfsd_find_all_delegations(clp, max - count, &reaplist);\n\t\tif (max != 0 && count >= max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&nn->client_lock);\n\tnfsd_forget_delegations(&reaplist);\n\treturn count;\n}\n\nstatic void\nnfsd_recall_delegations(struct list_head *reaplist)\n{\n\tstruct nfs4_client *clp;\n\tstruct nfs4_delegation *dp, *next;\n\n\tlist_for_each_entry_safe(dp, next, reaplist, dl_recall_lru) {\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tclp = dp->dl_stid.sc_client;\n\t\t/*\n\t\t * We skipped all entries that had a zero dl_time before,\n\t\t * so we can now reset the dl_time back to 0. If a delegation\n\t\t * break comes in now, then it won't make any difference since\n\t\t * we're recalling it either way.\n\t\t */\n\t\tspin_lock(&state_lock);\n\t\tdp->dl_time = 0;\n\t\tspin_unlock(&state_lock);\n\t\tnfsd_break_one_deleg(dp);\n\t\tput_client(clp);\n\t}\n}\n\nu64\nnfsd_inject_recall_client_delegations(struct sockaddr_storage *addr,\n\t\t\t\t      size_t addr_size)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tclp = nfsd_find_client(addr, addr_size);\n\tif (clp)\n\t\tcount = nfsd_find_all_delegations(clp, 0, &reaplist);\n\tspin_unlock(&nn->client_lock);\n\n\tnfsd_recall_delegations(&reaplist);\n\treturn count;\n}\n\nu64\nnfsd_inject_recall_delegations(u64 max)\n{\n\tu64 count = 0;\n\tstruct nfs4_client *clp, *next;\n\tstruct nfsd_net *nn = net_generic(current->nsproxy->net_ns,\n\t\t\t\t\t\tnfsd_net_id);\n\tLIST_HEAD(reaplist);\n\n\tif (!nfsd_netns_ready(nn))\n\t\treturn count;\n\n\tspin_lock(&nn->client_lock);\n\tlist_for_each_entry_safe(clp, next, &nn->client_lru, cl_lru) {\n\t\tcount += nfsd_find_all_delegations(clp, max - count, &reaplist);\n\t\tif (max != 0 && ++count >= max)\n\t\t\tbreak;\n\t}\n\tspin_unlock(&nn->client_lock);\n\tnfsd_recall_delegations(&reaplist);\n\treturn count;\n}\n#endif /* CONFIG_NFSD_FAULT_INJECTION */\n\n/*\n * Since the lifetime of a delegation isn't limited to that of an open, a\n * client may quite reasonably hang on to a delegation as long as it has\n * the inode cached.  This becomes an obvious problem the first time a\n * client's inode cache approaches the size of the server's total memory.\n *\n * For now we avoid this problem by imposing a hard limit on the number\n * of delegations, which varies according to the server's memory size.\n */\nstatic void\nset_max_delegations(void)\n{\n\t/*\n\t * Allow at most 4 delegations per megabyte of RAM.  Quick\n\t * estimates suggest that in the worst case (where every delegation\n\t * is for a different inode), a delegation could take about 1.5K,\n\t * giving a worst case usage of about 6% of memory.\n\t */\n\tmax_delegations = nr_free_buffer_pages() >> (20 - 2 - PAGE_SHIFT);\n}\n\nstatic int nfs4_state_create_net(struct net *net)\n{\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\tint i;\n\n\tnn->conf_id_hashtbl = kmalloc(sizeof(struct list_head) *\n\t\t\tCLIENT_HASH_SIZE, GFP_KERNEL);\n\tif (!nn->conf_id_hashtbl)\n\t\tgoto err;\n\tnn->unconf_id_hashtbl = kmalloc(sizeof(struct list_head) *\n\t\t\tCLIENT_HASH_SIZE, GFP_KERNEL);\n\tif (!nn->unconf_id_hashtbl)\n\t\tgoto err_unconf_id;\n\tnn->sessionid_hashtbl = kmalloc(sizeof(struct list_head) *\n\t\t\tSESSION_HASH_SIZE, GFP_KERNEL);\n\tif (!nn->sessionid_hashtbl)\n\t\tgoto err_sessionid;\n\n\tfor (i = 0; i < CLIENT_HASH_SIZE; i++) {\n\t\tINIT_LIST_HEAD(&nn->conf_id_hashtbl[i]);\n\t\tINIT_LIST_HEAD(&nn->unconf_id_hashtbl[i]);\n\t}\n\tfor (i = 0; i < SESSION_HASH_SIZE; i++)\n\t\tINIT_LIST_HEAD(&nn->sessionid_hashtbl[i]);\n\tnn->conf_name_tree = RB_ROOT;\n\tnn->unconf_name_tree = RB_ROOT;\n\tINIT_LIST_HEAD(&nn->client_lru);\n\tINIT_LIST_HEAD(&nn->close_lru);\n\tINIT_LIST_HEAD(&nn->del_recall_lru);\n\tspin_lock_init(&nn->client_lock);\n\n\tspin_lock_init(&nn->blocked_locks_lock);\n\tINIT_LIST_HEAD(&nn->blocked_locks_lru);\n\n\tINIT_DELAYED_WORK(&nn->laundromat_work, laundromat_main);\n\tget_net(net);\n\n\treturn 0;\n\nerr_sessionid:\n\tkfree(nn->unconf_id_hashtbl);\nerr_unconf_id:\n\tkfree(nn->conf_id_hashtbl);\nerr:\n\treturn -ENOMEM;\n}\n\nstatic void\nnfs4_state_destroy_net(struct net *net)\n{\n\tint i;\n\tstruct nfs4_client *clp = NULL;\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\n\tfor (i = 0; i < CLIENT_HASH_SIZE; i++) {\n\t\twhile (!list_empty(&nn->conf_id_hashtbl[i])) {\n\t\t\tclp = list_entry(nn->conf_id_hashtbl[i].next, struct nfs4_client, cl_idhash);\n\t\t\tdestroy_client(clp);\n\t\t}\n\t}\n\n\tfor (i = 0; i < CLIENT_HASH_SIZE; i++) {\n\t\twhile (!list_empty(&nn->unconf_id_hashtbl[i])) {\n\t\t\tclp = list_entry(nn->unconf_id_hashtbl[i].next, struct nfs4_client, cl_idhash);\n\t\t\tdestroy_client(clp);\n\t\t}\n\t}\n\n\tkfree(nn->sessionid_hashtbl);\n\tkfree(nn->unconf_id_hashtbl);\n\tkfree(nn->conf_id_hashtbl);\n\tput_net(net);\n}\n\nint\nnfs4_state_start_net(struct net *net)\n{\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\tint ret;\n\n\tret = nfs4_state_create_net(net);\n\tif (ret)\n\t\treturn ret;\n\tnn->boot_time = get_seconds();\n\tnn->grace_ended = false;\n\tnn->nfsd4_manager.block_opens = true;\n\tlocks_start_grace(net, &nn->nfsd4_manager);\n\tnfsd4_client_tracking_init(net);\n\tprintk(KERN_INFO \"NFSD: starting %ld-second grace period (net %p)\\n\",\n\t       nn->nfsd4_grace, net);\n\tqueue_delayed_work(laundry_wq, &nn->laundromat_work, nn->nfsd4_grace * HZ);\n\treturn 0;\n}\n\n/* initialization to perform when the nfsd service is started: */\n\nint\nnfs4_state_start(void)\n{\n\tint ret;\n\n\tret = set_callback_cred();\n\tif (ret)\n\t\treturn ret;\n\n\tlaundry_wq = alloc_workqueue(\"%s\", WQ_UNBOUND, 0, \"nfsd4\");\n\tif (laundry_wq == NULL) {\n\t\tret = -ENOMEM;\n\t\tgoto out_cleanup_cred;\n\t}\n\tret = nfsd4_create_callback_queue();\n\tif (ret)\n\t\tgoto out_free_laundry;\n\n\tset_max_delegations();\n\treturn 0;\n\nout_free_laundry:\n\tdestroy_workqueue(laundry_wq);\nout_cleanup_cred:\n\tcleanup_callback_cred();\n\treturn ret;\n}\n\nvoid\nnfs4_state_shutdown_net(struct net *net)\n{\n\tstruct nfs4_delegation *dp = NULL;\n\tstruct list_head *pos, *next, reaplist;\n\tstruct nfsd_net *nn = net_generic(net, nfsd_net_id);\n\tstruct nfsd4_blocked_lock *nbl;\n\n\tcancel_delayed_work_sync(&nn->laundromat_work);\n\tlocks_end_grace(&nn->nfsd4_manager);\n\n\tINIT_LIST_HEAD(&reaplist);\n\tspin_lock(&state_lock);\n\tlist_for_each_safe(pos, next, &nn->del_recall_lru) {\n\t\tdp = list_entry (pos, struct nfs4_delegation, dl_recall_lru);\n\t\tWARN_ON(!unhash_delegation_locked(dp));\n\t\tlist_add(&dp->dl_recall_lru, &reaplist);\n\t}\n\tspin_unlock(&state_lock);\n\tlist_for_each_safe(pos, next, &reaplist) {\n\t\tdp = list_entry (pos, struct nfs4_delegation, dl_recall_lru);\n\t\tlist_del_init(&dp->dl_recall_lru);\n\t\tput_clnt_odstate(dp->dl_clnt_odstate);\n\t\tnfs4_put_deleg_lease(dp->dl_stid.sc_file);\n\t\tnfs4_put_stid(&dp->dl_stid);\n\t}\n\n\tBUG_ON(!list_empty(&reaplist));\n\tspin_lock(&nn->blocked_locks_lock);\n\twhile (!list_empty(&nn->blocked_locks_lru)) {\n\t\tnbl = list_first_entry(&nn->blocked_locks_lru,\n\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_lru);\n\t\tlist_move(&nbl->nbl_lru, &reaplist);\n\t\tlist_del_init(&nbl->nbl_list);\n\t}\n\tspin_unlock(&nn->blocked_locks_lock);\n\n\twhile (!list_empty(&reaplist)) {\n\t\tnbl = list_first_entry(&nn->blocked_locks_lru,\n\t\t\t\t\tstruct nfsd4_blocked_lock, nbl_lru);\n\t\tlist_del_init(&nbl->nbl_lru);\n\t\tposix_unblock_lock(&nbl->nbl_lock);\n\t\tfree_blocked_lock(nbl);\n\t}\n\n\tnfsd4_client_tracking_exit(net);\n\tnfs4_state_destroy_net(net);\n}\n\nvoid\nnfs4_state_shutdown(void)\n{\n\tdestroy_workqueue(laundry_wq);\n\tnfsd4_destroy_callback_queue();\n\tcleanup_callback_cred();\n}\n\nstatic void\nget_stateid(struct nfsd4_compound_state *cstate, stateid_t *stateid)\n{\n\tif (HAS_STATE_ID(cstate, CURRENT_STATE_ID_FLAG) && CURRENT_STATEID(stateid))\n\t\tmemcpy(stateid, &cstate->current_stateid, sizeof(stateid_t));\n}\n\nstatic void\nput_stateid(struct nfsd4_compound_state *cstate, stateid_t *stateid)\n{\n\tif (cstate->minorversion) {\n\t\tmemcpy(&cstate->current_stateid, stateid, sizeof(stateid_t));\n\t\tSET_STATE_ID(cstate, CURRENT_STATE_ID_FLAG);\n\t}\n}\n\nvoid\nclear_current_stateid(struct nfsd4_compound_state *cstate)\n{\n\tCLEAR_STATE_ID(cstate, CURRENT_STATE_ID_FLAG);\n}\n\n/*\n * functions to set current state id\n */\nvoid\nnfsd4_set_opendowngradestateid(struct nfsd4_compound_state *cstate, struct nfsd4_open_downgrade *odp)\n{\n\tput_stateid(cstate, &odp->od_stateid);\n}\n\nvoid\nnfsd4_set_openstateid(struct nfsd4_compound_state *cstate, struct nfsd4_open *open)\n{\n\tput_stateid(cstate, &open->op_stateid);\n}\n\nvoid\nnfsd4_set_closestateid(struct nfsd4_compound_state *cstate, struct nfsd4_close *close)\n{\n\tput_stateid(cstate, &close->cl_stateid);\n}\n\nvoid\nnfsd4_set_lockstateid(struct nfsd4_compound_state *cstate, struct nfsd4_lock *lock)\n{\n\tput_stateid(cstate, &lock->lk_resp_stateid);\n}\n\n/*\n * functions to consume current state id\n */\n\nvoid\nnfsd4_get_opendowngradestateid(struct nfsd4_compound_state *cstate, struct nfsd4_open_downgrade *odp)\n{\n\tget_stateid(cstate, &odp->od_stateid);\n}\n\nvoid\nnfsd4_get_delegreturnstateid(struct nfsd4_compound_state *cstate, struct nfsd4_delegreturn *drp)\n{\n\tget_stateid(cstate, &drp->dr_stateid);\n}\n\nvoid\nnfsd4_get_freestateid(struct nfsd4_compound_state *cstate, struct nfsd4_free_stateid *fsp)\n{\n\tget_stateid(cstate, &fsp->fr_stateid);\n}\n\nvoid\nnfsd4_get_setattrstateid(struct nfsd4_compound_state *cstate, struct nfsd4_setattr *setattr)\n{\n\tget_stateid(cstate, &setattr->sa_stateid);\n}\n\nvoid\nnfsd4_get_closestateid(struct nfsd4_compound_state *cstate, struct nfsd4_close *close)\n{\n\tget_stateid(cstate, &close->cl_stateid);\n}\n\nvoid\nnfsd4_get_lockustateid(struct nfsd4_compound_state *cstate, struct nfsd4_locku *locku)\n{\n\tget_stateid(cstate, &locku->lu_stateid);\n}\n\nvoid\nnfsd4_get_readstateid(struct nfsd4_compound_state *cstate, struct nfsd4_read *read)\n{\n\tget_stateid(cstate, &read->rd_stateid);\n}\n\nvoid\nnfsd4_get_writestateid(struct nfsd4_compound_state *cstate, struct nfsd4_write *write)\n{\n\tget_stateid(cstate, &write->wr_stateid);\n}\n", "/*\n *  Server-side XDR for NFSv4\n *\n *  Copyright (c) 2002 The Regents of the University of Michigan.\n *  All rights reserved.\n *\n *  Kendrick Smith <kmsmith@umich.edu>\n *  Andy Adamson   <andros@umich.edu>\n *\n *  Redistribution and use in source and binary forms, with or without\n *  modification, are permitted provided that the following conditions\n *  are met:\n *\n *  1. Redistributions of source code must retain the above copyright\n *     notice, this list of conditions and the following disclaimer.\n *  2. Redistributions in binary form must reproduce the above copyright\n *     notice, this list of conditions and the following disclaimer in the\n *     documentation and/or other materials provided with the distribution.\n *  3. Neither the name of the University nor the names of its\n *     contributors may be used to endorse or promote products derived\n *     from this software without specific prior written permission.\n *\n *  THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESS OR IMPLIED\n *  WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n *  MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n *  DISCLAIMED. IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE\n *  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n *  CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n *  SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR\n *  BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n *  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n *  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n *  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n#include <linux/fs_struct.h>\n#include <linux/file.h>\n#include <linux/slab.h>\n#include <linux/namei.h>\n#include <linux/statfs.h>\n#include <linux/utsname.h>\n#include <linux/pagemap.h>\n#include <linux/sunrpc/svcauth_gss.h>\n\n#include \"idmap.h\"\n#include \"acl.h\"\n#include \"xdr4.h\"\n#include \"vfs.h\"\n#include \"state.h\"\n#include \"cache.h\"\n#include \"netns.h\"\n#include \"pnfs.h\"\n\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n#include <linux/security.h>\n#endif\n\n\n#define NFSDDBG_FACILITY\t\tNFSDDBG_XDR\n\nconst u32 nfsd_suppattrs[3][3] = {\n\t{NFSD4_SUPPORTED_ATTRS_WORD0,\n\t NFSD4_SUPPORTED_ATTRS_WORD1,\n\t NFSD4_SUPPORTED_ATTRS_WORD2},\n\n\t{NFSD4_1_SUPPORTED_ATTRS_WORD0,\n\t NFSD4_1_SUPPORTED_ATTRS_WORD1,\n\t NFSD4_1_SUPPORTED_ATTRS_WORD2},\n\n\t{NFSD4_1_SUPPORTED_ATTRS_WORD0,\n\t NFSD4_1_SUPPORTED_ATTRS_WORD1,\n\t NFSD4_2_SUPPORTED_ATTRS_WORD2},\n};\n\n/*\n * As per referral draft, the fsid for a referral MUST be different from the fsid of the containing\n * directory in order to indicate to the client that a filesystem boundary is present\n * We use a fixed fsid for a referral\n */\n#define NFS4_REFERRAL_FSID_MAJOR\t0x8000000ULL\n#define NFS4_REFERRAL_FSID_MINOR\t0x8000000ULL\n\nstatic __be32\ncheck_filename(char *str, int len)\n{\n\tint i;\n\n\tif (len == 0)\n\t\treturn nfserr_inval;\n\tif (isdotent(str, len))\n\t\treturn nfserr_badname;\n\tfor (i = 0; i < len; i++)\n\t\tif (str[i] == '/')\n\t\t\treturn nfserr_badname;\n\treturn 0;\n}\n\n#define DECODE_HEAD\t\t\t\t\\\n\t__be32 *p;\t\t\t\t\\\n\t__be32 status\n#define DECODE_TAIL\t\t\t\t\\\n\tstatus = 0;\t\t\t\t\\\nout:\t\t\t\t\t\t\\\n\treturn status;\t\t\t\t\\\nxdr_error:\t\t\t\t\t\\\n\tdprintk(\"NFSD: xdr error (%s:%d)\\n\",\t\\\n\t\t\t__FILE__, __LINE__);\t\\\n\tstatus = nfserr_bad_xdr;\t\t\\\n\tgoto out\n\n#define READMEM(x,nbytes) do {\t\t\t\\\n\tx = (char *)p;\t\t\t\t\\\n\tp += XDR_QUADLEN(nbytes);\t\t\\\n} while (0)\n#define SAVEMEM(x,nbytes) do {\t\t\t\\\n\tif (!(x = (p==argp->tmp || p == argp->tmpp) ? \\\n \t\tsavemem(argp, p, nbytes) :\t\\\n \t\t(char *)p)) {\t\t\t\\\n\t\tdprintk(\"NFSD: xdr error (%s:%d)\\n\", \\\n\t\t\t\t__FILE__, __LINE__); \\\n\t\tgoto xdr_error;\t\t\t\\\n\t\t}\t\t\t\t\\\n\tp += XDR_QUADLEN(nbytes);\t\t\\\n} while (0)\n#define COPYMEM(x,nbytes) do {\t\t\t\\\n\tmemcpy((x), p, nbytes);\t\t\t\\\n\tp += XDR_QUADLEN(nbytes);\t\t\\\n} while (0)\n\n/* READ_BUF, read_buf(): nbytes must be <= PAGE_SIZE */\n#define READ_BUF(nbytes)  do {\t\t\t\\\n\tif (nbytes <= (u32)((char *)argp->end - (char *)argp->p)) {\t\\\n\t\tp = argp->p;\t\t\t\\\n\t\targp->p += XDR_QUADLEN(nbytes);\t\\\n\t} else if (!(p = read_buf(argp, nbytes))) { \\\n\t\tdprintk(\"NFSD: xdr error (%s:%d)\\n\", \\\n\t\t\t\t__FILE__, __LINE__); \\\n\t\tgoto xdr_error;\t\t\t\\\n\t}\t\t\t\t\t\\\n} while (0)\n\nstatic void next_decode_page(struct nfsd4_compoundargs *argp)\n{\n\targp->p = page_address(argp->pagelist[0]);\n\targp->pagelist++;\n\tif (argp->pagelen < PAGE_SIZE) {\n\t\targp->end = argp->p + (argp->pagelen>>2);\n\t\targp->pagelen = 0;\n\t} else {\n\t\targp->end = argp->p + (PAGE_SIZE>>2);\n\t\targp->pagelen -= PAGE_SIZE;\n\t}\n}\n\nstatic __be32 *read_buf(struct nfsd4_compoundargs *argp, u32 nbytes)\n{\n\t/* We want more bytes than seem to be available.\n\t * Maybe we need a new page, maybe we have just run out\n\t */\n\tunsigned int avail = (char *)argp->end - (char *)argp->p;\n\t__be32 *p;\n\tif (avail + argp->pagelen < nbytes)\n\t\treturn NULL;\n\tif (avail + PAGE_SIZE < nbytes) /* need more than a page !! */\n\t\treturn NULL;\n\t/* ok, we can do it with the current plus the next page */\n\tif (nbytes <= sizeof(argp->tmp))\n\t\tp = argp->tmp;\n\telse {\n\t\tkfree(argp->tmpp);\n\t\tp = argp->tmpp = kmalloc(nbytes, GFP_KERNEL);\n\t\tif (!p)\n\t\t\treturn NULL;\n\t\t\n\t}\n\t/*\n\t * The following memcpy is safe because read_buf is always\n\t * called with nbytes > avail, and the two cases above both\n\t * guarantee p points to at least nbytes bytes.\n\t */\n\tmemcpy(p, argp->p, avail);\n\tnext_decode_page(argp);\n\tmemcpy(((char*)p)+avail, argp->p, (nbytes - avail));\n\targp->p += XDR_QUADLEN(nbytes - avail);\n\treturn p;\n}\n\nstatic int zero_clientid(clientid_t *clid)\n{\n\treturn (clid->cl_boot == 0) && (clid->cl_id == 0);\n}\n\n/**\n * svcxdr_tmpalloc - allocate memory to be freed after compound processing\n * @argp: NFSv4 compound argument structure\n * @p: pointer to be freed (with kfree())\n *\n * Marks @p to be freed when processing the compound operation\n * described in @argp finishes.\n */\nstatic void *\nsvcxdr_tmpalloc(struct nfsd4_compoundargs *argp, u32 len)\n{\n\tstruct svcxdr_tmpbuf *tb;\n\n\ttb = kmalloc(sizeof(*tb) + len, GFP_KERNEL);\n\tif (!tb)\n\t\treturn NULL;\n\ttb->next = argp->to_free;\n\targp->to_free = tb;\n\treturn tb->buf;\n}\n\n/*\n * For xdr strings that need to be passed to other kernel api's\n * as null-terminated strings.\n *\n * Note null-terminating in place usually isn't safe since the\n * buffer might end on a page boundary.\n */\nstatic char *\nsvcxdr_dupstr(struct nfsd4_compoundargs *argp, void *buf, u32 len)\n{\n\tchar *p = svcxdr_tmpalloc(argp, len + 1);\n\n\tif (!p)\n\t\treturn NULL;\n\tmemcpy(p, buf, len);\n\tp[len] = '\\0';\n\treturn p;\n}\n\n/**\n * savemem - duplicate a chunk of memory for later processing\n * @argp: NFSv4 compound argument structure to be freed with\n * @p: pointer to be duplicated\n * @nbytes: length to be duplicated\n *\n * Returns a pointer to a copy of @nbytes bytes of memory at @p\n * that are preserved until processing of the NFSv4 compound\n * operation described by @argp finishes.\n */\nstatic char *savemem(struct nfsd4_compoundargs *argp, __be32 *p, int nbytes)\n{\n\tvoid *ret;\n\n\tret = svcxdr_tmpalloc(argp, nbytes);\n\tif (!ret)\n\t\treturn NULL;\n\tmemcpy(ret, p, nbytes);\n\treturn ret;\n}\n\n/*\n * We require the high 32 bits of 'seconds' to be 0, and\n * we ignore all 32 bits of 'nseconds'.\n */\nstatic __be32\nnfsd4_decode_time(struct nfsd4_compoundargs *argp, struct timespec *tv)\n{\n\tDECODE_HEAD;\n\tu64 sec;\n\n\tREAD_BUF(12);\n\tp = xdr_decode_hyper(p, &sec);\n\ttv->tv_sec = sec;\n\ttv->tv_nsec = be32_to_cpup(p++);\n\tif (tv->tv_nsec >= (u32)1000000000)\n\t\treturn nfserr_inval;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_bitmap(struct nfsd4_compoundargs *argp, u32 *bmval)\n{\n\tu32 bmlen;\n\tDECODE_HEAD;\n\n\tbmval[0] = 0;\n\tbmval[1] = 0;\n\tbmval[2] = 0;\n\n\tREAD_BUF(4);\n\tbmlen = be32_to_cpup(p++);\n\tif (bmlen > 1000)\n\t\tgoto xdr_error;\n\n\tREAD_BUF(bmlen << 2);\n\tif (bmlen > 0)\n\t\tbmval[0] = be32_to_cpup(p++);\n\tif (bmlen > 1)\n\t\tbmval[1] = be32_to_cpup(p++);\n\tif (bmlen > 2)\n\t\tbmval[2] = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_fattr(struct nfsd4_compoundargs *argp, u32 *bmval,\n\t\t   struct iattr *iattr, struct nfs4_acl **acl,\n\t\t   struct xdr_netobj *label, int *umask)\n{\n\tint expected_len, len = 0;\n\tu32 dummy32;\n\tchar *buf;\n\n\tDECODE_HEAD;\n\tiattr->ia_valid = 0;\n\tif ((status = nfsd4_decode_bitmap(argp, bmval)))\n\t\treturn status;\n\n\tif (bmval[0] & ~NFSD_WRITEABLE_ATTRS_WORD0\n\t    || bmval[1] & ~NFSD_WRITEABLE_ATTRS_WORD1\n\t    || bmval[2] & ~NFSD_WRITEABLE_ATTRS_WORD2) {\n\t\tif (nfsd_attrs_supported(argp->minorversion, bmval))\n\t\t\treturn nfserr_inval;\n\t\treturn nfserr_attrnotsupp;\n\t}\n\n\tREAD_BUF(4);\n\texpected_len = be32_to_cpup(p++);\n\n\tif (bmval[0] & FATTR4_WORD0_SIZE) {\n\t\tREAD_BUF(8);\n\t\tlen += 8;\n\t\tp = xdr_decode_hyper(p, &iattr->ia_size);\n\t\tiattr->ia_valid |= ATTR_SIZE;\n\t}\n\tif (bmval[0] & FATTR4_WORD0_ACL) {\n\t\tu32 nace;\n\t\tstruct nfs4_ace *ace;\n\n\t\tREAD_BUF(4); len += 4;\n\t\tnace = be32_to_cpup(p++);\n\n\t\tif (nace > NFS4_ACL_MAX)\n\t\t\treturn nfserr_fbig;\n\n\t\t*acl = svcxdr_tmpalloc(argp, nfs4_acl_bytes(nace));\n\t\tif (*acl == NULL)\n\t\t\treturn nfserr_jukebox;\n\n\t\t(*acl)->naces = nace;\n\t\tfor (ace = (*acl)->aces; ace < (*acl)->aces + nace; ace++) {\n\t\t\tREAD_BUF(16); len += 16;\n\t\t\tace->type = be32_to_cpup(p++);\n\t\t\tace->flag = be32_to_cpup(p++);\n\t\t\tace->access_mask = be32_to_cpup(p++);\n\t\t\tdummy32 = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy32);\n\t\t\tlen += XDR_QUADLEN(dummy32) << 2;\n\t\t\tREADMEM(buf, dummy32);\n\t\t\tace->whotype = nfs4_acl_get_whotype(buf, dummy32);\n\t\t\tstatus = nfs_ok;\n\t\t\tif (ace->whotype != NFS4_ACL_WHO_NAMED)\n\t\t\t\t;\n\t\t\telse if (ace->flag & NFS4_ACE_IDENTIFIER_GROUP)\n\t\t\t\tstatus = nfsd_map_name_to_gid(argp->rqstp,\n\t\t\t\t\t\tbuf, dummy32, &ace->who_gid);\n\t\t\telse\n\t\t\t\tstatus = nfsd_map_name_to_uid(argp->rqstp,\n\t\t\t\t\t\tbuf, dummy32, &ace->who_uid);\n\t\t\tif (status)\n\t\t\t\treturn status;\n\t\t}\n\t} else\n\t\t*acl = NULL;\n\tif (bmval[1] & FATTR4_WORD1_MODE) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tiattr->ia_mode = be32_to_cpup(p++);\n\t\tiattr->ia_mode &= (S_IFMT | S_IALLUGO);\n\t\tiattr->ia_valid |= ATTR_MODE;\n\t}\n\tif (bmval[1] & FATTR4_WORD1_OWNER) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy32);\n\t\tlen += (XDR_QUADLEN(dummy32) << 2);\n\t\tREADMEM(buf, dummy32);\n\t\tif ((status = nfsd_map_name_to_uid(argp->rqstp, buf, dummy32, &iattr->ia_uid)))\n\t\t\treturn status;\n\t\tiattr->ia_valid |= ATTR_UID;\n\t}\n\tif (bmval[1] & FATTR4_WORD1_OWNER_GROUP) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy32);\n\t\tlen += (XDR_QUADLEN(dummy32) << 2);\n\t\tREADMEM(buf, dummy32);\n\t\tif ((status = nfsd_map_name_to_gid(argp->rqstp, buf, dummy32, &iattr->ia_gid)))\n\t\t\treturn status;\n\t\tiattr->ia_valid |= ATTR_GID;\n\t}\n\tif (bmval[1] & FATTR4_WORD1_TIME_ACCESS_SET) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tswitch (dummy32) {\n\t\tcase NFS4_SET_TO_CLIENT_TIME:\n\t\t\tlen += 12;\n\t\t\tstatus = nfsd4_decode_time(argp, &iattr->ia_atime);\n\t\t\tif (status)\n\t\t\t\treturn status;\n\t\t\tiattr->ia_valid |= (ATTR_ATIME | ATTR_ATIME_SET);\n\t\t\tbreak;\n\t\tcase NFS4_SET_TO_SERVER_TIME:\n\t\t\tiattr->ia_valid |= ATTR_ATIME;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto xdr_error;\n\t\t}\n\t}\n\tif (bmval[1] & FATTR4_WORD1_TIME_MODIFY_SET) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tswitch (dummy32) {\n\t\tcase NFS4_SET_TO_CLIENT_TIME:\n\t\t\tlen += 12;\n\t\t\tstatus = nfsd4_decode_time(argp, &iattr->ia_mtime);\n\t\t\tif (status)\n\t\t\t\treturn status;\n\t\t\tiattr->ia_valid |= (ATTR_MTIME | ATTR_MTIME_SET);\n\t\t\tbreak;\n\t\tcase NFS4_SET_TO_SERVER_TIME:\n\t\t\tiattr->ia_valid |= ATTR_MTIME;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto xdr_error;\n\t\t}\n\t}\n\n\tlabel->len = 0;\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n\tif (bmval[2] & FATTR4_WORD2_SECURITY_LABEL) {\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++); /* lfs: we don't use it */\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++); /* pi: we don't use it either */\n\t\tREAD_BUF(4);\n\t\tlen += 4;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy32);\n\t\tif (dummy32 > NFS4_MAXLABELLEN)\n\t\t\treturn nfserr_badlabel;\n\t\tlen += (XDR_QUADLEN(dummy32) << 2);\n\t\tREADMEM(buf, dummy32);\n\t\tlabel->len = dummy32;\n\t\tlabel->data = svcxdr_dupstr(argp, buf, dummy32);\n\t\tif (!label->data)\n\t\t\treturn nfserr_jukebox;\n\t}\n#endif\n\tif (bmval[2] & FATTR4_WORD2_MODE_UMASK) {\n\t\tif (!umask)\n\t\t\tgoto xdr_error;\n\t\tREAD_BUF(8);\n\t\tlen += 8;\n\t\tdummy32 = be32_to_cpup(p++);\n\t\tiattr->ia_mode = dummy32 & (S_IFMT | S_IALLUGO);\n\t\tdummy32 = be32_to_cpup(p++);\n\t\t*umask = dummy32 & S_IRWXUGO;\n\t\tiattr->ia_valid |= ATTR_MODE;\n\t}\n\tif (len != expected_len)\n\t\tgoto xdr_error;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_stateid(struct nfsd4_compoundargs *argp, stateid_t *sid)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(sizeof(stateid_t));\n\tsid->si_generation = be32_to_cpup(p++);\n\tCOPYMEM(&sid->si_opaque, sizeof(stateid_opaque_t));\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_access(struct nfsd4_compoundargs *argp, struct nfsd4_access *access)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\taccess->ac_req_access = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32 nfsd4_decode_cb_sec(struct nfsd4_compoundargs *argp, struct nfsd4_cb_sec *cbs)\n{\n\tDECODE_HEAD;\n\tu32 dummy, uid, gid;\n\tchar *machine_name;\n\tint i;\n\tint nr_secflavs;\n\n\t/* callback_sec_params4 */\n\tREAD_BUF(4);\n\tnr_secflavs = be32_to_cpup(p++);\n\tif (nr_secflavs)\n\t\tcbs->flavor = (u32)(-1);\n\telse\n\t\t/* Is this legal? Be generous, take it to mean AUTH_NONE: */\n\t\tcbs->flavor = 0;\n\tfor (i = 0; i < nr_secflavs; ++i) {\n\t\tREAD_BUF(4);\n\t\tdummy = be32_to_cpup(p++);\n\t\tswitch (dummy) {\n\t\tcase RPC_AUTH_NULL:\n\t\t\t/* Nothing to read */\n\t\t\tif (cbs->flavor == (u32)(-1))\n\t\t\t\tcbs->flavor = RPC_AUTH_NULL;\n\t\t\tbreak;\n\t\tcase RPC_AUTH_UNIX:\n\t\t\tREAD_BUF(8);\n\t\t\t/* stamp */\n\t\t\tdummy = be32_to_cpup(p++);\n\n\t\t\t/* machine name */\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy);\n\t\t\tSAVEMEM(machine_name, dummy);\n\n\t\t\t/* uid, gid */\n\t\t\tREAD_BUF(8);\n\t\t\tuid = be32_to_cpup(p++);\n\t\t\tgid = be32_to_cpup(p++);\n\n\t\t\t/* more gids */\n\t\t\tREAD_BUF(4);\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy * 4);\n\t\t\tif (cbs->flavor == (u32)(-1)) {\n\t\t\t\tkuid_t kuid = make_kuid(&init_user_ns, uid);\n\t\t\t\tkgid_t kgid = make_kgid(&init_user_ns, gid);\n\t\t\t\tif (uid_valid(kuid) && gid_valid(kgid)) {\n\t\t\t\t\tcbs->uid = kuid;\n\t\t\t\t\tcbs->gid = kgid;\n\t\t\t\t\tcbs->flavor = RPC_AUTH_UNIX;\n\t\t\t\t} else {\n\t\t\t\t\tdprintk(\"RPC_AUTH_UNIX with invalid\"\n\t\t\t\t\t\t\"uid or gid ignoring!\\n\");\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tcase RPC_AUTH_GSS:\n\t\t\tdprintk(\"RPC_AUTH_GSS callback secflavor \"\n\t\t\t\t\"not supported!\\n\");\n\t\t\tREAD_BUF(8);\n\t\t\t/* gcbp_service */\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\t/* gcbp_handle_from_server */\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy);\n\t\t\tp += XDR_QUADLEN(dummy);\n\t\t\t/* gcbp_handle_from_client */\n\t\t\tREAD_BUF(4);\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tdprintk(\"Illegal callback secflavor\\n\");\n\t\t\treturn nfserr_inval;\n\t\t}\n\t}\n\tDECODE_TAIL;\n}\n\nstatic __be32 nfsd4_decode_backchannel_ctl(struct nfsd4_compoundargs *argp, struct nfsd4_backchannel_ctl *bc)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tbc->bc_cb_program = be32_to_cpup(p++);\n\tnfsd4_decode_cb_sec(argp, &bc->bc_cb_sec);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32 nfsd4_decode_bind_conn_to_session(struct nfsd4_compoundargs *argp, struct nfsd4_bind_conn_to_session *bcts)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(NFS4_MAX_SESSIONID_LEN + 8);\n\tCOPYMEM(bcts->sessionid.data, NFS4_MAX_SESSIONID_LEN);\n\tbcts->dir = be32_to_cpup(p++);\n\t/* XXX: skipping ctsa_use_conn_in_rdma_mode.  Perhaps Tom Tucker\n\t * could help us figure out we should be using it. */\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_close(struct nfsd4_compoundargs *argp, struct nfsd4_close *close)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tclose->cl_seqid = be32_to_cpup(p++);\n\treturn nfsd4_decode_stateid(argp, &close->cl_stateid);\n\n\tDECODE_TAIL;\n}\n\n\nstatic __be32\nnfsd4_decode_commit(struct nfsd4_compoundargs *argp, struct nfsd4_commit *commit)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(12);\n\tp = xdr_decode_hyper(p, &commit->co_offset);\n\tcommit->co_count = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_create(struct nfsd4_compoundargs *argp, struct nfsd4_create *create)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tcreate->cr_type = be32_to_cpup(p++);\n\tswitch (create->cr_type) {\n\tcase NF4LNK:\n\t\tREAD_BUF(4);\n\t\tcreate->cr_datalen = be32_to_cpup(p++);\n\t\tREAD_BUF(create->cr_datalen);\n\t\tcreate->cr_data = svcxdr_dupstr(argp, p, create->cr_datalen);\n\t\tif (!create->cr_data)\n\t\t\treturn nfserr_jukebox;\n\t\tbreak;\n\tcase NF4BLK:\n\tcase NF4CHR:\n\t\tREAD_BUF(8);\n\t\tcreate->cr_specdata1 = be32_to_cpup(p++);\n\t\tcreate->cr_specdata2 = be32_to_cpup(p++);\n\t\tbreak;\n\tcase NF4SOCK:\n\tcase NF4FIFO:\n\tcase NF4DIR:\n\tdefault:\n\t\tbreak;\n\t}\n\n\tREAD_BUF(4);\n\tcreate->cr_namelen = be32_to_cpup(p++);\n\tREAD_BUF(create->cr_namelen);\n\tSAVEMEM(create->cr_name, create->cr_namelen);\n\tif ((status = check_filename(create->cr_name, create->cr_namelen)))\n\t\treturn status;\n\n\tstatus = nfsd4_decode_fattr(argp, create->cr_bmval, &create->cr_iattr,\n\t\t\t\t    &create->cr_acl, &create->cr_label,\n\t\t\t\t    &current->fs->umask);\n\tif (status)\n\t\tgoto out;\n\n\tDECODE_TAIL;\n}\n\nstatic inline __be32\nnfsd4_decode_delegreturn(struct nfsd4_compoundargs *argp, struct nfsd4_delegreturn *dr)\n{\n\treturn nfsd4_decode_stateid(argp, &dr->dr_stateid);\n}\n\nstatic inline __be32\nnfsd4_decode_getattr(struct nfsd4_compoundargs *argp, struct nfsd4_getattr *getattr)\n{\n\treturn nfsd4_decode_bitmap(argp, getattr->ga_bmval);\n}\n\nstatic __be32\nnfsd4_decode_link(struct nfsd4_compoundargs *argp, struct nfsd4_link *link)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tlink->li_namelen = be32_to_cpup(p++);\n\tREAD_BUF(link->li_namelen);\n\tSAVEMEM(link->li_name, link->li_namelen);\n\tif ((status = check_filename(link->li_name, link->li_namelen)))\n\t\treturn status;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_lock(struct nfsd4_compoundargs *argp, struct nfsd4_lock *lock)\n{\n\tDECODE_HEAD;\n\n\t/*\n\t* type, reclaim(boolean), offset, length, new_lock_owner(boolean)\n\t*/\n\tREAD_BUF(28);\n\tlock->lk_type = be32_to_cpup(p++);\n\tif ((lock->lk_type < NFS4_READ_LT) || (lock->lk_type > NFS4_WRITEW_LT))\n\t\tgoto xdr_error;\n\tlock->lk_reclaim = be32_to_cpup(p++);\n\tp = xdr_decode_hyper(p, &lock->lk_offset);\n\tp = xdr_decode_hyper(p, &lock->lk_length);\n\tlock->lk_is_new = be32_to_cpup(p++);\n\n\tif (lock->lk_is_new) {\n\t\tREAD_BUF(4);\n\t\tlock->lk_new_open_seqid = be32_to_cpup(p++);\n\t\tstatus = nfsd4_decode_stateid(argp, &lock->lk_new_open_stateid);\n\t\tif (status)\n\t\t\treturn status;\n\t\tREAD_BUF(8 + sizeof(clientid_t));\n\t\tlock->lk_new_lock_seqid = be32_to_cpup(p++);\n\t\tCOPYMEM(&lock->lk_new_clientid, sizeof(clientid_t));\n\t\tlock->lk_new_owner.len = be32_to_cpup(p++);\n\t\tREAD_BUF(lock->lk_new_owner.len);\n\t\tREADMEM(lock->lk_new_owner.data, lock->lk_new_owner.len);\n\t} else {\n\t\tstatus = nfsd4_decode_stateid(argp, &lock->lk_old_lock_stateid);\n\t\tif (status)\n\t\t\treturn status;\n\t\tREAD_BUF(4);\n\t\tlock->lk_old_lock_seqid = be32_to_cpup(p++);\n\t}\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_lockt(struct nfsd4_compoundargs *argp, struct nfsd4_lockt *lockt)\n{\n\tDECODE_HEAD;\n\t\t        \n\tREAD_BUF(32);\n\tlockt->lt_type = be32_to_cpup(p++);\n\tif((lockt->lt_type < NFS4_READ_LT) || (lockt->lt_type > NFS4_WRITEW_LT))\n\t\tgoto xdr_error;\n\tp = xdr_decode_hyper(p, &lockt->lt_offset);\n\tp = xdr_decode_hyper(p, &lockt->lt_length);\n\tCOPYMEM(&lockt->lt_clientid, 8);\n\tlockt->lt_owner.len = be32_to_cpup(p++);\n\tREAD_BUF(lockt->lt_owner.len);\n\tREADMEM(lockt->lt_owner.data, lockt->lt_owner.len);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_locku(struct nfsd4_compoundargs *argp, struct nfsd4_locku *locku)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(8);\n\tlocku->lu_type = be32_to_cpup(p++);\n\tif ((locku->lu_type < NFS4_READ_LT) || (locku->lu_type > NFS4_WRITEW_LT))\n\t\tgoto xdr_error;\n\tlocku->lu_seqid = be32_to_cpup(p++);\n\tstatus = nfsd4_decode_stateid(argp, &locku->lu_stateid);\n\tif (status)\n\t\treturn status;\n\tREAD_BUF(16);\n\tp = xdr_decode_hyper(p, &locku->lu_offset);\n\tp = xdr_decode_hyper(p, &locku->lu_length);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_lookup(struct nfsd4_compoundargs *argp, struct nfsd4_lookup *lookup)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tlookup->lo_len = be32_to_cpup(p++);\n\tREAD_BUF(lookup->lo_len);\n\tSAVEMEM(lookup->lo_name, lookup->lo_len);\n\tif ((status = check_filename(lookup->lo_name, lookup->lo_len)))\n\t\treturn status;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32 nfsd4_decode_share_access(struct nfsd4_compoundargs *argp, u32 *share_access, u32 *deleg_want, u32 *deleg_when)\n{\n\t__be32 *p;\n\tu32 w;\n\n\tREAD_BUF(4);\n\tw = be32_to_cpup(p++);\n\t*share_access = w & NFS4_SHARE_ACCESS_MASK;\n\t*deleg_want = w & NFS4_SHARE_WANT_MASK;\n\tif (deleg_when)\n\t\t*deleg_when = w & NFS4_SHARE_WHEN_MASK;\n\n\tswitch (w & NFS4_SHARE_ACCESS_MASK) {\n\tcase NFS4_SHARE_ACCESS_READ:\n\tcase NFS4_SHARE_ACCESS_WRITE:\n\tcase NFS4_SHARE_ACCESS_BOTH:\n\t\tbreak;\n\tdefault:\n\t\treturn nfserr_bad_xdr;\n\t}\n\tw &= ~NFS4_SHARE_ACCESS_MASK;\n\tif (!w)\n\t\treturn nfs_ok;\n\tif (!argp->minorversion)\n\t\treturn nfserr_bad_xdr;\n\tswitch (w & NFS4_SHARE_WANT_MASK) {\n\tcase NFS4_SHARE_WANT_NO_PREFERENCE:\n\tcase NFS4_SHARE_WANT_READ_DELEG:\n\tcase NFS4_SHARE_WANT_WRITE_DELEG:\n\tcase NFS4_SHARE_WANT_ANY_DELEG:\n\tcase NFS4_SHARE_WANT_NO_DELEG:\n\tcase NFS4_SHARE_WANT_CANCEL:\n\t\tbreak;\n\tdefault:\n\t\treturn nfserr_bad_xdr;\n\t}\n\tw &= ~NFS4_SHARE_WANT_MASK;\n\tif (!w)\n\t\treturn nfs_ok;\n\n\tif (!deleg_when)\t/* open_downgrade */\n\t\treturn nfserr_inval;\n\tswitch (w) {\n\tcase NFS4_SHARE_SIGNAL_DELEG_WHEN_RESRC_AVAIL:\n\tcase NFS4_SHARE_PUSH_DELEG_WHEN_UNCONTENDED:\n\tcase (NFS4_SHARE_SIGNAL_DELEG_WHEN_RESRC_AVAIL |\n\t      NFS4_SHARE_PUSH_DELEG_WHEN_UNCONTENDED):\n\t\treturn nfs_ok;\n\t}\nxdr_error:\n\treturn nfserr_bad_xdr;\n}\n\nstatic __be32 nfsd4_decode_share_deny(struct nfsd4_compoundargs *argp, u32 *x)\n{\n\t__be32 *p;\n\n\tREAD_BUF(4);\n\t*x = be32_to_cpup(p++);\n\t/* Note: unlinke access bits, deny bits may be zero. */\n\tif (*x & ~NFS4_SHARE_DENY_BOTH)\n\t\treturn nfserr_bad_xdr;\n\treturn nfs_ok;\nxdr_error:\n\treturn nfserr_bad_xdr;\n}\n\nstatic __be32 nfsd4_decode_opaque(struct nfsd4_compoundargs *argp, struct xdr_netobj *o)\n{\n\t__be32 *p;\n\n\tREAD_BUF(4);\n\to->len = be32_to_cpup(p++);\n\n\tif (o->len == 0 || o->len > NFS4_OPAQUE_LIMIT)\n\t\treturn nfserr_bad_xdr;\n\n\tREAD_BUF(o->len);\n\tSAVEMEM(o->data, o->len);\n\treturn nfs_ok;\nxdr_error:\n\treturn nfserr_bad_xdr;\n}\n\nstatic __be32\nnfsd4_decode_open(struct nfsd4_compoundargs *argp, struct nfsd4_open *open)\n{\n\tDECODE_HEAD;\n\tu32 dummy;\n\n\tmemset(open->op_bmval, 0, sizeof(open->op_bmval));\n\topen->op_iattr.ia_valid = 0;\n\topen->op_openowner = NULL;\n\n\topen->op_xdr_error = 0;\n\t/* seqid, share_access, share_deny, clientid, ownerlen */\n\tREAD_BUF(4);\n\topen->op_seqid = be32_to_cpup(p++);\n\t/* decode, yet ignore deleg_when until supported */\n\tstatus = nfsd4_decode_share_access(argp, &open->op_share_access,\n\t\t\t\t\t   &open->op_deleg_want, &dummy);\n\tif (status)\n\t\tgoto xdr_error;\n\tstatus = nfsd4_decode_share_deny(argp, &open->op_share_deny);\n\tif (status)\n\t\tgoto xdr_error;\n\tREAD_BUF(sizeof(clientid_t));\n\tCOPYMEM(&open->op_clientid, sizeof(clientid_t));\n\tstatus = nfsd4_decode_opaque(argp, &open->op_owner);\n\tif (status)\n\t\tgoto xdr_error;\n\tREAD_BUF(4);\n\topen->op_create = be32_to_cpup(p++);\n\tswitch (open->op_create) {\n\tcase NFS4_OPEN_NOCREATE:\n\t\tbreak;\n\tcase NFS4_OPEN_CREATE:\n\t\tcurrent->fs->umask = 0;\n\t\tREAD_BUF(4);\n\t\topen->op_createmode = be32_to_cpup(p++);\n\t\tswitch (open->op_createmode) {\n\t\tcase NFS4_CREATE_UNCHECKED:\n\t\tcase NFS4_CREATE_GUARDED:\n\t\t\tstatus = nfsd4_decode_fattr(argp, open->op_bmval,\n\t\t\t\t&open->op_iattr, &open->op_acl, &open->op_label,\n\t\t\t\t&current->fs->umask);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tcase NFS4_CREATE_EXCLUSIVE:\n\t\t\tREAD_BUF(NFS4_VERIFIER_SIZE);\n\t\t\tCOPYMEM(open->op_verf.data, NFS4_VERIFIER_SIZE);\n\t\t\tbreak;\n\t\tcase NFS4_CREATE_EXCLUSIVE4_1:\n\t\t\tif (argp->minorversion < 1)\n\t\t\t\tgoto xdr_error;\n\t\t\tREAD_BUF(NFS4_VERIFIER_SIZE);\n\t\t\tCOPYMEM(open->op_verf.data, NFS4_VERIFIER_SIZE);\n\t\t\tstatus = nfsd4_decode_fattr(argp, open->op_bmval,\n\t\t\t\t&open->op_iattr, &open->op_acl, &open->op_label,\n\t\t\t\t&current->fs->umask);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tgoto xdr_error;\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tgoto xdr_error;\n\t}\n\n\t/* open_claim */\n\tREAD_BUF(4);\n\topen->op_claim_type = be32_to_cpup(p++);\n\tswitch (open->op_claim_type) {\n\tcase NFS4_OPEN_CLAIM_NULL:\n\tcase NFS4_OPEN_CLAIM_DELEGATE_PREV:\n\t\tREAD_BUF(4);\n\t\topen->op_fname.len = be32_to_cpup(p++);\n\t\tREAD_BUF(open->op_fname.len);\n\t\tSAVEMEM(open->op_fname.data, open->op_fname.len);\n\t\tif ((status = check_filename(open->op_fname.data, open->op_fname.len)))\n\t\t\treturn status;\n\t\tbreak;\n\tcase NFS4_OPEN_CLAIM_PREVIOUS:\n\t\tREAD_BUF(4);\n\t\topen->op_delegate_type = be32_to_cpup(p++);\n\t\tbreak;\n\tcase NFS4_OPEN_CLAIM_DELEGATE_CUR:\n\t\tstatus = nfsd4_decode_stateid(argp, &open->op_delegate_stateid);\n\t\tif (status)\n\t\t\treturn status;\n\t\tREAD_BUF(4);\n\t\topen->op_fname.len = be32_to_cpup(p++);\n\t\tREAD_BUF(open->op_fname.len);\n\t\tSAVEMEM(open->op_fname.data, open->op_fname.len);\n\t\tif ((status = check_filename(open->op_fname.data, open->op_fname.len)))\n\t\t\treturn status;\n\t\tbreak;\n\tcase NFS4_OPEN_CLAIM_FH:\n\tcase NFS4_OPEN_CLAIM_DELEG_PREV_FH:\n\t\tif (argp->minorversion < 1)\n\t\t\tgoto xdr_error;\n\t\t/* void */\n\t\tbreak;\n\tcase NFS4_OPEN_CLAIM_DELEG_CUR_FH:\n\t\tif (argp->minorversion < 1)\n\t\t\tgoto xdr_error;\n\t\tstatus = nfsd4_decode_stateid(argp, &open->op_delegate_stateid);\n\t\tif (status)\n\t\t\treturn status;\n\t\tbreak;\n\tdefault:\n\t\tgoto xdr_error;\n\t}\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_open_confirm(struct nfsd4_compoundargs *argp, struct nfsd4_open_confirm *open_conf)\n{\n\tDECODE_HEAD;\n\n\tif (argp->minorversion >= 1)\n\t\treturn nfserr_notsupp;\n\n\tstatus = nfsd4_decode_stateid(argp, &open_conf->oc_req_stateid);\n\tif (status)\n\t\treturn status;\n\tREAD_BUF(4);\n\topen_conf->oc_seqid = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_open_downgrade(struct nfsd4_compoundargs *argp, struct nfsd4_open_downgrade *open_down)\n{\n\tDECODE_HEAD;\n\t\t    \n\tstatus = nfsd4_decode_stateid(argp, &open_down->od_stateid);\n\tif (status)\n\t\treturn status;\n\tREAD_BUF(4);\n\topen_down->od_seqid = be32_to_cpup(p++);\n\tstatus = nfsd4_decode_share_access(argp, &open_down->od_share_access,\n\t\t\t\t\t   &open_down->od_deleg_want, NULL);\n\tif (status)\n\t\treturn status;\n\tstatus = nfsd4_decode_share_deny(argp, &open_down->od_share_deny);\n\tif (status)\n\t\treturn status;\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_putfh(struct nfsd4_compoundargs *argp, struct nfsd4_putfh *putfh)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tputfh->pf_fhlen = be32_to_cpup(p++);\n\tif (putfh->pf_fhlen > NFS4_FHSIZE)\n\t\tgoto xdr_error;\n\tREAD_BUF(putfh->pf_fhlen);\n\tSAVEMEM(putfh->pf_fhval, putfh->pf_fhlen);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_putpubfh(struct nfsd4_compoundargs *argp, void *p)\n{\n\tif (argp->minorversion == 0)\n\t\treturn nfs_ok;\n\treturn nfserr_notsupp;\n}\n\nstatic __be32\nnfsd4_decode_read(struct nfsd4_compoundargs *argp, struct nfsd4_read *read)\n{\n\tDECODE_HEAD;\n\n\tstatus = nfsd4_decode_stateid(argp, &read->rd_stateid);\n\tif (status)\n\t\treturn status;\n\tREAD_BUF(12);\n\tp = xdr_decode_hyper(p, &read->rd_offset);\n\tread->rd_length = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_readdir(struct nfsd4_compoundargs *argp, struct nfsd4_readdir *readdir)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(24);\n\tp = xdr_decode_hyper(p, &readdir->rd_cookie);\n\tCOPYMEM(readdir->rd_verf.data, sizeof(readdir->rd_verf.data));\n\treaddir->rd_dircount = be32_to_cpup(p++);\n\treaddir->rd_maxcount = be32_to_cpup(p++);\n\tif ((status = nfsd4_decode_bitmap(argp, readdir->rd_bmval)))\n\t\tgoto out;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_remove(struct nfsd4_compoundargs *argp, struct nfsd4_remove *remove)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tremove->rm_namelen = be32_to_cpup(p++);\n\tREAD_BUF(remove->rm_namelen);\n\tSAVEMEM(remove->rm_name, remove->rm_namelen);\n\tif ((status = check_filename(remove->rm_name, remove->rm_namelen)))\n\t\treturn status;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_rename(struct nfsd4_compoundargs *argp, struct nfsd4_rename *rename)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\trename->rn_snamelen = be32_to_cpup(p++);\n\tREAD_BUF(rename->rn_snamelen);\n\tSAVEMEM(rename->rn_sname, rename->rn_snamelen);\n\tREAD_BUF(4);\n\trename->rn_tnamelen = be32_to_cpup(p++);\n\tREAD_BUF(rename->rn_tnamelen);\n\tSAVEMEM(rename->rn_tname, rename->rn_tnamelen);\n\tif ((status = check_filename(rename->rn_sname, rename->rn_snamelen)))\n\t\treturn status;\n\tif ((status = check_filename(rename->rn_tname, rename->rn_tnamelen)))\n\t\treturn status;\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_renew(struct nfsd4_compoundargs *argp, clientid_t *clientid)\n{\n\tDECODE_HEAD;\n\n\tif (argp->minorversion >= 1)\n\t\treturn nfserr_notsupp;\n\n\tREAD_BUF(sizeof(clientid_t));\n\tCOPYMEM(clientid, sizeof(clientid_t));\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_secinfo(struct nfsd4_compoundargs *argp,\n\t\t     struct nfsd4_secinfo *secinfo)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tsecinfo->si_namelen = be32_to_cpup(p++);\n\tREAD_BUF(secinfo->si_namelen);\n\tSAVEMEM(secinfo->si_name, secinfo->si_namelen);\n\tstatus = check_filename(secinfo->si_name, secinfo->si_namelen);\n\tif (status)\n\t\treturn status;\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_secinfo_no_name(struct nfsd4_compoundargs *argp,\n\t\t     struct nfsd4_secinfo_no_name *sin)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\tsin->sin_style = be32_to_cpup(p++);\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_setattr(struct nfsd4_compoundargs *argp, struct nfsd4_setattr *setattr)\n{\n\t__be32 status;\n\n\tstatus = nfsd4_decode_stateid(argp, &setattr->sa_stateid);\n\tif (status)\n\t\treturn status;\n\treturn nfsd4_decode_fattr(argp, setattr->sa_bmval, &setattr->sa_iattr,\n\t\t\t\t  &setattr->sa_acl, &setattr->sa_label, NULL);\n}\n\nstatic __be32\nnfsd4_decode_setclientid(struct nfsd4_compoundargs *argp, struct nfsd4_setclientid *setclientid)\n{\n\tDECODE_HEAD;\n\n\tif (argp->minorversion >= 1)\n\t\treturn nfserr_notsupp;\n\n\tREAD_BUF(NFS4_VERIFIER_SIZE);\n\tCOPYMEM(setclientid->se_verf.data, NFS4_VERIFIER_SIZE);\n\n\tstatus = nfsd4_decode_opaque(argp, &setclientid->se_name);\n\tif (status)\n\t\treturn nfserr_bad_xdr;\n\tREAD_BUF(8);\n\tsetclientid->se_callback_prog = be32_to_cpup(p++);\n\tsetclientid->se_callback_netid_len = be32_to_cpup(p++);\n\tREAD_BUF(setclientid->se_callback_netid_len);\n\tSAVEMEM(setclientid->se_callback_netid_val, setclientid->se_callback_netid_len);\n\tREAD_BUF(4);\n\tsetclientid->se_callback_addr_len = be32_to_cpup(p++);\n\n\tREAD_BUF(setclientid->se_callback_addr_len);\n\tSAVEMEM(setclientid->se_callback_addr_val, setclientid->se_callback_addr_len);\n\tREAD_BUF(4);\n\tsetclientid->se_callback_ident = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_setclientid_confirm(struct nfsd4_compoundargs *argp, struct nfsd4_setclientid_confirm *scd_c)\n{\n\tDECODE_HEAD;\n\n\tif (argp->minorversion >= 1)\n\t\treturn nfserr_notsupp;\n\n\tREAD_BUF(8 + NFS4_VERIFIER_SIZE);\n\tCOPYMEM(&scd_c->sc_clientid, 8);\n\tCOPYMEM(&scd_c->sc_confirm, NFS4_VERIFIER_SIZE);\n\n\tDECODE_TAIL;\n}\n\n/* Also used for NVERIFY */\nstatic __be32\nnfsd4_decode_verify(struct nfsd4_compoundargs *argp, struct nfsd4_verify *verify)\n{\n\tDECODE_HEAD;\n\n\tif ((status = nfsd4_decode_bitmap(argp, verify->ve_bmval)))\n\t\tgoto out;\n\n\t/* For convenience's sake, we compare raw xdr'd attributes in\n\t * nfsd4_proc_verify */\n\n\tREAD_BUF(4);\n\tverify->ve_attrlen = be32_to_cpup(p++);\n\tREAD_BUF(verify->ve_attrlen);\n\tSAVEMEM(verify->ve_attrval, verify->ve_attrlen);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_write(struct nfsd4_compoundargs *argp, struct nfsd4_write *write)\n{\n\tint avail;\n\tint len;\n\tDECODE_HEAD;\n\n\tstatus = nfsd4_decode_stateid(argp, &write->wr_stateid);\n\tif (status)\n\t\treturn status;\n\tREAD_BUF(16);\n\tp = xdr_decode_hyper(p, &write->wr_offset);\n\twrite->wr_stable_how = be32_to_cpup(p++);\n\tif (write->wr_stable_how > NFS_FILE_SYNC)\n\t\tgoto xdr_error;\n\twrite->wr_buflen = be32_to_cpup(p++);\n\n\t/* Sorry .. no magic macros for this.. *\n\t * READ_BUF(write->wr_buflen);\n\t * SAVEMEM(write->wr_buf, write->wr_buflen);\n\t */\n\tavail = (char*)argp->end - (char*)argp->p;\n\tif (avail + argp->pagelen < write->wr_buflen) {\n\t\tdprintk(\"NFSD: xdr error (%s:%d)\\n\",\n\t\t\t\t__FILE__, __LINE__);\n\t\tgoto xdr_error;\n\t}\n\twrite->wr_head.iov_base = p;\n\twrite->wr_head.iov_len = avail;\n\twrite->wr_pagelist = argp->pagelist;\n\n\tlen = XDR_QUADLEN(write->wr_buflen) << 2;\n\tif (len >= avail) {\n\t\tint pages;\n\n\t\tlen -= avail;\n\n\t\tpages = len >> PAGE_SHIFT;\n\t\targp->pagelist += pages;\n\t\targp->pagelen -= pages * PAGE_SIZE;\n\t\tlen -= pages * PAGE_SIZE;\n\n\t\targp->p = (__be32 *)page_address(argp->pagelist[0]);\n\t\targp->pagelist++;\n\t\targp->end = argp->p + XDR_QUADLEN(PAGE_SIZE);\n\t}\n\targp->p += XDR_QUADLEN(len);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_release_lockowner(struct nfsd4_compoundargs *argp, struct nfsd4_release_lockowner *rlockowner)\n{\n\tDECODE_HEAD;\n\n\tif (argp->minorversion >= 1)\n\t\treturn nfserr_notsupp;\n\n\tREAD_BUF(12);\n\tCOPYMEM(&rlockowner->rl_clientid, sizeof(clientid_t));\n\trlockowner->rl_owner.len = be32_to_cpup(p++);\n\tREAD_BUF(rlockowner->rl_owner.len);\n\tREADMEM(rlockowner->rl_owner.data, rlockowner->rl_owner.len);\n\n\tif (argp->minorversion && !zero_clientid(&rlockowner->rl_clientid))\n\t\treturn nfserr_inval;\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_exchange_id(struct nfsd4_compoundargs *argp,\n\t\t\t struct nfsd4_exchange_id *exid)\n{\n\tint dummy, tmp;\n\tDECODE_HEAD;\n\n\tREAD_BUF(NFS4_VERIFIER_SIZE);\n\tCOPYMEM(exid->verifier.data, NFS4_VERIFIER_SIZE);\n\n\tstatus = nfsd4_decode_opaque(argp, &exid->clname);\n\tif (status)\n\t\treturn nfserr_bad_xdr;\n\n\tREAD_BUF(4);\n\texid->flags = be32_to_cpup(p++);\n\n\t/* Ignore state_protect4_a */\n\tREAD_BUF(4);\n\texid->spa_how = be32_to_cpup(p++);\n\tswitch (exid->spa_how) {\n\tcase SP4_NONE:\n\t\tbreak;\n\tcase SP4_MACH_CRED:\n\t\t/* spo_must_enforce */\n\t\tstatus = nfsd4_decode_bitmap(argp,\n\t\t\t\t\texid->spo_must_enforce);\n\t\tif (status)\n\t\t\tgoto out;\n\t\t/* spo_must_allow */\n\t\tstatus = nfsd4_decode_bitmap(argp, exid->spo_must_allow);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tbreak;\n\tcase SP4_SSV:\n\t\t/* ssp_ops */\n\t\tREAD_BUF(4);\n\t\tdummy = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy * 4);\n\t\tp += dummy;\n\n\t\tREAD_BUF(4);\n\t\tdummy = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy * 4);\n\t\tp += dummy;\n\n\t\t/* ssp_hash_algs<> */\n\t\tREAD_BUF(4);\n\t\ttmp = be32_to_cpup(p++);\n\t\twhile (tmp--) {\n\t\t\tREAD_BUF(4);\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy);\n\t\t\tp += XDR_QUADLEN(dummy);\n\t\t}\n\n\t\t/* ssp_encr_algs<> */\n\t\tREAD_BUF(4);\n\t\ttmp = be32_to_cpup(p++);\n\t\twhile (tmp--) {\n\t\t\tREAD_BUF(4);\n\t\t\tdummy = be32_to_cpup(p++);\n\t\t\tREAD_BUF(dummy);\n\t\t\tp += XDR_QUADLEN(dummy);\n\t\t}\n\n\t\t/* ssp_window and ssp_num_gss_handles */\n\t\tREAD_BUF(8);\n\t\tdummy = be32_to_cpup(p++);\n\t\tdummy = be32_to_cpup(p++);\n\t\tbreak;\n\tdefault:\n\t\tgoto xdr_error;\n\t}\n\n\t/* Ignore Implementation ID */\n\tREAD_BUF(4);    /* nfs_impl_id4 array length */\n\tdummy = be32_to_cpup(p++);\n\n\tif (dummy > 1)\n\t\tgoto xdr_error;\n\n\tif (dummy == 1) {\n\t\t/* nii_domain */\n\t\tREAD_BUF(4);\n\t\tdummy = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy);\n\t\tp += XDR_QUADLEN(dummy);\n\n\t\t/* nii_name */\n\t\tREAD_BUF(4);\n\t\tdummy = be32_to_cpup(p++);\n\t\tREAD_BUF(dummy);\n\t\tp += XDR_QUADLEN(dummy);\n\n\t\t/* nii_date */\n\t\tREAD_BUF(12);\n\t\tp += 3;\n\t}\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_create_session(struct nfsd4_compoundargs *argp,\n\t\t\t    struct nfsd4_create_session *sess)\n{\n\tDECODE_HEAD;\n\tu32 dummy;\n\n\tREAD_BUF(16);\n\tCOPYMEM(&sess->clientid, 8);\n\tsess->seqid = be32_to_cpup(p++);\n\tsess->flags = be32_to_cpup(p++);\n\n\t/* Fore channel attrs */\n\tREAD_BUF(28);\n\tdummy = be32_to_cpup(p++); /* headerpadsz is always 0 */\n\tsess->fore_channel.maxreq_sz = be32_to_cpup(p++);\n\tsess->fore_channel.maxresp_sz = be32_to_cpup(p++);\n\tsess->fore_channel.maxresp_cached = be32_to_cpup(p++);\n\tsess->fore_channel.maxops = be32_to_cpup(p++);\n\tsess->fore_channel.maxreqs = be32_to_cpup(p++);\n\tsess->fore_channel.nr_rdma_attrs = be32_to_cpup(p++);\n\tif (sess->fore_channel.nr_rdma_attrs == 1) {\n\t\tREAD_BUF(4);\n\t\tsess->fore_channel.rdma_attrs = be32_to_cpup(p++);\n\t} else if (sess->fore_channel.nr_rdma_attrs > 1) {\n\t\tdprintk(\"Too many fore channel attr bitmaps!\\n\");\n\t\tgoto xdr_error;\n\t}\n\n\t/* Back channel attrs */\n\tREAD_BUF(28);\n\tdummy = be32_to_cpup(p++); /* headerpadsz is always 0 */\n\tsess->back_channel.maxreq_sz = be32_to_cpup(p++);\n\tsess->back_channel.maxresp_sz = be32_to_cpup(p++);\n\tsess->back_channel.maxresp_cached = be32_to_cpup(p++);\n\tsess->back_channel.maxops = be32_to_cpup(p++);\n\tsess->back_channel.maxreqs = be32_to_cpup(p++);\n\tsess->back_channel.nr_rdma_attrs = be32_to_cpup(p++);\n\tif (sess->back_channel.nr_rdma_attrs == 1) {\n\t\tREAD_BUF(4);\n\t\tsess->back_channel.rdma_attrs = be32_to_cpup(p++);\n\t} else if (sess->back_channel.nr_rdma_attrs > 1) {\n\t\tdprintk(\"Too many back channel attr bitmaps!\\n\");\n\t\tgoto xdr_error;\n\t}\n\n\tREAD_BUF(4);\n\tsess->callback_prog = be32_to_cpup(p++);\n\tnfsd4_decode_cb_sec(argp, &sess->cb_sec);\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_destroy_session(struct nfsd4_compoundargs *argp,\n\t\t\t     struct nfsd4_destroy_session *destroy_session)\n{\n\tDECODE_HEAD;\n\tREAD_BUF(NFS4_MAX_SESSIONID_LEN);\n\tCOPYMEM(destroy_session->sessionid.data, NFS4_MAX_SESSIONID_LEN);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_free_stateid(struct nfsd4_compoundargs *argp,\n\t\t\t  struct nfsd4_free_stateid *free_stateid)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(sizeof(stateid_t));\n\tfree_stateid->fr_stateid.si_generation = be32_to_cpup(p++);\n\tCOPYMEM(&free_stateid->fr_stateid.si_opaque, sizeof(stateid_opaque_t));\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_sequence(struct nfsd4_compoundargs *argp,\n\t\t      struct nfsd4_sequence *seq)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(NFS4_MAX_SESSIONID_LEN + 16);\n\tCOPYMEM(seq->sessionid.data, NFS4_MAX_SESSIONID_LEN);\n\tseq->seqid = be32_to_cpup(p++);\n\tseq->slotid = be32_to_cpup(p++);\n\tseq->maxslots = be32_to_cpup(p++);\n\tseq->cachethis = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_test_stateid(struct nfsd4_compoundargs *argp, struct nfsd4_test_stateid *test_stateid)\n{\n\tint i;\n\t__be32 *p, status;\n\tstruct nfsd4_test_stateid_id *stateid;\n\n\tREAD_BUF(4);\n\ttest_stateid->ts_num_ids = ntohl(*p++);\n\n\tINIT_LIST_HEAD(&test_stateid->ts_stateid_list);\n\n\tfor (i = 0; i < test_stateid->ts_num_ids; i++) {\n\t\tstateid = svcxdr_tmpalloc(argp, sizeof(*stateid));\n\t\tif (!stateid) {\n\t\t\tstatus = nfserrno(-ENOMEM);\n\t\t\tgoto out;\n\t\t}\n\n\t\tINIT_LIST_HEAD(&stateid->ts_id_list);\n\t\tlist_add_tail(&stateid->ts_id_list, &test_stateid->ts_stateid_list);\n\n\t\tstatus = nfsd4_decode_stateid(argp, &stateid->ts_id_stateid);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tstatus = 0;\nout:\n\treturn status;\nxdr_error:\n\tdprintk(\"NFSD: xdr error (%s:%d)\\n\", __FILE__, __LINE__);\n\tstatus = nfserr_bad_xdr;\n\tgoto out;\n}\n\nstatic __be32 nfsd4_decode_destroy_clientid(struct nfsd4_compoundargs *argp, struct nfsd4_destroy_clientid *dc)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(8);\n\tCOPYMEM(&dc->clientid, 8);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32 nfsd4_decode_reclaim_complete(struct nfsd4_compoundargs *argp, struct nfsd4_reclaim_complete *rc)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(4);\n\trc->rca_one_fs = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\n#ifdef CONFIG_NFSD_PNFS\nstatic __be32\nnfsd4_decode_getdeviceinfo(struct nfsd4_compoundargs *argp,\n\t\tstruct nfsd4_getdeviceinfo *gdev)\n{\n\tDECODE_HEAD;\n\tu32 num, i;\n\n\tREAD_BUF(sizeof(struct nfsd4_deviceid) + 3 * 4);\n\tCOPYMEM(&gdev->gd_devid, sizeof(struct nfsd4_deviceid));\n\tgdev->gd_layout_type = be32_to_cpup(p++);\n\tgdev->gd_maxcount = be32_to_cpup(p++);\n\tnum = be32_to_cpup(p++);\n\tif (num) {\n\t\tREAD_BUF(4 * num);\n\t\tgdev->gd_notify_types = be32_to_cpup(p++);\n\t\tfor (i = 1; i < num; i++) {\n\t\t\tif (be32_to_cpup(p++)) {\n\t\t\t\tstatus = nfserr_inval;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_layoutget(struct nfsd4_compoundargs *argp,\n\t\tstruct nfsd4_layoutget *lgp)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(36);\n\tlgp->lg_signal = be32_to_cpup(p++);\n\tlgp->lg_layout_type = be32_to_cpup(p++);\n\tlgp->lg_seg.iomode = be32_to_cpup(p++);\n\tp = xdr_decode_hyper(p, &lgp->lg_seg.offset);\n\tp = xdr_decode_hyper(p, &lgp->lg_seg.length);\n\tp = xdr_decode_hyper(p, &lgp->lg_minlength);\n\n\tstatus = nfsd4_decode_stateid(argp, &lgp->lg_sid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(4);\n\tlgp->lg_maxcount = be32_to_cpup(p++);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_layoutcommit(struct nfsd4_compoundargs *argp,\n\t\tstruct nfsd4_layoutcommit *lcp)\n{\n\tDECODE_HEAD;\n\tu32 timechange;\n\n\tREAD_BUF(20);\n\tp = xdr_decode_hyper(p, &lcp->lc_seg.offset);\n\tp = xdr_decode_hyper(p, &lcp->lc_seg.length);\n\tlcp->lc_reclaim = be32_to_cpup(p++);\n\n\tstatus = nfsd4_decode_stateid(argp, &lcp->lc_sid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(4);\n\tlcp->lc_newoffset = be32_to_cpup(p++);\n\tif (lcp->lc_newoffset) {\n\t\tREAD_BUF(8);\n\t\tp = xdr_decode_hyper(p, &lcp->lc_last_wr);\n\t} else\n\t\tlcp->lc_last_wr = 0;\n\tREAD_BUF(4);\n\ttimechange = be32_to_cpup(p++);\n\tif (timechange) {\n\t\tstatus = nfsd4_decode_time(argp, &lcp->lc_mtime);\n\t\tif (status)\n\t\t\treturn status;\n\t} else {\n\t\tlcp->lc_mtime.tv_nsec = UTIME_NOW;\n\t}\n\tREAD_BUF(8);\n\tlcp->lc_layout_type = be32_to_cpup(p++);\n\n\t/*\n\t * Save the layout update in XDR format and let the layout driver deal\n\t * with it later.\n\t */\n\tlcp->lc_up_len = be32_to_cpup(p++);\n\tif (lcp->lc_up_len > 0) {\n\t\tREAD_BUF(lcp->lc_up_len);\n\t\tREADMEM(lcp->lc_up_layout, lcp->lc_up_len);\n\t}\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_layoutreturn(struct nfsd4_compoundargs *argp,\n\t\tstruct nfsd4_layoutreturn *lrp)\n{\n\tDECODE_HEAD;\n\n\tREAD_BUF(16);\n\tlrp->lr_reclaim = be32_to_cpup(p++);\n\tlrp->lr_layout_type = be32_to_cpup(p++);\n\tlrp->lr_seg.iomode = be32_to_cpup(p++);\n\tlrp->lr_return_type = be32_to_cpup(p++);\n\tif (lrp->lr_return_type == RETURN_FILE) {\n\t\tREAD_BUF(16);\n\t\tp = xdr_decode_hyper(p, &lrp->lr_seg.offset);\n\t\tp = xdr_decode_hyper(p, &lrp->lr_seg.length);\n\n\t\tstatus = nfsd4_decode_stateid(argp, &lrp->lr_sid);\n\t\tif (status)\n\t\t\treturn status;\n\n\t\tREAD_BUF(4);\n\t\tlrp->lrf_body_len = be32_to_cpup(p++);\n\t\tif (lrp->lrf_body_len > 0) {\n\t\t\tREAD_BUF(lrp->lrf_body_len);\n\t\t\tREADMEM(lrp->lrf_body, lrp->lrf_body_len);\n\t\t}\n\t} else {\n\t\tlrp->lr_seg.offset = 0;\n\t\tlrp->lr_seg.length = NFS4_MAX_UINT64;\n\t}\n\n\tDECODE_TAIL;\n}\n#endif /* CONFIG_NFSD_PNFS */\n\nstatic __be32\nnfsd4_decode_fallocate(struct nfsd4_compoundargs *argp,\n\t\t       struct nfsd4_fallocate *fallocate)\n{\n\tDECODE_HEAD;\n\n\tstatus = nfsd4_decode_stateid(argp, &fallocate->falloc_stateid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(16);\n\tp = xdr_decode_hyper(p, &fallocate->falloc_offset);\n\txdr_decode_hyper(p, &fallocate->falloc_length);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_clone(struct nfsd4_compoundargs *argp, struct nfsd4_clone *clone)\n{\n\tDECODE_HEAD;\n\n\tstatus = nfsd4_decode_stateid(argp, &clone->cl_src_stateid);\n\tif (status)\n\t\treturn status;\n\tstatus = nfsd4_decode_stateid(argp, &clone->cl_dst_stateid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(8 + 8 + 8);\n\tp = xdr_decode_hyper(p, &clone->cl_src_pos);\n\tp = xdr_decode_hyper(p, &clone->cl_dst_pos);\n\tp = xdr_decode_hyper(p, &clone->cl_count);\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_copy(struct nfsd4_compoundargs *argp, struct nfsd4_copy *copy)\n{\n\tDECODE_HEAD;\n\tunsigned int tmp;\n\n\tstatus = nfsd4_decode_stateid(argp, &copy->cp_src_stateid);\n\tif (status)\n\t\treturn status;\n\tstatus = nfsd4_decode_stateid(argp, &copy->cp_dst_stateid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(8 + 8 + 8 + 4 + 4 + 4);\n\tp = xdr_decode_hyper(p, &copy->cp_src_pos);\n\tp = xdr_decode_hyper(p, &copy->cp_dst_pos);\n\tp = xdr_decode_hyper(p, &copy->cp_count);\n\tcopy->cp_consecutive = be32_to_cpup(p++);\n\tcopy->cp_synchronous = be32_to_cpup(p++);\n\ttmp = be32_to_cpup(p); /* Source server list not supported */\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_seek(struct nfsd4_compoundargs *argp, struct nfsd4_seek *seek)\n{\n\tDECODE_HEAD;\n\n\tstatus = nfsd4_decode_stateid(argp, &seek->seek_stateid);\n\tif (status)\n\t\treturn status;\n\n\tREAD_BUF(8 + 4);\n\tp = xdr_decode_hyper(p, &seek->seek_offset);\n\tseek->seek_whence = be32_to_cpup(p);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32\nnfsd4_decode_noop(struct nfsd4_compoundargs *argp, void *p)\n{\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_decode_notsupp(struct nfsd4_compoundargs *argp, void *p)\n{\n\treturn nfserr_notsupp;\n}\n\ntypedef __be32(*nfsd4_dec)(struct nfsd4_compoundargs *argp, void *);\n\nstatic nfsd4_dec nfsd4_dec_ops[] = {\n\t[OP_ACCESS]\t\t= (nfsd4_dec)nfsd4_decode_access,\n\t[OP_CLOSE]\t\t= (nfsd4_dec)nfsd4_decode_close,\n\t[OP_COMMIT]\t\t= (nfsd4_dec)nfsd4_decode_commit,\n\t[OP_CREATE]\t\t= (nfsd4_dec)nfsd4_decode_create,\n\t[OP_DELEGPURGE]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_DELEGRETURN]\t= (nfsd4_dec)nfsd4_decode_delegreturn,\n\t[OP_GETATTR]\t\t= (nfsd4_dec)nfsd4_decode_getattr,\n\t[OP_GETFH]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_LINK]\t\t= (nfsd4_dec)nfsd4_decode_link,\n\t[OP_LOCK]\t\t= (nfsd4_dec)nfsd4_decode_lock,\n\t[OP_LOCKT]\t\t= (nfsd4_dec)nfsd4_decode_lockt,\n\t[OP_LOCKU]\t\t= (nfsd4_dec)nfsd4_decode_locku,\n\t[OP_LOOKUP]\t\t= (nfsd4_dec)nfsd4_decode_lookup,\n\t[OP_LOOKUPP]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_NVERIFY]\t\t= (nfsd4_dec)nfsd4_decode_verify,\n\t[OP_OPEN]\t\t= (nfsd4_dec)nfsd4_decode_open,\n\t[OP_OPENATTR]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_OPEN_CONFIRM]\t= (nfsd4_dec)nfsd4_decode_open_confirm,\n\t[OP_OPEN_DOWNGRADE]\t= (nfsd4_dec)nfsd4_decode_open_downgrade,\n\t[OP_PUTFH]\t\t= (nfsd4_dec)nfsd4_decode_putfh,\n\t[OP_PUTPUBFH]\t\t= (nfsd4_dec)nfsd4_decode_putpubfh,\n\t[OP_PUTROOTFH]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_READ]\t\t= (nfsd4_dec)nfsd4_decode_read,\n\t[OP_READDIR]\t\t= (nfsd4_dec)nfsd4_decode_readdir,\n\t[OP_READLINK]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_REMOVE]\t\t= (nfsd4_dec)nfsd4_decode_remove,\n\t[OP_RENAME]\t\t= (nfsd4_dec)nfsd4_decode_rename,\n\t[OP_RENEW]\t\t= (nfsd4_dec)nfsd4_decode_renew,\n\t[OP_RESTOREFH]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_SAVEFH]\t\t= (nfsd4_dec)nfsd4_decode_noop,\n\t[OP_SECINFO]\t\t= (nfsd4_dec)nfsd4_decode_secinfo,\n\t[OP_SETATTR]\t\t= (nfsd4_dec)nfsd4_decode_setattr,\n\t[OP_SETCLIENTID]\t= (nfsd4_dec)nfsd4_decode_setclientid,\n\t[OP_SETCLIENTID_CONFIRM] = (nfsd4_dec)nfsd4_decode_setclientid_confirm,\n\t[OP_VERIFY]\t\t= (nfsd4_dec)nfsd4_decode_verify,\n\t[OP_WRITE]\t\t= (nfsd4_dec)nfsd4_decode_write,\n\t[OP_RELEASE_LOCKOWNER]\t= (nfsd4_dec)nfsd4_decode_release_lockowner,\n\n\t/* new operations for NFSv4.1 */\n\t[OP_BACKCHANNEL_CTL]\t= (nfsd4_dec)nfsd4_decode_backchannel_ctl,\n\t[OP_BIND_CONN_TO_SESSION]= (nfsd4_dec)nfsd4_decode_bind_conn_to_session,\n\t[OP_EXCHANGE_ID]\t= (nfsd4_dec)nfsd4_decode_exchange_id,\n\t[OP_CREATE_SESSION]\t= (nfsd4_dec)nfsd4_decode_create_session,\n\t[OP_DESTROY_SESSION]\t= (nfsd4_dec)nfsd4_decode_destroy_session,\n\t[OP_FREE_STATEID]\t= (nfsd4_dec)nfsd4_decode_free_stateid,\n\t[OP_GET_DIR_DELEGATION]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n#ifdef CONFIG_NFSD_PNFS\n\t[OP_GETDEVICEINFO]\t= (nfsd4_dec)nfsd4_decode_getdeviceinfo,\n\t[OP_GETDEVICELIST]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTCOMMIT]\t= (nfsd4_dec)nfsd4_decode_layoutcommit,\n\t[OP_LAYOUTGET]\t\t= (nfsd4_dec)nfsd4_decode_layoutget,\n\t[OP_LAYOUTRETURN]\t= (nfsd4_dec)nfsd4_decode_layoutreturn,\n#else\n\t[OP_GETDEVICEINFO]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_GETDEVICELIST]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTCOMMIT]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTGET]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTRETURN]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n#endif\n\t[OP_SECINFO_NO_NAME]\t= (nfsd4_dec)nfsd4_decode_secinfo_no_name,\n\t[OP_SEQUENCE]\t\t= (nfsd4_dec)nfsd4_decode_sequence,\n\t[OP_SET_SSV]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_TEST_STATEID]\t= (nfsd4_dec)nfsd4_decode_test_stateid,\n\t[OP_WANT_DELEGATION]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_DESTROY_CLIENTID]\t= (nfsd4_dec)nfsd4_decode_destroy_clientid,\n\t[OP_RECLAIM_COMPLETE]\t= (nfsd4_dec)nfsd4_decode_reclaim_complete,\n\n\t/* new operations for NFSv4.2 */\n\t[OP_ALLOCATE]\t\t= (nfsd4_dec)nfsd4_decode_fallocate,\n\t[OP_COPY]\t\t= (nfsd4_dec)nfsd4_decode_copy,\n\t[OP_COPY_NOTIFY]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_DEALLOCATE]\t\t= (nfsd4_dec)nfsd4_decode_fallocate,\n\t[OP_IO_ADVISE]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTERROR]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_LAYOUTSTATS]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_OFFLOAD_CANCEL]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_OFFLOAD_STATUS]\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_READ_PLUS]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_SEEK]\t\t= (nfsd4_dec)nfsd4_decode_seek,\n\t[OP_WRITE_SAME]\t\t= (nfsd4_dec)nfsd4_decode_notsupp,\n\t[OP_CLONE]\t\t= (nfsd4_dec)nfsd4_decode_clone,\n};\n\nstatic inline bool\nnfsd4_opnum_in_range(struct nfsd4_compoundargs *argp, struct nfsd4_op *op)\n{\n\tif (op->opnum < FIRST_NFS4_OP)\n\t\treturn false;\n\telse if (argp->minorversion == 0 && op->opnum > LAST_NFS40_OP)\n\t\treturn false;\n\telse if (argp->minorversion == 1 && op->opnum > LAST_NFS41_OP)\n\t\treturn false;\n\telse if (argp->minorversion == 2 && op->opnum > LAST_NFS42_OP)\n\t\treturn false;\n\treturn true;\n}\n\nstatic __be32\nnfsd4_decode_compound(struct nfsd4_compoundargs *argp)\n{\n\tDECODE_HEAD;\n\tstruct nfsd4_op *op;\n\tbool cachethis = false;\n\tint auth_slack= argp->rqstp->rq_auth_slack;\n\tint max_reply = auth_slack + 8; /* opcnt, status */\n\tint readcount = 0;\n\tint readbytes = 0;\n\tint i;\n\n\tREAD_BUF(4);\n\targp->taglen = be32_to_cpup(p++);\n\tREAD_BUF(argp->taglen);\n\tSAVEMEM(argp->tag, argp->taglen);\n\tREAD_BUF(8);\n\targp->minorversion = be32_to_cpup(p++);\n\targp->opcnt = be32_to_cpup(p++);\n\tmax_reply += 4 + (XDR_QUADLEN(argp->taglen) << 2);\n\n\tif (argp->taglen > NFSD4_MAX_TAGLEN)\n\t\tgoto xdr_error;\n\tif (argp->opcnt > 100)\n\t\tgoto xdr_error;\n\n\tif (argp->opcnt > ARRAY_SIZE(argp->iops)) {\n\t\targp->ops = kzalloc(argp->opcnt * sizeof(*argp->ops), GFP_KERNEL);\n\t\tif (!argp->ops) {\n\t\t\targp->ops = argp->iops;\n\t\t\tdprintk(\"nfsd: couldn't allocate room for COMPOUND\\n\");\n\t\t\tgoto xdr_error;\n\t\t}\n\t}\n\n\tif (argp->minorversion > NFSD_SUPPORTED_MINOR_VERSION)\n\t\targp->opcnt = 0;\n\n\tfor (i = 0; i < argp->opcnt; i++) {\n\t\top = &argp->ops[i];\n\t\top->replay = NULL;\n\n\t\tREAD_BUF(4);\n\t\top->opnum = be32_to_cpup(p++);\n\n\t\tif (nfsd4_opnum_in_range(argp, op))\n\t\t\top->status = nfsd4_dec_ops[op->opnum](argp, &op->u);\n\t\telse {\n\t\t\top->opnum = OP_ILLEGAL;\n\t\t\top->status = nfserr_op_illegal;\n\t\t}\n\t\t/*\n\t\t * We'll try to cache the result in the DRC if any one\n\t\t * op in the compound wants to be cached:\n\t\t */\n\t\tcachethis |= nfsd4_cache_this_op(op);\n\n\t\tif (op->opnum == OP_READ) {\n\t\t\treadcount++;\n\t\t\treadbytes += nfsd4_max_reply(argp->rqstp, op);\n\t\t} else\n\t\t\tmax_reply += nfsd4_max_reply(argp->rqstp, op);\n\t\t/*\n\t\t * OP_LOCK and OP_LOCKT may return a conflicting lock.\n\t\t * (Special case because it will just skip encoding this\n\t\t * if it runs out of xdr buffer space, and it is the only\n\t\t * operation that behaves this way.)\n\t\t */\n\t\tif (op->opnum == OP_LOCK || op->opnum == OP_LOCKT)\n\t\t\tmax_reply += NFS4_OPAQUE_LIMIT;\n\n\t\tif (op->status) {\n\t\t\targp->opcnt = i+1;\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Sessions make the DRC unnecessary: */\n\tif (argp->minorversion)\n\t\tcachethis = false;\n\tsvc_reserve(argp->rqstp, max_reply + readbytes);\n\targp->rqstp->rq_cachetype = cachethis ? RC_REPLBUFF : RC_NOCACHE;\n\n\tif (readcount > 1 || max_reply > PAGE_SIZE - auth_slack)\n\t\tclear_bit(RQ_SPLICE_OK, &argp->rqstp->rq_flags);\n\n\tDECODE_TAIL;\n}\n\nstatic __be32 *encode_change(__be32 *p, struct kstat *stat, struct inode *inode,\n\t\t\t     struct svc_export *exp)\n{\n\tif (exp->ex_flags & NFSEXP_V4ROOT) {\n\t\t*p++ = cpu_to_be32(convert_to_wallclock(exp->cd->flush_time));\n\t\t*p++ = 0;\n\t} else if (IS_I_VERSION(inode)) {\n\t\tp = xdr_encode_hyper(p, inode->i_version);\n\t} else {\n\t\t*p++ = cpu_to_be32(stat->ctime.tv_sec);\n\t\t*p++ = cpu_to_be32(stat->ctime.tv_nsec);\n\t}\n\treturn p;\n}\n\nstatic __be32 *encode_cinfo(__be32 *p, struct nfsd4_change_info *c)\n{\n\t*p++ = cpu_to_be32(c->atomic);\n\tif (c->change_supported) {\n\t\tp = xdr_encode_hyper(p, c->before_change);\n\t\tp = xdr_encode_hyper(p, c->after_change);\n\t} else {\n\t\t*p++ = cpu_to_be32(c->before_ctime_sec);\n\t\t*p++ = cpu_to_be32(c->before_ctime_nsec);\n\t\t*p++ = cpu_to_be32(c->after_ctime_sec);\n\t\t*p++ = cpu_to_be32(c->after_ctime_nsec);\n\t}\n\treturn p;\n}\n\n/* Encode as an array of strings the string given with components\n * separated @sep, escaped with esc_enter and esc_exit.\n */\nstatic __be32 nfsd4_encode_components_esc(struct xdr_stream *xdr, char sep,\n\t\t\t\t\t  char *components, char esc_enter,\n\t\t\t\t\t  char esc_exit)\n{\n\t__be32 *p;\n\t__be32 pathlen;\n\tint pathlen_offset;\n\tint strlen, count=0;\n\tchar *str, *end, *next;\n\n\tdprintk(\"nfsd4_encode_components(%s)\\n\", components);\n\n\tpathlen_offset = xdr->buf->len;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tp++; /* We will fill this in with @count later */\n\n\tend = str = components;\n\twhile (*end) {\n\t\tbool found_esc = false;\n\n\t\t/* try to parse as esc_start, ..., esc_end, sep */\n\t\tif (*str == esc_enter) {\n\t\t\tfor (; *end && (*end != esc_exit); end++)\n\t\t\t\t/* find esc_exit or end of string */;\n\t\t\tnext = end + 1;\n\t\t\tif (*end && (!*next || *next == sep)) {\n\t\t\t\tstr++;\n\t\t\t\tfound_esc = true;\n\t\t\t}\n\t\t}\n\n\t\tif (!found_esc)\n\t\t\tfor (; *end && (*end != sep); end++)\n\t\t\t\t/* find sep or end of string */;\n\n\t\tstrlen = end - str;\n\t\tif (strlen) {\n\t\t\tp = xdr_reserve_space(xdr, strlen + 4);\n\t\t\tif (!p)\n\t\t\t\treturn nfserr_resource;\n\t\t\tp = xdr_encode_opaque(p, str, strlen);\n\t\t\tcount++;\n\t\t}\n\t\telse\n\t\t\tend++;\n\t\tif (found_esc)\n\t\t\tend = next;\n\n\t\tstr = end;\n\t}\n\tpathlen = htonl(count);\n\twrite_bytes_to_xdr_buf(xdr->buf, pathlen_offset, &pathlen, 4);\n\treturn 0;\n}\n\n/* Encode as an array of strings the string given with components\n * separated @sep.\n */\nstatic __be32 nfsd4_encode_components(struct xdr_stream *xdr, char sep,\n\t\t\t\t      char *components)\n{\n\treturn nfsd4_encode_components_esc(xdr, sep, components, 0, 0);\n}\n\n/*\n * encode a location element of a fs_locations structure\n */\nstatic __be32 nfsd4_encode_fs_location4(struct xdr_stream *xdr,\n\t\t\t\t\tstruct nfsd4_fs_location *location)\n{\n\t__be32 status;\n\n\tstatus = nfsd4_encode_components_esc(xdr, ':', location->hosts,\n\t\t\t\t\t\t'[', ']');\n\tif (status)\n\t\treturn status;\n\tstatus = nfsd4_encode_components(xdr, '/', location->path);\n\tif (status)\n\t\treturn status;\n\treturn 0;\n}\n\n/*\n * Encode a path in RFC3530 'pathname4' format\n */\nstatic __be32 nfsd4_encode_path(struct xdr_stream *xdr,\n\t\t\t\tconst struct path *root,\n\t\t\t\tconst struct path *path)\n{\n\tstruct path cur = *path;\n\t__be32 *p;\n\tstruct dentry **components = NULL;\n\tunsigned int ncomponents = 0;\n\t__be32 err = nfserr_jukebox;\n\n\tdprintk(\"nfsd4_encode_components(\");\n\n\tpath_get(&cur);\n\t/* First walk the path up to the nfsd root, and store the\n\t * dentries/path components in an array.\n\t */\n\tfor (;;) {\n\t\tif (path_equal(&cur, root))\n\t\t\tbreak;\n\t\tif (cur.dentry == cur.mnt->mnt_root) {\n\t\t\tif (follow_up(&cur))\n\t\t\t\tcontinue;\n\t\t\tgoto out_free;\n\t\t}\n\t\tif ((ncomponents & 15) == 0) {\n\t\t\tstruct dentry **new;\n\t\t\tnew = krealloc(components,\n\t\t\t\t\tsizeof(*new) * (ncomponents + 16),\n\t\t\t\t\tGFP_KERNEL);\n\t\t\tif (!new)\n\t\t\t\tgoto out_free;\n\t\t\tcomponents = new;\n\t\t}\n\t\tcomponents[ncomponents++] = cur.dentry;\n\t\tcur.dentry = dget_parent(cur.dentry);\n\t}\n\terr = nfserr_resource;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\tgoto out_free;\n\t*p++ = cpu_to_be32(ncomponents);\n\n\twhile (ncomponents) {\n\t\tstruct dentry *dentry = components[ncomponents - 1];\n\t\tunsigned int len;\n\n\t\tspin_lock(&dentry->d_lock);\n\t\tlen = dentry->d_name.len;\n\t\tp = xdr_reserve_space(xdr, len + 4);\n\t\tif (!p) {\n\t\t\tspin_unlock(&dentry->d_lock);\n\t\t\tgoto out_free;\n\t\t}\n\t\tp = xdr_encode_opaque(p, dentry->d_name.name, len);\n\t\tdprintk(\"/%pd\", dentry);\n\t\tspin_unlock(&dentry->d_lock);\n\t\tdput(dentry);\n\t\tncomponents--;\n\t}\n\n\terr = 0;\nout_free:\n\tdprintk(\")\\n\");\n\twhile (ncomponents)\n\t\tdput(components[--ncomponents]);\n\tkfree(components);\n\tpath_put(&cur);\n\treturn err;\n}\n\nstatic __be32 nfsd4_encode_fsloc_fsroot(struct xdr_stream *xdr,\n\t\t\tstruct svc_rqst *rqstp, const struct path *path)\n{\n\tstruct svc_export *exp_ps;\n\t__be32 res;\n\n\texp_ps = rqst_find_fsidzero_export(rqstp);\n\tif (IS_ERR(exp_ps))\n\t\treturn nfserrno(PTR_ERR(exp_ps));\n\tres = nfsd4_encode_path(xdr, &exp_ps->ex_path, path);\n\texp_put(exp_ps);\n\treturn res;\n}\n\n/*\n *  encode a fs_locations structure\n */\nstatic __be32 nfsd4_encode_fs_locations(struct xdr_stream *xdr,\n\t\t\tstruct svc_rqst *rqstp, struct svc_export *exp)\n{\n\t__be32 status;\n\tint i;\n\t__be32 *p;\n\tstruct nfsd4_fs_locations *fslocs = &exp->ex_fslocs;\n\n\tstatus = nfsd4_encode_fsloc_fsroot(xdr, rqstp, &exp->ex_path);\n\tif (status)\n\t\treturn status;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(fslocs->locations_count);\n\tfor (i=0; i<fslocs->locations_count; i++) {\n\t\tstatus = nfsd4_encode_fs_location4(xdr, &fslocs->locations[i]);\n\t\tif (status)\n\t\t\treturn status;\n\t}\n\treturn 0;\n}\n\nstatic u32 nfs4_file_type(umode_t mode)\n{\n\tswitch (mode & S_IFMT) {\n\tcase S_IFIFO:\treturn NF4FIFO;\n\tcase S_IFCHR:\treturn NF4CHR;\n\tcase S_IFDIR:\treturn NF4DIR;\n\tcase S_IFBLK:\treturn NF4BLK;\n\tcase S_IFLNK:\treturn NF4LNK;\n\tcase S_IFREG:\treturn NF4REG;\n\tcase S_IFSOCK:\treturn NF4SOCK;\n\tdefault:\treturn NF4BAD;\n\t};\n}\n\nstatic inline __be32\nnfsd4_encode_aclname(struct xdr_stream *xdr, struct svc_rqst *rqstp,\n\t\t     struct nfs4_ace *ace)\n{\n\tif (ace->whotype != NFS4_ACL_WHO_NAMED)\n\t\treturn nfs4_acl_write_who(xdr, ace->whotype);\n\telse if (ace->flag & NFS4_ACE_IDENTIFIER_GROUP)\n\t\treturn nfsd4_encode_group(xdr, rqstp, ace->who_gid);\n\telse\n\t\treturn nfsd4_encode_user(xdr, rqstp, ace->who_uid);\n}\n\nstatic inline __be32\nnfsd4_encode_layout_types(struct xdr_stream *xdr, u32 layout_types)\n{\n\t__be32\t\t*p;\n\tunsigned long\ti = hweight_long(layout_types);\n\n\tp = xdr_reserve_space(xdr, 4 + 4 * i);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t*p++ = cpu_to_be32(i);\n\n\tfor (i = LAYOUT_NFSV4_1_FILES; i < LAYOUT_TYPE_MAX; ++i)\n\t\tif (layout_types & (1 << i))\n\t\t\t*p++ = cpu_to_be32(i);\n\n\treturn 0;\n}\n\n#define WORD0_ABSENT_FS_ATTRS (FATTR4_WORD0_FS_LOCATIONS | FATTR4_WORD0_FSID | \\\n\t\t\t      FATTR4_WORD0_RDATTR_ERROR)\n#define WORD1_ABSENT_FS_ATTRS FATTR4_WORD1_MOUNTED_ON_FILEID\n#define WORD2_ABSENT_FS_ATTRS 0\n\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\nstatic inline __be32\nnfsd4_encode_security_label(struct xdr_stream *xdr, struct svc_rqst *rqstp,\n\t\t\t    void *context, int len)\n{\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, len + 4 + 4 + 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t/*\n\t * For now we use a 0 here to indicate the null translation; in\n\t * the future we may place a call to translation code here.\n\t */\n\t*p++ = cpu_to_be32(0); /* lfs */\n\t*p++ = cpu_to_be32(0); /* pi */\n\tp = xdr_encode_opaque(p, context, len);\n\treturn 0;\n}\n#else\nstatic inline __be32\nnfsd4_encode_security_label(struct xdr_stream *xdr, struct svc_rqst *rqstp,\n\t\t\t    void *context, int len)\n{ return 0; }\n#endif\n\nstatic __be32 fattr_handle_absent_fs(u32 *bmval0, u32 *bmval1, u32 *bmval2, u32 *rdattr_err)\n{\n\t/* As per referral draft:  */\n\tif (*bmval0 & ~WORD0_ABSENT_FS_ATTRS ||\n\t    *bmval1 & ~WORD1_ABSENT_FS_ATTRS) {\n\t\tif (*bmval0 & FATTR4_WORD0_RDATTR_ERROR ||\n\t            *bmval0 & FATTR4_WORD0_FS_LOCATIONS)\n\t\t\t*rdattr_err = NFSERR_MOVED;\n\t\telse\n\t\t\treturn nfserr_moved;\n\t}\n\t*bmval0 &= WORD0_ABSENT_FS_ATTRS;\n\t*bmval1 &= WORD1_ABSENT_FS_ATTRS;\n\t*bmval2 &= WORD2_ABSENT_FS_ATTRS;\n\treturn 0;\n}\n\n\nstatic int get_parent_attributes(struct svc_export *exp, struct kstat *stat)\n{\n\tstruct path path = exp->ex_path;\n\tint err;\n\n\tpath_get(&path);\n\twhile (follow_up(&path)) {\n\t\tif (path.dentry != path.mnt->mnt_root)\n\t\t\tbreak;\n\t}\n\terr = vfs_getattr(&path, stat, STATX_BASIC_STATS, AT_STATX_SYNC_AS_STAT);\n\tpath_put(&path);\n\treturn err;\n}\n\nstatic __be32\nnfsd4_encode_bitmap(struct xdr_stream *xdr, u32 bmval0, u32 bmval1, u32 bmval2)\n{\n\t__be32 *p;\n\n\tif (bmval2) {\n\t\tp = xdr_reserve_space(xdr, 16);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(3);\n\t\t*p++ = cpu_to_be32(bmval0);\n\t\t*p++ = cpu_to_be32(bmval1);\n\t\t*p++ = cpu_to_be32(bmval2);\n\t} else if (bmval1) {\n\t\tp = xdr_reserve_space(xdr, 12);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(2);\n\t\t*p++ = cpu_to_be32(bmval0);\n\t\t*p++ = cpu_to_be32(bmval1);\n\t} else {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t\t*p++ = cpu_to_be32(bmval0);\n\t}\n\n\treturn 0;\nout_resource:\n\treturn nfserr_resource;\n}\n\n/*\n * Note: @fhp can be NULL; in this case, we might have to compose the filehandle\n * ourselves.\n */\nstatic __be32\nnfsd4_encode_fattr(struct xdr_stream *xdr, struct svc_fh *fhp,\n\t\tstruct svc_export *exp,\n\t\tstruct dentry *dentry, u32 *bmval,\n\t\tstruct svc_rqst *rqstp, int ignore_crossmnt)\n{\n\tu32 bmval0 = bmval[0];\n\tu32 bmval1 = bmval[1];\n\tu32 bmval2 = bmval[2];\n\tstruct kstat stat;\n\tstruct svc_fh *tempfh = NULL;\n\tstruct kstatfs statfs;\n\t__be32 *p;\n\tint starting_len = xdr->buf->len;\n\tint attrlen_offset;\n\t__be32 attrlen;\n\tu32 dummy;\n\tu64 dummy64;\n\tu32 rdattr_err = 0;\n\t__be32 status;\n\tint err;\n\tstruct nfs4_acl *acl = NULL;\n\tvoid *context = NULL;\n\tint contextlen;\n\tbool contextsupport = false;\n\tstruct nfsd4_compoundres *resp = rqstp->rq_resp;\n\tu32 minorversion = resp->cstate.minorversion;\n\tstruct path path = {\n\t\t.mnt\t= exp->ex_path.mnt,\n\t\t.dentry\t= dentry,\n\t};\n\tstruct nfsd_net *nn = net_generic(SVC_NET(rqstp), nfsd_net_id);\n\n\tBUG_ON(bmval1 & NFSD_WRITEONLY_ATTRS_WORD1);\n\tBUG_ON(!nfsd_attrs_supported(minorversion, bmval));\n\n\tif (exp->ex_fslocs.migrated) {\n\t\tstatus = fattr_handle_absent_fs(&bmval0, &bmval1, &bmval2, &rdattr_err);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\terr = vfs_getattr(&path, &stat, STATX_BASIC_STATS, AT_STATX_SYNC_AS_STAT);\n\tif (err)\n\t\tgoto out_nfserr;\n\tif ((bmval0 & (FATTR4_WORD0_FILES_AVAIL | FATTR4_WORD0_FILES_FREE |\n\t\t\tFATTR4_WORD0_FILES_TOTAL | FATTR4_WORD0_MAXNAME)) ||\n\t    (bmval1 & (FATTR4_WORD1_SPACE_AVAIL | FATTR4_WORD1_SPACE_FREE |\n\t\t       FATTR4_WORD1_SPACE_TOTAL))) {\n\t\terr = vfs_statfs(&path, &statfs);\n\t\tif (err)\n\t\t\tgoto out_nfserr;\n\t}\n\tif ((bmval0 & (FATTR4_WORD0_FILEHANDLE | FATTR4_WORD0_FSID)) && !fhp) {\n\t\ttempfh = kmalloc(sizeof(struct svc_fh), GFP_KERNEL);\n\t\tstatus = nfserr_jukebox;\n\t\tif (!tempfh)\n\t\t\tgoto out;\n\t\tfh_init(tempfh, NFS4_FHSIZE);\n\t\tstatus = fh_compose(tempfh, exp, dentry, NULL);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tfhp = tempfh;\n\t}\n\tif (bmval0 & FATTR4_WORD0_ACL) {\n\t\terr = nfsd4_get_nfs4_acl(rqstp, dentry, &acl);\n\t\tif (err == -EOPNOTSUPP)\n\t\t\tbmval0 &= ~FATTR4_WORD0_ACL;\n\t\telse if (err == -EINVAL) {\n\t\t\tstatus = nfserr_attrnotsupp;\n\t\t\tgoto out;\n\t\t} else if (err != 0)\n\t\t\tgoto out_nfserr;\n\t}\n\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n\tif ((bmval2 & FATTR4_WORD2_SECURITY_LABEL) ||\n\t     bmval0 & FATTR4_WORD0_SUPPORTED_ATTRS) {\n\t\tif (exp->ex_flags & NFSEXP_SECURITY_LABEL)\n\t\t\terr = security_inode_getsecctx(d_inode(dentry),\n\t\t\t\t\t\t&context, &contextlen);\n\t\telse\n\t\t\terr = -EOPNOTSUPP;\n\t\tcontextsupport = (err == 0);\n\t\tif (bmval2 & FATTR4_WORD2_SECURITY_LABEL) {\n\t\t\tif (err == -EOPNOTSUPP)\n\t\t\t\tbmval2 &= ~FATTR4_WORD2_SECURITY_LABEL;\n\t\t\telse if (err)\n\t\t\t\tgoto out_nfserr;\n\t\t}\n\t}\n#endif /* CONFIG_NFSD_V4_SECURITY_LABEL */\n\n\tstatus = nfsd4_encode_bitmap(xdr, bmval0, bmval1, bmval2);\n\tif (status)\n\t\tgoto out;\n\n\tattrlen_offset = xdr->buf->len;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\tgoto out_resource;\n\tp++;                /* to be backfilled later */\n\n\tif (bmval0 & FATTR4_WORD0_SUPPORTED_ATTRS) {\n\t\tu32 supp[3];\n\n\t\tmemcpy(supp, nfsd_suppattrs[minorversion], sizeof(supp));\n\n\t\tif (!IS_POSIXACL(dentry->d_inode))\n\t\t\tsupp[0] &= ~FATTR4_WORD0_ACL;\n\t\tif (!contextsupport)\n\t\t\tsupp[2] &= ~FATTR4_WORD2_SECURITY_LABEL;\n\t\tif (!supp[2]) {\n\t\t\tp = xdr_reserve_space(xdr, 12);\n\t\t\tif (!p)\n\t\t\t\tgoto out_resource;\n\t\t\t*p++ = cpu_to_be32(2);\n\t\t\t*p++ = cpu_to_be32(supp[0]);\n\t\t\t*p++ = cpu_to_be32(supp[1]);\n\t\t} else {\n\t\t\tp = xdr_reserve_space(xdr, 16);\n\t\t\tif (!p)\n\t\t\t\tgoto out_resource;\n\t\t\t*p++ = cpu_to_be32(3);\n\t\t\t*p++ = cpu_to_be32(supp[0]);\n\t\t\t*p++ = cpu_to_be32(supp[1]);\n\t\t\t*p++ = cpu_to_be32(supp[2]);\n\t\t}\n\t}\n\tif (bmval0 & FATTR4_WORD0_TYPE) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tdummy = nfs4_file_type(stat.mode);\n\t\tif (dummy == NF4BAD) {\n\t\t\tstatus = nfserr_serverfault;\n\t\t\tgoto out;\n\t\t}\n\t\t*p++ = cpu_to_be32(dummy);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FH_EXPIRE_TYPE) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tif (exp->ex_flags & NFSEXP_NOSUBTREECHECK)\n\t\t\t*p++ = cpu_to_be32(NFS4_FH_PERSISTENT);\n\t\telse\n\t\t\t*p++ = cpu_to_be32(NFS4_FH_PERSISTENT|\n\t\t\t\t\t\tNFS4_FH_VOL_RENAME);\n\t}\n\tif (bmval0 & FATTR4_WORD0_CHANGE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = encode_change(p, &stat, d_inode(dentry), exp);\n\t}\n\tif (bmval0 & FATTR4_WORD0_SIZE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, stat.size);\n\t}\n\tif (bmval0 & FATTR4_WORD0_LINK_SUPPORT) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_SYMLINK_SUPPORT) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_NAMED_ATTR) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FSID) {\n\t\tp = xdr_reserve_space(xdr, 16);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tif (exp->ex_fslocs.migrated) {\n\t\t\tp = xdr_encode_hyper(p, NFS4_REFERRAL_FSID_MAJOR);\n\t\t\tp = xdr_encode_hyper(p, NFS4_REFERRAL_FSID_MINOR);\n\t\t} else switch(fsid_source(fhp)) {\n\t\tcase FSIDSOURCE_FSID:\n\t\t\tp = xdr_encode_hyper(p, (u64)exp->ex_fsid);\n\t\t\tp = xdr_encode_hyper(p, (u64)0);\n\t\t\tbreak;\n\t\tcase FSIDSOURCE_DEV:\n\t\t\t*p++ = cpu_to_be32(0);\n\t\t\t*p++ = cpu_to_be32(MAJOR(stat.dev));\n\t\t\t*p++ = cpu_to_be32(0);\n\t\t\t*p++ = cpu_to_be32(MINOR(stat.dev));\n\t\t\tbreak;\n\t\tcase FSIDSOURCE_UUID:\n\t\t\tp = xdr_encode_opaque_fixed(p, exp->ex_uuid,\n\t\t\t\t\t\t\t\tEX_UUID_LEN);\n\t\t\tbreak;\n\t\t}\n\t}\n\tif (bmval0 & FATTR4_WORD0_UNIQUE_HANDLES) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\tif (bmval0 & FATTR4_WORD0_LEASE_TIME) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(nn->nfsd4_lease);\n\t}\n\tif (bmval0 & FATTR4_WORD0_RDATTR_ERROR) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(rdattr_err);\n\t}\n\tif (bmval0 & FATTR4_WORD0_ACL) {\n\t\tstruct nfs4_ace *ace;\n\n\t\tif (acl == NULL) {\n\t\t\tp = xdr_reserve_space(xdr, 4);\n\t\t\tif (!p)\n\t\t\t\tgoto out_resource;\n\n\t\t\t*p++ = cpu_to_be32(0);\n\t\t\tgoto out_acl;\n\t\t}\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(acl->naces);\n\n\t\tfor (ace = acl->aces; ace < acl->aces + acl->naces; ace++) {\n\t\t\tp = xdr_reserve_space(xdr, 4*3);\n\t\t\tif (!p)\n\t\t\t\tgoto out_resource;\n\t\t\t*p++ = cpu_to_be32(ace->type);\n\t\t\t*p++ = cpu_to_be32(ace->flag);\n\t\t\t*p++ = cpu_to_be32(ace->access_mask &\n\t\t\t\t\t\t\tNFS4_ACE_MASK_ALL);\n\t\t\tstatus = nfsd4_encode_aclname(xdr, rqstp, ace);\n\t\t\tif (status)\n\t\t\t\tgoto out;\n\t\t}\n\t}\nout_acl:\n\tif (bmval0 & FATTR4_WORD0_ACLSUPPORT) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(IS_POSIXACL(dentry->d_inode) ?\n\t\t\tACL4_SUPPORT_ALLOW_ACL|ACL4_SUPPORT_DENY_ACL : 0);\n\t}\n\tif (bmval0 & FATTR4_WORD0_CANSETTIME) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_CASE_INSENSITIVE) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\tif (bmval0 & FATTR4_WORD0_CASE_PRESERVING) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_CHOWN_RESTRICTED) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FILEHANDLE) {\n\t\tp = xdr_reserve_space(xdr, fhp->fh_handle.fh_size + 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_opaque(p, &fhp->fh_handle.fh_base,\n\t\t\t\t\tfhp->fh_handle.fh_size);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FILEID) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, stat.ino);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FILES_AVAIL) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (u64) statfs.f_ffree);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FILES_FREE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (u64) statfs.f_ffree);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FILES_TOTAL) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (u64) statfs.f_files);\n\t}\n\tif (bmval0 & FATTR4_WORD0_FS_LOCATIONS) {\n\t\tstatus = nfsd4_encode_fs_locations(xdr, rqstp, exp);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\tif (bmval0 & FATTR4_WORD0_HOMOGENEOUS) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval0 & FATTR4_WORD0_MAXFILESIZE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, exp->ex_path.mnt->mnt_sb->s_maxbytes);\n\t}\n\tif (bmval0 & FATTR4_WORD0_MAXLINK) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(255);\n\t}\n\tif (bmval0 & FATTR4_WORD0_MAXNAME) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(statfs.f_namelen);\n\t}\n\tif (bmval0 & FATTR4_WORD0_MAXREAD) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (u64) svc_max_payload(rqstp));\n\t}\n\tif (bmval0 & FATTR4_WORD0_MAXWRITE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (u64) svc_max_payload(rqstp));\n\t}\n\tif (bmval1 & FATTR4_WORD1_MODE) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(stat.mode & S_IALLUGO);\n\t}\n\tif (bmval1 & FATTR4_WORD1_NO_TRUNC) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(1);\n\t}\n\tif (bmval1 & FATTR4_WORD1_NUMLINKS) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(stat.nlink);\n\t}\n\tif (bmval1 & FATTR4_WORD1_OWNER) {\n\t\tstatus = nfsd4_encode_user(xdr, rqstp, stat.uid);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\tif (bmval1 & FATTR4_WORD1_OWNER_GROUP) {\n\t\tstatus = nfsd4_encode_group(xdr, rqstp, stat.gid);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\tif (bmval1 & FATTR4_WORD1_RAWDEV) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32((u32) MAJOR(stat.rdev));\n\t\t*p++ = cpu_to_be32((u32) MINOR(stat.rdev));\n\t}\n\tif (bmval1 & FATTR4_WORD1_SPACE_AVAIL) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tdummy64 = (u64)statfs.f_bavail * (u64)statfs.f_bsize;\n\t\tp = xdr_encode_hyper(p, dummy64);\n\t}\n\tif (bmval1 & FATTR4_WORD1_SPACE_FREE) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tdummy64 = (u64)statfs.f_bfree * (u64)statfs.f_bsize;\n\t\tp = xdr_encode_hyper(p, dummy64);\n\t}\n\tif (bmval1 & FATTR4_WORD1_SPACE_TOTAL) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tdummy64 = (u64)statfs.f_blocks * (u64)statfs.f_bsize;\n\t\tp = xdr_encode_hyper(p, dummy64);\n\t}\n\tif (bmval1 & FATTR4_WORD1_SPACE_USED) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tdummy64 = (u64)stat.blocks << 9;\n\t\tp = xdr_encode_hyper(p, dummy64);\n\t}\n\tif (bmval1 & FATTR4_WORD1_TIME_ACCESS) {\n\t\tp = xdr_reserve_space(xdr, 12);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (s64)stat.atime.tv_sec);\n\t\t*p++ = cpu_to_be32(stat.atime.tv_nsec);\n\t}\n\tif (bmval1 & FATTR4_WORD1_TIME_DELTA) {\n\t\tp = xdr_reserve_space(xdr, 12);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(1);\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\tif (bmval1 & FATTR4_WORD1_TIME_METADATA) {\n\t\tp = xdr_reserve_space(xdr, 12);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (s64)stat.ctime.tv_sec);\n\t\t*p++ = cpu_to_be32(stat.ctime.tv_nsec);\n\t}\n\tif (bmval1 & FATTR4_WORD1_TIME_MODIFY) {\n\t\tp = xdr_reserve_space(xdr, 12);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\tp = xdr_encode_hyper(p, (s64)stat.mtime.tv_sec);\n\t\t*p++ = cpu_to_be32(stat.mtime.tv_nsec);\n\t}\n\tif (bmval1 & FATTR4_WORD1_MOUNTED_ON_FILEID) {\n\t\tstruct kstat parent_stat;\n\t\tu64 ino = stat.ino;\n\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n                \tgoto out_resource;\n\t\t/*\n\t\t * Get parent's attributes if not ignoring crossmount\n\t\t * and this is the root of a cross-mounted filesystem.\n\t\t */\n\t\tif (ignore_crossmnt == 0 &&\n\t\t    dentry == exp->ex_path.mnt->mnt_root) {\n\t\t\terr = get_parent_attributes(exp, &parent_stat);\n\t\t\tif (err)\n\t\t\t\tgoto out_nfserr;\n\t\t\tino = parent_stat.ino;\n\t\t}\n\t\tp = xdr_encode_hyper(p, ino);\n\t}\n#ifdef CONFIG_NFSD_PNFS\n\tif (bmval1 & FATTR4_WORD1_FS_LAYOUT_TYPES) {\n\t\tstatus = nfsd4_encode_layout_types(xdr, exp->ex_layout_types);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tif (bmval2 & FATTR4_WORD2_LAYOUT_TYPES) {\n\t\tstatus = nfsd4_encode_layout_types(xdr, exp->ex_layout_types);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tif (bmval2 & FATTR4_WORD2_LAYOUT_BLKSIZE) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out_resource;\n\t\t*p++ = cpu_to_be32(stat.blksize);\n\t}\n#endif /* CONFIG_NFSD_PNFS */\n\tif (bmval2 & FATTR4_WORD2_SUPPATTR_EXCLCREAT) {\n\t\tu32 supp[3];\n\n\t\tmemcpy(supp, nfsd_suppattrs[minorversion], sizeof(supp));\n\t\tsupp[0] &= NFSD_SUPPATTR_EXCLCREAT_WORD0;\n\t\tsupp[1] &= NFSD_SUPPATTR_EXCLCREAT_WORD1;\n\t\tsupp[2] &= NFSD_SUPPATTR_EXCLCREAT_WORD2;\n\n\t\tstatus = nfsd4_encode_bitmap(xdr, supp[0], supp[1], supp[2]);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tif (bmval2 & FATTR4_WORD2_SECURITY_LABEL) {\n\t\tstatus = nfsd4_encode_security_label(xdr, rqstp, context,\n\t\t\t\t\t\t\t\tcontextlen);\n\t\tif (status)\n\t\t\tgoto out;\n\t}\n\n\tattrlen = htonl(xdr->buf->len - attrlen_offset - 4);\n\twrite_bytes_to_xdr_buf(xdr->buf, attrlen_offset, &attrlen, 4);\n\tstatus = nfs_ok;\n\nout:\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n\tif (context)\n\t\tsecurity_release_secctx(context, contextlen);\n#endif /* CONFIG_NFSD_V4_SECURITY_LABEL */\n\tkfree(acl);\n\tif (tempfh) {\n\t\tfh_put(tempfh);\n\t\tkfree(tempfh);\n\t}\n\tif (status)\n\t\txdr_truncate_encode(xdr, starting_len);\n\treturn status;\nout_nfserr:\n\tstatus = nfserrno(err);\n\tgoto out;\nout_resource:\n\tstatus = nfserr_resource;\n\tgoto out;\n}\n\nstatic void svcxdr_init_encode_from_buffer(struct xdr_stream *xdr,\n\t\t\t\tstruct xdr_buf *buf, __be32 *p, int bytes)\n{\n\txdr->scratch.iov_len = 0;\n\tmemset(buf, 0, sizeof(struct xdr_buf));\n\tbuf->head[0].iov_base = p;\n\tbuf->head[0].iov_len = 0;\n\tbuf->len = 0;\n\txdr->buf = buf;\n\txdr->iov = buf->head;\n\txdr->p = p;\n\txdr->end = (void *)p + bytes;\n\tbuf->buflen = bytes;\n}\n\n__be32 nfsd4_encode_fattr_to_buf(__be32 **p, int words,\n\t\t\tstruct svc_fh *fhp, struct svc_export *exp,\n\t\t\tstruct dentry *dentry, u32 *bmval,\n\t\t\tstruct svc_rqst *rqstp, int ignore_crossmnt)\n{\n\tstruct xdr_buf dummy;\n\tstruct xdr_stream xdr;\n\t__be32 ret;\n\n\tsvcxdr_init_encode_from_buffer(&xdr, &dummy, *p, words << 2);\n\tret = nfsd4_encode_fattr(&xdr, fhp, exp, dentry, bmval, rqstp,\n\t\t\t\t\t\t\tignore_crossmnt);\n\t*p = xdr.p;\n\treturn ret;\n}\n\nstatic inline int attributes_need_mount(u32 *bmval)\n{\n\tif (bmval[0] & ~(FATTR4_WORD0_RDATTR_ERROR | FATTR4_WORD0_LEASE_TIME))\n\t\treturn 1;\n\tif (bmval[1] & ~FATTR4_WORD1_MOUNTED_ON_FILEID)\n\t\treturn 1;\n\treturn 0;\n}\n\nstatic __be32\nnfsd4_encode_dirent_fattr(struct xdr_stream *xdr, struct nfsd4_readdir *cd,\n\t\t\tconst char *name, int namlen)\n{\n\tstruct svc_export *exp = cd->rd_fhp->fh_export;\n\tstruct dentry *dentry;\n\t__be32 nfserr;\n\tint ignore_crossmnt = 0;\n\n\tdentry = lookup_one_len_unlocked(name, cd->rd_fhp->fh_dentry, namlen);\n\tif (IS_ERR(dentry))\n\t\treturn nfserrno(PTR_ERR(dentry));\n\tif (d_really_is_negative(dentry)) {\n\t\t/*\n\t\t * we're not holding the i_mutex here, so there's\n\t\t * a window where this directory entry could have gone\n\t\t * away.\n\t\t */\n\t\tdput(dentry);\n\t\treturn nfserr_noent;\n\t}\n\n\texp_get(exp);\n\t/*\n\t * In the case of a mountpoint, the client may be asking for\n\t * attributes that are only properties of the underlying filesystem\n\t * as opposed to the cross-mounted file system. In such a case,\n\t * we will not follow the cross mount and will fill the attribtutes\n\t * directly from the mountpoint dentry.\n\t */\n\tif (nfsd_mountpoint(dentry, exp)) {\n\t\tint err;\n\n\t\tif (!(exp->ex_flags & NFSEXP_V4ROOT)\n\t\t\t\t&& !attributes_need_mount(cd->rd_bmval)) {\n\t\t\tignore_crossmnt = 1;\n\t\t\tgoto out_encode;\n\t\t}\n\t\t/*\n\t\t * Why the heck aren't we just using nfsd_lookup??\n\t\t * Different \".\"/\"..\" handling?  Something else?\n\t\t * At least, add a comment here to explain....\n\t\t */\n\t\terr = nfsd_cross_mnt(cd->rd_rqstp, &dentry, &exp);\n\t\tif (err) {\n\t\t\tnfserr = nfserrno(err);\n\t\t\tgoto out_put;\n\t\t}\n\t\tnfserr = check_nfsd_access(exp, cd->rd_rqstp);\n\t\tif (nfserr)\n\t\t\tgoto out_put;\n\n\t}\nout_encode:\n\tnfserr = nfsd4_encode_fattr(xdr, NULL, exp, dentry, cd->rd_bmval,\n\t\t\t\t\tcd->rd_rqstp, ignore_crossmnt);\nout_put:\n\tdput(dentry);\n\texp_put(exp);\n\treturn nfserr;\n}\n\nstatic __be32 *\nnfsd4_encode_rdattr_error(struct xdr_stream *xdr, __be32 nfserr)\n{\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, 20);\n\tif (!p)\n\t\treturn NULL;\n\t*p++ = htonl(2);\n\t*p++ = htonl(FATTR4_WORD0_RDATTR_ERROR); /* bmval0 */\n\t*p++ = htonl(0);\t\t\t /* bmval1 */\n\n\t*p++ = htonl(4);     /* attribute length */\n\t*p++ = nfserr;       /* no htonl */\n\treturn p;\n}\n\nstatic int\nnfsd4_encode_dirent(void *ccdv, const char *name, int namlen,\n\t\t    loff_t offset, u64 ino, unsigned int d_type)\n{\n\tstruct readdir_cd *ccd = ccdv;\n\tstruct nfsd4_readdir *cd = container_of(ccd, struct nfsd4_readdir, common);\n\tstruct xdr_stream *xdr = cd->xdr;\n\tint start_offset = xdr->buf->len;\n\tint cookie_offset;\n\tu32 name_and_cookie;\n\tint entry_bytes;\n\t__be32 nfserr = nfserr_toosmall;\n\t__be64 wire_offset;\n\t__be32 *p;\n\n\t/* In nfsv4, \".\" and \"..\" never make it onto the wire.. */\n\tif (name && isdotent(name, namlen)) {\n\t\tcd->common.err = nfs_ok;\n\t\treturn 0;\n\t}\n\n\tif (cd->cookie_offset) {\n\t\twire_offset = cpu_to_be64(offset);\n\t\twrite_bytes_to_xdr_buf(xdr->buf, cd->cookie_offset,\n\t\t\t\t\t\t\t&wire_offset, 8);\n\t}\n\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\tgoto fail;\n\t*p++ = xdr_one;                             /* mark entry present */\n\tcookie_offset = xdr->buf->len;\n\tp = xdr_reserve_space(xdr, 3*4 + namlen);\n\tif (!p)\n\t\tgoto fail;\n\tp = xdr_encode_hyper(p, NFS_OFFSET_MAX);    /* offset of next entry */\n\tp = xdr_encode_array(p, name, namlen);      /* name length & name */\n\n\tnfserr = nfsd4_encode_dirent_fattr(xdr, cd, name, namlen);\n\tswitch (nfserr) {\n\tcase nfs_ok:\n\t\tbreak;\n\tcase nfserr_resource:\n\t\tnfserr = nfserr_toosmall;\n\t\tgoto fail;\n\tcase nfserr_noent:\n\t\txdr_truncate_encode(xdr, start_offset);\n\t\tgoto skip_entry;\n\tdefault:\n\t\t/*\n\t\t * If the client requested the RDATTR_ERROR attribute,\n\t\t * we stuff the error code into this attribute\n\t\t * and continue.  If this attribute was not requested,\n\t\t * then in accordance with the spec, we fail the\n\t\t * entire READDIR operation(!)\n\t\t */\n\t\tif (!(cd->rd_bmval[0] & FATTR4_WORD0_RDATTR_ERROR))\n\t\t\tgoto fail;\n\t\tp = nfsd4_encode_rdattr_error(xdr, nfserr);\n\t\tif (p == NULL) {\n\t\t\tnfserr = nfserr_toosmall;\n\t\t\tgoto fail;\n\t\t}\n\t}\n\tnfserr = nfserr_toosmall;\n\tentry_bytes = xdr->buf->len - start_offset;\n\tif (entry_bytes > cd->rd_maxcount)\n\t\tgoto fail;\n\tcd->rd_maxcount -= entry_bytes;\n\t/*\n\t * RFC 3530 14.2.24 describes rd_dircount as only a \"hint\", so\n\t * let's always let through the first entry, at least:\n\t */\n\tif (!cd->rd_dircount)\n\t\tgoto fail;\n\tname_and_cookie = 4 + 4 * XDR_QUADLEN(namlen) + 8;\n\tif (name_and_cookie > cd->rd_dircount && cd->cookie_offset)\n\t\tgoto fail;\n\tcd->rd_dircount -= min(cd->rd_dircount, name_and_cookie);\n\n\tcd->cookie_offset = cookie_offset;\nskip_entry:\n\tcd->common.err = nfs_ok;\n\treturn 0;\nfail:\n\txdr_truncate_encode(xdr, start_offset);\n\tcd->common.err = nfserr;\n\treturn -EINVAL;\n}\n\nstatic __be32\nnfsd4_encode_stateid(struct xdr_stream *xdr, stateid_t *sid)\n{\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, sizeof(stateid_t));\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(sid->si_generation);\n\tp = xdr_encode_opaque_fixed(p, &sid->si_opaque,\n\t\t\t\t\tsizeof(stateid_opaque_t));\n\treturn 0;\n}\n\nstatic __be32\nnfsd4_encode_access(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_access *access)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(access->ac_supported);\n\t\t*p++ = cpu_to_be32(access->ac_resp_access);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32 nfsd4_encode_bind_conn_to_session(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_bind_conn_to_session *bcts)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, NFS4_MAX_SESSIONID_LEN + 8);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = xdr_encode_opaque_fixed(p, bcts->sessionid.data,\n\t\t\t\t\t\tNFS4_MAX_SESSIONID_LEN);\n\t\t*p++ = cpu_to_be32(bcts->dir);\n\t\t/* Upshifting from TCP to RDMA is not supported */\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_close(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_close *close)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (!nfserr)\n\t\tnfserr = nfsd4_encode_stateid(xdr, &close->cl_stateid);\n\n\treturn nfserr;\n}\n\n\nstatic __be32\nnfsd4_encode_commit(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_commit *commit)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, NFS4_VERIFIER_SIZE);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = xdr_encode_opaque_fixed(p, commit->co_verf.data,\n\t\t\t\t\t\tNFS4_VERIFIER_SIZE);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_create(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_create *create)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 20);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tencode_cinfo(p, &create->cr_cinfo);\n\t\tnfserr = nfsd4_encode_bitmap(xdr, create->cr_bmval[0],\n\t\t\t\tcreate->cr_bmval[1], create->cr_bmval[2]);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_getattr(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_getattr *getattr)\n{\n\tstruct svc_fh *fhp = getattr->ga_fhp;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tnfserr = nfsd4_encode_fattr(xdr, fhp, fhp->fh_export, fhp->fh_dentry,\n\t\t\t\t    getattr->ga_bmval,\n\t\t\t\t    resp->rqstp, 0);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_getfh(struct nfsd4_compoundres *resp, __be32 nfserr, struct svc_fh **fhpp)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct svc_fh *fhp = *fhpp;\n\tunsigned int len;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tlen = fhp->fh_handle.fh_size;\n\t\tp = xdr_reserve_space(xdr, len + 4);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = xdr_encode_opaque(p, &fhp->fh_handle.fh_base, len);\n\t}\n\treturn nfserr;\n}\n\n/*\n* Including all fields other than the name, a LOCK4denied structure requires\n*   8(clientid) + 4(namelen) + 8(offset) + 8(length) + 4(type) = 32 bytes.\n*/\nstatic __be32\nnfsd4_encode_lock_denied(struct xdr_stream *xdr, struct nfsd4_lock_denied *ld)\n{\n\tstruct xdr_netobj *conf = &ld->ld_owner;\n\t__be32 *p;\n\nagain:\n\tp = xdr_reserve_space(xdr, 32 + XDR_LEN(conf->len));\n\tif (!p) {\n\t\t/*\n\t\t * Don't fail to return the result just because we can't\n\t\t * return the conflicting open:\n\t\t */\n\t\tif (conf->len) {\n\t\t\tkfree(conf->data);\n\t\t\tconf->len = 0;\n\t\t\tconf->data = NULL;\n\t\t\tgoto again;\n\t\t}\n\t\treturn nfserr_resource;\n\t}\n\tp = xdr_encode_hyper(p, ld->ld_start);\n\tp = xdr_encode_hyper(p, ld->ld_length);\n\t*p++ = cpu_to_be32(ld->ld_type);\n\tif (conf->len) {\n\t\tp = xdr_encode_opaque_fixed(p, &ld->ld_clientid, 8);\n\t\tp = xdr_encode_opaque(p, conf->data, conf->len);\n\t\tkfree(conf->data);\n\t}  else {  /* non - nfsv4 lock in conflict, no clientid nor owner */\n\t\tp = xdr_encode_hyper(p, (u64)0); /* clientid */\n\t\t*p++ = cpu_to_be32(0); /* length of owner name */\n\t}\n\treturn nfserr_denied;\n}\n\nstatic __be32\nnfsd4_encode_lock(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_lock *lock)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (!nfserr)\n\t\tnfserr = nfsd4_encode_stateid(xdr, &lock->lk_resp_stateid);\n\telse if (nfserr == nfserr_denied)\n\t\tnfserr = nfsd4_encode_lock_denied(xdr, &lock->lk_denied);\n\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_lockt(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_lockt *lockt)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (nfserr == nfserr_denied)\n\t\tnfsd4_encode_lock_denied(xdr, &lockt->lt_denied);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_locku(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_locku *locku)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (!nfserr)\n\t\tnfserr = nfsd4_encode_stateid(xdr, &locku->lu_stateid);\n\n\treturn nfserr;\n}\n\n\nstatic __be32\nnfsd4_encode_link(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_link *link)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 20);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = encode_cinfo(p, &link->li_cinfo);\n\t}\n\treturn nfserr;\n}\n\n\nstatic __be32\nnfsd4_encode_open(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_open *open)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfsd4_encode_stateid(xdr, &open->op_stateid);\n\tif (nfserr)\n\t\tgoto out;\n\tp = xdr_reserve_space(xdr, 24);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tp = encode_cinfo(p, &open->op_cinfo);\n\t*p++ = cpu_to_be32(open->op_rflags);\n\n\tnfserr = nfsd4_encode_bitmap(xdr, open->op_bmval[0], open->op_bmval[1],\n\t\t\t\t\topen->op_bmval[2]);\n\tif (nfserr)\n\t\tgoto out;\n\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t*p++ = cpu_to_be32(open->op_delegate_type);\n\tswitch (open->op_delegate_type) {\n\tcase NFS4_OPEN_DELEGATE_NONE:\n\t\tbreak;\n\tcase NFS4_OPEN_DELEGATE_READ:\n\t\tnfserr = nfsd4_encode_stateid(xdr, &open->op_delegate_stateid);\n\t\tif (nfserr)\n\t\t\treturn nfserr;\n\t\tp = xdr_reserve_space(xdr, 20);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(open->op_recall);\n\n\t\t/*\n\t\t * TODO: ACE's in delegations\n\t\t */\n\t\t*p++ = cpu_to_be32(NFS4_ACE_ACCESS_ALLOWED_ACE_TYPE);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);   /* XXX: is NULL principal ok? */\n\t\tbreak;\n\tcase NFS4_OPEN_DELEGATE_WRITE:\n\t\tnfserr = nfsd4_encode_stateid(xdr, &open->op_delegate_stateid);\n\t\tif (nfserr)\n\t\t\treturn nfserr;\n\t\tp = xdr_reserve_space(xdr, 32);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(0);\n\n\t\t/*\n\t\t * TODO: space_limit's in delegations\n\t\t */\n\t\t*p++ = cpu_to_be32(NFS4_LIMIT_SIZE);\n\t\t*p++ = cpu_to_be32(~(u32)0);\n\t\t*p++ = cpu_to_be32(~(u32)0);\n\n\t\t/*\n\t\t * TODO: ACE's in delegations\n\t\t */\n\t\t*p++ = cpu_to_be32(NFS4_ACE_ACCESS_ALLOWED_ACE_TYPE);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);   /* XXX: is NULL principal ok? */\n\t\tbreak;\n\tcase NFS4_OPEN_DELEGATE_NONE_EXT: /* 4.1 */\n\t\tswitch (open->op_why_no_deleg) {\n\t\tcase WND4_CONTENTION:\n\t\tcase WND4_RESOURCE:\n\t\t\tp = xdr_reserve_space(xdr, 8);\n\t\t\tif (!p)\n\t\t\t\treturn nfserr_resource;\n\t\t\t*p++ = cpu_to_be32(open->op_why_no_deleg);\n\t\t\t/* deleg signaling not supported yet: */\n\t\t\t*p++ = cpu_to_be32(0);\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tp = xdr_reserve_space(xdr, 4);\n\t\t\tif (!p)\n\t\t\t\treturn nfserr_resource;\n\t\t\t*p++ = cpu_to_be32(open->op_why_no_deleg);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\t/* XXX save filehandle here */\nout:\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_open_confirm(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_open_confirm *oc)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (!nfserr)\n\t\tnfserr = nfsd4_encode_stateid(xdr, &oc->oc_resp_stateid);\n\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_open_downgrade(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_open_downgrade *od)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\tif (!nfserr)\n\t\tnfserr = nfsd4_encode_stateid(xdr, &od->od_stateid);\n\n\treturn nfserr;\n}\n\nstatic __be32 nfsd4_encode_splice_read(\n\t\t\t\tstruct nfsd4_compoundres *resp,\n\t\t\t\tstruct nfsd4_read *read,\n\t\t\t\tstruct file *file, unsigned long maxcount)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct xdr_buf *buf = xdr->buf;\n\tu32 eof;\n\tlong len;\n\tint space_left;\n\t__be32 nfserr;\n\t__be32 *p = xdr->p - 2;\n\n\t/* Make sure there will be room for padding if needed */\n\tif (xdr->end - xdr->p < 1)\n\t\treturn nfserr_resource;\n\n\tlen = maxcount;\n\tnfserr = nfsd_splice_read(read->rd_rqstp, file,\n\t\t\t\t  read->rd_offset, &maxcount);\n\tif (nfserr) {\n\t\t/*\n\t\t * nfsd_splice_actor may have already messed with the\n\t\t * page length; reset it so as not to confuse\n\t\t * xdr_truncate_encode:\n\t\t */\n\t\tbuf->page_len = 0;\n\t\treturn nfserr;\n\t}\n\n\teof = nfsd_eof_on_read(len, maxcount, read->rd_offset,\n\t\t\t\td_inode(read->rd_fhp->fh_dentry)->i_size);\n\n\t*(p++) = htonl(eof);\n\t*(p++) = htonl(maxcount);\n\n\tbuf->page_len = maxcount;\n\tbuf->len += maxcount;\n\txdr->page_ptr += (buf->page_base + maxcount + PAGE_SIZE - 1)\n\t\t\t\t\t\t\t/ PAGE_SIZE;\n\n\t/* Use rest of head for padding and remaining ops: */\n\tbuf->tail[0].iov_base = xdr->p;\n\tbuf->tail[0].iov_len = 0;\n\txdr->iov = buf->tail;\n\tif (maxcount&3) {\n\t\tint pad = 4 - (maxcount&3);\n\n\t\t*(xdr->p++) = 0;\n\n\t\tbuf->tail[0].iov_base += maxcount&3;\n\t\tbuf->tail[0].iov_len = pad;\n\t\tbuf->len += pad;\n\t}\n\n\tspace_left = min_t(int, (void *)xdr->end - (void *)xdr->p,\n\t\t\t\tbuf->buflen - buf->len);\n\tbuf->buflen = buf->len + space_left;\n\txdr->end = (__be32 *)((void *)xdr->end + space_left);\n\n\treturn 0;\n}\n\nstatic __be32 nfsd4_encode_readv(struct nfsd4_compoundres *resp,\n\t\t\t\t struct nfsd4_read *read,\n\t\t\t\t struct file *file, unsigned long maxcount)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tu32 eof;\n\tint v;\n\tint starting_len = xdr->buf->len - 8;\n\tlong len;\n\tint thislen;\n\t__be32 nfserr;\n\t__be32 tmp;\n\t__be32 *p;\n\tu32 zzz = 0;\n\tint pad;\n\n\tlen = maxcount;\n\tv = 0;\n\n\tthislen = min_t(long, len, ((void *)xdr->end - (void *)xdr->p));\n\tp = xdr_reserve_space(xdr, (thislen+3)&~3);\n\tWARN_ON_ONCE(!p);\n\tresp->rqstp->rq_vec[v].iov_base = p;\n\tresp->rqstp->rq_vec[v].iov_len = thislen;\n\tv++;\n\tlen -= thislen;\n\n\twhile (len) {\n\t\tthislen = min_t(long, len, PAGE_SIZE);\n\t\tp = xdr_reserve_space(xdr, (thislen+3)&~3);\n\t\tWARN_ON_ONCE(!p);\n\t\tresp->rqstp->rq_vec[v].iov_base = p;\n\t\tresp->rqstp->rq_vec[v].iov_len = thislen;\n\t\tv++;\n\t\tlen -= thislen;\n\t}\n\tread->rd_vlen = v;\n\n\tlen = maxcount;\n\tnfserr = nfsd_readv(file, read->rd_offset, resp->rqstp->rq_vec,\n\t\t\tread->rd_vlen, &maxcount);\n\tif (nfserr)\n\t\treturn nfserr;\n\txdr_truncate_encode(xdr, starting_len + 8 + ((maxcount+3)&~3));\n\n\teof = nfsd_eof_on_read(len, maxcount, read->rd_offset,\n\t\t\t\td_inode(read->rd_fhp->fh_dentry)->i_size);\n\n\ttmp = htonl(eof);\n\twrite_bytes_to_xdr_buf(xdr->buf, starting_len    , &tmp, 4);\n\ttmp = htonl(maxcount);\n\twrite_bytes_to_xdr_buf(xdr->buf, starting_len + 4, &tmp, 4);\n\n\tpad = (maxcount&3) ? 4 - (maxcount&3) : 0;\n\twrite_bytes_to_xdr_buf(xdr->buf, starting_len + 8 + maxcount,\n\t\t\t\t\t\t\t\t&zzz, pad);\n\treturn 0;\n\n}\n\nstatic __be32\nnfsd4_encode_read(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t  struct nfsd4_read *read)\n{\n\tunsigned long maxcount;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct file *file = read->rd_filp;\n\tint starting_len = xdr->buf->len;\n\tstruct raparms *ra = NULL;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\tgoto out;\n\n\tp = xdr_reserve_space(xdr, 8); /* eof flag and byte count */\n\tif (!p) {\n\t\tWARN_ON_ONCE(test_bit(RQ_SPLICE_OK, &resp->rqstp->rq_flags));\n\t\tnfserr = nfserr_resource;\n\t\tgoto out;\n\t}\n\tif (resp->xdr.buf->page_len &&\n\t    test_bit(RQ_SPLICE_OK, &resp->rqstp->rq_flags)) {\n\t\tWARN_ON_ONCE(1);\n\t\tnfserr = nfserr_resource;\n\t\tgoto out;\n\t}\n\txdr_commit_encode(xdr);\n\n\tmaxcount = svc_max_payload(resp->rqstp);\n\tmaxcount = min_t(unsigned long, maxcount,\n\t\t\t (xdr->buf->buflen - xdr->buf->len));\n\tmaxcount = min_t(unsigned long, maxcount, read->rd_length);\n\n\tif (read->rd_tmp_file)\n\t\tra = nfsd_init_raparms(file);\n\n\tif (file->f_op->splice_read &&\n\t    test_bit(RQ_SPLICE_OK, &resp->rqstp->rq_flags))\n\t\tnfserr = nfsd4_encode_splice_read(resp, read, file, maxcount);\n\telse\n\t\tnfserr = nfsd4_encode_readv(resp, read, file, maxcount);\n\n\tif (ra)\n\t\tnfsd_put_raparams(file, ra);\n\n\tif (nfserr)\n\t\txdr_truncate_encode(xdr, starting_len);\n\nout:\n\tif (file)\n\t\tfput(file);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_readlink(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_readlink *readlink)\n{\n\tint maxcount;\n\t__be32 wire_count;\n\tint zero = 0;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tint length_offset = xdr->buf->len;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tmaxcount = PAGE_SIZE;\n\n\tp = xdr_reserve_space(xdr, maxcount);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t/*\n\t * XXX: By default, vfs_readlink() will truncate symlinks if they\n\t * would overflow the buffer.  Is this kosher in NFSv4?  If not, one\n\t * easy fix is: if vfs_readlink() precisely fills the buffer, assume\n\t * that truncation occurred, and return NFS4ERR_RESOURCE.\n\t */\n\tnfserr = nfsd_readlink(readlink->rl_rqstp, readlink->rl_fhp,\n\t\t\t\t\t\t(char *)p, &maxcount);\n\tif (nfserr == nfserr_isdir)\n\t\tnfserr = nfserr_inval;\n\tif (nfserr) {\n\t\txdr_truncate_encode(xdr, length_offset);\n\t\treturn nfserr;\n\t}\n\n\twire_count = htonl(maxcount);\n\twrite_bytes_to_xdr_buf(xdr->buf, length_offset, &wire_count, 4);\n\txdr_truncate_encode(xdr, length_offset + 4 + ALIGN(maxcount, 4));\n\tif (maxcount & 3)\n\t\twrite_bytes_to_xdr_buf(xdr->buf, length_offset + 4 + maxcount,\n\t\t\t\t\t\t&zero, 4 - (maxcount&3));\n\treturn 0;\n}\n\nstatic __be32\nnfsd4_encode_readdir(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_readdir *readdir)\n{\n\tint maxcount;\n\tint bytes_left;\n\tloff_t offset;\n\t__be64 wire_offset;\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tint starting_len = xdr->buf->len;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, NFS4_VERIFIER_SIZE);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t/* XXX: Following NFSv3, we ignore the READDIR verifier for now. */\n\t*p++ = cpu_to_be32(0);\n\t*p++ = cpu_to_be32(0);\n\tresp->xdr.buf->head[0].iov_len = ((char *)resp->xdr.p)\n\t\t\t\t- (char *)resp->xdr.buf->head[0].iov_base;\n\n\t/*\n\t * Number of bytes left for directory entries allowing for the\n\t * final 8 bytes of the readdir and a following failed op:\n\t */\n\tbytes_left = xdr->buf->buflen - xdr->buf->len\n\t\t\t- COMPOUND_ERR_SLACK_SPACE - 8;\n\tif (bytes_left < 0) {\n\t\tnfserr = nfserr_resource;\n\t\tgoto err_no_verf;\n\t}\n\tmaxcount = min_t(u32, readdir->rd_maxcount, INT_MAX);\n\t/*\n\t * Note the rfc defines rd_maxcount as the size of the\n\t * READDIR4resok structure, which includes the verifier above\n\t * and the 8 bytes encoded at the end of this function:\n\t */\n\tif (maxcount < 16) {\n\t\tnfserr = nfserr_toosmall;\n\t\tgoto err_no_verf;\n\t}\n\tmaxcount = min_t(int, maxcount-16, bytes_left);\n\n\t/* RFC 3530 14.2.24 allows us to ignore dircount when it's 0: */\n\tif (!readdir->rd_dircount)\n\t\treaddir->rd_dircount = INT_MAX;\n\n\treaddir->xdr = xdr;\n\treaddir->rd_maxcount = maxcount;\n\treaddir->common.err = 0;\n\treaddir->cookie_offset = 0;\n\n\toffset = readdir->rd_cookie;\n\tnfserr = nfsd_readdir(readdir->rd_rqstp, readdir->rd_fhp,\n\t\t\t      &offset,\n\t\t\t      &readdir->common, nfsd4_encode_dirent);\n\tif (nfserr == nfs_ok &&\n\t    readdir->common.err == nfserr_toosmall &&\n\t    xdr->buf->len == starting_len + 8) {\n\t\t/* nothing encoded; which limit did we hit?: */\n\t\tif (maxcount - 16 < bytes_left)\n\t\t\t/* It was the fault of rd_maxcount: */\n\t\t\tnfserr = nfserr_toosmall;\n\t\telse\n\t\t\t/* We ran out of buffer space: */\n\t\t\tnfserr = nfserr_resource;\n\t}\n\tif (nfserr)\n\t\tgoto err_no_verf;\n\n\tif (readdir->cookie_offset) {\n\t\twire_offset = cpu_to_be64(offset);\n\t\twrite_bytes_to_xdr_buf(xdr->buf, readdir->cookie_offset,\n\t\t\t\t\t\t\t&wire_offset, 8);\n\t}\n\n\tp = xdr_reserve_space(xdr, 8);\n\tif (!p) {\n\t\tWARN_ON_ONCE(1);\n\t\tgoto err_no_verf;\n\t}\n\t*p++ = 0;\t/* no more entries */\n\t*p++ = htonl(readdir->common.err == nfserr_eof);\n\n\treturn 0;\nerr_no_verf:\n\txdr_truncate_encode(xdr, starting_len);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_remove(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_remove *remove)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 20);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = encode_cinfo(p, &remove->rm_cinfo);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_rename(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_rename *rename)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 40);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = encode_cinfo(p, &rename->rn_sinfo);\n\t\tp = encode_cinfo(p, &rename->rn_tinfo);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_do_encode_secinfo(struct xdr_stream *xdr,\n\t\t\t __be32 nfserr, struct svc_export *exp)\n{\n\tu32 i, nflavs, supported;\n\tstruct exp_flavor_info *flavs;\n\tstruct exp_flavor_info def_flavs[2];\n\t__be32 *p, *flavorsp;\n\tstatic bool report = true;\n\n\tif (nfserr)\n\t\tgoto out;\n\tnfserr = nfserr_resource;\n\tif (exp->ex_nflavors) {\n\t\tflavs = exp->ex_flavors;\n\t\tnflavs = exp->ex_nflavors;\n\t} else { /* Handling of some defaults in absence of real secinfo: */\n\t\tflavs = def_flavs;\n\t\tif (exp->ex_client->flavour->flavour == RPC_AUTH_UNIX) {\n\t\t\tnflavs = 2;\n\t\t\tflavs[0].pseudoflavor = RPC_AUTH_UNIX;\n\t\t\tflavs[1].pseudoflavor = RPC_AUTH_NULL;\n\t\t} else if (exp->ex_client->flavour->flavour == RPC_AUTH_GSS) {\n\t\t\tnflavs = 1;\n\t\t\tflavs[0].pseudoflavor\n\t\t\t\t\t= svcauth_gss_flavor(exp->ex_client);\n\t\t} else {\n\t\t\tnflavs = 1;\n\t\t\tflavs[0].pseudoflavor\n\t\t\t\t\t= exp->ex_client->flavour->flavour;\n\t\t}\n\t}\n\n\tsupported = 0;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\tgoto out;\n\tflavorsp = p++;\t\t/* to be backfilled later */\n\n\tfor (i = 0; i < nflavs; i++) {\n\t\trpc_authflavor_t pf = flavs[i].pseudoflavor;\n\t\tstruct rpcsec_gss_info info;\n\n\t\tif (rpcauth_get_gssinfo(pf, &info) == 0) {\n\t\t\tsupported++;\n\t\t\tp = xdr_reserve_space(xdr, 4 + 4 +\n\t\t\t\t\t      XDR_LEN(info.oid.len) + 4 + 4);\n\t\t\tif (!p)\n\t\t\t\tgoto out;\n\t\t\t*p++ = cpu_to_be32(RPC_AUTH_GSS);\n\t\t\tp = xdr_encode_opaque(p,  info.oid.data, info.oid.len);\n\t\t\t*p++ = cpu_to_be32(info.qop);\n\t\t\t*p++ = cpu_to_be32(info.service);\n\t\t} else if (pf < RPC_AUTH_MAXFLAVOR) {\n\t\t\tsupported++;\n\t\t\tp = xdr_reserve_space(xdr, 4);\n\t\t\tif (!p)\n\t\t\t\tgoto out;\n\t\t\t*p++ = cpu_to_be32(pf);\n\t\t} else {\n\t\t\tif (report)\n\t\t\t\tpr_warn(\"NFS: SECINFO: security flavor %u \"\n\t\t\t\t\t\"is not supported\\n\", pf);\n\t\t}\n\t}\n\n\tif (nflavs != supported)\n\t\treport = false;\n\t*flavorsp = htonl(supported);\n\tnfserr = 0;\nout:\n\tif (exp)\n\t\texp_put(exp);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_secinfo(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t     struct nfsd4_secinfo *secinfo)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\treturn nfsd4_do_encode_secinfo(xdr, nfserr, secinfo->si_exp);\n}\n\nstatic __be32\nnfsd4_encode_secinfo_no_name(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t     struct nfsd4_secinfo_no_name *secinfo)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\n\treturn nfsd4_do_encode_secinfo(xdr, nfserr, secinfo->sin_exp);\n}\n\n/*\n * The SETATTR encode routine is special -- it always encodes a bitmap,\n * regardless of the error status.\n */\nstatic __be32\nnfsd4_encode_setattr(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_setattr *setattr)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, 16);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tif (nfserr) {\n\t\t*p++ = cpu_to_be32(3);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\telse {\n\t\t*p++ = cpu_to_be32(3);\n\t\t*p++ = cpu_to_be32(setattr->sa_bmval[0]);\n\t\t*p++ = cpu_to_be32(setattr->sa_bmval[1]);\n\t\t*p++ = cpu_to_be32(setattr->sa_bmval[2]);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_setclientid(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_setclientid *scd)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 8 + NFS4_VERIFIER_SIZE);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = xdr_encode_opaque_fixed(p, &scd->se_clientid, 8);\n\t\tp = xdr_encode_opaque_fixed(p, &scd->se_confirm,\n\t\t\t\t\t\tNFS4_VERIFIER_SIZE);\n\t}\n\telse if (nfserr == nfserr_clid_inuse) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(0);\n\t\t*p++ = cpu_to_be32(0);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_write(struct nfsd4_compoundres *resp, __be32 nfserr, struct nfsd4_write *write)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tp = xdr_reserve_space(xdr, 16);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(write->wr_bytes_written);\n\t\t*p++ = cpu_to_be32(write->wr_how_written);\n\t\tp = xdr_encode_opaque_fixed(p, write->wr_verifier.data,\n\t\t\t\t\t\t\tNFS4_VERIFIER_SIZE);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_exchange_id(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t\t struct nfsd4_exchange_id *exid)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\tchar *major_id;\n\tchar *server_scope;\n\tint major_id_sz;\n\tint server_scope_sz;\n\tint status = 0;\n\tuint64_t minor_id = 0;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tmajor_id = utsname()->nodename;\n\tmajor_id_sz = strlen(major_id);\n\tserver_scope = utsname()->nodename;\n\tserver_scope_sz = strlen(server_scope);\n\n\tp = xdr_reserve_space(xdr,\n\t\t8 /* eir_clientid */ +\n\t\t4 /* eir_sequenceid */ +\n\t\t4 /* eir_flags */ +\n\t\t4 /* spr_how */);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\tp = xdr_encode_opaque_fixed(p, &exid->clientid, 8);\n\t*p++ = cpu_to_be32(exid->seqid);\n\t*p++ = cpu_to_be32(exid->flags);\n\n\t*p++ = cpu_to_be32(exid->spa_how);\n\n\tswitch (exid->spa_how) {\n\tcase SP4_NONE:\n\t\tbreak;\n\tcase SP4_MACH_CRED:\n\t\t/* spo_must_enforce bitmap: */\n\t\tstatus = nfsd4_encode_bitmap(xdr,\n\t\t\t\t\texid->spo_must_enforce[0],\n\t\t\t\t\texid->spo_must_enforce[1],\n\t\t\t\t\texid->spo_must_enforce[2]);\n\t\tif (status)\n\t\t\tgoto out;\n\t\t/* spo_must_allow bitmap: */\n\t\tstatus = nfsd4_encode_bitmap(xdr,\n\t\t\t\t\texid->spo_must_allow[0],\n\t\t\t\t\texid->spo_must_allow[1],\n\t\t\t\t\texid->spo_must_allow[2]);\n\t\tif (status)\n\t\t\tgoto out;\n\t\tbreak;\n\tdefault:\n\t\tWARN_ON_ONCE(1);\n\t}\n\n\tp = xdr_reserve_space(xdr,\n\t\t8 /* so_minor_id */ +\n\t\t4 /* so_major_id.len */ +\n\t\t(XDR_QUADLEN(major_id_sz) * 4) +\n\t\t4 /* eir_server_scope.len */ +\n\t\t(XDR_QUADLEN(server_scope_sz) * 4) +\n\t\t4 /* eir_server_impl_id.count (0) */);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t/* The server_owner struct */\n\tp = xdr_encode_hyper(p, minor_id);      /* Minor id */\n\t/* major id */\n\tp = xdr_encode_opaque(p, major_id, major_id_sz);\n\n\t/* Server scope */\n\tp = xdr_encode_opaque(p, server_scope, server_scope_sz);\n\n\t/* Implementation id */\n\t*p++ = cpu_to_be32(0);\t/* zero length nfs_impl_id4 array */\n\treturn 0;\nout:\n\treturn status;\n}\n\nstatic __be32\nnfsd4_encode_create_session(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t\t    struct nfsd4_create_session *sess)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, 24);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tp = xdr_encode_opaque_fixed(p, sess->sessionid.data,\n\t\t\t\t\tNFS4_MAX_SESSIONID_LEN);\n\t*p++ = cpu_to_be32(sess->seqid);\n\t*p++ = cpu_to_be32(sess->flags);\n\n\tp = xdr_reserve_space(xdr, 28);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(0); /* headerpadsz */\n\t*p++ = cpu_to_be32(sess->fore_channel.maxreq_sz);\n\t*p++ = cpu_to_be32(sess->fore_channel.maxresp_sz);\n\t*p++ = cpu_to_be32(sess->fore_channel.maxresp_cached);\n\t*p++ = cpu_to_be32(sess->fore_channel.maxops);\n\t*p++ = cpu_to_be32(sess->fore_channel.maxreqs);\n\t*p++ = cpu_to_be32(sess->fore_channel.nr_rdma_attrs);\n\n\tif (sess->fore_channel.nr_rdma_attrs) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(sess->fore_channel.rdma_attrs);\n\t}\n\n\tp = xdr_reserve_space(xdr, 28);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(0); /* headerpadsz */\n\t*p++ = cpu_to_be32(sess->back_channel.maxreq_sz);\n\t*p++ = cpu_to_be32(sess->back_channel.maxresp_sz);\n\t*p++ = cpu_to_be32(sess->back_channel.maxresp_cached);\n\t*p++ = cpu_to_be32(sess->back_channel.maxops);\n\t*p++ = cpu_to_be32(sess->back_channel.maxreqs);\n\t*p++ = cpu_to_be32(sess->back_channel.nr_rdma_attrs);\n\n\tif (sess->back_channel.nr_rdma_attrs) {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\t*p++ = cpu_to_be32(sess->back_channel.rdma_attrs);\n\t}\n\treturn 0;\n}\n\nstatic __be32\nnfsd4_encode_sequence(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t      struct nfsd4_sequence *seq)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, NFS4_MAX_SESSIONID_LEN + 20);\n\tif (!p)\n\t\treturn nfserr_resource;\n\tp = xdr_encode_opaque_fixed(p, seq->sessionid.data,\n\t\t\t\t\tNFS4_MAX_SESSIONID_LEN);\n\t*p++ = cpu_to_be32(seq->seqid);\n\t*p++ = cpu_to_be32(seq->slotid);\n\t/* Note slotid's are numbered from zero: */\n\t*p++ = cpu_to_be32(seq->maxslots - 1); /* sr_highest_slotid */\n\t*p++ = cpu_to_be32(seq->maxslots - 1); /* sr_target_highest_slotid */\n\t*p++ = cpu_to_be32(seq->status_flags);\n\n\tresp->cstate.data_offset = xdr->buf->len; /* DRC cache data pointer */\n\treturn 0;\n}\n\nstatic __be32\nnfsd4_encode_test_stateid(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t\t  struct nfsd4_test_stateid *test_stateid)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct nfsd4_test_stateid_id *stateid, *next;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, 4 + (4 * test_stateid->ts_num_ids));\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = htonl(test_stateid->ts_num_ids);\n\n\tlist_for_each_entry_safe(stateid, next, &test_stateid->ts_stateid_list, ts_id_list) {\n\t\t*p++ = stateid->ts_id_status;\n\t}\n\n\treturn nfserr;\n}\n\n#ifdef CONFIG_NFSD_PNFS\nstatic __be32\nnfsd4_encode_getdeviceinfo(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\tstruct nfsd4_getdeviceinfo *gdev)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tconst struct nfsd4_layout_ops *ops;\n\tu32 starting_len = xdr->buf->len, needed_len;\n\t__be32 *p;\n\n\tdprintk(\"%s: err %d\\n\", __func__, be32_to_cpu(nfserr));\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfserr_resource;\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\tgoto out;\n\n\t*p++ = cpu_to_be32(gdev->gd_layout_type);\n\n\t/* If maxcount is 0 then just update notifications */\n\tif (gdev->gd_maxcount != 0) {\n\t\tops = nfsd4_layout_ops[gdev->gd_layout_type];\n\t\tnfserr = ops->encode_getdeviceinfo(xdr, gdev);\n\t\tif (nfserr) {\n\t\t\t/*\n\t\t\t * We don't bother to burden the layout drivers with\n\t\t\t * enforcing gd_maxcount, just tell the client to\n\t\t\t * come back with a bigger buffer if it's not enough.\n\t\t\t */\n\t\t\tif (xdr->buf->len + 4 > gdev->gd_maxcount)\n\t\t\t\tgoto toosmall;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tnfserr = nfserr_resource;\n\tif (gdev->gd_notify_types) {\n\t\tp = xdr_reserve_space(xdr, 4 + 4);\n\t\tif (!p)\n\t\t\tgoto out;\n\t\t*p++ = cpu_to_be32(1);\t\t\t/* bitmap length */\n\t\t*p++ = cpu_to_be32(gdev->gd_notify_types);\n\t} else {\n\t\tp = xdr_reserve_space(xdr, 4);\n\t\tif (!p)\n\t\t\tgoto out;\n\t\t*p++ = 0;\n\t}\n\n\tnfserr = 0;\nout:\n\tkfree(gdev->gd_device);\n\tdprintk(\"%s: done: %d\\n\", __func__, be32_to_cpu(nfserr));\n\treturn nfserr;\n\ntoosmall:\n\tdprintk(\"%s: maxcount too small\\n\", __func__);\n\tneeded_len = xdr->buf->len + 4 /* notifications */;\n\txdr_truncate_encode(xdr, starting_len);\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p) {\n\t\tnfserr = nfserr_resource;\n\t} else {\n\t\t*p++ = cpu_to_be32(needed_len);\n\t\tnfserr = nfserr_toosmall;\n\t}\n\tgoto out;\n}\n\nstatic __be32\nnfsd4_encode_layoutget(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\tstruct nfsd4_layoutget *lgp)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tconst struct nfsd4_layout_ops *ops;\n\t__be32 *p;\n\n\tdprintk(\"%s: err %d\\n\", __func__, nfserr);\n\tif (nfserr)\n\t\tgoto out;\n\n\tnfserr = nfserr_resource;\n\tp = xdr_reserve_space(xdr, 36 + sizeof(stateid_opaque_t));\n\tif (!p)\n\t\tgoto out;\n\n\t*p++ = cpu_to_be32(1);\t/* we always set return-on-close */\n\t*p++ = cpu_to_be32(lgp->lg_sid.si_generation);\n\tp = xdr_encode_opaque_fixed(p, &lgp->lg_sid.si_opaque,\n\t\t\t\t    sizeof(stateid_opaque_t));\n\n\t*p++ = cpu_to_be32(1);\t/* we always return a single layout */\n\tp = xdr_encode_hyper(p, lgp->lg_seg.offset);\n\tp = xdr_encode_hyper(p, lgp->lg_seg.length);\n\t*p++ = cpu_to_be32(lgp->lg_seg.iomode);\n\t*p++ = cpu_to_be32(lgp->lg_layout_type);\n\n\tops = nfsd4_layout_ops[lgp->lg_layout_type];\n\tnfserr = ops->encode_layoutget(xdr, lgp);\nout:\n\tkfree(lgp->lg_content);\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_layoutcommit(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t\t  struct nfsd4_layoutcommit *lcp)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(lcp->lc_size_chg);\n\tif (lcp->lc_size_chg) {\n\t\tp = xdr_reserve_space(xdr, 8);\n\t\tif (!p)\n\t\t\treturn nfserr_resource;\n\t\tp = xdr_encode_hyper(p, lcp->lc_newsize);\n\t}\n\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_encode_layoutreturn(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\tstruct nfsd4_layoutreturn *lrp)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(xdr, 4);\n\tif (!p)\n\t\treturn nfserr_resource;\n\t*p++ = cpu_to_be32(lrp->lrs_present);\n\tif (lrp->lrs_present)\n\t\treturn nfsd4_encode_stateid(xdr, &lrp->lr_sid);\n\treturn nfs_ok;\n}\n#endif /* CONFIG_NFSD_PNFS */\n\nstatic __be32\nnfsd42_encode_write_res(struct nfsd4_compoundres *resp, struct nfsd42_write_res *write)\n{\n\t__be32 *p;\n\n\tp = xdr_reserve_space(&resp->xdr, 4 + 8 + 4 + NFS4_VERIFIER_SIZE);\n\tif (!p)\n\t\treturn nfserr_resource;\n\n\t*p++ = cpu_to_be32(0);\n\tp = xdr_encode_hyper(p, write->wr_bytes_written);\n\t*p++ = cpu_to_be32(write->wr_stable_how);\n\tp = xdr_encode_opaque_fixed(p, write->wr_verifier.data,\n\t\t\t\t    NFS4_VERIFIER_SIZE);\n\treturn nfs_ok;\n}\n\nstatic __be32\nnfsd4_encode_copy(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t  struct nfsd4_copy *copy)\n{\n\t__be32 *p;\n\n\tif (!nfserr) {\n\t\tnfserr = nfsd42_encode_write_res(resp, &copy->cp_res);\n\t\tif (nfserr)\n\t\t\treturn nfserr;\n\n\t\tp = xdr_reserve_space(&resp->xdr, 4 + 4);\n\t\t*p++ = cpu_to_be32(copy->cp_consecutive);\n\t\t*p++ = cpu_to_be32(copy->cp_synchronous);\n\t}\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_seek(struct nfsd4_compoundres *resp, __be32 nfserr,\n\t\t  struct nfsd4_seek *seek)\n{\n\t__be32 *p;\n\n\tif (nfserr)\n\t\treturn nfserr;\n\n\tp = xdr_reserve_space(&resp->xdr, 4 + 8);\n\t*p++ = cpu_to_be32(seek->seek_eof);\n\tp = xdr_encode_hyper(p, seek->seek_pos);\n\n\treturn nfserr;\n}\n\nstatic __be32\nnfsd4_encode_noop(struct nfsd4_compoundres *resp, __be32 nfserr, void *p)\n{\n\treturn nfserr;\n}\n\ntypedef __be32(* nfsd4_enc)(struct nfsd4_compoundres *, __be32, void *);\n\n/*\n * Note: nfsd4_enc_ops vector is shared for v4.0 and v4.1\n * since we don't need to filter out obsolete ops as this is\n * done in the decoding phase.\n */\nstatic nfsd4_enc nfsd4_enc_ops[] = {\n\t[OP_ACCESS]\t\t= (nfsd4_enc)nfsd4_encode_access,\n\t[OP_CLOSE]\t\t= (nfsd4_enc)nfsd4_encode_close,\n\t[OP_COMMIT]\t\t= (nfsd4_enc)nfsd4_encode_commit,\n\t[OP_CREATE]\t\t= (nfsd4_enc)nfsd4_encode_create,\n\t[OP_DELEGPURGE]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_DELEGRETURN]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_GETATTR]\t\t= (nfsd4_enc)nfsd4_encode_getattr,\n\t[OP_GETFH]\t\t= (nfsd4_enc)nfsd4_encode_getfh,\n\t[OP_LINK]\t\t= (nfsd4_enc)nfsd4_encode_link,\n\t[OP_LOCK]\t\t= (nfsd4_enc)nfsd4_encode_lock,\n\t[OP_LOCKT]\t\t= (nfsd4_enc)nfsd4_encode_lockt,\n\t[OP_LOCKU]\t\t= (nfsd4_enc)nfsd4_encode_locku,\n\t[OP_LOOKUP]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LOOKUPP]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_NVERIFY]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_OPEN]\t\t= (nfsd4_enc)nfsd4_encode_open,\n\t[OP_OPENATTR]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_OPEN_CONFIRM]\t= (nfsd4_enc)nfsd4_encode_open_confirm,\n\t[OP_OPEN_DOWNGRADE]\t= (nfsd4_enc)nfsd4_encode_open_downgrade,\n\t[OP_PUTFH]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_PUTPUBFH]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_PUTROOTFH]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_READ]\t\t= (nfsd4_enc)nfsd4_encode_read,\n\t[OP_READDIR]\t\t= (nfsd4_enc)nfsd4_encode_readdir,\n\t[OP_READLINK]\t\t= (nfsd4_enc)nfsd4_encode_readlink,\n\t[OP_REMOVE]\t\t= (nfsd4_enc)nfsd4_encode_remove,\n\t[OP_RENAME]\t\t= (nfsd4_enc)nfsd4_encode_rename,\n\t[OP_RENEW]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_RESTOREFH]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_SAVEFH]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_SECINFO]\t\t= (nfsd4_enc)nfsd4_encode_secinfo,\n\t[OP_SETATTR]\t\t= (nfsd4_enc)nfsd4_encode_setattr,\n\t[OP_SETCLIENTID]\t= (nfsd4_enc)nfsd4_encode_setclientid,\n\t[OP_SETCLIENTID_CONFIRM] = (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_VERIFY]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_WRITE]\t\t= (nfsd4_enc)nfsd4_encode_write,\n\t[OP_RELEASE_LOCKOWNER]\t= (nfsd4_enc)nfsd4_encode_noop,\n\n\t/* NFSv4.1 operations */\n\t[OP_BACKCHANNEL_CTL]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_BIND_CONN_TO_SESSION] = (nfsd4_enc)nfsd4_encode_bind_conn_to_session,\n\t[OP_EXCHANGE_ID]\t= (nfsd4_enc)nfsd4_encode_exchange_id,\n\t[OP_CREATE_SESSION]\t= (nfsd4_enc)nfsd4_encode_create_session,\n\t[OP_DESTROY_SESSION]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_FREE_STATEID]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_GET_DIR_DELEGATION]\t= (nfsd4_enc)nfsd4_encode_noop,\n#ifdef CONFIG_NFSD_PNFS\n\t[OP_GETDEVICEINFO]\t= (nfsd4_enc)nfsd4_encode_getdeviceinfo,\n\t[OP_GETDEVICELIST]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTCOMMIT]\t= (nfsd4_enc)nfsd4_encode_layoutcommit,\n\t[OP_LAYOUTGET]\t\t= (nfsd4_enc)nfsd4_encode_layoutget,\n\t[OP_LAYOUTRETURN]\t= (nfsd4_enc)nfsd4_encode_layoutreturn,\n#else\n\t[OP_GETDEVICEINFO]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_GETDEVICELIST]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTCOMMIT]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTGET]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTRETURN]\t= (nfsd4_enc)nfsd4_encode_noop,\n#endif\n\t[OP_SECINFO_NO_NAME]\t= (nfsd4_enc)nfsd4_encode_secinfo_no_name,\n\t[OP_SEQUENCE]\t\t= (nfsd4_enc)nfsd4_encode_sequence,\n\t[OP_SET_SSV]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_TEST_STATEID]\t= (nfsd4_enc)nfsd4_encode_test_stateid,\n\t[OP_WANT_DELEGATION]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_DESTROY_CLIENTID]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_RECLAIM_COMPLETE]\t= (nfsd4_enc)nfsd4_encode_noop,\n\n\t/* NFSv4.2 operations */\n\t[OP_ALLOCATE]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_COPY]\t\t= (nfsd4_enc)nfsd4_encode_copy,\n\t[OP_COPY_NOTIFY]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_DEALLOCATE]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_IO_ADVISE]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTERROR]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_LAYOUTSTATS]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_OFFLOAD_CANCEL]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_OFFLOAD_STATUS]\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_READ_PLUS]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_SEEK]\t\t= (nfsd4_enc)nfsd4_encode_seek,\n\t[OP_WRITE_SAME]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n\t[OP_CLONE]\t\t= (nfsd4_enc)nfsd4_encode_noop,\n};\n\n/*\n * Calculate whether we still have space to encode repsize bytes.\n * There are two considerations:\n *     - For NFS versions >=4.1, the size of the reply must stay within\n *       session limits\n *     - For all NFS versions, we must stay within limited preallocated\n *       buffer space.\n *\n * This is called before the operation is processed, so can only provide\n * an upper estimate.  For some nonidempotent operations (such as\n * getattr), it's not necessarily a problem if that estimate is wrong,\n * as we can fail it after processing without significant side effects.\n */\n__be32 nfsd4_check_resp_size(struct nfsd4_compoundres *resp, u32 respsize)\n{\n\tstruct xdr_buf *buf = &resp->rqstp->rq_res;\n\tstruct nfsd4_slot *slot = resp->cstate.slot;\n\n\tif (buf->len + respsize <= buf->buflen)\n\t\treturn nfs_ok;\n\tif (!nfsd4_has_session(&resp->cstate))\n\t\treturn nfserr_resource;\n\tif (slot->sl_flags & NFSD4_SLOT_CACHETHIS) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn nfserr_rep_too_big_to_cache;\n\t}\n\treturn nfserr_rep_too_big;\n}\n\nvoid\nnfsd4_encode_operation(struct nfsd4_compoundres *resp, struct nfsd4_op *op)\n{\n\tstruct xdr_stream *xdr = &resp->xdr;\n\tstruct nfs4_stateowner *so = resp->cstate.replay_owner;\n\tstruct svc_rqst *rqstp = resp->rqstp;\n\tint post_err_offset;\n\tnfsd4_enc encoder;\n\t__be32 *p;\n\n\tp = xdr_reserve_space(xdr, 8);\n\tif (!p) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\t*p++ = cpu_to_be32(op->opnum);\n\tpost_err_offset = xdr->buf->len;\n\n\tif (op->opnum == OP_ILLEGAL)\n\t\tgoto status;\n\tBUG_ON(op->opnum < 0 || op->opnum >= ARRAY_SIZE(nfsd4_enc_ops) ||\n\t       !nfsd4_enc_ops[op->opnum]);\n\tencoder = nfsd4_enc_ops[op->opnum];\n\top->status = encoder(resp, op->status, &op->u);\n\txdr_commit_encode(xdr);\n\n\t/* nfsd4_check_resp_size guarantees enough room for error status */\n\tif (!op->status) {\n\t\tint space_needed = 0;\n\t\tif (!nfsd4_last_compound_op(rqstp))\n\t\t\tspace_needed = COMPOUND_ERR_SLACK_SPACE;\n\t\top->status = nfsd4_check_resp_size(resp, space_needed);\n\t}\n\tif (op->status == nfserr_resource && nfsd4_has_session(&resp->cstate)) {\n\t\tstruct nfsd4_slot *slot = resp->cstate.slot;\n\n\t\tif (slot->sl_flags & NFSD4_SLOT_CACHETHIS)\n\t\t\top->status = nfserr_rep_too_big_to_cache;\n\t\telse\n\t\t\top->status = nfserr_rep_too_big;\n\t}\n\tif (op->status == nfserr_resource ||\n\t    op->status == nfserr_rep_too_big ||\n\t    op->status == nfserr_rep_too_big_to_cache) {\n\t\t/*\n\t\t * The operation may have already been encoded or\n\t\t * partially encoded.  No op returns anything additional\n\t\t * in the case of one of these three errors, so we can\n\t\t * just truncate back to after the status.  But it's a\n\t\t * bug if we had to do this on a non-idempotent op:\n\t\t */\n\t\twarn_on_nonidempotent_op(op);\n\t\txdr_truncate_encode(xdr, post_err_offset);\n\t}\n\tif (so) {\n\t\tint len = xdr->buf->len - post_err_offset;\n\n\t\tso->so_replay.rp_status = op->status;\n\t\tso->so_replay.rp_buflen = len;\n\t\tread_bytes_from_xdr_buf(xdr->buf, post_err_offset,\n\t\t\t\t\t\tso->so_replay.rp_buf, len);\n\t}\nstatus:\n\t/* Note that op->status is already in network byte order: */\n\twrite_bytes_to_xdr_buf(xdr->buf, post_err_offset - 4, &op->status, 4);\n}\n\n/* \n * Encode the reply stored in the stateowner reply cache \n * \n * XDR note: do not encode rp->rp_buflen: the buffer contains the\n * previously sent already encoded operation.\n */\nvoid\nnfsd4_encode_replay(struct xdr_stream *xdr, struct nfsd4_op *op)\n{\n\t__be32 *p;\n\tstruct nfs4_replay *rp = op->replay;\n\n\tBUG_ON(!rp);\n\n\tp = xdr_reserve_space(xdr, 8 + rp->rp_buflen);\n\tif (!p) {\n\t\tWARN_ON_ONCE(1);\n\t\treturn;\n\t}\n\t*p++ = cpu_to_be32(op->opnum);\n\t*p++ = rp->rp_status;  /* already xdr'ed */\n\n\tp = xdr_encode_opaque_fixed(p, rp->rp_buf, rp->rp_buflen);\n}\n\nint\nnfs4svc_encode_voidres(struct svc_rqst *rqstp, __be32 *p, void *dummy)\n{\n        return xdr_ressize_check(rqstp, p);\n}\n\nint nfsd4_release_compoundargs(void *rq, __be32 *p, void *resp)\n{\n\tstruct svc_rqst *rqstp = rq;\n\tstruct nfsd4_compoundargs *args = rqstp->rq_argp;\n\n\tif (args->ops != args->iops) {\n\t\tkfree(args->ops);\n\t\targs->ops = args->iops;\n\t}\n\tkfree(args->tmpp);\n\targs->tmpp = NULL;\n\twhile (args->to_free) {\n\t\tstruct svcxdr_tmpbuf *tb = args->to_free;\n\t\targs->to_free = tb->next;\n\t\tkfree(tb);\n\t}\n\treturn 1;\n}\n\nint\nnfs4svc_decode_compoundargs(struct svc_rqst *rqstp, __be32 *p, struct nfsd4_compoundargs *args)\n{\n\tif (rqstp->rq_arg.head[0].iov_len % 4) {\n\t\t/* client is nuts */\n\t\tdprintk(\"%s: compound not properly padded! (peeraddr=%pISc xid=0x%x)\",\n\t\t\t__func__, svc_addr(rqstp), be32_to_cpu(rqstp->rq_xid));\n\t\treturn 0;\n\t}\n\targs->p = p;\n\targs->end = rqstp->rq_arg.head[0].iov_base + rqstp->rq_arg.head[0].iov_len;\n\targs->pagelist = rqstp->rq_arg.pages;\n\targs->pagelen = rqstp->rq_arg.page_len;\n\targs->tmpp = NULL;\n\targs->to_free = NULL;\n\targs->ops = args->iops;\n\targs->rqstp = rqstp;\n\n\treturn !nfsd4_decode_compound(args);\n}\n\nint\nnfs4svc_encode_compoundres(struct svc_rqst *rqstp, __be32 *p, struct nfsd4_compoundres *resp)\n{\n\t/*\n\t * All that remains is to write the tag and operation count...\n\t */\n\tstruct xdr_buf *buf = resp->xdr.buf;\n\n\tWARN_ON_ONCE(buf->len != buf->head[0].iov_len + buf->page_len +\n\t\t\t\t buf->tail[0].iov_len);\n\n\trqstp->rq_next_page = resp->xdr.page_ptr + 1;\n\n\tp = resp->tagp;\n\t*p++ = htonl(resp->taglen);\n\tmemcpy(p, resp->tag, resp->taglen);\n\tp += XDR_QUADLEN(resp->taglen);\n\t*p++ = htonl(resp->opcnt);\n\n\tnfsd4_sequence_done(resp);\n\treturn 1;\n}\n\n/*\n * Local variables:\n *  c-basic-offset: 8\n * End:\n */\n", "/*\n * XDR support for nfsd\n *\n * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>\n */\n\n#include \"vfs.h\"\n#include \"xdr.h\"\n#include \"auth.h\"\n\n#define NFSDDBG_FACILITY\t\tNFSDDBG_XDR\n\n/*\n * Mapping of S_IF* types to NFS file types\n */\nstatic u32\tnfs_ftypes[] = {\n\tNFNON,  NFCHR,  NFCHR, NFBAD,\n\tNFDIR,  NFBAD,  NFBLK, NFBAD,\n\tNFREG,  NFBAD,  NFLNK, NFBAD,\n\tNFSOCK, NFBAD,  NFLNK, NFBAD,\n};\n\n\n/*\n * XDR functions for basic NFS types\n */\nstatic __be32 *\ndecode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\tfh_init(fhp, NFS_FHSIZE);\n\tmemcpy(&fhp->fh_handle.fh_base, p, NFS_FHSIZE);\n\tfhp->fh_handle.fh_size = NFS_FHSIZE;\n\n\t/* FIXME: Look up export pointer here and verify\n\t * Sun Secure RPC if requested */\n\treturn p + (NFS_FHSIZE >> 2);\n}\n\n/* Helper function for NFSv2 ACL code */\n__be32 *nfs2svc_decode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\treturn decode_fh(p, fhp);\n}\n\nstatic __be32 *\nencode_fh(__be32 *p, struct svc_fh *fhp)\n{\n\tmemcpy(p, &fhp->fh_handle.fh_base, NFS_FHSIZE);\n\treturn p + (NFS_FHSIZE>> 2);\n}\n\n/*\n * Decode a file name and make sure that the path contains\n * no slashes or null bytes.\n */\nstatic __be32 *\ndecode_filename(__be32 *p, char **namp, unsigned int *lenp)\n{\n\tchar\t\t*name;\n\tunsigned int\ti;\n\n\tif ((p = xdr_decode_string_inplace(p, namp, lenp, NFS_MAXNAMLEN)) != NULL) {\n\t\tfor (i = 0, name = *namp; i < *lenp; i++, name++) {\n\t\t\tif (*name == '\\0' || *name == '/')\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn p;\n}\n\nstatic __be32 *\ndecode_pathname(__be32 *p, char **namp, unsigned int *lenp)\n{\n\tchar\t\t*name;\n\tunsigned int\ti;\n\n\tif ((p = xdr_decode_string_inplace(p, namp, lenp, NFS_MAXPATHLEN)) != NULL) {\n\t\tfor (i = 0, name = *namp; i < *lenp; i++, name++) {\n\t\t\tif (*name == '\\0')\n\t\t\t\treturn NULL;\n\t\t}\n\t}\n\n\treturn p;\n}\n\nstatic __be32 *\ndecode_sattr(__be32 *p, struct iattr *iap)\n{\n\tu32\ttmp, tmp1;\n\n\tiap->ia_valid = 0;\n\n\t/* Sun client bug compatibility check: some sun clients seem to\n\t * put 0xffff in the mode field when they mean 0xffffffff.\n\t * Quoting the 4.4BSD nfs server code: Nah nah nah nah na nah.\n\t */\n\tif ((tmp = ntohl(*p++)) != (u32)-1 && tmp != 0xffff) {\n\t\tiap->ia_valid |= ATTR_MODE;\n\t\tiap->ia_mode = tmp;\n\t}\n\tif ((tmp = ntohl(*p++)) != (u32)-1) {\n\t\tiap->ia_uid = make_kuid(&init_user_ns, tmp);\n\t\tif (uid_valid(iap->ia_uid))\n\t\t\tiap->ia_valid |= ATTR_UID;\n\t}\n\tif ((tmp = ntohl(*p++)) != (u32)-1) {\n\t\tiap->ia_gid = make_kgid(&init_user_ns, tmp);\n\t\tif (gid_valid(iap->ia_gid))\n\t\t\tiap->ia_valid |= ATTR_GID;\n\t}\n\tif ((tmp = ntohl(*p++)) != (u32)-1) {\n\t\tiap->ia_valid |= ATTR_SIZE;\n\t\tiap->ia_size = tmp;\n\t}\n\ttmp  = ntohl(*p++); tmp1 = ntohl(*p++);\n\tif (tmp != (u32)-1 && tmp1 != (u32)-1) {\n\t\tiap->ia_valid |= ATTR_ATIME | ATTR_ATIME_SET;\n\t\tiap->ia_atime.tv_sec = tmp;\n\t\tiap->ia_atime.tv_nsec = tmp1 * 1000; \n\t}\n\ttmp  = ntohl(*p++); tmp1 = ntohl(*p++);\n\tif (tmp != (u32)-1 && tmp1 != (u32)-1) {\n\t\tiap->ia_valid |= ATTR_MTIME | ATTR_MTIME_SET;\n\t\tiap->ia_mtime.tv_sec = tmp;\n\t\tiap->ia_mtime.tv_nsec = tmp1 * 1000; \n\t\t/*\n\t\t * Passing the invalid value useconds=1000000 for mtime\n\t\t * is a Sun convention for \"set both mtime and atime to\n\t\t * current server time\".  It's needed to make permissions\n\t\t * checks for the \"touch\" program across v2 mounts to\n\t\t * Solaris and Irix boxes work correctly. See description of\n\t\t * sattr in section 6.1 of \"NFS Illustrated\" by\n\t\t * Brent Callaghan, Addison-Wesley, ISBN 0-201-32750-5\n\t\t */\n\t\tif (tmp1 == 1000000)\n\t\t\tiap->ia_valid &= ~(ATTR_ATIME_SET|ATTR_MTIME_SET);\n\t}\n\treturn p;\n}\n\nstatic __be32 *\nencode_fattr(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp,\n\t     struct kstat *stat)\n{\n\tstruct dentry\t*dentry = fhp->fh_dentry;\n\tint type;\n\tstruct timespec time;\n\tu32 f;\n\n\ttype = (stat->mode & S_IFMT);\n\n\t*p++ = htonl(nfs_ftypes[type >> 12]);\n\t*p++ = htonl((u32) stat->mode);\n\t*p++ = htonl((u32) stat->nlink);\n\t*p++ = htonl((u32) from_kuid(&init_user_ns, stat->uid));\n\t*p++ = htonl((u32) from_kgid(&init_user_ns, stat->gid));\n\n\tif (S_ISLNK(type) && stat->size > NFS_MAXPATHLEN) {\n\t\t*p++ = htonl(NFS_MAXPATHLEN);\n\t} else {\n\t\t*p++ = htonl((u32) stat->size);\n\t}\n\t*p++ = htonl((u32) stat->blksize);\n\tif (S_ISCHR(type) || S_ISBLK(type))\n\t\t*p++ = htonl(new_encode_dev(stat->rdev));\n\telse\n\t\t*p++ = htonl(0xffffffff);\n\t*p++ = htonl((u32) stat->blocks);\n\tswitch (fsid_source(fhp)) {\n\tdefault:\n\tcase FSIDSOURCE_DEV:\n\t\t*p++ = htonl(new_encode_dev(stat->dev));\n\t\tbreak;\n\tcase FSIDSOURCE_FSID:\n\t\t*p++ = htonl((u32) fhp->fh_export->ex_fsid);\n\t\tbreak;\n\tcase FSIDSOURCE_UUID:\n\t\tf = ((u32*)fhp->fh_export->ex_uuid)[0];\n\t\tf ^= ((u32*)fhp->fh_export->ex_uuid)[1];\n\t\tf ^= ((u32*)fhp->fh_export->ex_uuid)[2];\n\t\tf ^= ((u32*)fhp->fh_export->ex_uuid)[3];\n\t\t*p++ = htonl(f);\n\t\tbreak;\n\t}\n\t*p++ = htonl((u32) stat->ino);\n\t*p++ = htonl((u32) stat->atime.tv_sec);\n\t*p++ = htonl(stat->atime.tv_nsec ? stat->atime.tv_nsec / 1000 : 0);\n\tlease_get_mtime(d_inode(dentry), &time); \n\t*p++ = htonl((u32) time.tv_sec);\n\t*p++ = htonl(time.tv_nsec ? time.tv_nsec / 1000 : 0); \n\t*p++ = htonl((u32) stat->ctime.tv_sec);\n\t*p++ = htonl(stat->ctime.tv_nsec ? stat->ctime.tv_nsec / 1000 : 0);\n\n\treturn p;\n}\n\n/* Helper function for NFSv2 ACL code */\n__be32 *nfs2svc_encode_fattr(struct svc_rqst *rqstp, __be32 *p, struct svc_fh *fhp, struct kstat *stat)\n{\n\treturn encode_fattr(rqstp, p, fhp, stat);\n}\n\n/*\n * XDR decode functions\n */\nint\nnfssvc_decode_void(struct svc_rqst *rqstp, __be32 *p, void *dummy)\n{\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_fhandle(struct svc_rqst *rqstp, __be32 *p, struct nfsd_fhandle *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_sattrargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_sattrargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tp = decode_sattr(p, &args->attrs);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_diropargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_diropargs *args)\n{\n\tif (!(p = decode_fh(p, &args->fh))\n\t || !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_readargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_readargs *args)\n{\n\tunsigned int len;\n\tint v;\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\n\targs->offset    = ntohl(*p++);\n\tlen = args->count     = ntohl(*p++);\n\tp++; /* totalcount - unused */\n\n\tif (!xdr_argsize_check(rqstp, p))\n\t\treturn 0;\n\n\tlen = min_t(unsigned int, len, NFSSVC_MAXBLKSIZE_V2);\n\n\t/* set up somewhere to store response.\n\t * We take pages, put them on reslist and include in iovec\n\t */\n\tv=0;\n\twhile (len > 0) {\n\t\tstruct page *p = *(rqstp->rq_next_page++);\n\n\t\trqstp->rq_vec[v].iov_base = page_address(p);\n\t\trqstp->rq_vec[v].iov_len = min_t(unsigned int, len, PAGE_SIZE);\n\t\tlen -= rqstp->rq_vec[v].iov_len;\n\t\tv++;\n\t}\n\targs->vlen = v;\n\treturn 1;\n}\n\nint\nnfssvc_decode_writeargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_writeargs *args)\n{\n\tunsigned int len, hdr, dlen;\n\tstruct kvec *head = rqstp->rq_arg.head;\n\tint v;\n\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\n\tp++;\t\t\t\t/* beginoffset */\n\targs->offset = ntohl(*p++);\t/* offset */\n\tp++;\t\t\t\t/* totalcount */\n\tlen = args->len = ntohl(*p++);\n\t/*\n\t * The protocol specifies a maximum of 8192 bytes.\n\t */\n\tif (len > NFSSVC_MAXBLKSIZE_V2)\n\t\treturn 0;\n\n\t/*\n\t * Check to make sure that we got the right number of\n\t * bytes.\n\t */\n\thdr = (void*)p - head->iov_base;\n\tif (hdr > head->iov_len)\n\t\treturn 0;\n\tdlen = head->iov_len + rqstp->rq_arg.page_len - hdr;\n\n\t/*\n\t * Round the length of the data which was specified up to\n\t * the next multiple of XDR units and then compare that\n\t * against the length which was actually received.\n\t * Note that when RPCSEC/GSS (for example) is used, the\n\t * data buffer can be padded so dlen might be larger\n\t * than required.  It must never be smaller.\n\t */\n\tif (dlen < XDR_QUADLEN(len)*4)\n\t\treturn 0;\n\n\trqstp->rq_vec[0].iov_base = (void*)p;\n\trqstp->rq_vec[0].iov_len = head->iov_len - hdr;\n\tv = 0;\n\twhile (len > rqstp->rq_vec[v].iov_len) {\n\t\tlen -= rqstp->rq_vec[v].iov_len;\n\t\tv++;\n\t\trqstp->rq_vec[v].iov_base = page_address(rqstp->rq_pages[v]);\n\t\trqstp->rq_vec[v].iov_len = PAGE_SIZE;\n\t}\n\trqstp->rq_vec[v].iov_len = len;\n\targs->vlen = v + 1;\n\treturn 1;\n}\n\nint\nnfssvc_decode_createargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_createargs *args)\n{\n\tif (   !(p = decode_fh(p, &args->fh))\n\t    || !(p = decode_filename(p, &args->name, &args->len)))\n\t\treturn 0;\n\tp = decode_sattr(p, &args->attrs);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_renameargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_renameargs *args)\n{\n\tif (!(p = decode_fh(p, &args->ffh))\n\t || !(p = decode_filename(p, &args->fname, &args->flen))\n\t || !(p = decode_fh(p, &args->tfh))\n\t || !(p = decode_filename(p, &args->tname, &args->tlen)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_readlinkargs(struct svc_rqst *rqstp, __be32 *p, struct nfsd_readlinkargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\tif (!xdr_argsize_check(rqstp, p))\n\t\treturn 0;\n\targs->buffer = page_address(*(rqstp->rq_next_page++));\n\n\treturn 1;\n}\n\nint\nnfssvc_decode_linkargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_linkargs *args)\n{\n\tif (!(p = decode_fh(p, &args->ffh))\n\t || !(p = decode_fh(p, &args->tfh))\n\t || !(p = decode_filename(p, &args->tname, &args->tlen)))\n\t\treturn 0;\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_symlinkargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_symlinkargs *args)\n{\n\tif (   !(p = decode_fh(p, &args->ffh))\n\t    || !(p = decode_filename(p, &args->fname, &args->flen))\n\t    || !(p = decode_pathname(p, &args->tname, &args->tlen)))\n\t\treturn 0;\n\tp = decode_sattr(p, &args->attrs);\n\n\treturn xdr_argsize_check(rqstp, p);\n}\n\nint\nnfssvc_decode_readdirargs(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_readdirargs *args)\n{\n\tp = decode_fh(p, &args->fh);\n\tif (!p)\n\t\treturn 0;\n\targs->cookie = ntohl(*p++);\n\targs->count  = ntohl(*p++);\n\targs->count  = min_t(u32, args->count, PAGE_SIZE);\n\tif (!xdr_argsize_check(rqstp, p))\n\t\treturn 0;\n\targs->buffer = page_address(*(rqstp->rq_next_page++));\n\n\treturn 1;\n}\n\n/*\n * XDR encode functions\n */\nint\nnfssvc_encode_void(struct svc_rqst *rqstp, __be32 *p, void *dummy)\n{\n\treturn xdr_ressize_check(rqstp, p);\n}\n\nint\nnfssvc_encode_attrstat(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_attrstat *resp)\n{\n\tp = encode_fattr(rqstp, p, &resp->fh, &resp->stat);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\nint\nnfssvc_encode_diropres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_diropres *resp)\n{\n\tp = encode_fh(p, &resp->fh);\n\tp = encode_fattr(rqstp, p, &resp->fh, &resp->stat);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\nint\nnfssvc_encode_readlinkres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_readlinkres *resp)\n{\n\t*p++ = htonl(resp->len);\n\txdr_ressize_check(rqstp, p);\n\trqstp->rq_res.page_len = resp->len;\n\tif (resp->len & 3) {\n\t\t/* need to pad the tail */\n\t\trqstp->rq_res.tail[0].iov_base = p;\n\t\t*p = 0;\n\t\trqstp->rq_res.tail[0].iov_len = 4 - (resp->len&3);\n\t}\n\treturn 1;\n}\n\nint\nnfssvc_encode_readres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_readres *resp)\n{\n\tp = encode_fattr(rqstp, p, &resp->fh, &resp->stat);\n\t*p++ = htonl(resp->count);\n\txdr_ressize_check(rqstp, p);\n\n\t/* now update rqstp->rq_res to reflect data as well */\n\trqstp->rq_res.page_len = resp->count;\n\tif (resp->count & 3) {\n\t\t/* need to pad the tail */\n\t\trqstp->rq_res.tail[0].iov_base = p;\n\t\t*p = 0;\n\t\trqstp->rq_res.tail[0].iov_len = 4 - (resp->count&3);\n\t}\n\treturn 1;\n}\n\nint\nnfssvc_encode_readdirres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_readdirres *resp)\n{\n\txdr_ressize_check(rqstp, p);\n\tp = resp->buffer;\n\t*p++ = 0;\t\t\t/* no more entries */\n\t*p++ = htonl((resp->common.err == nfserr_eof));\n\trqstp->rq_res.page_len = (((unsigned long)p-1) & ~PAGE_MASK)+1;\n\n\treturn 1;\n}\n\nint\nnfssvc_encode_statfsres(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_statfsres *resp)\n{\n\tstruct kstatfs\t*stat = &resp->stats;\n\n\t*p++ = htonl(NFSSVC_MAXBLKSIZE_V2);\t/* max transfer size */\n\t*p++ = htonl(stat->f_bsize);\n\t*p++ = htonl(stat->f_blocks);\n\t*p++ = htonl(stat->f_bfree);\n\t*p++ = htonl(stat->f_bavail);\n\treturn xdr_ressize_check(rqstp, p);\n}\n\nint\nnfssvc_encode_entry(void *ccdv, const char *name,\n\t\t    int namlen, loff_t offset, u64 ino, unsigned int d_type)\n{\n\tstruct readdir_cd *ccd = ccdv;\n\tstruct nfsd_readdirres *cd = container_of(ccd, struct nfsd_readdirres, common);\n\t__be32\t*p = cd->buffer;\n\tint\tbuflen, slen;\n\n\t/*\n\tdprintk(\"nfsd: entry(%.*s off %ld ino %ld)\\n\",\n\t\t\tnamlen, name, offset, ino);\n\t */\n\n\tif (offset > ~((u32) 0)) {\n\t\tcd->common.err = nfserr_fbig;\n\t\treturn -EINVAL;\n\t}\n\tif (cd->offset)\n\t\t*cd->offset = htonl(offset);\n\n\t/* truncate filename */\n\tnamlen = min(namlen, NFS2_MAXNAMLEN);\n\tslen = XDR_QUADLEN(namlen);\n\n\tif ((buflen = cd->buflen - slen - 4) < 0) {\n\t\tcd->common.err = nfserr_toosmall;\n\t\treturn -EINVAL;\n\t}\n\tif (ino > ~((u32) 0)) {\n\t\tcd->common.err = nfserr_fbig;\n\t\treturn -EINVAL;\n\t}\n\t*p++ = xdr_one;\t\t\t\t/* mark entry present */\n\t*p++ = htonl((u32) ino);\t\t/* file id */\n\tp    = xdr_encode_array(p, name, namlen);/* name length & name */\n\tcd->offset = p;\t\t\t/* remember pointer */\n\t*p++ = htonl(~0U);\t\t/* offset of next entry */\n\n\tcd->buflen = buflen;\n\tcd->buffer = p;\n\tcd->common.err = nfs_ok;\n\treturn 0;\n}\n\n/*\n * XDR release functions\n */\nint\nnfssvc_release_fhandle(struct svc_rqst *rqstp, __be32 *p,\n\t\t\t\t\tstruct nfsd_fhandle *resp)\n{\n\tfh_put(&resp->fh);\n\treturn 1;\n}\n", "/*\n * File operations used by nfsd. Some of these have been ripped from\n * other parts of the kernel because they weren't exported, others\n * are partial duplicates with added or changed functionality.\n *\n * Note that several functions dget() the dentry upon which they want\n * to act, most notably those that create directory entries. Response\n * dentry's are dput()'d if necessary in the release callback.\n * So if you notice code paths that apparently fail to dput() the\n * dentry, don't worry--they have been taken care of.\n *\n * Copyright (C) 1995-1999 Olaf Kirch <okir@monad.swb.de>\n * Zerocpy NFS support (C) 2002 Hirokazu Takahashi <taka@valinux.co.jp>\n */\n\n#include <linux/fs.h>\n#include <linux/file.h>\n#include <linux/splice.h>\n#include <linux/falloc.h>\n#include <linux/fcntl.h>\n#include <linux/namei.h>\n#include <linux/delay.h>\n#include <linux/fsnotify.h>\n#include <linux/posix_acl_xattr.h>\n#include <linux/xattr.h>\n#include <linux/jhash.h>\n#include <linux/ima.h>\n#include <linux/slab.h>\n#include <linux/uaccess.h>\n#include <linux/exportfs.h>\n#include <linux/writeback.h>\n#include <linux/security.h>\n\n#ifdef CONFIG_NFSD_V3\n#include \"xdr3.h\"\n#endif /* CONFIG_NFSD_V3 */\n\n#ifdef CONFIG_NFSD_V4\n#include \"../internal.h\"\n#include \"acl.h\"\n#include \"idmap.h\"\n#endif /* CONFIG_NFSD_V4 */\n\n#include \"nfsd.h\"\n#include \"vfs.h\"\n#include \"trace.h\"\n\n#define NFSDDBG_FACILITY\t\tNFSDDBG_FILEOP\n\n\n/*\n * This is a cache of readahead params that help us choose the proper\n * readahead strategy. Initially, we set all readahead parameters to 0\n * and let the VFS handle things.\n * If you increase the number of cached files very much, you'll need to\n * add a hash table here.\n */\nstruct raparms {\n\tstruct raparms\t\t*p_next;\n\tunsigned int\t\tp_count;\n\tino_t\t\t\tp_ino;\n\tdev_t\t\t\tp_dev;\n\tint\t\t\tp_set;\n\tstruct file_ra_state\tp_ra;\n\tunsigned int\t\tp_hindex;\n};\n\nstruct raparm_hbucket {\n\tstruct raparms\t\t*pb_head;\n\tspinlock_t\t\tpb_lock;\n} ____cacheline_aligned_in_smp;\n\n#define RAPARM_HASH_BITS\t4\n#define RAPARM_HASH_SIZE\t(1<<RAPARM_HASH_BITS)\n#define RAPARM_HASH_MASK\t(RAPARM_HASH_SIZE-1)\nstatic struct raparm_hbucket\traparm_hash[RAPARM_HASH_SIZE];\n\n/* \n * Called from nfsd_lookup and encode_dirent. Check if we have crossed \n * a mount point.\n * Returns -EAGAIN or -ETIMEDOUT leaving *dpp and *expp unchanged,\n *  or nfs_ok having possibly changed *dpp and *expp\n */\nint\nnfsd_cross_mnt(struct svc_rqst *rqstp, struct dentry **dpp, \n\t\t        struct svc_export **expp)\n{\n\tstruct svc_export *exp = *expp, *exp2 = NULL;\n\tstruct dentry *dentry = *dpp;\n\tstruct path path = {.mnt = mntget(exp->ex_path.mnt),\n\t\t\t    .dentry = dget(dentry)};\n\tint err = 0;\n\n\terr = follow_down(&path);\n\tif (err < 0)\n\t\tgoto out;\n\tif (path.mnt == exp->ex_path.mnt && path.dentry == dentry &&\n\t    nfsd_mountpoint(dentry, exp) == 2) {\n\t\t/* This is only a mountpoint in some other namespace */\n\t\tpath_put(&path);\n\t\tgoto out;\n\t}\n\n\texp2 = rqst_exp_get_by_name(rqstp, &path);\n\tif (IS_ERR(exp2)) {\n\t\terr = PTR_ERR(exp2);\n\t\t/*\n\t\t * We normally allow NFS clients to continue\n\t\t * \"underneath\" a mountpoint that is not exported.\n\t\t * The exception is V4ROOT, where no traversal is ever\n\t\t * allowed without an explicit export of the new\n\t\t * directory.\n\t\t */\n\t\tif (err == -ENOENT && !(exp->ex_flags & NFSEXP_V4ROOT))\n\t\t\terr = 0;\n\t\tpath_put(&path);\n\t\tgoto out;\n\t}\n\tif (nfsd_v4client(rqstp) ||\n\t\t(exp->ex_flags & NFSEXP_CROSSMOUNT) || EX_NOHIDE(exp2)) {\n\t\t/* successfully crossed mount point */\n\t\t/*\n\t\t * This is subtle: path.dentry is *not* on path.mnt\n\t\t * at this point.  The only reason we are safe is that\n\t\t * original mnt is pinned down by exp, so we should\n\t\t * put path *before* putting exp\n\t\t */\n\t\t*dpp = path.dentry;\n\t\tpath.dentry = dentry;\n\t\t*expp = exp2;\n\t\texp2 = exp;\n\t}\n\tpath_put(&path);\n\texp_put(exp2);\nout:\n\treturn err;\n}\n\nstatic void follow_to_parent(struct path *path)\n{\n\tstruct dentry *dp;\n\n\twhile (path->dentry == path->mnt->mnt_root && follow_up(path))\n\t\t;\n\tdp = dget_parent(path->dentry);\n\tdput(path->dentry);\n\tpath->dentry = dp;\n}\n\nstatic int nfsd_lookup_parent(struct svc_rqst *rqstp, struct dentry *dparent, struct svc_export **exp, struct dentry **dentryp)\n{\n\tstruct svc_export *exp2;\n\tstruct path path = {.mnt = mntget((*exp)->ex_path.mnt),\n\t\t\t    .dentry = dget(dparent)};\n\n\tfollow_to_parent(&path);\n\n\texp2 = rqst_exp_parent(rqstp, &path);\n\tif (PTR_ERR(exp2) == -ENOENT) {\n\t\t*dentryp = dget(dparent);\n\t} else if (IS_ERR(exp2)) {\n\t\tpath_put(&path);\n\t\treturn PTR_ERR(exp2);\n\t} else {\n\t\t*dentryp = dget(path.dentry);\n\t\texp_put(*exp);\n\t\t*exp = exp2;\n\t}\n\tpath_put(&path);\n\treturn 0;\n}\n\n/*\n * For nfsd purposes, we treat V4ROOT exports as though there was an\n * export at *every* directory.\n * We return:\n * '1' if this dentry *must* be an export point,\n * '2' if it might be, if there is really a mount here, and\n * '0' if there is no chance of an export point here.\n */\nint nfsd_mountpoint(struct dentry *dentry, struct svc_export *exp)\n{\n\tif (!d_inode(dentry))\n\t\treturn 0;\n\tif (exp->ex_flags & NFSEXP_V4ROOT)\n\t\treturn 1;\n\tif (nfsd4_is_junction(dentry))\n\t\treturn 1;\n\tif (d_mountpoint(dentry))\n\t\t/*\n\t\t * Might only be a mountpoint in a different namespace,\n\t\t * but we need to check.\n\t\t */\n\t\treturn 2;\n\treturn 0;\n}\n\n__be32\nnfsd_lookup_dentry(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\t   const char *name, unsigned int len,\n\t\t   struct svc_export **exp_ret, struct dentry **dentry_ret)\n{\n\tstruct svc_export\t*exp;\n\tstruct dentry\t\t*dparent;\n\tstruct dentry\t\t*dentry;\n\tint\t\t\thost_err;\n\n\tdprintk(\"nfsd: nfsd_lookup(fh %s, %.*s)\\n\", SVCFH_fmt(fhp), len,name);\n\n\tdparent = fhp->fh_dentry;\n\texp = exp_get(fhp->fh_export);\n\n\t/* Lookup the name, but don't follow links */\n\tif (isdotent(name, len)) {\n\t\tif (len==1)\n\t\t\tdentry = dget(dparent);\n\t\telse if (dparent != exp->ex_path.dentry)\n\t\t\tdentry = dget_parent(dparent);\n\t\telse if (!EX_NOHIDE(exp) && !nfsd_v4client(rqstp))\n\t\t\tdentry = dget(dparent); /* .. == . just like at / */\n\t\telse {\n\t\t\t/* checking mountpoint crossing is very different when stepping up */\n\t\t\thost_err = nfsd_lookup_parent(rqstp, dparent, &exp, &dentry);\n\t\t\tif (host_err)\n\t\t\t\tgoto out_nfserr;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * In the nfsd4_open() case, this may be held across\n\t\t * subsequent open and delegation acquisition which may\n\t\t * need to take the child's i_mutex:\n\t\t */\n\t\tfh_lock_nested(fhp, I_MUTEX_PARENT);\n\t\tdentry = lookup_one_len(name, dparent, len);\n\t\thost_err = PTR_ERR(dentry);\n\t\tif (IS_ERR(dentry))\n\t\t\tgoto out_nfserr;\n\t\tif (nfsd_mountpoint(dentry, exp)) {\n\t\t\t/*\n\t\t\t * We don't need the i_mutex after all.  It's\n\t\t\t * still possible we could open this (regular\n\t\t\t * files can be mountpoints too), but the\n\t\t\t * i_mutex is just there to prevent renames of\n\t\t\t * something that we might be about to delegate,\n\t\t\t * and a mountpoint won't be renamed:\n\t\t\t */\n\t\t\tfh_unlock(fhp);\n\t\t\tif ((host_err = nfsd_cross_mnt(rqstp, &dentry, &exp))) {\n\t\t\t\tdput(dentry);\n\t\t\t\tgoto out_nfserr;\n\t\t\t}\n\t\t}\n\t}\n\t*dentry_ret = dentry;\n\t*exp_ret = exp;\n\treturn 0;\n\nout_nfserr:\n\texp_put(exp);\n\treturn nfserrno(host_err);\n}\n\n/*\n * Look up one component of a pathname.\n * N.B. After this call _both_ fhp and resfh need an fh_put\n *\n * If the lookup would cross a mountpoint, and the mounted filesystem\n * is exported to the client with NFSEXP_NOHIDE, then the lookup is\n * accepted as it stands and the mounted directory is\n * returned. Otherwise the covered directory is returned.\n * NOTE: this mountpoint crossing is not supported properly by all\n *   clients and is explicitly disallowed for NFSv3\n *      NeilBrown <neilb@cse.unsw.edu.au>\n */\n__be32\nnfsd_lookup(struct svc_rqst *rqstp, struct svc_fh *fhp, const char *name,\n\t\t\t\tunsigned int len, struct svc_fh *resfh)\n{\n\tstruct svc_export\t*exp;\n\tstruct dentry\t\t*dentry;\n\t__be32 err;\n\n\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_EXEC);\n\tif (err)\n\t\treturn err;\n\terr = nfsd_lookup_dentry(rqstp, fhp, name, len, &exp, &dentry);\n\tif (err)\n\t\treturn err;\n\terr = check_nfsd_access(exp, rqstp);\n\tif (err)\n\t\tgoto out;\n\t/*\n\t * Note: we compose the file handle now, but as the\n\t * dentry may be negative, it may need to be updated.\n\t */\n\terr = fh_compose(resfh, exp, dentry, fhp);\n\tif (!err && d_really_is_negative(dentry))\n\t\terr = nfserr_noent;\nout:\n\tdput(dentry);\n\texp_put(exp);\n\treturn err;\n}\n\n/*\n * Commit metadata changes to stable storage.\n */\nstatic int\ncommit_metadata(struct svc_fh *fhp)\n{\n\tstruct inode *inode = d_inode(fhp->fh_dentry);\n\tconst struct export_operations *export_ops = inode->i_sb->s_export_op;\n\n\tif (!EX_ISSYNC(fhp->fh_export))\n\t\treturn 0;\n\n\tif (export_ops->commit_metadata)\n\t\treturn export_ops->commit_metadata(inode);\n\treturn sync_inode_metadata(inode, 1);\n}\n\n/*\n * Go over the attributes and take care of the small differences between\n * NFS semantics and what Linux expects.\n */\nstatic void\nnfsd_sanitize_attrs(struct inode *inode, struct iattr *iap)\n{\n\t/* sanitize the mode change */\n\tif (iap->ia_valid & ATTR_MODE) {\n\t\tiap->ia_mode &= S_IALLUGO;\n\t\tiap->ia_mode |= (inode->i_mode & ~S_IALLUGO);\n\t}\n\n\t/* Revoke setuid/setgid on chown */\n\tif (!S_ISDIR(inode->i_mode) &&\n\t    ((iap->ia_valid & ATTR_UID) || (iap->ia_valid & ATTR_GID))) {\n\t\tiap->ia_valid |= ATTR_KILL_PRIV;\n\t\tif (iap->ia_valid & ATTR_MODE) {\n\t\t\t/* we're setting mode too, just clear the s*id bits */\n\t\t\tiap->ia_mode &= ~S_ISUID;\n\t\t\tif (iap->ia_mode & S_IXGRP)\n\t\t\t\tiap->ia_mode &= ~S_ISGID;\n\t\t} else {\n\t\t\t/* set ATTR_KILL_* bits and let VFS handle it */\n\t\t\tiap->ia_valid |= (ATTR_KILL_SUID | ATTR_KILL_SGID);\n\t\t}\n\t}\n}\n\nstatic __be32\nnfsd_get_write_access(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tstruct iattr *iap)\n{\n\tstruct inode *inode = d_inode(fhp->fh_dentry);\n\tint host_err;\n\n\tif (iap->ia_size < inode->i_size) {\n\t\t__be32 err;\n\n\t\terr = nfsd_permission(rqstp, fhp->fh_export, fhp->fh_dentry,\n\t\t\t\tNFSD_MAY_TRUNC | NFSD_MAY_OWNER_OVERRIDE);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\thost_err = get_write_access(inode);\n\tif (host_err)\n\t\tgoto out_nfserrno;\n\n\thost_err = locks_verify_truncate(inode, NULL, iap->ia_size);\n\tif (host_err)\n\t\tgoto out_put_write_access;\n\treturn 0;\n\nout_put_write_access:\n\tput_write_access(inode);\nout_nfserrno:\n\treturn nfserrno(host_err);\n}\n\n/*\n * Set various file attributes.  After this call fhp needs an fh_put.\n */\n__be32\nnfsd_setattr(struct svc_rqst *rqstp, struct svc_fh *fhp, struct iattr *iap,\n\t     int check_guard, time_t guardtime)\n{\n\tstruct dentry\t*dentry;\n\tstruct inode\t*inode;\n\tint\t\taccmode = NFSD_MAY_SATTR;\n\tumode_t\t\tftype = 0;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\tbool\t\tget_write_count;\n\tbool\t\tsize_change = (iap->ia_valid & ATTR_SIZE);\n\n\tif (iap->ia_valid & (ATTR_ATIME | ATTR_MTIME | ATTR_SIZE))\n\t\taccmode |= NFSD_MAY_WRITE|NFSD_MAY_OWNER_OVERRIDE;\n\tif (iap->ia_valid & ATTR_SIZE)\n\t\tftype = S_IFREG;\n\n\t/* Callers that do fh_verify should do the fh_want_write: */\n\tget_write_count = !fhp->fh_dentry;\n\n\t/* Get inode */\n\terr = fh_verify(rqstp, fhp, ftype, accmode);\n\tif (err)\n\t\treturn err;\n\tif (get_write_count) {\n\t\thost_err = fh_want_write(fhp);\n\t\tif (host_err)\n\t\t\tgoto out;\n\t}\n\n\tdentry = fhp->fh_dentry;\n\tinode = d_inode(dentry);\n\n\t/* Ignore any mode updates on symlinks */\n\tif (S_ISLNK(inode->i_mode))\n\t\tiap->ia_valid &= ~ATTR_MODE;\n\n\tif (!iap->ia_valid)\n\t\treturn 0;\n\n\tnfsd_sanitize_attrs(inode, iap);\n\n\tif (check_guard && guardtime != inode->i_ctime.tv_sec)\n\t\treturn nfserr_notsync;\n\n\t/*\n\t * The size case is special, it changes the file in addition to the\n\t * attributes, and file systems don't expect it to be mixed with\n\t * \"random\" attribute changes.  We thus split out the size change\n\t * into a separate call to ->setattr, and do the rest as a separate\n\t * setattr call.\n\t */\n\tif (size_change) {\n\t\terr = nfsd_get_write_access(rqstp, fhp, iap);\n\t\tif (err)\n\t\t\treturn err;\n\t}\n\n\tfh_lock(fhp);\n\tif (size_change) {\n\t\t/*\n\t\t * RFC5661, Section 18.30.4:\n\t\t *   Changing the size of a file with SETATTR indirectly\n\t\t *   changes the time_modify and change attributes.\n\t\t *\n\t\t * (and similar for the older RFCs)\n\t\t */\n\t\tstruct iattr size_attr = {\n\t\t\t.ia_valid\t= ATTR_SIZE | ATTR_CTIME | ATTR_MTIME,\n\t\t\t.ia_size\t= iap->ia_size,\n\t\t};\n\n\t\thost_err = notify_change(dentry, &size_attr, NULL);\n\t\tif (host_err)\n\t\t\tgoto out_unlock;\n\t\tiap->ia_valid &= ~ATTR_SIZE;\n\n\t\t/*\n\t\t * Avoid the additional setattr call below if the only other\n\t\t * attribute that the client sends is the mtime, as we update\n\t\t * it as part of the size change above.\n\t\t */\n\t\tif ((iap->ia_valid & ~ATTR_MTIME) == 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\tiap->ia_valid |= ATTR_CTIME;\n\thost_err = notify_change(dentry, iap, NULL);\n\nout_unlock:\n\tfh_unlock(fhp);\n\tif (size_change)\n\t\tput_write_access(inode);\nout:\n\tif (!host_err)\n\t\thost_err = commit_metadata(fhp);\n\treturn nfserrno(host_err);\n}\n\n#if defined(CONFIG_NFSD_V4)\n/*\n * NFS junction information is stored in an extended attribute.\n */\n#define NFSD_JUNCTION_XATTR_NAME\tXATTR_TRUSTED_PREFIX \"junction.nfs\"\n\n/**\n * nfsd4_is_junction - Test if an object could be an NFS junction\n *\n * @dentry: object to test\n *\n * Returns 1 if \"dentry\" appears to contain NFS junction information.\n * Otherwise 0 is returned.\n */\nint nfsd4_is_junction(struct dentry *dentry)\n{\n\tstruct inode *inode = d_inode(dentry);\n\n\tif (inode == NULL)\n\t\treturn 0;\n\tif (inode->i_mode & S_IXUGO)\n\t\treturn 0;\n\tif (!(inode->i_mode & S_ISVTX))\n\t\treturn 0;\n\tif (vfs_getxattr(dentry, NFSD_JUNCTION_XATTR_NAME, NULL, 0) <= 0)\n\t\treturn 0;\n\treturn 1;\n}\n#ifdef CONFIG_NFSD_V4_SECURITY_LABEL\n__be32 nfsd4_set_nfs4_label(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tstruct xdr_netobj *label)\n{\n\t__be32 error;\n\tint host_error;\n\tstruct dentry *dentry;\n\n\terror = fh_verify(rqstp, fhp, 0 /* S_IFREG */, NFSD_MAY_SATTR);\n\tif (error)\n\t\treturn error;\n\n\tdentry = fhp->fh_dentry;\n\n\tinode_lock(d_inode(dentry));\n\thost_error = security_inode_setsecctx(dentry, label->data, label->len);\n\tinode_unlock(d_inode(dentry));\n\treturn nfserrno(host_error);\n}\n#else\n__be32 nfsd4_set_nfs4_label(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tstruct xdr_netobj *label)\n{\n\treturn nfserr_notsupp;\n}\n#endif\n\n__be32 nfsd4_clone_file_range(struct file *src, u64 src_pos, struct file *dst,\n\t\tu64 dst_pos, u64 count)\n{\n\treturn nfserrno(do_clone_file_range(src, src_pos, dst, dst_pos, count));\n}\n\nssize_t nfsd_copy_file_range(struct file *src, u64 src_pos, struct file *dst,\n\t\t\t     u64 dst_pos, u64 count)\n{\n\n\t/*\n\t * Limit copy to 4MB to prevent indefinitely blocking an nfsd\n\t * thread and client rpc slot.  The choice of 4MB is somewhat\n\t * arbitrary.  We might instead base this on r/wsize, or make it\n\t * tunable, or use a time instead of a byte limit, or implement\n\t * asynchronous copy.  In theory a client could also recognize a\n\t * limit like this and pipeline multiple COPY requests.\n\t */\n\tcount = min_t(u64, count, 1 << 22);\n\treturn vfs_copy_file_range(src, src_pos, dst, dst_pos, count, 0);\n}\n\n__be32 nfsd4_vfs_fallocate(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\t\t   struct file *file, loff_t offset, loff_t len,\n\t\t\t   int flags)\n{\n\tint error;\n\n\tif (!S_ISREG(file_inode(file)->i_mode))\n\t\treturn nfserr_inval;\n\n\terror = vfs_fallocate(file, flags, offset, len);\n\tif (!error)\n\t\terror = commit_metadata(fhp);\n\n\treturn nfserrno(error);\n}\n#endif /* defined(CONFIG_NFSD_V4) */\n\n#ifdef CONFIG_NFSD_V3\n/*\n * Check server access rights to a file system object\n */\nstruct accessmap {\n\tu32\t\taccess;\n\tint\t\thow;\n};\nstatic struct accessmap\tnfs3_regaccess[] = {\n    {\tNFS3_ACCESS_READ,\tNFSD_MAY_READ\t\t\t},\n    {\tNFS3_ACCESS_EXECUTE,\tNFSD_MAY_EXEC\t\t\t},\n    {\tNFS3_ACCESS_MODIFY,\tNFSD_MAY_WRITE|NFSD_MAY_TRUNC\t},\n    {\tNFS3_ACCESS_EXTEND,\tNFSD_MAY_WRITE\t\t\t},\n\n    {\t0,\t\t\t0\t\t\t\t}\n};\n\nstatic struct accessmap\tnfs3_diraccess[] = {\n    {\tNFS3_ACCESS_READ,\tNFSD_MAY_READ\t\t\t},\n    {\tNFS3_ACCESS_LOOKUP,\tNFSD_MAY_EXEC\t\t\t},\n    {\tNFS3_ACCESS_MODIFY,\tNFSD_MAY_EXEC|NFSD_MAY_WRITE|NFSD_MAY_TRUNC},\n    {\tNFS3_ACCESS_EXTEND,\tNFSD_MAY_EXEC|NFSD_MAY_WRITE\t},\n    {\tNFS3_ACCESS_DELETE,\tNFSD_MAY_REMOVE\t\t\t},\n\n    {\t0,\t\t\t0\t\t\t\t}\n};\n\nstatic struct accessmap\tnfs3_anyaccess[] = {\n\t/* Some clients - Solaris 2.6 at least, make an access call\n\t * to the server to check for access for things like /dev/null\n\t * (which really, the server doesn't care about).  So\n\t * We provide simple access checking for them, looking\n\t * mainly at mode bits, and we make sure to ignore read-only\n\t * filesystem checks\n\t */\n    {\tNFS3_ACCESS_READ,\tNFSD_MAY_READ\t\t\t},\n    {\tNFS3_ACCESS_EXECUTE,\tNFSD_MAY_EXEC\t\t\t},\n    {\tNFS3_ACCESS_MODIFY,\tNFSD_MAY_WRITE|NFSD_MAY_LOCAL_ACCESS\t},\n    {\tNFS3_ACCESS_EXTEND,\tNFSD_MAY_WRITE|NFSD_MAY_LOCAL_ACCESS\t},\n\n    {\t0,\t\t\t0\t\t\t\t}\n};\n\n__be32\nnfsd_access(struct svc_rqst *rqstp, struct svc_fh *fhp, u32 *access, u32 *supported)\n{\n\tstruct accessmap\t*map;\n\tstruct svc_export\t*export;\n\tstruct dentry\t\t*dentry;\n\tu32\t\t\tquery, result = 0, sresult = 0;\n\t__be32\t\t\terror;\n\n\terror = fh_verify(rqstp, fhp, 0, NFSD_MAY_NOP);\n\tif (error)\n\t\tgoto out;\n\n\texport = fhp->fh_export;\n\tdentry = fhp->fh_dentry;\n\n\tif (d_is_reg(dentry))\n\t\tmap = nfs3_regaccess;\n\telse if (d_is_dir(dentry))\n\t\tmap = nfs3_diraccess;\n\telse\n\t\tmap = nfs3_anyaccess;\n\n\n\tquery = *access;\n\tfor  (; map->access; map++) {\n\t\tif (map->access & query) {\n\t\t\t__be32 err2;\n\n\t\t\tsresult |= map->access;\n\n\t\t\terr2 = nfsd_permission(rqstp, export, dentry, map->how);\n\t\t\tswitch (err2) {\n\t\t\tcase nfs_ok:\n\t\t\t\tresult |= map->access;\n\t\t\t\tbreak;\n\t\t\t\t\n\t\t\t/* the following error codes just mean the access was not allowed,\n\t\t\t * rather than an error occurred */\n\t\t\tcase nfserr_rofs:\n\t\t\tcase nfserr_acces:\n\t\t\tcase nfserr_perm:\n\t\t\t\t/* simply don't \"or\" in the access bit. */\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\terror = err2;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t}\n\t}\n\t*access = result;\n\tif (supported)\n\t\t*supported = sresult;\n\n out:\n\treturn error;\n}\n#endif /* CONFIG_NFSD_V3 */\n\nstatic int nfsd_open_break_lease(struct inode *inode, int access)\n{\n\tunsigned int mode;\n\n\tif (access & NFSD_MAY_NOT_BREAK_LEASE)\n\t\treturn 0;\n\tmode = (access & NFSD_MAY_WRITE) ? O_WRONLY : O_RDONLY;\n\treturn break_lease(inode, mode | O_NONBLOCK);\n}\n\n/*\n * Open an existing file or directory.\n * The may_flags argument indicates the type of open (read/write/lock)\n * and additional flags.\n * N.B. After this call fhp needs an fh_put\n */\n__be32\nnfsd_open(struct svc_rqst *rqstp, struct svc_fh *fhp, umode_t type,\n\t\t\tint may_flags, struct file **filp)\n{\n\tstruct path\tpath;\n\tstruct inode\t*inode;\n\tstruct file\t*file;\n\tint\t\tflags = O_RDONLY|O_LARGEFILE;\n\t__be32\t\terr;\n\tint\t\thost_err = 0;\n\n\tvalidate_process_creds();\n\n\t/*\n\t * If we get here, then the client has already done an \"open\",\n\t * and (hopefully) checked permission - so allow OWNER_OVERRIDE\n\t * in case a chmod has now revoked permission.\n\t *\n\t * Arguably we should also allow the owner override for\n\t * directories, but we never have and it doesn't seem to have\n\t * caused anyone a problem.  If we were to change this, note\n\t * also that our filldir callbacks would need a variant of\n\t * lookup_one_len that doesn't check permissions.\n\t */\n\tif (type == S_IFREG)\n\t\tmay_flags |= NFSD_MAY_OWNER_OVERRIDE;\n\terr = fh_verify(rqstp, fhp, type, may_flags);\n\tif (err)\n\t\tgoto out;\n\n\tpath.mnt = fhp->fh_export->ex_path.mnt;\n\tpath.dentry = fhp->fh_dentry;\n\tinode = d_inode(path.dentry);\n\n\t/* Disallow write access to files with the append-only bit set\n\t * or any access when mandatory locking enabled\n\t */\n\terr = nfserr_perm;\n\tif (IS_APPEND(inode) && (may_flags & NFSD_MAY_WRITE))\n\t\tgoto out;\n\t/*\n\t * We must ignore files (but only files) which might have mandatory\n\t * locks on them because there is no way to know if the accesser has\n\t * the lock.\n\t */\n\tif (S_ISREG((inode)->i_mode) && mandatory_lock(inode))\n\t\tgoto out;\n\n\tif (!inode->i_fop)\n\t\tgoto out;\n\n\thost_err = nfsd_open_break_lease(inode, may_flags);\n\tif (host_err) /* NOMEM or WOULDBLOCK */\n\t\tgoto out_nfserr;\n\n\tif (may_flags & NFSD_MAY_WRITE) {\n\t\tif (may_flags & NFSD_MAY_READ)\n\t\t\tflags = O_RDWR|O_LARGEFILE;\n\t\telse\n\t\t\tflags = O_WRONLY|O_LARGEFILE;\n\t}\n\n\tfile = dentry_open(&path, flags, current_cred());\n\tif (IS_ERR(file)) {\n\t\thost_err = PTR_ERR(file);\n\t\tgoto out_nfserr;\n\t}\n\n\thost_err = ima_file_check(file, may_flags, 0);\n\tif (host_err) {\n\t\tfput(file);\n\t\tgoto out_nfserr;\n\t}\n\n\tif (may_flags & NFSD_MAY_64BIT_COOKIE)\n\t\tfile->f_mode |= FMODE_64BITHASH;\n\telse\n\t\tfile->f_mode |= FMODE_32BITHASH;\n\n\t*filp = file;\nout_nfserr:\n\terr = nfserrno(host_err);\nout:\n\tvalidate_process_creds();\n\treturn err;\n}\n\nstruct raparms *\nnfsd_init_raparms(struct file *file)\n{\n\tstruct inode *inode = file_inode(file);\n\tdev_t dev = inode->i_sb->s_dev;\n\tino_t ino = inode->i_ino;\n\tstruct raparms\t*ra, **rap, **frap = NULL;\n\tint depth = 0;\n\tunsigned int hash;\n\tstruct raparm_hbucket *rab;\n\n\thash = jhash_2words(dev, ino, 0xfeedbeef) & RAPARM_HASH_MASK;\n\trab = &raparm_hash[hash];\n\n\tspin_lock(&rab->pb_lock);\n\tfor (rap = &rab->pb_head; (ra = *rap); rap = &ra->p_next) {\n\t\tif (ra->p_ino == ino && ra->p_dev == dev)\n\t\t\tgoto found;\n\t\tdepth++;\n\t\tif (ra->p_count == 0)\n\t\t\tfrap = rap;\n\t}\n\tdepth = nfsdstats.ra_size;\n\tif (!frap) {\t\n\t\tspin_unlock(&rab->pb_lock);\n\t\treturn NULL;\n\t}\n\trap = frap;\n\tra = *frap;\n\tra->p_dev = dev;\n\tra->p_ino = ino;\n\tra->p_set = 0;\n\tra->p_hindex = hash;\nfound:\n\tif (rap != &rab->pb_head) {\n\t\t*rap = ra->p_next;\n\t\tra->p_next   = rab->pb_head;\n\t\trab->pb_head = ra;\n\t}\n\tra->p_count++;\n\tnfsdstats.ra_depth[depth*10/nfsdstats.ra_size]++;\n\tspin_unlock(&rab->pb_lock);\n\n\tif (ra->p_set)\n\t\tfile->f_ra = ra->p_ra;\n\treturn ra;\n}\n\nvoid nfsd_put_raparams(struct file *file, struct raparms *ra)\n{\n\tstruct raparm_hbucket *rab = &raparm_hash[ra->p_hindex];\n\n\tspin_lock(&rab->pb_lock);\n\tra->p_ra = file->f_ra;\n\tra->p_set = 1;\n\tra->p_count--;\n\tspin_unlock(&rab->pb_lock);\n}\n\n/*\n * Grab and keep cached pages associated with a file in the svc_rqst\n * so that they can be passed to the network sendmsg/sendpage routines\n * directly. They will be released after the sending has completed.\n */\nstatic int\nnfsd_splice_actor(struct pipe_inode_info *pipe, struct pipe_buffer *buf,\n\t\t  struct splice_desc *sd)\n{\n\tstruct svc_rqst *rqstp = sd->u.data;\n\tstruct page **pp = rqstp->rq_next_page;\n\tstruct page *page = buf->page;\n\tsize_t size;\n\n\tsize = sd->len;\n\n\tif (rqstp->rq_res.page_len == 0) {\n\t\tget_page(page);\n\t\tput_page(*rqstp->rq_next_page);\n\t\t*(rqstp->rq_next_page++) = page;\n\t\trqstp->rq_res.page_base = buf->offset;\n\t\trqstp->rq_res.page_len = size;\n\t} else if (page != pp[-1]) {\n\t\tget_page(page);\n\t\tif (*rqstp->rq_next_page)\n\t\t\tput_page(*rqstp->rq_next_page);\n\t\t*(rqstp->rq_next_page++) = page;\n\t\trqstp->rq_res.page_len += size;\n\t} else\n\t\trqstp->rq_res.page_len += size;\n\n\treturn size;\n}\n\nstatic int nfsd_direct_splice_actor(struct pipe_inode_info *pipe,\n\t\t\t\t    struct splice_desc *sd)\n{\n\treturn __splice_from_pipe(pipe, sd, nfsd_splice_actor);\n}\n\nstatic __be32\nnfsd_finish_read(struct file *file, unsigned long *count, int host_err)\n{\n\tif (host_err >= 0) {\n\t\tnfsdstats.io_read += host_err;\n\t\t*count = host_err;\n\t\tfsnotify_access(file);\n\t\treturn 0;\n\t} else \n\t\treturn nfserrno(host_err);\n}\n\n__be32 nfsd_splice_read(struct svc_rqst *rqstp,\n\t\t     struct file *file, loff_t offset, unsigned long *count)\n{\n\tstruct splice_desc sd = {\n\t\t.len\t\t= 0,\n\t\t.total_len\t= *count,\n\t\t.pos\t\t= offset,\n\t\t.u.data\t\t= rqstp,\n\t};\n\tint host_err;\n\n\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\thost_err = splice_direct_to_actor(file, &sd, nfsd_direct_splice_actor);\n\treturn nfsd_finish_read(file, count, host_err);\n}\n\n__be32 nfsd_readv(struct file *file, loff_t offset, struct kvec *vec, int vlen,\n\t\tunsigned long *count)\n{\n\tmm_segment_t oldfs;\n\tint host_err;\n\n\toldfs = get_fs();\n\tset_fs(KERNEL_DS);\n\thost_err = vfs_readv(file, (struct iovec __user *)vec, vlen, &offset, 0);\n\tset_fs(oldfs);\n\treturn nfsd_finish_read(file, count, host_err);\n}\n\nstatic __be32\nnfsd_vfs_read(struct svc_rqst *rqstp, struct file *file,\n\t      loff_t offset, struct kvec *vec, int vlen, unsigned long *count)\n{\n\tif (file->f_op->splice_read && test_bit(RQ_SPLICE_OK, &rqstp->rq_flags))\n\t\treturn nfsd_splice_read(rqstp, file, offset, count);\n\telse\n\t\treturn nfsd_readv(file, offset, vec, vlen, count);\n}\n\n/*\n * Gathered writes: If another process is currently writing to the file,\n * there's a high chance this is another nfsd (triggered by a bulk write\n * from a client's biod). Rather than syncing the file with each write\n * request, we sleep for 10 msec.\n *\n * I don't know if this roughly approximates C. Juszak's idea of\n * gathered writes, but it's a nice and simple solution (IMHO), and it\n * seems to work:-)\n *\n * Note: we do this only in the NFSv2 case, since v3 and higher have a\n * better tool (separate unstable writes and commits) for solving this\n * problem.\n */\nstatic int wait_for_concurrent_writes(struct file *file)\n{\n\tstruct inode *inode = file_inode(file);\n\tstatic ino_t last_ino;\n\tstatic dev_t last_dev;\n\tint err = 0;\n\n\tif (atomic_read(&inode->i_writecount) > 1\n\t    || (last_ino == inode->i_ino && last_dev == inode->i_sb->s_dev)) {\n\t\tdprintk(\"nfsd: write defer %d\\n\", task_pid_nr(current));\n\t\tmsleep(10);\n\t\tdprintk(\"nfsd: write resume %d\\n\", task_pid_nr(current));\n\t}\n\n\tif (inode->i_state & I_DIRTY) {\n\t\tdprintk(\"nfsd: write sync %d\\n\", task_pid_nr(current));\n\t\terr = vfs_fsync(file, 0);\n\t}\n\tlast_ino = inode->i_ino;\n\tlast_dev = inode->i_sb->s_dev;\n\treturn err;\n}\n\n__be32\nnfsd_vfs_write(struct svc_rqst *rqstp, struct svc_fh *fhp, struct file *file,\n\t\t\t\tloff_t offset, struct kvec *vec, int vlen,\n\t\t\t\tunsigned long *cnt, int stable)\n{\n\tstruct svc_export\t*exp;\n\tmm_segment_t\t\toldfs;\n\t__be32\t\t\terr = 0;\n\tint\t\t\thost_err;\n\tint\t\t\tuse_wgather;\n\tloff_t\t\t\tpos = offset;\n\tunsigned int\t\tpflags = current->flags;\n\tint\t\t\tflags = 0;\n\n\tif (test_bit(RQ_LOCAL, &rqstp->rq_flags))\n\t\t/*\n\t\t * We want less throttling in balance_dirty_pages()\n\t\t * and shrink_inactive_list() so that nfs to\n\t\t * localhost doesn't cause nfsd to lock up due to all\n\t\t * the client's dirty pages or its congested queue.\n\t\t */\n\t\tcurrent->flags |= PF_LESS_THROTTLE;\n\n\texp = fhp->fh_export;\n\tuse_wgather = (rqstp->rq_vers == 2) && EX_WGATHER(exp);\n\n\tif (!EX_ISSYNC(exp))\n\t\tstable = NFS_UNSTABLE;\n\n\tif (stable && !use_wgather)\n\t\tflags |= RWF_SYNC;\n\n\t/* Write the data. */\n\toldfs = get_fs(); set_fs(KERNEL_DS);\n\thost_err = vfs_writev(file, (struct iovec __user *)vec, vlen, &pos, flags);\n\tset_fs(oldfs);\n\tif (host_err < 0)\n\t\tgoto out_nfserr;\n\t*cnt = host_err;\n\tnfsdstats.io_write += host_err;\n\tfsnotify_modify(file);\n\n\tif (stable && use_wgather)\n\t\thost_err = wait_for_concurrent_writes(file);\n\nout_nfserr:\n\tdprintk(\"nfsd: write complete host_err=%d\\n\", host_err);\n\tif (host_err >= 0)\n\t\terr = 0;\n\telse\n\t\terr = nfserrno(host_err);\n\tif (test_bit(RQ_LOCAL, &rqstp->rq_flags))\n\t\tcurrent_restore_flags(pflags, PF_LESS_THROTTLE);\n\treturn err;\n}\n\n/*\n * Read data from a file. count must contain the requested read count\n * on entry. On return, *count contains the number of bytes actually read.\n * N.B. After this call fhp needs an fh_put\n */\n__be32 nfsd_read(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\tloff_t offset, struct kvec *vec, int vlen, unsigned long *count)\n{\n\tstruct file *file;\n\tstruct raparms\t*ra;\n\t__be32 err;\n\n\ttrace_read_start(rqstp, fhp, offset, vlen);\n\terr = nfsd_open(rqstp, fhp, S_IFREG, NFSD_MAY_READ, &file);\n\tif (err)\n\t\treturn err;\n\n\tra = nfsd_init_raparms(file);\n\n\ttrace_read_opened(rqstp, fhp, offset, vlen);\n\terr = nfsd_vfs_read(rqstp, file, offset, vec, vlen, count);\n\ttrace_read_io_done(rqstp, fhp, offset, vlen);\n\n\tif (ra)\n\t\tnfsd_put_raparams(file, ra);\n\tfput(file);\n\n\ttrace_read_done(rqstp, fhp, offset, vlen);\n\n\treturn err;\n}\n\n/*\n * Write data to a file.\n * The stable flag requests synchronous writes.\n * N.B. After this call fhp needs an fh_put\n */\n__be32\nnfsd_write(struct svc_rqst *rqstp, struct svc_fh *fhp, loff_t offset,\n\t   struct kvec *vec, int vlen, unsigned long *cnt, int stable)\n{\n\tstruct file *file = NULL;\n\t__be32 err = 0;\n\n\ttrace_write_start(rqstp, fhp, offset, vlen);\n\n\terr = nfsd_open(rqstp, fhp, S_IFREG, NFSD_MAY_WRITE, &file);\n\tif (err)\n\t\tgoto out;\n\n\ttrace_write_opened(rqstp, fhp, offset, vlen);\n\terr = nfsd_vfs_write(rqstp, fhp, file, offset, vec, vlen, cnt, stable);\n\ttrace_write_io_done(rqstp, fhp, offset, vlen);\n\tfput(file);\nout:\n\ttrace_write_done(rqstp, fhp, offset, vlen);\n\treturn err;\n}\n\n#ifdef CONFIG_NFSD_V3\n/*\n * Commit all pending writes to stable storage.\n *\n * Note: we only guarantee that data that lies within the range specified\n * by the 'offset' and 'count' parameters will be synced.\n *\n * Unfortunately we cannot lock the file to make sure we return full WCC\n * data to the client, as locking happens lower down in the filesystem.\n */\n__be32\nnfsd_commit(struct svc_rqst *rqstp, struct svc_fh *fhp,\n               loff_t offset, unsigned long count)\n{\n\tstruct file\t*file;\n\tloff_t\t\tend = LLONG_MAX;\n\t__be32\t\terr = nfserr_inval;\n\n\tif (offset < 0)\n\t\tgoto out;\n\tif (count != 0) {\n\t\tend = offset + (loff_t)count - 1;\n\t\tif (end < offset)\n\t\t\tgoto out;\n\t}\n\n\terr = nfsd_open(rqstp, fhp, S_IFREG,\n\t\t\tNFSD_MAY_WRITE|NFSD_MAY_NOT_BREAK_LEASE, &file);\n\tif (err)\n\t\tgoto out;\n\tif (EX_ISSYNC(fhp->fh_export)) {\n\t\tint err2 = vfs_fsync_range(file, offset, end, 0);\n\n\t\tif (err2 != -EINVAL)\n\t\t\terr = nfserrno(err2);\n\t\telse\n\t\t\terr = nfserr_notsupp;\n\t}\n\n\tfput(file);\nout:\n\treturn err;\n}\n#endif /* CONFIG_NFSD_V3 */\n\nstatic __be32\nnfsd_create_setattr(struct svc_rqst *rqstp, struct svc_fh *resfhp,\n\t\t\tstruct iattr *iap)\n{\n\t/*\n\t * Mode has already been set earlier in create:\n\t */\n\tiap->ia_valid &= ~ATTR_MODE;\n\t/*\n\t * Setting uid/gid works only for root.  Irix appears to\n\t * send along the gid on create when it tries to implement\n\t * setgid directories via NFS:\n\t */\n\tif (!uid_eq(current_fsuid(), GLOBAL_ROOT_UID))\n\t\tiap->ia_valid &= ~(ATTR_UID|ATTR_GID);\n\tif (iap->ia_valid)\n\t\treturn nfsd_setattr(rqstp, resfhp, iap, 0, (time_t)0);\n\t/* Callers expect file metadata to be committed here */\n\treturn nfserrno(commit_metadata(resfhp));\n}\n\n/* HPUX client sometimes creates a file in mode 000, and sets size to 0.\n * setting size to 0 may fail for some specific file systems by the permission\n * checking which requires WRITE permission but the mode is 000.\n * we ignore the resizing(to 0) on the just new created file, since the size is\n * 0 after file created.\n *\n * call this only after vfs_create() is called.\n * */\nstatic void\nnfsd_check_ignore_resizing(struct iattr *iap)\n{\n\tif ((iap->ia_valid & ATTR_SIZE) && (iap->ia_size == 0))\n\t\tiap->ia_valid &= ~ATTR_SIZE;\n}\n\n/* The parent directory should already be locked: */\n__be32\nnfsd_create_locked(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tchar *fname, int flen, struct iattr *iap,\n\t\tint type, dev_t rdev, struct svc_fh *resfhp)\n{\n\tstruct dentry\t*dentry, *dchild;\n\tstruct inode\t*dirp;\n\t__be32\t\terr;\n\t__be32\t\terr2;\n\tint\t\thost_err;\n\n\tdentry = fhp->fh_dentry;\n\tdirp = d_inode(dentry);\n\n\tdchild = dget(resfhp->fh_dentry);\n\tif (!fhp->fh_locked) {\n\t\tWARN_ONCE(1, \"nfsd_create: parent %pd2 not locked!\\n\",\n\t\t\t\tdentry);\n\t\terr = nfserr_io;\n\t\tgoto out;\n\t}\n\n\terr = nfsd_permission(rqstp, fhp->fh_export, dentry, NFSD_MAY_CREATE);\n\tif (err)\n\t\tgoto out;\n\n\tif (!(iap->ia_valid & ATTR_MODE))\n\t\tiap->ia_mode = 0;\n\tiap->ia_mode = (iap->ia_mode & S_IALLUGO) | type;\n\n\terr = 0;\n\thost_err = 0;\n\tswitch (type) {\n\tcase S_IFREG:\n\t\thost_err = vfs_create(dirp, dchild, iap->ia_mode, true);\n\t\tif (!host_err)\n\t\t\tnfsd_check_ignore_resizing(iap);\n\t\tbreak;\n\tcase S_IFDIR:\n\t\thost_err = vfs_mkdir(dirp, dchild, iap->ia_mode);\n\t\tbreak;\n\tcase S_IFCHR:\n\tcase S_IFBLK:\n\tcase S_IFIFO:\n\tcase S_IFSOCK:\n\t\thost_err = vfs_mknod(dirp, dchild, iap->ia_mode, rdev);\n\t\tbreak;\n\tdefault:\n\t\tprintk(KERN_WARNING \"nfsd: bad file type %o in nfsd_create\\n\",\n\t\t       type);\n\t\thost_err = -EINVAL;\n\t}\n\tif (host_err < 0)\n\t\tgoto out_nfserr;\n\n\terr = nfsd_create_setattr(rqstp, resfhp, iap);\n\n\t/*\n\t * nfsd_create_setattr already committed the child.  Transactional\n\t * filesystems had a chance to commit changes for both parent and\n\t * child simultaneously making the following commit_metadata a\n\t * noop.\n\t */\n\terr2 = nfserrno(commit_metadata(fhp));\n\tif (err2)\n\t\terr = err2;\n\t/*\n\t * Update the file handle to get the new inode info.\n\t */\n\tif (!err)\n\t\terr = fh_update(resfhp);\nout:\n\tdput(dchild);\n\treturn err;\n\nout_nfserr:\n\terr = nfserrno(host_err);\n\tgoto out;\n}\n\n/*\n * Create a filesystem object (regular, directory, special).\n * Note that the parent directory is left locked.\n *\n * N.B. Every call to nfsd_create needs an fh_put for _both_ fhp and resfhp\n */\n__be32\nnfsd_create(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tchar *fname, int flen, struct iattr *iap,\n\t\tint type, dev_t rdev, struct svc_fh *resfhp)\n{\n\tstruct dentry\t*dentry, *dchild = NULL;\n\tstruct inode\t*dirp;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\n\tif (isdotent(fname, flen))\n\t\treturn nfserr_exist;\n\n\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_NOP);\n\tif (err)\n\t\treturn err;\n\n\tdentry = fhp->fh_dentry;\n\tdirp = d_inode(dentry);\n\n\thost_err = fh_want_write(fhp);\n\tif (host_err)\n\t\treturn nfserrno(host_err);\n\n\tfh_lock_nested(fhp, I_MUTEX_PARENT);\n\tdchild = lookup_one_len(fname, dentry, flen);\n\thost_err = PTR_ERR(dchild);\n\tif (IS_ERR(dchild))\n\t\treturn nfserrno(host_err);\n\terr = fh_compose(resfhp, fhp->fh_export, dchild, fhp);\n\t/*\n\t * We unconditionally drop our ref to dchild as fh_compose will have\n\t * already grabbed its own ref for it.\n\t */\n\tdput(dchild);\n\tif (err)\n\t\treturn err;\n\treturn nfsd_create_locked(rqstp, fhp, fname, flen, iap, type,\n\t\t\t\t\trdev, resfhp);\n}\n\n#ifdef CONFIG_NFSD_V3\n\n/*\n * NFSv3 and NFSv4 version of nfsd_create\n */\n__be32\ndo_nfsd_create(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\tchar *fname, int flen, struct iattr *iap,\n\t\tstruct svc_fh *resfhp, int createmode, u32 *verifier,\n\t        bool *truncp, bool *created)\n{\n\tstruct dentry\t*dentry, *dchild = NULL;\n\tstruct inode\t*dirp;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\t__u32\t\tv_mtime=0, v_atime=0;\n\n\terr = nfserr_perm;\n\tif (!flen)\n\t\tgoto out;\n\terr = nfserr_exist;\n\tif (isdotent(fname, flen))\n\t\tgoto out;\n\tif (!(iap->ia_valid & ATTR_MODE))\n\t\tiap->ia_mode = 0;\n\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_EXEC);\n\tif (err)\n\t\tgoto out;\n\n\tdentry = fhp->fh_dentry;\n\tdirp = d_inode(dentry);\n\n\thost_err = fh_want_write(fhp);\n\tif (host_err)\n\t\tgoto out_nfserr;\n\n\tfh_lock_nested(fhp, I_MUTEX_PARENT);\n\n\t/*\n\t * Compose the response file handle.\n\t */\n\tdchild = lookup_one_len(fname, dentry, flen);\n\thost_err = PTR_ERR(dchild);\n\tif (IS_ERR(dchild))\n\t\tgoto out_nfserr;\n\n\t/* If file doesn't exist, check for permissions to create one */\n\tif (d_really_is_negative(dchild)) {\n\t\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_CREATE);\n\t\tif (err)\n\t\t\tgoto out;\n\t}\n\n\terr = fh_compose(resfhp, fhp->fh_export, dchild, fhp);\n\tif (err)\n\t\tgoto out;\n\n\tif (nfsd_create_is_exclusive(createmode)) {\n\t\t/* solaris7 gets confused (bugid 4218508) if these have\n\t\t * the high bit set, so just clear the high bits. If this is\n\t\t * ever changed to use different attrs for storing the\n\t\t * verifier, then do_open_lookup() will also need to be fixed\n\t\t * accordingly.\n\t\t */\n\t\tv_mtime = verifier[0]&0x7fffffff;\n\t\tv_atime = verifier[1]&0x7fffffff;\n\t}\n\t\n\tif (d_really_is_positive(dchild)) {\n\t\terr = 0;\n\n\t\tswitch (createmode) {\n\t\tcase NFS3_CREATE_UNCHECKED:\n\t\t\tif (! d_is_reg(dchild))\n\t\t\t\tgoto out;\n\t\t\telse if (truncp) {\n\t\t\t\t/* in nfsv4, we need to treat this case a little\n\t\t\t\t * differently.  we don't want to truncate the\n\t\t\t\t * file now; this would be wrong if the OPEN\n\t\t\t\t * fails for some other reason.  furthermore,\n\t\t\t\t * if the size is nonzero, we should ignore it\n\t\t\t\t * according to spec!\n\t\t\t\t */\n\t\t\t\t*truncp = (iap->ia_valid & ATTR_SIZE) && !iap->ia_size;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tiap->ia_valid &= ATTR_SIZE;\n\t\t\t\tgoto set_attr;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFS3_CREATE_EXCLUSIVE:\n\t\t\tif (   d_inode(dchild)->i_mtime.tv_sec == v_mtime\n\t\t\t    && d_inode(dchild)->i_atime.tv_sec == v_atime\n\t\t\t    && d_inode(dchild)->i_size  == 0 ) {\n\t\t\t\tif (created)\n\t\t\t\t\t*created = 1;\n\t\t\t\tbreak;\n\t\t\t}\n\t\tcase NFS4_CREATE_EXCLUSIVE4_1:\n\t\t\tif (   d_inode(dchild)->i_mtime.tv_sec == v_mtime\n\t\t\t    && d_inode(dchild)->i_atime.tv_sec == v_atime\n\t\t\t    && d_inode(dchild)->i_size  == 0 ) {\n\t\t\t\tif (created)\n\t\t\t\t\t*created = 1;\n\t\t\t\tgoto set_attr;\n\t\t\t}\n\t\t\t /* fallthru */\n\t\tcase NFS3_CREATE_GUARDED:\n\t\t\terr = nfserr_exist;\n\t\t}\n\t\tfh_drop_write(fhp);\n\t\tgoto out;\n\t}\n\n\thost_err = vfs_create(dirp, dchild, iap->ia_mode, true);\n\tif (host_err < 0) {\n\t\tfh_drop_write(fhp);\n\t\tgoto out_nfserr;\n\t}\n\tif (created)\n\t\t*created = 1;\n\n\tnfsd_check_ignore_resizing(iap);\n\n\tif (nfsd_create_is_exclusive(createmode)) {\n\t\t/* Cram the verifier into atime/mtime */\n\t\tiap->ia_valid = ATTR_MTIME|ATTR_ATIME\n\t\t\t| ATTR_MTIME_SET|ATTR_ATIME_SET;\n\t\t/* XXX someone who knows this better please fix it for nsec */ \n\t\tiap->ia_mtime.tv_sec = v_mtime;\n\t\tiap->ia_atime.tv_sec = v_atime;\n\t\tiap->ia_mtime.tv_nsec = 0;\n\t\tiap->ia_atime.tv_nsec = 0;\n\t}\n\n set_attr:\n\terr = nfsd_create_setattr(rqstp, resfhp, iap);\n\n\t/*\n\t * nfsd_create_setattr already committed the child\n\t * (and possibly also the parent).\n\t */\n\tif (!err)\n\t\terr = nfserrno(commit_metadata(fhp));\n\n\t/*\n\t * Update the filehandle to get the new inode info.\n\t */\n\tif (!err)\n\t\terr = fh_update(resfhp);\n\n out:\n\tfh_unlock(fhp);\n\tif (dchild && !IS_ERR(dchild))\n\t\tdput(dchild);\n\tfh_drop_write(fhp);\n \treturn err;\n \n out_nfserr:\n\terr = nfserrno(host_err);\n\tgoto out;\n}\n#endif /* CONFIG_NFSD_V3 */\n\n/*\n * Read a symlink. On entry, *lenp must contain the maximum path length that\n * fits into the buffer. On return, it contains the true length.\n * N.B. After this call fhp needs an fh_put\n */\n__be32\nnfsd_readlink(struct svc_rqst *rqstp, struct svc_fh *fhp, char *buf, int *lenp)\n{\n\tmm_segment_t\toldfs;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\tstruct path path;\n\n\terr = fh_verify(rqstp, fhp, S_IFLNK, NFSD_MAY_NOP);\n\tif (err)\n\t\tgoto out;\n\n\tpath.mnt = fhp->fh_export->ex_path.mnt;\n\tpath.dentry = fhp->fh_dentry;\n\n\terr = nfserr_inval;\n\tif (!d_is_symlink(path.dentry))\n\t\tgoto out;\n\n\ttouch_atime(&path);\n\t/* N.B. Why does this call need a get_fs()??\n\t * Remove the set_fs and watch the fireworks:-) --okir\n\t */\n\n\toldfs = get_fs(); set_fs(KERNEL_DS);\n\thost_err = vfs_readlink(path.dentry, (char __user *)buf, *lenp);\n\tset_fs(oldfs);\n\n\tif (host_err < 0)\n\t\tgoto out_nfserr;\n\t*lenp = host_err;\n\terr = 0;\nout:\n\treturn err;\n\nout_nfserr:\n\terr = nfserrno(host_err);\n\tgoto out;\n}\n\n/*\n * Create a symlink and look up its inode\n * N.B. After this call _both_ fhp and resfhp need an fh_put\n */\n__be32\nnfsd_symlink(struct svc_rqst *rqstp, struct svc_fh *fhp,\n\t\t\t\tchar *fname, int flen,\n\t\t\t\tchar *path,\n\t\t\t\tstruct svc_fh *resfhp)\n{\n\tstruct dentry\t*dentry, *dnew;\n\t__be32\t\terr, cerr;\n\tint\t\thost_err;\n\n\terr = nfserr_noent;\n\tif (!flen || path[0] == '\\0')\n\t\tgoto out;\n\terr = nfserr_exist;\n\tif (isdotent(fname, flen))\n\t\tgoto out;\n\n\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_CREATE);\n\tif (err)\n\t\tgoto out;\n\n\thost_err = fh_want_write(fhp);\n\tif (host_err)\n\t\tgoto out_nfserr;\n\n\tfh_lock(fhp);\n\tdentry = fhp->fh_dentry;\n\tdnew = lookup_one_len(fname, dentry, flen);\n\thost_err = PTR_ERR(dnew);\n\tif (IS_ERR(dnew))\n\t\tgoto out_nfserr;\n\n\thost_err = vfs_symlink(d_inode(dentry), dnew, path);\n\terr = nfserrno(host_err);\n\tif (!err)\n\t\terr = nfserrno(commit_metadata(fhp));\n\tfh_unlock(fhp);\n\n\tfh_drop_write(fhp);\n\n\tcerr = fh_compose(resfhp, fhp->fh_export, dnew, fhp);\n\tdput(dnew);\n\tif (err==0) err = cerr;\nout:\n\treturn err;\n\nout_nfserr:\n\terr = nfserrno(host_err);\n\tgoto out;\n}\n\n/*\n * Create a hardlink\n * N.B. After this call _both_ ffhp and tfhp need an fh_put\n */\n__be32\nnfsd_link(struct svc_rqst *rqstp, struct svc_fh *ffhp,\n\t\t\t\tchar *name, int len, struct svc_fh *tfhp)\n{\n\tstruct dentry\t*ddir, *dnew, *dold;\n\tstruct inode\t*dirp;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\n\terr = fh_verify(rqstp, ffhp, S_IFDIR, NFSD_MAY_CREATE);\n\tif (err)\n\t\tgoto out;\n\terr = fh_verify(rqstp, tfhp, 0, NFSD_MAY_NOP);\n\tif (err)\n\t\tgoto out;\n\terr = nfserr_isdir;\n\tif (d_is_dir(tfhp->fh_dentry))\n\t\tgoto out;\n\terr = nfserr_perm;\n\tif (!len)\n\t\tgoto out;\n\terr = nfserr_exist;\n\tif (isdotent(name, len))\n\t\tgoto out;\n\n\thost_err = fh_want_write(tfhp);\n\tif (host_err) {\n\t\terr = nfserrno(host_err);\n\t\tgoto out;\n\t}\n\n\tfh_lock_nested(ffhp, I_MUTEX_PARENT);\n\tddir = ffhp->fh_dentry;\n\tdirp = d_inode(ddir);\n\n\tdnew = lookup_one_len(name, ddir, len);\n\thost_err = PTR_ERR(dnew);\n\tif (IS_ERR(dnew))\n\t\tgoto out_nfserr;\n\n\tdold = tfhp->fh_dentry;\n\n\terr = nfserr_noent;\n\tif (d_really_is_negative(dold))\n\t\tgoto out_dput;\n\thost_err = vfs_link(dold, dirp, dnew, NULL);\n\tif (!host_err) {\n\t\terr = nfserrno(commit_metadata(ffhp));\n\t\tif (!err)\n\t\t\terr = nfserrno(commit_metadata(tfhp));\n\t} else {\n\t\tif (host_err == -EXDEV && rqstp->rq_vers == 2)\n\t\t\terr = nfserr_acces;\n\t\telse\n\t\t\terr = nfserrno(host_err);\n\t}\nout_dput:\n\tdput(dnew);\nout_unlock:\n\tfh_unlock(ffhp);\n\tfh_drop_write(tfhp);\nout:\n\treturn err;\n\nout_nfserr:\n\terr = nfserrno(host_err);\n\tgoto out_unlock;\n}\n\n/*\n * Rename a file\n * N.B. After this call _both_ ffhp and tfhp need an fh_put\n */\n__be32\nnfsd_rename(struct svc_rqst *rqstp, struct svc_fh *ffhp, char *fname, int flen,\n\t\t\t    struct svc_fh *tfhp, char *tname, int tlen)\n{\n\tstruct dentry\t*fdentry, *tdentry, *odentry, *ndentry, *trap;\n\tstruct inode\t*fdir, *tdir;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\n\terr = fh_verify(rqstp, ffhp, S_IFDIR, NFSD_MAY_REMOVE);\n\tif (err)\n\t\tgoto out;\n\terr = fh_verify(rqstp, tfhp, S_IFDIR, NFSD_MAY_CREATE);\n\tif (err)\n\t\tgoto out;\n\n\tfdentry = ffhp->fh_dentry;\n\tfdir = d_inode(fdentry);\n\n\ttdentry = tfhp->fh_dentry;\n\ttdir = d_inode(tdentry);\n\n\terr = nfserr_perm;\n\tif (!flen || isdotent(fname, flen) || !tlen || isdotent(tname, tlen))\n\t\tgoto out;\n\n\thost_err = fh_want_write(ffhp);\n\tif (host_err) {\n\t\terr = nfserrno(host_err);\n\t\tgoto out;\n\t}\n\n\t/* cannot use fh_lock as we need deadlock protective ordering\n\t * so do it by hand */\n\ttrap = lock_rename(tdentry, fdentry);\n\tffhp->fh_locked = tfhp->fh_locked = true;\n\tfill_pre_wcc(ffhp);\n\tfill_pre_wcc(tfhp);\n\n\todentry = lookup_one_len(fname, fdentry, flen);\n\thost_err = PTR_ERR(odentry);\n\tif (IS_ERR(odentry))\n\t\tgoto out_nfserr;\n\n\thost_err = -ENOENT;\n\tif (d_really_is_negative(odentry))\n\t\tgoto out_dput_old;\n\thost_err = -EINVAL;\n\tif (odentry == trap)\n\t\tgoto out_dput_old;\n\n\tndentry = lookup_one_len(tname, tdentry, tlen);\n\thost_err = PTR_ERR(ndentry);\n\tif (IS_ERR(ndentry))\n\t\tgoto out_dput_old;\n\thost_err = -ENOTEMPTY;\n\tif (ndentry == trap)\n\t\tgoto out_dput_new;\n\n\thost_err = -EXDEV;\n\tif (ffhp->fh_export->ex_path.mnt != tfhp->fh_export->ex_path.mnt)\n\t\tgoto out_dput_new;\n\tif (ffhp->fh_export->ex_path.dentry != tfhp->fh_export->ex_path.dentry)\n\t\tgoto out_dput_new;\n\n\thost_err = vfs_rename(fdir, odentry, tdir, ndentry, NULL, 0);\n\tif (!host_err) {\n\t\thost_err = commit_metadata(tfhp);\n\t\tif (!host_err)\n\t\t\thost_err = commit_metadata(ffhp);\n\t}\n out_dput_new:\n\tdput(ndentry);\n out_dput_old:\n\tdput(odentry);\n out_nfserr:\n\terr = nfserrno(host_err);\n\t/*\n\t * We cannot rely on fh_unlock on the two filehandles,\n\t * as that would do the wrong thing if the two directories\n\t * were the same, so again we do it by hand.\n\t */\n\tfill_post_wcc(ffhp);\n\tfill_post_wcc(tfhp);\n\tunlock_rename(tdentry, fdentry);\n\tffhp->fh_locked = tfhp->fh_locked = false;\n\tfh_drop_write(ffhp);\n\nout:\n\treturn err;\n}\n\n/*\n * Unlink a file or directory\n * N.B. After this call fhp needs an fh_put\n */\n__be32\nnfsd_unlink(struct svc_rqst *rqstp, struct svc_fh *fhp, int type,\n\t\t\t\tchar *fname, int flen)\n{\n\tstruct dentry\t*dentry, *rdentry;\n\tstruct inode\t*dirp;\n\t__be32\t\terr;\n\tint\t\thost_err;\n\n\terr = nfserr_acces;\n\tif (!flen || isdotent(fname, flen))\n\t\tgoto out;\n\terr = fh_verify(rqstp, fhp, S_IFDIR, NFSD_MAY_REMOVE);\n\tif (err)\n\t\tgoto out;\n\n\thost_err = fh_want_write(fhp);\n\tif (host_err)\n\t\tgoto out_nfserr;\n\n\tfh_lock_nested(fhp, I_MUTEX_PARENT);\n\tdentry = fhp->fh_dentry;\n\tdirp = d_inode(dentry);\n\n\trdentry = lookup_one_len(fname, dentry, flen);\n\thost_err = PTR_ERR(rdentry);\n\tif (IS_ERR(rdentry))\n\t\tgoto out_nfserr;\n\n\tif (d_really_is_negative(rdentry)) {\n\t\tdput(rdentry);\n\t\terr = nfserr_noent;\n\t\tgoto out;\n\t}\n\n\tif (!type)\n\t\ttype = d_inode(rdentry)->i_mode & S_IFMT;\n\n\tif (type != S_IFDIR)\n\t\thost_err = vfs_unlink(dirp, rdentry, NULL);\n\telse\n\t\thost_err = vfs_rmdir(dirp, rdentry);\n\tif (!host_err)\n\t\thost_err = commit_metadata(fhp);\n\tdput(rdentry);\n\nout_nfserr:\n\terr = nfserrno(host_err);\nout:\n\treturn err;\n}\n\n/*\n * We do this buffering because we must not call back into the file\n * system's ->lookup() method from the filldir callback. That may well\n * deadlock a number of file systems.\n *\n * This is based heavily on the implementation of same in XFS.\n */\nstruct buffered_dirent {\n\tu64\t\tino;\n\tloff_t\t\toffset;\n\tint\t\tnamlen;\n\tunsigned int\td_type;\n\tchar\t\tname[];\n};\n\nstruct readdir_data {\n\tstruct dir_context ctx;\n\tchar\t\t*dirent;\n\tsize_t\t\tused;\n\tint\t\tfull;\n};\n\nstatic int nfsd_buffered_filldir(struct dir_context *ctx, const char *name,\n\t\t\t\t int namlen, loff_t offset, u64 ino,\n\t\t\t\t unsigned int d_type)\n{\n\tstruct readdir_data *buf =\n\t\tcontainer_of(ctx, struct readdir_data, ctx);\n\tstruct buffered_dirent *de = (void *)(buf->dirent + buf->used);\n\tunsigned int reclen;\n\n\treclen = ALIGN(sizeof(struct buffered_dirent) + namlen, sizeof(u64));\n\tif (buf->used + reclen > PAGE_SIZE) {\n\t\tbuf->full = 1;\n\t\treturn -EINVAL;\n\t}\n\n\tde->namlen = namlen;\n\tde->offset = offset;\n\tde->ino = ino;\n\tde->d_type = d_type;\n\tmemcpy(de->name, name, namlen);\n\tbuf->used += reclen;\n\n\treturn 0;\n}\n\nstatic __be32 nfsd_buffered_readdir(struct file *file, nfsd_filldir_t func,\n\t\t\t\t    struct readdir_cd *cdp, loff_t *offsetp)\n{\n\tstruct buffered_dirent *de;\n\tint host_err;\n\tint size;\n\tloff_t offset;\n\tstruct readdir_data buf = {\n\t\t.ctx.actor = nfsd_buffered_filldir,\n\t\t.dirent = (void *)__get_free_page(GFP_KERNEL)\n\t};\n\n\tif (!buf.dirent)\n\t\treturn nfserrno(-ENOMEM);\n\n\toffset = *offsetp;\n\n\twhile (1) {\n\t\tunsigned int reclen;\n\n\t\tcdp->err = nfserr_eof; /* will be cleared on successful read */\n\t\tbuf.used = 0;\n\t\tbuf.full = 0;\n\n\t\thost_err = iterate_dir(file, &buf.ctx);\n\t\tif (buf.full)\n\t\t\thost_err = 0;\n\n\t\tif (host_err < 0)\n\t\t\tbreak;\n\n\t\tsize = buf.used;\n\n\t\tif (!size)\n\t\t\tbreak;\n\n\t\tde = (struct buffered_dirent *)buf.dirent;\n\t\twhile (size > 0) {\n\t\t\toffset = de->offset;\n\n\t\t\tif (func(cdp, de->name, de->namlen, de->offset,\n\t\t\t\t de->ino, de->d_type))\n\t\t\t\tbreak;\n\n\t\t\tif (cdp->err != nfs_ok)\n\t\t\t\tbreak;\n\n\t\t\treclen = ALIGN(sizeof(*de) + de->namlen,\n\t\t\t\t       sizeof(u64));\n\t\t\tsize -= reclen;\n\t\t\tde = (struct buffered_dirent *)((char *)de + reclen);\n\t\t}\n\t\tif (size > 0) /* We bailed out early */\n\t\t\tbreak;\n\n\t\toffset = vfs_llseek(file, 0, SEEK_CUR);\n\t}\n\n\tfree_page((unsigned long)(buf.dirent));\n\n\tif (host_err)\n\t\treturn nfserrno(host_err);\n\n\t*offsetp = offset;\n\treturn cdp->err;\n}\n\n/*\n * Read entries from a directory.\n * The  NFSv3/4 verifier we ignore for now.\n */\n__be32\nnfsd_readdir(struct svc_rqst *rqstp, struct svc_fh *fhp, loff_t *offsetp, \n\t     struct readdir_cd *cdp, nfsd_filldir_t func)\n{\n\t__be32\t\terr;\n\tstruct file\t*file;\n\tloff_t\t\toffset = *offsetp;\n\tint             may_flags = NFSD_MAY_READ;\n\n\t/* NFSv2 only supports 32 bit cookies */\n\tif (rqstp->rq_vers > 2)\n\t\tmay_flags |= NFSD_MAY_64BIT_COOKIE;\n\n\terr = nfsd_open(rqstp, fhp, S_IFDIR, may_flags, &file);\n\tif (err)\n\t\tgoto out;\n\n\toffset = vfs_llseek(file, offset, SEEK_SET);\n\tif (offset < 0) {\n\t\terr = nfserrno((int)offset);\n\t\tgoto out_close;\n\t}\n\n\terr = nfsd_buffered_readdir(file, func, cdp, offsetp);\n\n\tif (err == nfserr_eof || err == nfserr_toosmall)\n\t\terr = nfs_ok; /* can still be found in ->err */\nout_close:\n\tfput(file);\nout:\n\treturn err;\n}\n\n/*\n * Get file system stats\n * N.B. After this call fhp needs an fh_put\n */\n__be32\nnfsd_statfs(struct svc_rqst *rqstp, struct svc_fh *fhp, struct kstatfs *stat, int access)\n{\n\t__be32 err;\n\n\terr = fh_verify(rqstp, fhp, 0, NFSD_MAY_NOP | access);\n\tif (!err) {\n\t\tstruct path path = {\n\t\t\t.mnt\t= fhp->fh_export->ex_path.mnt,\n\t\t\t.dentry\t= fhp->fh_dentry,\n\t\t};\n\t\tif (vfs_statfs(&path, stat))\n\t\t\terr = nfserr_io;\n\t}\n\treturn err;\n}\n\nstatic int exp_rdonly(struct svc_rqst *rqstp, struct svc_export *exp)\n{\n\treturn nfsexp_flags(rqstp, exp) & NFSEXP_READONLY;\n}\n\n/*\n * Check for a user's access permissions to this inode.\n */\n__be32\nnfsd_permission(struct svc_rqst *rqstp, struct svc_export *exp,\n\t\t\t\t\tstruct dentry *dentry, int acc)\n{\n\tstruct inode\t*inode = d_inode(dentry);\n\tint\t\terr;\n\n\tif ((acc & NFSD_MAY_MASK) == NFSD_MAY_NOP)\n\t\treturn 0;\n#if 0\n\tdprintk(\"nfsd: permission 0x%x%s%s%s%s%s%s%s mode 0%o%s%s%s\\n\",\n\t\tacc,\n\t\t(acc & NFSD_MAY_READ)?\t\" read\"  : \"\",\n\t\t(acc & NFSD_MAY_WRITE)?\t\" write\" : \"\",\n\t\t(acc & NFSD_MAY_EXEC)?\t\" exec\"  : \"\",\n\t\t(acc & NFSD_MAY_SATTR)?\t\" sattr\" : \"\",\n\t\t(acc & NFSD_MAY_TRUNC)?\t\" trunc\" : \"\",\n\t\t(acc & NFSD_MAY_LOCK)?\t\" lock\"  : \"\",\n\t\t(acc & NFSD_MAY_OWNER_OVERRIDE)? \" owneroverride\" : \"\",\n\t\tinode->i_mode,\n\t\tIS_IMMUTABLE(inode)?\t\" immut\" : \"\",\n\t\tIS_APPEND(inode)?\t\" append\" : \"\",\n\t\t__mnt_is_readonly(exp->ex_path.mnt)?\t\" ro\" : \"\");\n\tdprintk(\"      owner %d/%d user %d/%d\\n\",\n\t\tinode->i_uid, inode->i_gid, current_fsuid(), current_fsgid());\n#endif\n\n\t/* Normally we reject any write/sattr etc access on a read-only file\n\t * system.  But if it is IRIX doing check on write-access for a \n\t * device special file, we ignore rofs.\n\t */\n\tif (!(acc & NFSD_MAY_LOCAL_ACCESS))\n\t\tif (acc & (NFSD_MAY_WRITE | NFSD_MAY_SATTR | NFSD_MAY_TRUNC)) {\n\t\t\tif (exp_rdonly(rqstp, exp) ||\n\t\t\t    __mnt_is_readonly(exp->ex_path.mnt))\n\t\t\t\treturn nfserr_rofs;\n\t\t\tif (/* (acc & NFSD_MAY_WRITE) && */ IS_IMMUTABLE(inode))\n\t\t\t\treturn nfserr_perm;\n\t\t}\n\tif ((acc & NFSD_MAY_TRUNC) && IS_APPEND(inode))\n\t\treturn nfserr_perm;\n\n\tif (acc & NFSD_MAY_LOCK) {\n\t\t/* If we cannot rely on authentication in NLM requests,\n\t\t * just allow locks, otherwise require read permission, or\n\t\t * ownership\n\t\t */\n\t\tif (exp->ex_flags & NFSEXP_NOAUTHNLM)\n\t\t\treturn 0;\n\t\telse\n\t\t\tacc = NFSD_MAY_READ | NFSD_MAY_OWNER_OVERRIDE;\n\t}\n\t/*\n\t * The file owner always gets access permission for accesses that\n\t * would normally be checked at open time. This is to make\n\t * file access work even when the client has done a fchmod(fd, 0).\n\t *\n\t * However, `cp foo bar' should fail nevertheless when bar is\n\t * readonly. A sensible way to do this might be to reject all\n\t * attempts to truncate a read-only file, because a creat() call\n\t * always implies file truncation.\n\t * ... but this isn't really fair.  A process may reasonably call\n\t * ftruncate on an open file descriptor on a file with perm 000.\n\t * We must trust the client to do permission checking - using \"ACCESS\"\n\t * with NFSv3.\n\t */\n\tif ((acc & NFSD_MAY_OWNER_OVERRIDE) &&\n\t    uid_eq(inode->i_uid, current_fsuid()))\n\t\treturn 0;\n\n\t/* This assumes  NFSD_MAY_{READ,WRITE,EXEC} == MAY_{READ,WRITE,EXEC} */\n\terr = inode_permission(inode, acc & (MAY_READ|MAY_WRITE|MAY_EXEC));\n\n\t/* Allow read access to binaries even when mode 111 */\n\tif (err == -EACCES && S_ISREG(inode->i_mode) &&\n\t     (acc == (NFSD_MAY_READ | NFSD_MAY_OWNER_OVERRIDE) ||\n\t      acc == (NFSD_MAY_READ | NFSD_MAY_READ_IF_EXEC)))\n\t\terr = inode_permission(inode, MAY_EXEC);\n\n\treturn err? nfserrno(err) : 0;\n}\n\nvoid\nnfsd_racache_shutdown(void)\n{\n\tstruct raparms *raparm, *last_raparm;\n\tunsigned int i;\n\n\tdprintk(\"nfsd: freeing readahead buffers.\\n\");\n\n\tfor (i = 0; i < RAPARM_HASH_SIZE; i++) {\n\t\traparm = raparm_hash[i].pb_head;\n\t\twhile(raparm) {\n\t\t\tlast_raparm = raparm;\n\t\t\traparm = raparm->p_next;\n\t\t\tkfree(last_raparm);\n\t\t}\n\t\traparm_hash[i].pb_head = NULL;\n\t}\n}\n/*\n * Initialize readahead param cache\n */\nint\nnfsd_racache_init(int cache_size)\n{\n\tint\ti;\n\tint\tj = 0;\n\tint\tnperbucket;\n\tstruct raparms **raparm = NULL;\n\n\n\tif (raparm_hash[0].pb_head)\n\t\treturn 0;\n\tnperbucket = DIV_ROUND_UP(cache_size, RAPARM_HASH_SIZE);\n\tnperbucket = max(2, nperbucket);\n\tcache_size = nperbucket * RAPARM_HASH_SIZE;\n\n\tdprintk(\"nfsd: allocating %d readahead buffers.\\n\", cache_size);\n\n\tfor (i = 0; i < RAPARM_HASH_SIZE; i++) {\n\t\tspin_lock_init(&raparm_hash[i].pb_lock);\n\n\t\traparm = &raparm_hash[i].pb_head;\n\t\tfor (j = 0; j < nperbucket; j++) {\n\t\t\t*raparm = kzalloc(sizeof(struct raparms), GFP_KERNEL);\n\t\t\tif (!*raparm)\n\t\t\t\tgoto out_nomem;\n\t\t\traparm = &(*raparm)->p_next;\n\t\t}\n\t\t*raparm = NULL;\n\t}\n\n\tnfsdstats.ra_size = cache_size;\n\treturn 0;\n\nout_nomem:\n\tdprintk(\"nfsd: kmalloc failed, freeing readahead buffers\\n\");\n\tnfsd_racache_shutdown();\n\treturn -ENOMEM;\n}\n", "/*\n * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n#ifndef _LINUX_SUNRPC_RPC_RDMA_H\n#define _LINUX_SUNRPC_RPC_RDMA_H\n\n#include <linux/types.h>\n#include <linux/bitops.h>\n\n#define RPCRDMA_VERSION\t\t1\n#define rpcrdma_version\t\tcpu_to_be32(RPCRDMA_VERSION)\n\nenum {\n\tRPCRDMA_V1_DEF_INLINE_SIZE\t= 1024,\n};\n\nstruct rpcrdma_segment {\n\t__be32 rs_handle;\t/* Registered memory handle */\n\t__be32 rs_length;\t/* Length of the chunk in bytes */\n\t__be64 rs_offset;\t/* Chunk virtual address or offset */\n};\n\n/*\n * read chunk(s), encoded as a linked list.\n */\nstruct rpcrdma_read_chunk {\n\t__be32 rc_discrim;\t/* 1 indicates presence */\n\t__be32 rc_position;\t/* Position in XDR stream */\n\tstruct rpcrdma_segment rc_target;\n};\n\n/*\n * write chunk, and reply chunk.\n */\nstruct rpcrdma_write_chunk {\n\tstruct rpcrdma_segment wc_target;\n};\n\n/*\n * write chunk(s), encoded as a counted array.\n */\nstruct rpcrdma_write_array {\n\t__be32 wc_discrim;\t/* 1 indicates presence */\n\t__be32 wc_nchunks;\t/* Array count */\n\tstruct rpcrdma_write_chunk wc_array[0];\n};\n\nstruct rpcrdma_msg {\n\t__be32 rm_xid;\t/* Mirrors the RPC header xid */\n\t__be32 rm_vers;\t/* Version of this protocol */\n\t__be32 rm_credit;\t/* Buffers requested/granted */\n\t__be32 rm_type;\t/* Type of message (enum rpcrdma_proc) */\n\tunion {\n\n\t\tstruct {\t\t\t/* no chunks */\n\t\t\t__be32 rm_empty[3];\t/* 3 empty chunk lists */\n\t\t} rm_nochunks;\n\n\t\tstruct {\t\t\t/* no chunks and padded */\n\t\t\t__be32 rm_align;\t/* Padding alignment */\n\t\t\t__be32 rm_thresh;\t/* Padding threshold */\n\t\t\t__be32 rm_pempty[3];\t/* 3 empty chunk lists */\n\t\t} rm_padded;\n\n\t\tstruct {\n\t\t\t__be32 rm_err;\n\t\t\t__be32 rm_vers_low;\n\t\t\t__be32 rm_vers_high;\n\t\t} rm_error;\n\n\t\t__be32 rm_chunks[0];\t/* read, write and reply chunks */\n\n\t} rm_body;\n};\n\n/*\n * XDR sizes, in quads\n */\nenum {\n\trpcrdma_fixed_maxsz\t= 4,\n\trpcrdma_segment_maxsz\t= 4,\n\trpcrdma_readchunk_maxsz\t= 2 + rpcrdma_segment_maxsz,\n};\n\n/*\n * Smallest RPC/RDMA header: rm_xid through rm_type, then rm_nochunks\n */\n#define RPCRDMA_HDRLEN_MIN\t(sizeof(__be32) * 7)\n#define RPCRDMA_HDRLEN_ERR\t(sizeof(__be32) * 5)\n\nenum rpcrdma_errcode {\n\tERR_VERS = 1,\n\tERR_CHUNK = 2\n};\n\nenum rpcrdma_proc {\n\tRDMA_MSG = 0,\t\t/* An RPC call or reply msg */\n\tRDMA_NOMSG = 1,\t\t/* An RPC call or reply msg - separate body */\n\tRDMA_MSGP = 2,\t\t/* An RPC call or reply msg with padding */\n\tRDMA_DONE = 3,\t\t/* Client signals reply completion */\n\tRDMA_ERROR = 4\t\t/* An RPC RDMA encoding error */\n};\n\n#define rdma_msg\tcpu_to_be32(RDMA_MSG)\n#define rdma_nomsg\tcpu_to_be32(RDMA_NOMSG)\n#define rdma_msgp\tcpu_to_be32(RDMA_MSGP)\n#define rdma_done\tcpu_to_be32(RDMA_DONE)\n#define rdma_error\tcpu_to_be32(RDMA_ERROR)\n\n#define err_vers\tcpu_to_be32(ERR_VERS)\n#define err_chunk\tcpu_to_be32(ERR_CHUNK)\n\n/*\n * Private extension to RPC-over-RDMA Version One.\n * Message passed during RDMA-CM connection set-up.\n *\n * Add new fields at the end, and don't permute existing\n * fields.\n */\nstruct rpcrdma_connect_private {\n\t__be32\t\t\tcp_magic;\n\tu8\t\t\tcp_version;\n\tu8\t\t\tcp_flags;\n\tu8\t\t\tcp_send_size;\n\tu8\t\t\tcp_recv_size;\n} __packed;\n\n#define rpcrdma_cmp_magic\t__cpu_to_be32(0xf6ab0e18)\n\nenum {\n\tRPCRDMA_CMP_VERSION\t\t= 1,\n\tRPCRDMA_CMP_F_SND_W_INV_OK\t= BIT(0),\n};\n\nstatic inline u8\nrpcrdma_encode_buffer_size(unsigned int size)\n{\n\treturn (size >> 10) - 1;\n}\n\nstatic inline unsigned int\nrpcrdma_decode_buffer_size(u8 val)\n{\n\treturn ((unsigned int)val + 1) << 10;\n}\n\n#endif\t\t\t\t/* _LINUX_SUNRPC_RPC_RDMA_H */\n", "/*\n * linux/include/linux/sunrpc/svc.h\n *\n * RPC server declarations.\n *\n * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>\n */\n\n\n#ifndef SUNRPC_SVC_H\n#define SUNRPC_SVC_H\n\n#include <linux/in.h>\n#include <linux/in6.h>\n#include <linux/sunrpc/types.h>\n#include <linux/sunrpc/xdr.h>\n#include <linux/sunrpc/auth.h>\n#include <linux/sunrpc/svcauth.h>\n#include <linux/wait.h>\n#include <linux/mm.h>\n\n/* statistics for svc_pool structures */\nstruct svc_pool_stats {\n\tatomic_long_t\tpackets;\n\tunsigned long\tsockets_queued;\n\tatomic_long_t\tthreads_woken;\n\tatomic_long_t\tthreads_timedout;\n};\n\n/*\n *\n * RPC service thread pool.\n *\n * Pool of threads and temporary sockets.  Generally there is only\n * a single one of these per RPC service, but on NUMA machines those\n * services that can benefit from it (i.e. nfs but not lockd) will\n * have one pool per NUMA node.  This optimisation reduces cross-\n * node traffic on multi-node NUMA NFS servers.\n */\nstruct svc_pool {\n\tunsigned int\t\tsp_id;\t    \t/* pool id; also node id on NUMA */\n\tspinlock_t\t\tsp_lock;\t/* protects all fields */\n\tstruct list_head\tsp_sockets;\t/* pending sockets */\n\tunsigned int\t\tsp_nrthreads;\t/* # of threads in pool */\n\tstruct list_head\tsp_all_threads;\t/* all server threads */\n\tstruct svc_pool_stats\tsp_stats;\t/* statistics on pool operation */\n#define\tSP_TASK_PENDING\t\t(0)\t\t/* still work to do even if no\n\t\t\t\t\t\t * xprt is queued. */\n\tunsigned long\t\tsp_flags;\n} ____cacheline_aligned_in_smp;\n\nstruct svc_serv;\n\nstruct svc_serv_ops {\n\t/* Callback to use when last thread exits. */\n\tvoid\t\t(*svo_shutdown)(struct svc_serv *, struct net *);\n\n\t/* function for service threads to run */\n\tint\t\t(*svo_function)(void *);\n\n\t/* queue up a transport for servicing */\n\tvoid\t\t(*svo_enqueue_xprt)(struct svc_xprt *);\n\n\t/* set up thread (or whatever) execution context */\n\tint\t\t(*svo_setup)(struct svc_serv *, struct svc_pool *, int);\n\n\t/* optional module to count when adding threads (pooled svcs only) */\n\tstruct module\t*svo_module;\n};\n\n/*\n * RPC service.\n *\n * An RPC service is a ``daemon,'' possibly multithreaded, which\n * receives and processes incoming RPC messages.\n * It has one or more transport sockets associated with it, and maintains\n * a list of idle threads waiting for input.\n *\n * We currently do not support more than one RPC program per daemon.\n */\nstruct svc_serv {\n\tstruct svc_program *\tsv_program;\t/* RPC program */\n\tstruct svc_stat *\tsv_stats;\t/* RPC statistics */\n\tspinlock_t\t\tsv_lock;\n\tunsigned int\t\tsv_nrthreads;\t/* # of server threads */\n\tunsigned int\t\tsv_maxconn;\t/* max connections allowed or\n\t\t\t\t\t\t * '0' causing max to be based\n\t\t\t\t\t\t * on number of threads. */\n\n\tunsigned int\t\tsv_max_payload;\t/* datagram payload size */\n\tunsigned int\t\tsv_max_mesg;\t/* max_payload + 1 page for overheads */\n\tunsigned int\t\tsv_xdrsize;\t/* XDR buffer size */\n\tstruct list_head\tsv_permsocks;\t/* all permanent sockets */\n\tstruct list_head\tsv_tempsocks;\t/* all temporary sockets */\n\tint\t\t\tsv_tmpcnt;\t/* count of temporary sockets */\n\tstruct timer_list\tsv_temptimer;\t/* timer for aging temporary sockets */\n\n\tchar *\t\t\tsv_name;\t/* service name */\n\n\tunsigned int\t\tsv_nrpools;\t/* number of thread pools */\n\tstruct svc_pool *\tsv_pools;\t/* array of thread pools */\n\tstruct svc_serv_ops\t*sv_ops;\t/* server operations */\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\n\tstruct list_head\tsv_cb_list;\t/* queue for callback requests\n\t\t\t\t\t\t * that arrive over the same\n\t\t\t\t\t\t * connection */\n\tspinlock_t\t\tsv_cb_lock;\t/* protects the svc_cb_list */\n\twait_queue_head_t\tsv_cb_waitq;\t/* sleep here if there are no\n\t\t\t\t\t\t * entries in the svc_cb_list */\n\tstruct svc_xprt\t\t*sv_bc_xprt;\t/* callback on fore channel */\n#endif /* CONFIG_SUNRPC_BACKCHANNEL */\n};\n\n/*\n * We use sv_nrthreads as a reference count.  svc_destroy() drops\n * this refcount, so we need to bump it up around operations that\n * change the number of threads.  Horrible, but there it is.\n * Should be called with the \"service mutex\" held.\n */\nstatic inline void svc_get(struct svc_serv *serv)\n{\n\tserv->sv_nrthreads++;\n}\n\n/*\n * Maximum payload size supported by a kernel RPC server.\n * This is use to determine the max number of pages nfsd is\n * willing to return in a single READ operation.\n *\n * These happen to all be powers of 2, which is not strictly\n * necessary but helps enforce the real limitation, which is\n * that they should be multiples of PAGE_SIZE.\n *\n * For UDP transports, a block plus NFS,RPC, and UDP headers\n * has to fit into the IP datagram limit of 64K.  The largest\n * feasible number for all known page sizes is probably 48K,\n * but we choose 32K here.  This is the same as the historical\n * Linux limit; someone who cares more about NFS/UDP performance\n * can test a larger number.\n *\n * For TCP transports we have more freedom.  A size of 1MB is\n * chosen to match the client limit.  Other OSes are known to\n * have larger limits, but those numbers are probably beyond\n * the point of diminishing returns.\n */\n#define RPCSVC_MAXPAYLOAD\t(1*1024*1024u)\n#define RPCSVC_MAXPAYLOAD_TCP\tRPCSVC_MAXPAYLOAD\n#define RPCSVC_MAXPAYLOAD_UDP\t(32*1024u)\n\nextern u32 svc_max_payload(const struct svc_rqst *rqstp);\n\n/*\n * RPC Requsts and replies are stored in one or more pages.\n * We maintain an array of pages for each server thread.\n * Requests are copied into these pages as they arrive.  Remaining\n * pages are available to write the reply into.\n *\n * Pages are sent using ->sendpage so each server thread needs to\n * allocate more to replace those used in sending.  To help keep track\n * of these pages we have a receive list where all pages initialy live,\n * and a send list where pages are moved to when there are to be part\n * of a reply.\n *\n * We use xdr_buf for holding responses as it fits well with NFS\n * read responses (that have a header, and some data pages, and possibly\n * a tail) and means we can share some client side routines.\n *\n * The xdr_buf.head kvec always points to the first page in the rq_*pages\n * list.  The xdr_buf.pages pointer points to the second page on that\n * list.  xdr_buf.tail points to the end of the first page.\n * This assumes that the non-page part of an rpc reply will fit\n * in a page - NFSd ensures this.  lockd also has no trouble.\n *\n * Each request/reply pair can have at most one \"payload\", plus two pages,\n * one for the request, and one for the reply.\n * We using ->sendfile to return read data, we might need one extra page\n * if the request is not page-aligned.  So add another '1'.\n */\n#define RPCSVC_MAXPAGES\t\t((RPCSVC_MAXPAYLOAD+PAGE_SIZE-1)/PAGE_SIZE \\\n\t\t\t\t+ 2 + 1)\n\nstatic inline u32 svc_getnl(struct kvec *iov)\n{\n\t__be32 val, *vp;\n\tvp = iov->iov_base;\n\tval = *vp++;\n\tiov->iov_base = (void*)vp;\n\tiov->iov_len -= sizeof(__be32);\n\treturn ntohl(val);\n}\n\nstatic inline void svc_putnl(struct kvec *iov, u32 val)\n{\n\t__be32 *vp = iov->iov_base + iov->iov_len;\n\t*vp = htonl(val);\n\tiov->iov_len += sizeof(__be32);\n}\n\nstatic inline __be32 svc_getu32(struct kvec *iov)\n{\n\t__be32 val, *vp;\n\tvp = iov->iov_base;\n\tval = *vp++;\n\tiov->iov_base = (void*)vp;\n\tiov->iov_len -= sizeof(__be32);\n\treturn val;\n}\n\nstatic inline void svc_ungetu32(struct kvec *iov)\n{\n\t__be32 *vp = (__be32 *)iov->iov_base;\n\tiov->iov_base = (void *)(vp - 1);\n\tiov->iov_len += sizeof(*vp);\n}\n\nstatic inline void svc_putu32(struct kvec *iov, __be32 val)\n{\n\t__be32 *vp = iov->iov_base + iov->iov_len;\n\t*vp = val;\n\tiov->iov_len += sizeof(__be32);\n}\n\n/*\n * The context of a single thread, including the request currently being\n * processed.\n */\nstruct svc_rqst {\n\tstruct list_head\trq_all;\t\t/* all threads list */\n\tstruct rcu_head\t\trq_rcu_head;\t/* for RCU deferred kfree */\n\tstruct svc_xprt *\trq_xprt;\t/* transport ptr */\n\n\tstruct sockaddr_storage\trq_addr;\t/* peer address */\n\tsize_t\t\t\trq_addrlen;\n\tstruct sockaddr_storage\trq_daddr;\t/* dest addr of request\n\t\t\t\t\t\t *  - reply from here */\n\tsize_t\t\t\trq_daddrlen;\n\n\tstruct svc_serv *\trq_server;\t/* RPC service definition */\n\tstruct svc_pool *\trq_pool;\t/* thread pool */\n\tstruct svc_procedure *\trq_procinfo;\t/* procedure info */\n\tstruct auth_ops *\trq_authop;\t/* authentication flavour */\n\tstruct svc_cred\t\trq_cred;\t/* auth info */\n\tvoid *\t\t\trq_xprt_ctxt;\t/* transport specific context ptr */\n\tstruct svc_deferred_req*rq_deferred;\t/* deferred request we are replaying */\n\n\tsize_t\t\t\trq_xprt_hlen;\t/* xprt header len */\n\tstruct xdr_buf\t\trq_arg;\n\tstruct xdr_buf\t\trq_res;\n\tstruct page *\t\trq_pages[RPCSVC_MAXPAGES];\n\tstruct page *\t\t*rq_respages;\t/* points into rq_pages */\n\tstruct page *\t\t*rq_next_page; /* next reply page to use */\n\tstruct page *\t\t*rq_page_end;  /* one past the last page */\n\n\tstruct kvec\t\trq_vec[RPCSVC_MAXPAGES]; /* generally useful.. */\n\n\t__be32\t\t\trq_xid;\t\t/* transmission id */\n\tu32\t\t\trq_prog;\t/* program number */\n\tu32\t\t\trq_vers;\t/* program version */\n\tu32\t\t\trq_proc;\t/* procedure number */\n\tu32\t\t\trq_prot;\t/* IP protocol */\n\tint\t\t\trq_cachetype;\t/* catering to nfsd */\n#define\tRQ_SECURE\t(0)\t\t\t/* secure port */\n#define\tRQ_LOCAL\t(1)\t\t\t/* local request */\n#define\tRQ_USEDEFERRAL\t(2)\t\t\t/* use deferral */\n#define\tRQ_DROPME\t(3)\t\t\t/* drop current reply */\n#define\tRQ_SPLICE_OK\t(4)\t\t\t/* turned off in gss privacy\n\t\t\t\t\t\t * to prevent encrypting page\n\t\t\t\t\t\t * cache pages */\n#define\tRQ_VICTIM\t(5)\t\t\t/* about to be shut down */\n#define\tRQ_BUSY\t\t(6)\t\t\t/* request is busy */\n#define\tRQ_DATA\t\t(7)\t\t\t/* request has data */\n\tunsigned long\t\trq_flags;\t/* flags field */\n\n\tvoid *\t\t\trq_argp;\t/* decoded arguments */\n\tvoid *\t\t\trq_resp;\t/* xdr'd results */\n\tvoid *\t\t\trq_auth_data;\t/* flavor-specific data */\n\tint\t\t\trq_auth_slack;\t/* extra space xdr code\n\t\t\t\t\t\t * should leave in head\n\t\t\t\t\t\t * for krb5i, krb5p.\n\t\t\t\t\t\t */\n\tint\t\t\trq_reserved;\t/* space on socket outq\n\t\t\t\t\t\t * reserved for this request\n\t\t\t\t\t\t */\n\n\tstruct cache_req\trq_chandle;\t/* handle passed to caches for \n\t\t\t\t\t\t * request delaying \n\t\t\t\t\t\t */\n\t/* Catering to nfsd */\n\tstruct auth_domain *\trq_client;\t/* RPC peer info */\n\tstruct auth_domain *\trq_gssclient;\t/* \"gss/\"-style peer info */\n\tstruct svc_cacherep *\trq_cacherep;\t/* cache info */\n\tstruct task_struct\t*rq_task;\t/* service thread */\n\tspinlock_t\t\trq_lock;\t/* per-request lock */\n};\n\n#define SVC_NET(svc_rqst)\t(svc_rqst->rq_xprt->xpt_net)\n\n/*\n * Rigorous type checking on sockaddr type conversions\n */\nstatic inline struct sockaddr_in *svc_addr_in(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr_in *) &rqst->rq_addr;\n}\n\nstatic inline struct sockaddr_in6 *svc_addr_in6(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr_in6 *) &rqst->rq_addr;\n}\n\nstatic inline struct sockaddr *svc_addr(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr *) &rqst->rq_addr;\n}\n\nstatic inline struct sockaddr_in *svc_daddr_in(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr_in *) &rqst->rq_daddr;\n}\n\nstatic inline struct sockaddr_in6 *svc_daddr_in6(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr_in6 *) &rqst->rq_daddr;\n}\n\nstatic inline struct sockaddr *svc_daddr(const struct svc_rqst *rqst)\n{\n\treturn (struct sockaddr *) &rqst->rq_daddr;\n}\n\n/*\n * Check buffer bounds after decoding arguments\n */\nstatic inline int\nxdr_argsize_check(struct svc_rqst *rqstp, __be32 *p)\n{\n\tchar *cp = (char *)p;\n\tstruct kvec *vec = &rqstp->rq_arg.head[0];\n\treturn cp == (char *)vec->iov_base + vec->iov_len;\n}\n\nstatic inline int\nxdr_ressize_check(struct svc_rqst *rqstp, __be32 *p)\n{\n\tstruct kvec *vec = &rqstp->rq_res.head[0];\n\tchar *cp = (char*)p;\n\n\tvec->iov_len = cp - (char*)vec->iov_base;\n\n\treturn vec->iov_len <= PAGE_SIZE;\n}\n\nstatic inline void svc_free_res_pages(struct svc_rqst *rqstp)\n{\n\twhile (rqstp->rq_next_page != rqstp->rq_respages) {\n\t\tstruct page **pp = --rqstp->rq_next_page;\n\t\tif (*pp) {\n\t\t\tput_page(*pp);\n\t\t\t*pp = NULL;\n\t\t}\n\t}\n}\n\nstruct svc_deferred_req {\n\tu32\t\t\tprot;\t/* protocol (UDP or TCP) */\n\tstruct svc_xprt\t\t*xprt;\n\tstruct sockaddr_storage\taddr;\t/* where reply must go */\n\tsize_t\t\t\taddrlen;\n\tstruct sockaddr_storage\tdaddr;\t/* where reply must come from */\n\tsize_t\t\t\tdaddrlen;\n\tstruct cache_deferred_req handle;\n\tsize_t\t\t\txprt_hlen;\n\tint\t\t\targslen;\n\t__be32\t\t\targs[0];\n};\n\n/*\n * List of RPC programs on the same transport endpoint\n */\nstruct svc_program {\n\tstruct svc_program *\tpg_next;\t/* other programs (same xprt) */\n\tu32\t\t\tpg_prog;\t/* program number */\n\tunsigned int\t\tpg_lovers;\t/* lowest version */\n\tunsigned int\t\tpg_hivers;\t/* highest version */\n\tunsigned int\t\tpg_nvers;\t/* number of versions */\n\tstruct svc_version **\tpg_vers;\t/* version array */\n\tchar *\t\t\tpg_name;\t/* service name */\n\tchar *\t\t\tpg_class;\t/* class name: services sharing authentication */\n\tstruct svc_stat *\tpg_stats;\t/* rpc statistics */\n\tint\t\t\t(*pg_authenticate)(struct svc_rqst *);\n};\n\n/*\n * RPC program version\n */\nstruct svc_version {\n\tu32\t\t\tvs_vers;\t/* version number */\n\tu32\t\t\tvs_nproc;\t/* number of procedures */\n\tstruct svc_procedure *\tvs_proc;\t/* per-procedure info */\n\tu32\t\t\tvs_xdrsize;\t/* xdrsize needed for this version */\n\n\t/* Don't register with rpcbind */\n\tbool\t\t\tvs_hidden;\n\n\t/* Don't care if the rpcbind registration fails */\n\tbool\t\t\tvs_rpcb_optnl;\n\n\t/* Need xprt with congestion control */\n\tbool\t\t\tvs_need_cong_ctrl;\n\n\t/* Override dispatch function (e.g. when caching replies).\n\t * A return value of 0 means drop the request. \n\t * vs_dispatch == NULL means use default dispatcher.\n\t */\n\tint\t\t\t(*vs_dispatch)(struct svc_rqst *, __be32 *);\n};\n\n/*\n * RPC procedure info\n */\ntypedef __be32\t(*svc_procfunc)(struct svc_rqst *, void *argp, void *resp);\nstruct svc_procedure {\n\tsvc_procfunc\t\tpc_func;\t/* process the request */\n\tkxdrproc_t\t\tpc_decode;\t/* XDR decode args */\n\tkxdrproc_t\t\tpc_encode;\t/* XDR encode result */\n\tkxdrproc_t\t\tpc_release;\t/* XDR free result */\n\tunsigned int\t\tpc_argsize;\t/* argument struct size */\n\tunsigned int\t\tpc_ressize;\t/* result struct size */\n\tunsigned int\t\tpc_count;\t/* call count */\n\tunsigned int\t\tpc_cachetype;\t/* cache info (NFS) */\n\tunsigned int\t\tpc_xdrressize;\t/* maximum size of XDR reply */\n};\n\n/*\n * Mode for mapping cpus to pools.\n */\nenum {\n\tSVC_POOL_AUTO = -1,\t/* choose one of the others */\n\tSVC_POOL_GLOBAL,\t/* no mapping, just a single global pool\n\t\t\t\t * (legacy & UP mode) */\n\tSVC_POOL_PERCPU,\t/* one pool per cpu */\n\tSVC_POOL_PERNODE\t/* one pool per numa node */\n};\n\nstruct svc_pool_map {\n\tint count;\t\t\t/* How many svc_servs use us */\n\tint mode;\t\t\t/* Note: int not enum to avoid\n\t\t\t\t\t * warnings about \"enumeration value\n\t\t\t\t\t * not handled in switch\" */\n\tunsigned int npools;\n\tunsigned int *pool_to;\t\t/* maps pool id to cpu or node */\n\tunsigned int *to_pool;\t\t/* maps cpu or node to pool id */\n};\n\nextern struct svc_pool_map svc_pool_map;\n\n/*\n * Function prototypes.\n */\nint svc_rpcb_setup(struct svc_serv *serv, struct net *net);\nvoid svc_rpcb_cleanup(struct svc_serv *serv, struct net *net);\nint svc_bind(struct svc_serv *serv, struct net *net);\nstruct svc_serv *svc_create(struct svc_program *, unsigned int,\n\t\t\t    struct svc_serv_ops *);\nstruct svc_rqst *svc_rqst_alloc(struct svc_serv *serv,\n\t\t\t\t\tstruct svc_pool *pool, int node);\nstruct svc_rqst *svc_prepare_thread(struct svc_serv *serv,\n\t\t\t\t\tstruct svc_pool *pool, int node);\nvoid\t\t   svc_rqst_free(struct svc_rqst *);\nvoid\t\t   svc_exit_thread(struct svc_rqst *);\nunsigned int\t   svc_pool_map_get(void);\nvoid\t\t   svc_pool_map_put(void);\nstruct svc_serv *  svc_create_pooled(struct svc_program *, unsigned int,\n\t\t\tstruct svc_serv_ops *);\nint\t\t   svc_set_num_threads(struct svc_serv *, struct svc_pool *, int);\nint\t\t   svc_set_num_threads_sync(struct svc_serv *, struct svc_pool *, int);\nint\t\t   svc_pool_stats_open(struct svc_serv *serv, struct file *file);\nvoid\t\t   svc_destroy(struct svc_serv *);\nvoid\t\t   svc_shutdown_net(struct svc_serv *, struct net *);\nint\t\t   svc_process(struct svc_rqst *);\nint\t\t   bc_svc_process(struct svc_serv *, struct rpc_rqst *,\n\t\t\tstruct svc_rqst *);\nint\t\t   svc_register(const struct svc_serv *, struct net *, const int,\n\t\t\t\tconst unsigned short, const unsigned short);\n\nvoid\t\t   svc_wake_up(struct svc_serv *);\nvoid\t\t   svc_reserve(struct svc_rqst *rqstp, int space);\nstruct svc_pool *  svc_pool_for_cpu(struct svc_serv *serv, int cpu);\nchar *\t\t   svc_print_addr(struct svc_rqst *, char *, size_t);\n\n#define\tRPC_MAX_ADDRBUFLEN\t(63U)\n\n/*\n * When we want to reduce the size of the reserved space in the response\n * buffer, we need to take into account the size of any checksum data that\n * may be at the end of the packet. This is difficult to determine exactly\n * for all cases without actually generating the checksum, so we just use a\n * static value.\n */\nstatic inline void svc_reserve_auth(struct svc_rqst *rqstp, int space)\n{\n\tsvc_reserve(rqstp, space + rqstp->rq_auth_slack);\n}\n\n#endif /* SUNRPC_SVC_H */\n", "/*\n * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n#ifndef SVC_RDMA_H\n#define SVC_RDMA_H\n#include <linux/sunrpc/xdr.h>\n#include <linux/sunrpc/svcsock.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n#define SVCRDMA_DEBUG\n\n/* Default and maximum inline threshold sizes */\nenum {\n\tRPCRDMA_DEF_INLINE_THRESH = 4096,\n\tRPCRDMA_MAX_INLINE_THRESH = 65536\n};\n\n/* RPC/RDMA parameters and stats */\nextern unsigned int svcrdma_ord;\nextern unsigned int svcrdma_max_requests;\nextern unsigned int svcrdma_max_bc_requests;\nextern unsigned int svcrdma_max_req_size;\n\nextern atomic_t rdma_stat_recv;\nextern atomic_t rdma_stat_read;\nextern atomic_t rdma_stat_write;\nextern atomic_t rdma_stat_sq_starve;\nextern atomic_t rdma_stat_rq_starve;\nextern atomic_t rdma_stat_rq_poll;\nextern atomic_t rdma_stat_rq_prod;\nextern atomic_t rdma_stat_sq_poll;\nextern atomic_t rdma_stat_sq_prod;\n\n/*\n * Contexts are built when an RDMA request is created and are a\n * record of the resources that can be recovered when the request\n * completes.\n */\nstruct svc_rdma_op_ctxt {\n\tstruct list_head list;\n\tstruct svc_rdma_op_ctxt *read_hdr;\n\tstruct svc_rdma_fastreg_mr *frmr;\n\tint hdr_count;\n\tstruct xdr_buf arg;\n\tstruct ib_cqe cqe;\n\tstruct ib_cqe reg_cqe;\n\tstruct ib_cqe inv_cqe;\n\tu32 byte_len;\n\tu32 position;\n\tstruct svcxprt_rdma *xprt;\n\tunsigned long flags;\n\tenum dma_data_direction direction;\n\tint count;\n\tunsigned int mapped_sges;\n\tstruct ib_send_wr send_wr;\n\tstruct ib_sge sge[1 + RPCRDMA_MAX_INLINE_THRESH / PAGE_SIZE];\n\tstruct page *pages[RPCSVC_MAXPAGES];\n};\n\nstruct svc_rdma_fastreg_mr {\n\tstruct ib_mr *mr;\n\tstruct scatterlist *sg;\n\tint sg_nents;\n\tunsigned long access_flags;\n\tenum dma_data_direction direction;\n\tstruct list_head frmr_list;\n};\n\n#define RDMACTXT_F_LAST_CTXT\t2\n\n#define\tSVCRDMA_DEVCAP_FAST_REG\t\t1\t/* fast mr registration */\n#define\tSVCRDMA_DEVCAP_READ_W_INV\t2\t/* read w/ invalidate */\n\nstruct svcxprt_rdma {\n\tstruct svc_xprt      sc_xprt;\t\t/* SVC transport structure */\n\tstruct rdma_cm_id    *sc_cm_id;\t\t/* RDMA connection id */\n\tstruct list_head     sc_accept_q;\t/* Conn. waiting accept */\n\tint\t\t     sc_ord;\t\t/* RDMA read limit */\n\tint                  sc_max_sge;\n\tint                  sc_max_sge_rd;\t/* max sge for read target */\n\tbool\t\t     sc_snd_w_inv;\t/* OK to use Send With Invalidate */\n\n\tatomic_t             sc_sq_avail;\t/* SQEs ready to be consumed */\n\tunsigned int\t     sc_sq_depth;\t/* Depth of SQ */\n\tunsigned int\t     sc_rq_depth;\t/* Depth of RQ */\n\t__be32\t\t     sc_fc_credits;\t/* Forward credits */\n\tu32\t\t     sc_max_requests;\t/* Max requests */\n\tu32\t\t     sc_max_bc_requests;/* Backward credits */\n\tint                  sc_max_req_size;\t/* Size of each RQ WR buf */\n\tu8\t\t     sc_port_num;\n\n\tstruct ib_pd         *sc_pd;\n\n\tspinlock_t\t     sc_ctxt_lock;\n\tstruct list_head     sc_ctxts;\n\tint\t\t     sc_ctxt_used;\n\tspinlock_t\t     sc_rw_ctxt_lock;\n\tstruct list_head     sc_rw_ctxts;\n\n\tstruct list_head     sc_rq_dto_q;\n\tspinlock_t\t     sc_rq_dto_lock;\n\tstruct ib_qp         *sc_qp;\n\tstruct ib_cq         *sc_rq_cq;\n\tstruct ib_cq         *sc_sq_cq;\n\tint\t\t     (*sc_reader)(struct svcxprt_rdma *,\n\t\t\t\t\t  struct svc_rqst *,\n\t\t\t\t\t  struct svc_rdma_op_ctxt *,\n\t\t\t\t\t  int *, u32 *, u32, u32, u64, bool);\n\tu32\t\t     sc_dev_caps;\t/* distilled device caps */\n\tunsigned int\t     sc_frmr_pg_list_len;\n\tstruct list_head     sc_frmr_q;\n\tspinlock_t\t     sc_frmr_q_lock;\n\n\tspinlock_t\t     sc_lock;\t\t/* transport lock */\n\n\twait_queue_head_t    sc_send_wait;\t/* SQ exhaustion waitlist */\n\tunsigned long\t     sc_flags;\n\tstruct list_head     sc_read_complete_q;\n\tstruct work_struct   sc_work;\n};\n/* sc_flags */\n#define RDMAXPRT_CONN_PENDING\t3\n\n#define RPCRDMA_LISTEN_BACKLOG  10\n/* The default ORD value is based on two outstanding full-size writes with a\n * page size of 4k, or 32k * 2 ops / 4k = 16 outstanding RDMA_READ.  */\n#define RPCRDMA_ORD             (64/4)\n#define RPCRDMA_MAX_REQUESTS    32\n\n/* Typical ULP usage of BC requests is NFSv4.1 backchannel. Our\n * current NFSv4.1 implementation supports one backchannel slot.\n */\n#define RPCRDMA_MAX_BC_REQUESTS\t2\n\n#define RPCSVC_MAXPAYLOAD_RDMA\tRPCSVC_MAXPAYLOAD\n\n/* Track DMA maps for this transport and context */\nstatic inline void svc_rdma_count_mappings(struct svcxprt_rdma *rdma,\n\t\t\t\t\t   struct svc_rdma_op_ctxt *ctxt)\n{\n\tctxt->mapped_sges++;\n}\n\n/* svc_rdma_backchannel.c */\nextern int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt,\n\t\t\t\t    __be32 *rdma_resp,\n\t\t\t\t    struct xdr_buf *rcvbuf);\n\n/* svc_rdma_marshal.c */\nextern int svc_rdma_xdr_decode_req(struct xdr_buf *);\n\n/* svc_rdma_recvfrom.c */\nextern int svc_rdma_recvfrom(struct svc_rqst *);\nextern int rdma_read_chunk_lcl(struct svcxprt_rdma *, struct svc_rqst *,\n\t\t\t       struct svc_rdma_op_ctxt *, int *, u32 *,\n\t\t\t       u32, u32, u64, bool);\nextern int rdma_read_chunk_frmr(struct svcxprt_rdma *, struct svc_rqst *,\n\t\t\t\tstruct svc_rdma_op_ctxt *, int *, u32 *,\n\t\t\t\tu32, u32, u64, bool);\n\n/* svc_rdma_rw.c */\nextern void svc_rdma_destroy_rw_ctxts(struct svcxprt_rdma *rdma);\nextern int svc_rdma_send_write_chunk(struct svcxprt_rdma *rdma,\n\t\t\t\t     __be32 *wr_ch, struct xdr_buf *xdr);\nextern int svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma,\n\t\t\t\t     __be32 *rp_ch, bool writelist,\n\t\t\t\t     struct xdr_buf *xdr);\n\n/* svc_rdma_sendto.c */\nextern int svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,\n\t\t\t\t  struct svc_rdma_op_ctxt *ctxt,\n\t\t\t\t  __be32 *rdma_resp, unsigned int len);\nextern int svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,\n\t\t\t\t struct svc_rdma_op_ctxt *ctxt,\n\t\t\t\t int num_sge, u32 inv_rkey);\nextern int svc_rdma_sendto(struct svc_rqst *);\n\n/* svc_rdma_transport.c */\nextern void svc_rdma_wc_send(struct ib_cq *, struct ib_wc *);\nextern void svc_rdma_wc_reg(struct ib_cq *, struct ib_wc *);\nextern void svc_rdma_wc_read(struct ib_cq *, struct ib_wc *);\nextern void svc_rdma_wc_inv(struct ib_cq *, struct ib_wc *);\nextern int svc_rdma_send(struct svcxprt_rdma *, struct ib_send_wr *);\nextern int svc_rdma_post_recv(struct svcxprt_rdma *, gfp_t);\nextern int svc_rdma_repost_recv(struct svcxprt_rdma *, gfp_t);\nextern int svc_rdma_create_listen(struct svc_serv *, int, struct sockaddr *);\nextern struct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *);\nextern void svc_rdma_put_context(struct svc_rdma_op_ctxt *, int);\nextern void svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt);\nextern struct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *);\nextern void svc_rdma_put_frmr(struct svcxprt_rdma *,\n\t\t\t      struct svc_rdma_fastreg_mr *);\nextern void svc_sq_reap(struct svcxprt_rdma *);\nextern void svc_rq_reap(struct svcxprt_rdma *);\nextern void svc_rdma_prep_reply_hdr(struct svc_rqst *);\n\nextern struct svc_xprt_class svc_rdma_class;\n#ifdef CONFIG_SUNRPC_BACKCHANNEL\nextern struct svc_xprt_class svc_rdma_bc_class;\n#endif\n\n/* svc_rdma.c */\nextern struct workqueue_struct *svc_rdma_wq;\nextern int svc_rdma_init(void);\nextern void svc_rdma_cleanup(void);\n\n#endif\n", "/*\n * Upcall description for nfsdcld communication\n *\n * Copyright (c) 2012 Red Hat, Inc.\n * Author(s): Jeff Layton <jlayton@redhat.com>\n *\n *  This program is free software; you can redistribute it and/or modify\n *  it under the terms of the GNU General Public License as published by\n *  the Free Software Foundation; either version 2 of the License, or\n *  (at your option) any later version.\n *\n *  This program is distributed in the hope that it will be useful,\n *  but WITHOUT ANY WARRANTY; without even the implied warranty of\n *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *  GNU General Public License for more details.\n *\n *  You should have received a copy of the GNU General Public License\n *  along with this program; if not, write to the Free Software\n *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n */\n\n#ifndef _NFSD_CLD_H\n#define _NFSD_CLD_H\n\n#include <linux/types.h>\n\n/* latest upcall version available */\n#define CLD_UPCALL_VERSION 1\n\n/* defined by RFC3530 */\n#define NFS4_OPAQUE_LIMIT 1024\n\nenum cld_command {\n\tCld_Create,\t\t/* create a record for this cm_id */\n\tCld_Remove,\t\t/* remove record of this cm_id */\n\tCld_Check,\t\t/* is this cm_id allowed? */\n\tCld_GraceDone,\t\t/* grace period is complete */\n};\n\n/* representation of long-form NFSv4 client ID */\nstruct cld_name {\n\t__u16\t\tcn_len;\t\t\t\t/* length of cm_id */\n\tunsigned char\tcn_id[NFS4_OPAQUE_LIMIT];\t/* client-provided */\n} __attribute__((packed));\n\n/* message struct for communication with userspace */\nstruct cld_msg {\n\t__u8\t\tcm_vers;\t\t/* upcall version */\n\t__u8\t\tcm_cmd;\t\t\t/* upcall command */\n\t__s16\t\tcm_status;\t\t/* return code */\n\t__u32\t\tcm_xid;\t\t\t/* transaction id */\n\tunion {\n\t\t__s64\t\tcm_gracetime;\t/* grace period start time */\n\t\tstruct cld_name\tcm_name;\n\t} __attribute__((packed)) cm_u;\n} __attribute__((packed));\n\n#endif /* !_NFSD_CLD_H */\n", "config SUNRPC\n\ttristate\n\tdepends on MULTIUSER\n\nconfig SUNRPC_GSS\n\ttristate\n\tselect OID_REGISTRY\n\tdepends on MULTIUSER\n\nconfig SUNRPC_BACKCHANNEL\n\tbool\n\tdepends on SUNRPC\n\nconfig SUNRPC_SWAP\n\tbool\n\tdepends on SUNRPC\n\nconfig RPCSEC_GSS_KRB5\n\ttristate \"Secure RPC: Kerberos V mechanism\"\n\tdepends on SUNRPC && CRYPTO\n\tdepends on CRYPTO_MD5 && CRYPTO_DES && CRYPTO_CBC && CRYPTO_CTS\n\tdepends on CRYPTO_ECB && CRYPTO_HMAC && CRYPTO_SHA1 && CRYPTO_AES\n\tdepends on CRYPTO_ARC4\n\tdefault y\n\tselect SUNRPC_GSS\n\thelp\n\t  Choose Y here to enable Secure RPC using the Kerberos version 5\n\t  GSS-API mechanism (RFC 1964).\n\n\t  Secure RPC calls with Kerberos require an auxiliary user-space\n\t  daemon which may be found in the Linux nfs-utils package\n\t  available from http://linux-nfs.org/.  In addition, user-space\n\t  Kerberos support should be installed.\n\n\t  If unsure, say Y.\n\nconfig SUNRPC_DEBUG\n\tbool \"RPC: Enable dprintk debugging\"\n\tdepends on SUNRPC && SYSCTL\n\tselect DEBUG_FS\n\thelp\n\t  This option enables a sysctl-based debugging interface\n\t  that is be used by the 'rpcdebug' utility to turn on or off\n\t  logging of different aspects of the kernel RPC activity.\n\n\t  Disabling this option will make your kernel slightly smaller,\n\t  but makes troubleshooting NFS issues significantly harder.\n\n\t  If unsure, say Y.\n\nconfig SUNRPC_XPRT_RDMA\n\ttristate \"RPC-over-RDMA transport\"\n\tdepends on SUNRPC && INFINIBAND && INFINIBAND_ADDR_TRANS\n\tdefault SUNRPC && INFINIBAND\n\tselect SG_POOL\n\thelp\n\t  This option allows the NFS client and server to use RDMA\n\t  transports (InfiniBand, iWARP, or RoCE).\n\n\t  To compile this support as a module, choose M. The module\n\t  will be called rpcrdma.ko.\n\n\t  If unsure, or you know there is no RDMA capability on your\n\t  hardware platform, say N.\n", "/*\n * linux/net/sunrpc/svc.c\n *\n * High-level RPC service routines\n *\n * Copyright (C) 1995, 1996 Olaf Kirch <okir@monad.swb.de>\n *\n * Multiple threads pools and NUMAisation\n * Copyright (c) 2006 Silicon Graphics, Inc.\n * by Greg Banks <gnb@melbourne.sgi.com>\n */\n\n#include <linux/linkage.h>\n#include <linux/sched/signal.h>\n#include <linux/errno.h>\n#include <linux/net.h>\n#include <linux/in.h>\n#include <linux/mm.h>\n#include <linux/interrupt.h>\n#include <linux/module.h>\n#include <linux/kthread.h>\n#include <linux/slab.h>\n\n#include <linux/sunrpc/types.h>\n#include <linux/sunrpc/xdr.h>\n#include <linux/sunrpc/stats.h>\n#include <linux/sunrpc/svcsock.h>\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/bc_xprt.h>\n\n#include <trace/events/sunrpc.h>\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCDSP\n\nstatic void svc_unregister(const struct svc_serv *serv, struct net *net);\n\n#define svc_serv_is_pooled(serv)    ((serv)->sv_ops->svo_function)\n\n#define SVC_POOL_DEFAULT\tSVC_POOL_GLOBAL\n\n/*\n * Structure for mapping cpus to pools and vice versa.\n * Setup once during sunrpc initialisation.\n */\nstruct svc_pool_map svc_pool_map = {\n\t.mode = SVC_POOL_DEFAULT\n};\nEXPORT_SYMBOL_GPL(svc_pool_map);\n\nstatic DEFINE_MUTEX(svc_pool_map_mutex);/* protects svc_pool_map.count only */\n\nstatic int\nparam_set_pool_mode(const char *val, struct kernel_param *kp)\n{\n\tint *ip = (int *)kp->arg;\n\tstruct svc_pool_map *m = &svc_pool_map;\n\tint err;\n\n\tmutex_lock(&svc_pool_map_mutex);\n\n\terr = -EBUSY;\n\tif (m->count)\n\t\tgoto out;\n\n\terr = 0;\n\tif (!strncmp(val, \"auto\", 4))\n\t\t*ip = SVC_POOL_AUTO;\n\telse if (!strncmp(val, \"global\", 6))\n\t\t*ip = SVC_POOL_GLOBAL;\n\telse if (!strncmp(val, \"percpu\", 6))\n\t\t*ip = SVC_POOL_PERCPU;\n\telse if (!strncmp(val, \"pernode\", 7))\n\t\t*ip = SVC_POOL_PERNODE;\n\telse\n\t\terr = -EINVAL;\n\nout:\n\tmutex_unlock(&svc_pool_map_mutex);\n\treturn err;\n}\n\nstatic int\nparam_get_pool_mode(char *buf, struct kernel_param *kp)\n{\n\tint *ip = (int *)kp->arg;\n\n\tswitch (*ip)\n\t{\n\tcase SVC_POOL_AUTO:\n\t\treturn strlcpy(buf, \"auto\", 20);\n\tcase SVC_POOL_GLOBAL:\n\t\treturn strlcpy(buf, \"global\", 20);\n\tcase SVC_POOL_PERCPU:\n\t\treturn strlcpy(buf, \"percpu\", 20);\n\tcase SVC_POOL_PERNODE:\n\t\treturn strlcpy(buf, \"pernode\", 20);\n\tdefault:\n\t\treturn sprintf(buf, \"%d\", *ip);\n\t}\n}\n\nmodule_param_call(pool_mode, param_set_pool_mode, param_get_pool_mode,\n\t\t &svc_pool_map.mode, 0644);\n\n/*\n * Detect best pool mapping mode heuristically,\n * according to the machine's topology.\n */\nstatic int\nsvc_pool_map_choose_mode(void)\n{\n\tunsigned int node;\n\n\tif (nr_online_nodes > 1) {\n\t\t/*\n\t\t * Actually have multiple NUMA nodes,\n\t\t * so split pools on NUMA node boundaries\n\t\t */\n\t\treturn SVC_POOL_PERNODE;\n\t}\n\n\tnode = first_online_node;\n\tif (nr_cpus_node(node) > 2) {\n\t\t/*\n\t\t * Non-trivial SMP, or CONFIG_NUMA on\n\t\t * non-NUMA hardware, e.g. with a generic\n\t\t * x86_64 kernel on Xeons.  In this case we\n\t\t * want to divide the pools on cpu boundaries.\n\t\t */\n\t\treturn SVC_POOL_PERCPU;\n\t}\n\n\t/* default: one global pool */\n\treturn SVC_POOL_GLOBAL;\n}\n\n/*\n * Allocate the to_pool[] and pool_to[] arrays.\n * Returns 0 on success or an errno.\n */\nstatic int\nsvc_pool_map_alloc_arrays(struct svc_pool_map *m, unsigned int maxpools)\n{\n\tm->to_pool = kcalloc(maxpools, sizeof(unsigned int), GFP_KERNEL);\n\tif (!m->to_pool)\n\t\tgoto fail;\n\tm->pool_to = kcalloc(maxpools, sizeof(unsigned int), GFP_KERNEL);\n\tif (!m->pool_to)\n\t\tgoto fail_free;\n\n\treturn 0;\n\nfail_free:\n\tkfree(m->to_pool);\n\tm->to_pool = NULL;\nfail:\n\treturn -ENOMEM;\n}\n\n/*\n * Initialise the pool map for SVC_POOL_PERCPU mode.\n * Returns number of pools or <0 on error.\n */\nstatic int\nsvc_pool_map_init_percpu(struct svc_pool_map *m)\n{\n\tunsigned int maxpools = nr_cpu_ids;\n\tunsigned int pidx = 0;\n\tunsigned int cpu;\n\tint err;\n\n\terr = svc_pool_map_alloc_arrays(m, maxpools);\n\tif (err)\n\t\treturn err;\n\n\tfor_each_online_cpu(cpu) {\n\t\tBUG_ON(pidx >= maxpools);\n\t\tm->to_pool[cpu] = pidx;\n\t\tm->pool_to[pidx] = cpu;\n\t\tpidx++;\n\t}\n\t/* cpus brought online later all get mapped to pool0, sorry */\n\n\treturn pidx;\n};\n\n\n/*\n * Initialise the pool map for SVC_POOL_PERNODE mode.\n * Returns number of pools or <0 on error.\n */\nstatic int\nsvc_pool_map_init_pernode(struct svc_pool_map *m)\n{\n\tunsigned int maxpools = nr_node_ids;\n\tunsigned int pidx = 0;\n\tunsigned int node;\n\tint err;\n\n\terr = svc_pool_map_alloc_arrays(m, maxpools);\n\tif (err)\n\t\treturn err;\n\n\tfor_each_node_with_cpus(node) {\n\t\t/* some architectures (e.g. SN2) have cpuless nodes */\n\t\tBUG_ON(pidx > maxpools);\n\t\tm->to_pool[node] = pidx;\n\t\tm->pool_to[pidx] = node;\n\t\tpidx++;\n\t}\n\t/* nodes brought online later all get mapped to pool0, sorry */\n\n\treturn pidx;\n}\n\n\n/*\n * Add a reference to the global map of cpus to pools (and\n * vice versa).  Initialise the map if we're the first user.\n * Returns the number of pools.\n */\nunsigned int\nsvc_pool_map_get(void)\n{\n\tstruct svc_pool_map *m = &svc_pool_map;\n\tint npools = -1;\n\n\tmutex_lock(&svc_pool_map_mutex);\n\n\tif (m->count++) {\n\t\tmutex_unlock(&svc_pool_map_mutex);\n\t\treturn m->npools;\n\t}\n\n\tif (m->mode == SVC_POOL_AUTO)\n\t\tm->mode = svc_pool_map_choose_mode();\n\n\tswitch (m->mode) {\n\tcase SVC_POOL_PERCPU:\n\t\tnpools = svc_pool_map_init_percpu(m);\n\t\tbreak;\n\tcase SVC_POOL_PERNODE:\n\t\tnpools = svc_pool_map_init_pernode(m);\n\t\tbreak;\n\t}\n\n\tif (npools < 0) {\n\t\t/* default, or memory allocation failure */\n\t\tnpools = 1;\n\t\tm->mode = SVC_POOL_GLOBAL;\n\t}\n\tm->npools = npools;\n\n\tmutex_unlock(&svc_pool_map_mutex);\n\treturn m->npools;\n}\nEXPORT_SYMBOL_GPL(svc_pool_map_get);\n\n/*\n * Drop a reference to the global map of cpus to pools.\n * When the last reference is dropped, the map data is\n * freed; this allows the sysadmin to change the pool\n * mode using the pool_mode module option without\n * rebooting or re-loading sunrpc.ko.\n */\nvoid\nsvc_pool_map_put(void)\n{\n\tstruct svc_pool_map *m = &svc_pool_map;\n\n\tmutex_lock(&svc_pool_map_mutex);\n\n\tif (!--m->count) {\n\t\tkfree(m->to_pool);\n\t\tm->to_pool = NULL;\n\t\tkfree(m->pool_to);\n\t\tm->pool_to = NULL;\n\t\tm->npools = 0;\n\t}\n\n\tmutex_unlock(&svc_pool_map_mutex);\n}\nEXPORT_SYMBOL_GPL(svc_pool_map_put);\n\nstatic int svc_pool_map_get_node(unsigned int pidx)\n{\n\tconst struct svc_pool_map *m = &svc_pool_map;\n\n\tif (m->count) {\n\t\tif (m->mode == SVC_POOL_PERCPU)\n\t\t\treturn cpu_to_node(m->pool_to[pidx]);\n\t\tif (m->mode == SVC_POOL_PERNODE)\n\t\t\treturn m->pool_to[pidx];\n\t}\n\treturn NUMA_NO_NODE;\n}\n/*\n * Set the given thread's cpus_allowed mask so that it\n * will only run on cpus in the given pool.\n */\nstatic inline void\nsvc_pool_map_set_cpumask(struct task_struct *task, unsigned int pidx)\n{\n\tstruct svc_pool_map *m = &svc_pool_map;\n\tunsigned int node = m->pool_to[pidx];\n\n\t/*\n\t * The caller checks for sv_nrpools > 1, which\n\t * implies that we've been initialized.\n\t */\n\tWARN_ON_ONCE(m->count == 0);\n\tif (m->count == 0)\n\t\treturn;\n\n\tswitch (m->mode) {\n\tcase SVC_POOL_PERCPU:\n\t{\n\t\tset_cpus_allowed_ptr(task, cpumask_of(node));\n\t\tbreak;\n\t}\n\tcase SVC_POOL_PERNODE:\n\t{\n\t\tset_cpus_allowed_ptr(task, cpumask_of_node(node));\n\t\tbreak;\n\t}\n\t}\n}\n\n/*\n * Use the mapping mode to choose a pool for a given CPU.\n * Used when enqueueing an incoming RPC.  Always returns\n * a non-NULL pool pointer.\n */\nstruct svc_pool *\nsvc_pool_for_cpu(struct svc_serv *serv, int cpu)\n{\n\tstruct svc_pool_map *m = &svc_pool_map;\n\tunsigned int pidx = 0;\n\n\t/*\n\t * An uninitialised map happens in a pure client when\n\t * lockd is brought up, so silently treat it the\n\t * same as SVC_POOL_GLOBAL.\n\t */\n\tif (svc_serv_is_pooled(serv)) {\n\t\tswitch (m->mode) {\n\t\tcase SVC_POOL_PERCPU:\n\t\t\tpidx = m->to_pool[cpu];\n\t\t\tbreak;\n\t\tcase SVC_POOL_PERNODE:\n\t\t\tpidx = m->to_pool[cpu_to_node(cpu)];\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn &serv->sv_pools[pidx % serv->sv_nrpools];\n}\n\nint svc_rpcb_setup(struct svc_serv *serv, struct net *net)\n{\n\tint err;\n\n\terr = rpcb_create_local(net);\n\tif (err)\n\t\treturn err;\n\n\t/* Remove any stale portmap registrations */\n\tsvc_unregister(serv, net);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(svc_rpcb_setup);\n\nvoid svc_rpcb_cleanup(struct svc_serv *serv, struct net *net)\n{\n\tsvc_unregister(serv, net);\n\trpcb_put_local(net);\n}\nEXPORT_SYMBOL_GPL(svc_rpcb_cleanup);\n\nstatic int svc_uses_rpcbind(struct svc_serv *serv)\n{\n\tstruct svc_program\t*progp;\n\tunsigned int\t\ti;\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next) {\n\t\tfor (i = 0; i < progp->pg_nvers; i++) {\n\t\t\tif (progp->pg_vers[i] == NULL)\n\t\t\t\tcontinue;\n\t\t\tif (!progp->pg_vers[i]->vs_hidden)\n\t\t\t\treturn 1;\n\t\t}\n\t}\n\n\treturn 0;\n}\n\nint svc_bind(struct svc_serv *serv, struct net *net)\n{\n\tif (!svc_uses_rpcbind(serv))\n\t\treturn 0;\n\treturn svc_rpcb_setup(serv, net);\n}\nEXPORT_SYMBOL_GPL(svc_bind);\n\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\nstatic void\n__svc_init_bc(struct svc_serv *serv)\n{\n\tINIT_LIST_HEAD(&serv->sv_cb_list);\n\tspin_lock_init(&serv->sv_cb_lock);\n\tinit_waitqueue_head(&serv->sv_cb_waitq);\n}\n#else\nstatic void\n__svc_init_bc(struct svc_serv *serv)\n{\n}\n#endif\n\n/*\n * Create an RPC service\n */\nstatic struct svc_serv *\n__svc_create(struct svc_program *prog, unsigned int bufsize, int npools,\n\t     struct svc_serv_ops *ops)\n{\n\tstruct svc_serv\t*serv;\n\tunsigned int vers;\n\tunsigned int xdrsize;\n\tunsigned int i;\n\n\tif (!(serv = kzalloc(sizeof(*serv), GFP_KERNEL)))\n\t\treturn NULL;\n\tserv->sv_name      = prog->pg_name;\n\tserv->sv_program   = prog;\n\tserv->sv_nrthreads = 1;\n\tserv->sv_stats     = prog->pg_stats;\n\tif (bufsize > RPCSVC_MAXPAYLOAD)\n\t\tbufsize = RPCSVC_MAXPAYLOAD;\n\tserv->sv_max_payload = bufsize? bufsize : 4096;\n\tserv->sv_max_mesg  = roundup(serv->sv_max_payload + PAGE_SIZE, PAGE_SIZE);\n\tserv->sv_ops = ops;\n\txdrsize = 0;\n\twhile (prog) {\n\t\tprog->pg_lovers = prog->pg_nvers-1;\n\t\tfor (vers=0; vers<prog->pg_nvers ; vers++)\n\t\t\tif (prog->pg_vers[vers]) {\n\t\t\t\tprog->pg_hivers = vers;\n\t\t\t\tif (prog->pg_lovers > vers)\n\t\t\t\t\tprog->pg_lovers = vers;\n\t\t\t\tif (prog->pg_vers[vers]->vs_xdrsize > xdrsize)\n\t\t\t\t\txdrsize = prog->pg_vers[vers]->vs_xdrsize;\n\t\t\t}\n\t\tprog = prog->pg_next;\n\t}\n\tserv->sv_xdrsize   = xdrsize;\n\tINIT_LIST_HEAD(&serv->sv_tempsocks);\n\tINIT_LIST_HEAD(&serv->sv_permsocks);\n\tinit_timer(&serv->sv_temptimer);\n\tspin_lock_init(&serv->sv_lock);\n\n\t__svc_init_bc(serv);\n\n\tserv->sv_nrpools = npools;\n\tserv->sv_pools =\n\t\tkcalloc(serv->sv_nrpools, sizeof(struct svc_pool),\n\t\t\tGFP_KERNEL);\n\tif (!serv->sv_pools) {\n\t\tkfree(serv);\n\t\treturn NULL;\n\t}\n\n\tfor (i = 0; i < serv->sv_nrpools; i++) {\n\t\tstruct svc_pool *pool = &serv->sv_pools[i];\n\n\t\tdprintk(\"svc: initialising pool %u for %s\\n\",\n\t\t\t\ti, serv->sv_name);\n\n\t\tpool->sp_id = i;\n\t\tINIT_LIST_HEAD(&pool->sp_sockets);\n\t\tINIT_LIST_HEAD(&pool->sp_all_threads);\n\t\tspin_lock_init(&pool->sp_lock);\n\t}\n\n\treturn serv;\n}\n\nstruct svc_serv *\nsvc_create(struct svc_program *prog, unsigned int bufsize,\n\t   struct svc_serv_ops *ops)\n{\n\treturn __svc_create(prog, bufsize, /*npools*/1, ops);\n}\nEXPORT_SYMBOL_GPL(svc_create);\n\nstruct svc_serv *\nsvc_create_pooled(struct svc_program *prog, unsigned int bufsize,\n\t\t  struct svc_serv_ops *ops)\n{\n\tstruct svc_serv *serv;\n\tunsigned int npools = svc_pool_map_get();\n\n\tserv = __svc_create(prog, bufsize, npools, ops);\n\tif (!serv)\n\t\tgoto out_err;\n\treturn serv;\nout_err:\n\tsvc_pool_map_put();\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(svc_create_pooled);\n\nvoid svc_shutdown_net(struct svc_serv *serv, struct net *net)\n{\n\tsvc_close_net(serv, net);\n\n\tif (serv->sv_ops->svo_shutdown)\n\t\tserv->sv_ops->svo_shutdown(serv, net);\n}\nEXPORT_SYMBOL_GPL(svc_shutdown_net);\n\n/*\n * Destroy an RPC service. Should be called with appropriate locking to\n * protect the sv_nrthreads, sv_permsocks and sv_tempsocks.\n */\nvoid\nsvc_destroy(struct svc_serv *serv)\n{\n\tdprintk(\"svc: svc_destroy(%s, %d)\\n\",\n\t\t\t\tserv->sv_program->pg_name,\n\t\t\t\tserv->sv_nrthreads);\n\n\tif (serv->sv_nrthreads) {\n\t\tif (--(serv->sv_nrthreads) != 0) {\n\t\t\tsvc_sock_update_bufs(serv);\n\t\t\treturn;\n\t\t}\n\t} else\n\t\tprintk(\"svc_destroy: no threads for serv=%p!\\n\", serv);\n\n\tdel_timer_sync(&serv->sv_temptimer);\n\n\t/*\n\t * The last user is gone and thus all sockets have to be destroyed to\n\t * the point. Check this.\n\t */\n\tBUG_ON(!list_empty(&serv->sv_permsocks));\n\tBUG_ON(!list_empty(&serv->sv_tempsocks));\n\n\tcache_clean_deferred(serv);\n\n\tif (svc_serv_is_pooled(serv))\n\t\tsvc_pool_map_put();\n\n\tkfree(serv->sv_pools);\n\tkfree(serv);\n}\nEXPORT_SYMBOL_GPL(svc_destroy);\n\n/*\n * Allocate an RPC server's buffer space.\n * We allocate pages and place them in rq_argpages.\n */\nstatic int\nsvc_init_buffer(struct svc_rqst *rqstp, unsigned int size, int node)\n{\n\tunsigned int pages, arghi;\n\n\t/* bc_xprt uses fore channel allocated buffers */\n\tif (svc_is_backchannel(rqstp))\n\t\treturn 1;\n\n\tpages = size / PAGE_SIZE + 1; /* extra page as we hold both request and reply.\n\t\t\t\t       * We assume one is at most one page\n\t\t\t\t       */\n\targhi = 0;\n\tWARN_ON_ONCE(pages > RPCSVC_MAXPAGES);\n\tif (pages > RPCSVC_MAXPAGES)\n\t\tpages = RPCSVC_MAXPAGES;\n\twhile (pages) {\n\t\tstruct page *p = alloc_pages_node(node, GFP_KERNEL, 0);\n\t\tif (!p)\n\t\t\tbreak;\n\t\trqstp->rq_pages[arghi++] = p;\n\t\tpages--;\n\t}\n\treturn pages == 0;\n}\n\n/*\n * Release an RPC server buffer\n */\nstatic void\nsvc_release_buffer(struct svc_rqst *rqstp)\n{\n\tunsigned int i;\n\n\tfor (i = 0; i < ARRAY_SIZE(rqstp->rq_pages); i++)\n\t\tif (rqstp->rq_pages[i])\n\t\t\tput_page(rqstp->rq_pages[i]);\n}\n\nstruct svc_rqst *\nsvc_rqst_alloc(struct svc_serv *serv, struct svc_pool *pool, int node)\n{\n\tstruct svc_rqst\t*rqstp;\n\n\trqstp = kzalloc_node(sizeof(*rqstp), GFP_KERNEL, node);\n\tif (!rqstp)\n\t\treturn rqstp;\n\n\t__set_bit(RQ_BUSY, &rqstp->rq_flags);\n\tspin_lock_init(&rqstp->rq_lock);\n\trqstp->rq_server = serv;\n\trqstp->rq_pool = pool;\n\n\trqstp->rq_argp = kmalloc_node(serv->sv_xdrsize, GFP_KERNEL, node);\n\tif (!rqstp->rq_argp)\n\t\tgoto out_enomem;\n\n\trqstp->rq_resp = kmalloc_node(serv->sv_xdrsize, GFP_KERNEL, node);\n\tif (!rqstp->rq_resp)\n\t\tgoto out_enomem;\n\n\tif (!svc_init_buffer(rqstp, serv->sv_max_mesg, node))\n\t\tgoto out_enomem;\n\n\treturn rqstp;\nout_enomem:\n\tsvc_rqst_free(rqstp);\n\treturn NULL;\n}\nEXPORT_SYMBOL_GPL(svc_rqst_alloc);\n\nstruct svc_rqst *\nsvc_prepare_thread(struct svc_serv *serv, struct svc_pool *pool, int node)\n{\n\tstruct svc_rqst\t*rqstp;\n\n\trqstp = svc_rqst_alloc(serv, pool, node);\n\tif (!rqstp)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tserv->sv_nrthreads++;\n\tspin_lock_bh(&pool->sp_lock);\n\tpool->sp_nrthreads++;\n\tlist_add_rcu(&rqstp->rq_all, &pool->sp_all_threads);\n\tspin_unlock_bh(&pool->sp_lock);\n\treturn rqstp;\n}\nEXPORT_SYMBOL_GPL(svc_prepare_thread);\n\n/*\n * Choose a pool in which to create a new thread, for svc_set_num_threads\n */\nstatic inline struct svc_pool *\nchoose_pool(struct svc_serv *serv, struct svc_pool *pool, unsigned int *state)\n{\n\tif (pool != NULL)\n\t\treturn pool;\n\n\treturn &serv->sv_pools[(*state)++ % serv->sv_nrpools];\n}\n\n/*\n * Choose a thread to kill, for svc_set_num_threads\n */\nstatic inline struct task_struct *\nchoose_victim(struct svc_serv *serv, struct svc_pool *pool, unsigned int *state)\n{\n\tunsigned int i;\n\tstruct task_struct *task = NULL;\n\n\tif (pool != NULL) {\n\t\tspin_lock_bh(&pool->sp_lock);\n\t} else {\n\t\t/* choose a pool in round-robin fashion */\n\t\tfor (i = 0; i < serv->sv_nrpools; i++) {\n\t\t\tpool = &serv->sv_pools[--(*state) % serv->sv_nrpools];\n\t\t\tspin_lock_bh(&pool->sp_lock);\n\t\t\tif (!list_empty(&pool->sp_all_threads))\n\t\t\t\tgoto found_pool;\n\t\t\tspin_unlock_bh(&pool->sp_lock);\n\t\t}\n\t\treturn NULL;\n\t}\n\nfound_pool:\n\tif (!list_empty(&pool->sp_all_threads)) {\n\t\tstruct svc_rqst *rqstp;\n\n\t\t/*\n\t\t * Remove from the pool->sp_all_threads list\n\t\t * so we don't try to kill it again.\n\t\t */\n\t\trqstp = list_entry(pool->sp_all_threads.next, struct svc_rqst, rq_all);\n\t\tset_bit(RQ_VICTIM, &rqstp->rq_flags);\n\t\tlist_del_rcu(&rqstp->rq_all);\n\t\ttask = rqstp->rq_task;\n\t}\n\tspin_unlock_bh(&pool->sp_lock);\n\n\treturn task;\n}\n\n/* create new threads */\nstatic int\nsvc_start_kthreads(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n{\n\tstruct svc_rqst\t*rqstp;\n\tstruct task_struct *task;\n\tstruct svc_pool *chosen_pool;\n\tunsigned int state = serv->sv_nrthreads-1;\n\tint node;\n\n\tdo {\n\t\tnrservs--;\n\t\tchosen_pool = choose_pool(serv, pool, &state);\n\n\t\tnode = svc_pool_map_get_node(chosen_pool->sp_id);\n\t\trqstp = svc_prepare_thread(serv, chosen_pool, node);\n\t\tif (IS_ERR(rqstp))\n\t\t\treturn PTR_ERR(rqstp);\n\n\t\t__module_get(serv->sv_ops->svo_module);\n\t\ttask = kthread_create_on_node(serv->sv_ops->svo_function, rqstp,\n\t\t\t\t\t      node, \"%s\", serv->sv_name);\n\t\tif (IS_ERR(task)) {\n\t\t\tmodule_put(serv->sv_ops->svo_module);\n\t\t\tsvc_exit_thread(rqstp);\n\t\t\treturn PTR_ERR(task);\n\t\t}\n\n\t\trqstp->rq_task = task;\n\t\tif (serv->sv_nrpools > 1)\n\t\t\tsvc_pool_map_set_cpumask(task, chosen_pool->sp_id);\n\n\t\tsvc_sock_update_bufs(serv);\n\t\twake_up_process(task);\n\t} while (nrservs > 0);\n\n\treturn 0;\n}\n\n\n/* destroy old threads */\nstatic int\nsvc_signal_kthreads(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n{\n\tstruct task_struct *task;\n\tunsigned int state = serv->sv_nrthreads-1;\n\n\t/* destroy old threads */\n\tdo {\n\t\ttask = choose_victim(serv, pool, &state);\n\t\tif (task == NULL)\n\t\t\tbreak;\n\t\tsend_sig(SIGINT, task, 1);\n\t\tnrservs++;\n\t} while (nrservs < 0);\n\n\treturn 0;\n}\n\n/*\n * Create or destroy enough new threads to make the number\n * of threads the given number.  If `pool' is non-NULL, applies\n * only to threads in that pool, otherwise round-robins between\n * all pools.  Caller must ensure that mutual exclusion between this and\n * server startup or shutdown.\n *\n * Destroying threads relies on the service threads filling in\n * rqstp->rq_task, which only the nfs ones do.  Assumes the serv\n * has been created using svc_create_pooled().\n *\n * Based on code that used to be in nfsd_svc() but tweaked\n * to be pool-aware.\n */\nint\nsvc_set_num_threads(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n{\n\tif (pool == NULL) {\n\t\t/* The -1 assumes caller has done a svc_get() */\n\t\tnrservs -= (serv->sv_nrthreads-1);\n\t} else {\n\t\tspin_lock_bh(&pool->sp_lock);\n\t\tnrservs -= pool->sp_nrthreads;\n\t\tspin_unlock_bh(&pool->sp_lock);\n\t}\n\n\tif (nrservs > 0)\n\t\treturn svc_start_kthreads(serv, pool, nrservs);\n\tif (nrservs < 0)\n\t\treturn svc_signal_kthreads(serv, pool, nrservs);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(svc_set_num_threads);\n\n/* destroy old threads */\nstatic int\nsvc_stop_kthreads(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n{\n\tstruct task_struct *task;\n\tunsigned int state = serv->sv_nrthreads-1;\n\n\t/* destroy old threads */\n\tdo {\n\t\ttask = choose_victim(serv, pool, &state);\n\t\tif (task == NULL)\n\t\t\tbreak;\n\t\tkthread_stop(task);\n\t\tnrservs++;\n\t} while (nrservs < 0);\n\treturn 0;\n}\n\nint\nsvc_set_num_threads_sync(struct svc_serv *serv, struct svc_pool *pool, int nrservs)\n{\n\tif (pool == NULL) {\n\t\t/* The -1 assumes caller has done a svc_get() */\n\t\tnrservs -= (serv->sv_nrthreads-1);\n\t} else {\n\t\tspin_lock_bh(&pool->sp_lock);\n\t\tnrservs -= pool->sp_nrthreads;\n\t\tspin_unlock_bh(&pool->sp_lock);\n\t}\n\n\tif (nrservs > 0)\n\t\treturn svc_start_kthreads(serv, pool, nrservs);\n\tif (nrservs < 0)\n\t\treturn svc_stop_kthreads(serv, pool, nrservs);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(svc_set_num_threads_sync);\n\n/*\n * Called from a server thread as it's exiting. Caller must hold the \"service\n * mutex\" for the service.\n */\nvoid\nsvc_rqst_free(struct svc_rqst *rqstp)\n{\n\tsvc_release_buffer(rqstp);\n\tkfree(rqstp->rq_resp);\n\tkfree(rqstp->rq_argp);\n\tkfree(rqstp->rq_auth_data);\n\tkfree_rcu(rqstp, rq_rcu_head);\n}\nEXPORT_SYMBOL_GPL(svc_rqst_free);\n\nvoid\nsvc_exit_thread(struct svc_rqst *rqstp)\n{\n\tstruct svc_serv\t*serv = rqstp->rq_server;\n\tstruct svc_pool\t*pool = rqstp->rq_pool;\n\n\tspin_lock_bh(&pool->sp_lock);\n\tpool->sp_nrthreads--;\n\tif (!test_and_set_bit(RQ_VICTIM, &rqstp->rq_flags))\n\t\tlist_del_rcu(&rqstp->rq_all);\n\tspin_unlock_bh(&pool->sp_lock);\n\n\tsvc_rqst_free(rqstp);\n\n\t/* Release the server */\n\tif (serv)\n\t\tsvc_destroy(serv);\n}\nEXPORT_SYMBOL_GPL(svc_exit_thread);\n\n/*\n * Register an \"inet\" protocol family netid with the local\n * rpcbind daemon via an rpcbind v4 SET request.\n *\n * No netconfig infrastructure is available in the kernel, so\n * we map IP_ protocol numbers to netids by hand.\n *\n * Returns zero on success; a negative errno value is returned\n * if any error occurs.\n */\nstatic int __svc_rpcb_register4(struct net *net, const u32 program,\n\t\t\t\tconst u32 version,\n\t\t\t\tconst unsigned short protocol,\n\t\t\t\tconst unsigned short port)\n{\n\tconst struct sockaddr_in sin = {\n\t\t.sin_family\t\t= AF_INET,\n\t\t.sin_addr.s_addr\t= htonl(INADDR_ANY),\n\t\t.sin_port\t\t= htons(port),\n\t};\n\tconst char *netid;\n\tint error;\n\n\tswitch (protocol) {\n\tcase IPPROTO_UDP:\n\t\tnetid = RPCBIND_NETID_UDP;\n\t\tbreak;\n\tcase IPPROTO_TCP:\n\t\tnetid = RPCBIND_NETID_TCP;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\terror = rpcb_v4_register(net, program, version,\n\t\t\t\t\t(const struct sockaddr *)&sin, netid);\n\n\t/*\n\t * User space didn't support rpcbind v4, so retry this\n\t * registration request with the legacy rpcbind v2 protocol.\n\t */\n\tif (error == -EPROTONOSUPPORT)\n\t\terror = rpcb_register(net, program, version, protocol, port);\n\n\treturn error;\n}\n\n#if IS_ENABLED(CONFIG_IPV6)\n/*\n * Register an \"inet6\" protocol family netid with the local\n * rpcbind daemon via an rpcbind v4 SET request.\n *\n * No netconfig infrastructure is available in the kernel, so\n * we map IP_ protocol numbers to netids by hand.\n *\n * Returns zero on success; a negative errno value is returned\n * if any error occurs.\n */\nstatic int __svc_rpcb_register6(struct net *net, const u32 program,\n\t\t\t\tconst u32 version,\n\t\t\t\tconst unsigned short protocol,\n\t\t\t\tconst unsigned short port)\n{\n\tconst struct sockaddr_in6 sin6 = {\n\t\t.sin6_family\t\t= AF_INET6,\n\t\t.sin6_addr\t\t= IN6ADDR_ANY_INIT,\n\t\t.sin6_port\t\t= htons(port),\n\t};\n\tconst char *netid;\n\tint error;\n\n\tswitch (protocol) {\n\tcase IPPROTO_UDP:\n\t\tnetid = RPCBIND_NETID_UDP6;\n\t\tbreak;\n\tcase IPPROTO_TCP:\n\t\tnetid = RPCBIND_NETID_TCP6;\n\t\tbreak;\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n\n\terror = rpcb_v4_register(net, program, version,\n\t\t\t\t\t(const struct sockaddr *)&sin6, netid);\n\n\t/*\n\t * User space didn't support rpcbind version 4, so we won't\n\t * use a PF_INET6 listener.\n\t */\n\tif (error == -EPROTONOSUPPORT)\n\t\terror = -EAFNOSUPPORT;\n\n\treturn error;\n}\n#endif\t/* IS_ENABLED(CONFIG_IPV6) */\n\n/*\n * Register a kernel RPC service via rpcbind version 4.\n *\n * Returns zero on success; a negative errno value is returned\n * if any error occurs.\n */\nstatic int __svc_register(struct net *net, const char *progname,\n\t\t\t  const u32 program, const u32 version,\n\t\t\t  const int family,\n\t\t\t  const unsigned short protocol,\n\t\t\t  const unsigned short port)\n{\n\tint error = -EAFNOSUPPORT;\n\n\tswitch (family) {\n\tcase PF_INET:\n\t\terror = __svc_rpcb_register4(net, program, version,\n\t\t\t\t\t\tprotocol, port);\n\t\tbreak;\n#if IS_ENABLED(CONFIG_IPV6)\n\tcase PF_INET6:\n\t\terror = __svc_rpcb_register6(net, program, version,\n\t\t\t\t\t\tprotocol, port);\n#endif\n\t}\n\n\treturn error;\n}\n\n/**\n * svc_register - register an RPC service with the local portmapper\n * @serv: svc_serv struct for the service to register\n * @net: net namespace for the service to register\n * @family: protocol family of service's listener socket\n * @proto: transport protocol number to advertise\n * @port: port to advertise\n *\n * Service is registered for any address in the passed-in protocol family\n */\nint svc_register(const struct svc_serv *serv, struct net *net,\n\t\t const int family, const unsigned short proto,\n\t\t const unsigned short port)\n{\n\tstruct svc_program\t*progp;\n\tstruct svc_version\t*vers;\n\tunsigned int\t\ti;\n\tint\t\t\terror = 0;\n\n\tWARN_ON_ONCE(proto == 0 && port == 0);\n\tif (proto == 0 && port == 0)\n\t\treturn -EINVAL;\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next) {\n\t\tfor (i = 0; i < progp->pg_nvers; i++) {\n\t\t\tvers = progp->pg_vers[i];\n\t\t\tif (vers == NULL)\n\t\t\t\tcontinue;\n\n\t\t\tdprintk(\"svc: svc_register(%sv%d, %s, %u, %u)%s\\n\",\n\t\t\t\t\tprogp->pg_name,\n\t\t\t\t\ti,\n\t\t\t\t\tproto == IPPROTO_UDP?  \"udp\" : \"tcp\",\n\t\t\t\t\tport,\n\t\t\t\t\tfamily,\n\t\t\t\t\tvers->vs_hidden ?\n\t\t\t\t\t\" (but not telling portmap)\" : \"\");\n\n\t\t\tif (vers->vs_hidden)\n\t\t\t\tcontinue;\n\n\t\t\t/*\n\t\t\t * Don't register a UDP port if we need congestion\n\t\t\t * control.\n\t\t\t */\n\t\t\tif (vers->vs_need_cong_ctrl && proto == IPPROTO_UDP)\n\t\t\t\tcontinue;\n\n\t\t\terror = __svc_register(net, progp->pg_name, progp->pg_prog,\n\t\t\t\t\t\ti, family, proto, port);\n\n\t\t\tif (vers->vs_rpcb_optnl) {\n\t\t\t\terror = 0;\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (error < 0) {\n\t\t\t\tprintk(KERN_WARNING \"svc: failed to register \"\n\t\t\t\t\t\"%sv%u RPC service (errno %d).\\n\",\n\t\t\t\t\tprogp->pg_name, i, -error);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn error;\n}\n\n/*\n * If user space is running rpcbind, it should take the v4 UNSET\n * and clear everything for this [program, version].  If user space\n * is running portmap, it will reject the v4 UNSET, but won't have\n * any \"inet6\" entries anyway.  So a PMAP_UNSET should be sufficient\n * in this case to clear all existing entries for [program, version].\n */\nstatic void __svc_unregister(struct net *net, const u32 program, const u32 version,\n\t\t\t     const char *progname)\n{\n\tint error;\n\n\terror = rpcb_v4_register(net, program, version, NULL, \"\");\n\n\t/*\n\t * User space didn't support rpcbind v4, so retry this\n\t * request with the legacy rpcbind v2 protocol.\n\t */\n\tif (error == -EPROTONOSUPPORT)\n\t\terror = rpcb_register(net, program, version, 0, 0);\n\n\tdprintk(\"svc: %s(%sv%u), error %d\\n\",\n\t\t\t__func__, progname, version, error);\n}\n\n/*\n * All netids, bind addresses and ports registered for [program, version]\n * are removed from the local rpcbind database (if the service is not\n * hidden) to make way for a new instance of the service.\n *\n * The result of unregistration is reported via dprintk for those who want\n * verification of the result, but is otherwise not important.\n */\nstatic void svc_unregister(const struct svc_serv *serv, struct net *net)\n{\n\tstruct svc_program *progp;\n\tunsigned long flags;\n\tunsigned int i;\n\n\tclear_thread_flag(TIF_SIGPENDING);\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next) {\n\t\tfor (i = 0; i < progp->pg_nvers; i++) {\n\t\t\tif (progp->pg_vers[i] == NULL)\n\t\t\t\tcontinue;\n\t\t\tif (progp->pg_vers[i]->vs_hidden)\n\t\t\t\tcontinue;\n\n\t\t\tdprintk(\"svc: attempting to unregister %sv%u\\n\",\n\t\t\t\tprogp->pg_name, i);\n\t\t\t__svc_unregister(net, progp->pg_prog, i, progp->pg_name);\n\t\t}\n\t}\n\n\tspin_lock_irqsave(&current->sighand->siglock, flags);\n\trecalc_sigpending();\n\tspin_unlock_irqrestore(&current->sighand->siglock, flags);\n}\n\n/*\n * dprintk the given error with the address of the client that caused it.\n */\n#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)\nstatic __printf(2, 3)\nvoid svc_printk(struct svc_rqst *rqstp, const char *fmt, ...)\n{\n\tstruct va_format vaf;\n\tva_list args;\n\tchar \tbuf[RPC_MAX_ADDRBUFLEN];\n\n\tva_start(args, fmt);\n\n\tvaf.fmt = fmt;\n\tvaf.va = &args;\n\n\tdprintk(\"svc: %s: %pV\", svc_print_addr(rqstp, buf, sizeof(buf)), &vaf);\n\n\tva_end(args);\n}\n#else\nstatic __printf(2,3) void svc_printk(struct svc_rqst *rqstp, const char *fmt, ...) {}\n#endif\n\n/*\n * Common routine for processing the RPC request.\n */\nstatic int\nsvc_process_common(struct svc_rqst *rqstp, struct kvec *argv, struct kvec *resv)\n{\n\tstruct svc_program\t*progp;\n\tstruct svc_version\t*versp = NULL;\t/* compiler food */\n\tstruct svc_procedure\t*procp = NULL;\n\tstruct svc_serv\t\t*serv = rqstp->rq_server;\n\tkxdrproc_t\t\txdr;\n\t__be32\t\t\t*statp;\n\tu32\t\t\tprog, vers, proc;\n\t__be32\t\t\tauth_stat, rpc_stat;\n\tint\t\t\tauth_res;\n\t__be32\t\t\t*reply_statp;\n\n\trpc_stat = rpc_success;\n\n\tif (argv->iov_len < 6*4)\n\t\tgoto err_short_len;\n\n\t/* Will be turned off only in gss privacy case: */\n\tset_bit(RQ_SPLICE_OK, &rqstp->rq_flags);\n\t/* Will be turned off only when NFSv4 Sessions are used */\n\tset_bit(RQ_USEDEFERRAL, &rqstp->rq_flags);\n\tclear_bit(RQ_DROPME, &rqstp->rq_flags);\n\n\t/* Setup reply header */\n\trqstp->rq_xprt->xpt_ops->xpo_prep_reply_hdr(rqstp);\n\n\tsvc_putu32(resv, rqstp->rq_xid);\n\n\tvers = svc_getnl(argv);\n\n\t/* First words of reply: */\n\tsvc_putnl(resv, 1);\t\t/* REPLY */\n\n\tif (vers != 2)\t\t/* RPC version number */\n\t\tgoto err_bad_rpc;\n\n\t/* Save position in case we later decide to reject: */\n\treply_statp = resv->iov_base + resv->iov_len;\n\n\tsvc_putnl(resv, 0);\t\t/* ACCEPT */\n\n\trqstp->rq_prog = prog = svc_getnl(argv);\t/* program number */\n\trqstp->rq_vers = vers = svc_getnl(argv);\t/* version number */\n\trqstp->rq_proc = proc = svc_getnl(argv);\t/* procedure number */\n\n\tfor (progp = serv->sv_program; progp; progp = progp->pg_next)\n\t\tif (prog == progp->pg_prog)\n\t\t\tbreak;\n\n\t/*\n\t * Decode auth data, and add verifier to reply buffer.\n\t * We do this before anything else in order to get a decent\n\t * auth verifier.\n\t */\n\tauth_res = svc_authenticate(rqstp, &auth_stat);\n\t/* Also give the program a chance to reject this call: */\n\tif (auth_res == SVC_OK && progp) {\n\t\tauth_stat = rpc_autherr_badcred;\n\t\tauth_res = progp->pg_authenticate(rqstp);\n\t}\n\tswitch (auth_res) {\n\tcase SVC_OK:\n\t\tbreak;\n\tcase SVC_GARBAGE:\n\t\tgoto err_garbage;\n\tcase SVC_SYSERR:\n\t\trpc_stat = rpc_system_err;\n\t\tgoto err_bad;\n\tcase SVC_DENIED:\n\t\tgoto err_bad_auth;\n\tcase SVC_CLOSE:\n\t\tgoto close;\n\tcase SVC_DROP:\n\t\tgoto dropit;\n\tcase SVC_COMPLETE:\n\t\tgoto sendit;\n\t}\n\n\tif (progp == NULL)\n\t\tgoto err_bad_prog;\n\n\tif (vers >= progp->pg_nvers ||\n\t  !(versp = progp->pg_vers[vers]))\n\t\tgoto err_bad_vers;\n\n\t/*\n\t * Some protocol versions (namely NFSv4) require some form of\n\t * congestion control.  (See RFC 7530 section 3.1 paragraph 2)\n\t * In other words, UDP is not allowed. We mark those when setting\n\t * up the svc_xprt, and verify that here.\n\t *\n\t * The spec is not very clear about what error should be returned\n\t * when someone tries to access a server that is listening on UDP\n\t * for lower versions. RPC_PROG_MISMATCH seems to be the closest\n\t * fit.\n\t */\n\tif (versp->vs_need_cong_ctrl &&\n\t    !test_bit(XPT_CONG_CTRL, &rqstp->rq_xprt->xpt_flags))\n\t\tgoto err_bad_vers;\n\n\tprocp = versp->vs_proc + proc;\n\tif (proc >= versp->vs_nproc || !procp->pc_func)\n\t\tgoto err_bad_proc;\n\trqstp->rq_procinfo = procp;\n\n\t/* Syntactic check complete */\n\tserv->sv_stats->rpccnt++;\n\n\t/* Build the reply header. */\n\tstatp = resv->iov_base +resv->iov_len;\n\tsvc_putnl(resv, RPC_SUCCESS);\n\n\t/* Bump per-procedure stats counter */\n\tprocp->pc_count++;\n\n\t/* Initialize storage for argp and resp */\n\tmemset(rqstp->rq_argp, 0, procp->pc_argsize);\n\tmemset(rqstp->rq_resp, 0, procp->pc_ressize);\n\n\t/* un-reserve some of the out-queue now that we have a\n\t * better idea of reply size\n\t */\n\tif (procp->pc_xdrressize)\n\t\tsvc_reserve_auth(rqstp, procp->pc_xdrressize<<2);\n\n\t/* Call the function that processes the request. */\n\tif (!versp->vs_dispatch) {\n\t\t/* Decode arguments */\n\t\txdr = procp->pc_decode;\n\t\tif (xdr && !xdr(rqstp, argv->iov_base, rqstp->rq_argp))\n\t\t\tgoto err_garbage;\n\n\t\t*statp = procp->pc_func(rqstp, rqstp->rq_argp, rqstp->rq_resp);\n\n\t\t/* Encode reply */\n\t\tif (*statp == rpc_drop_reply ||\n\t\t    test_bit(RQ_DROPME, &rqstp->rq_flags)) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp, NULL, rqstp->rq_resp);\n\t\t\tgoto dropit;\n\t\t}\n\t\tif (*statp == rpc_autherr_badcred) {\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp, NULL, rqstp->rq_resp);\n\t\t\tgoto err_bad_auth;\n\t\t}\n\t\tif (*statp == rpc_success &&\n\t\t    (xdr = procp->pc_encode) &&\n\t\t    !xdr(rqstp, resv->iov_base+resv->iov_len, rqstp->rq_resp)) {\n\t\t\tdprintk(\"svc: failed to encode reply\\n\");\n\t\t\t/* serv->sv_stats->rpcsystemerr++; */\n\t\t\t*statp = rpc_system_err;\n\t\t}\n\t} else {\n\t\tdprintk(\"svc: calling dispatcher\\n\");\n\t\tif (!versp->vs_dispatch(rqstp, statp)) {\n\t\t\t/* Release reply info */\n\t\t\tif (procp->pc_release)\n\t\t\t\tprocp->pc_release(rqstp, NULL, rqstp->rq_resp);\n\t\t\tgoto dropit;\n\t\t}\n\t}\n\n\t/* Check RPC status result */\n\tif (*statp != rpc_success)\n\t\tresv->iov_len = ((void*)statp)  - resv->iov_base + 4;\n\n\t/* Release reply info */\n\tif (procp->pc_release)\n\t\tprocp->pc_release(rqstp, NULL, rqstp->rq_resp);\n\n\tif (procp->pc_encode == NULL)\n\t\tgoto dropit;\n\n sendit:\n\tif (svc_authorise(rqstp))\n\t\tgoto close;\n\treturn 1;\t\t/* Caller can now send it */\n\n dropit:\n\tsvc_authorise(rqstp);\t/* doesn't hurt to call this twice */\n\tdprintk(\"svc: svc_process dropit\\n\");\n\treturn 0;\n\n close:\n\tif (test_bit(XPT_TEMP, &rqstp->rq_xprt->xpt_flags))\n\t\tsvc_close_xprt(rqstp->rq_xprt);\n\tdprintk(\"svc: svc_process close\\n\");\n\treturn 0;\n\nerr_short_len:\n\tsvc_printk(rqstp, \"short len %zd, dropping request\\n\",\n\t\t\targv->iov_len);\n\tgoto close;\n\nerr_bad_rpc:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 0);\t/* RPC_MISMATCH */\n\tsvc_putnl(resv, 2);\t/* Only RPCv2 supported */\n\tsvc_putnl(resv, 2);\n\tgoto sendit;\n\nerr_bad_auth:\n\tdprintk(\"svc: authentication failed (%d)\\n\", ntohl(auth_stat));\n\tserv->sv_stats->rpcbadauth++;\n\t/* Restore write pointer to location of accept status: */\n\txdr_ressize_check(rqstp, reply_statp);\n\tsvc_putnl(resv, 1);\t/* REJECT */\n\tsvc_putnl(resv, 1);\t/* AUTH_ERROR */\n\tsvc_putnl(resv, ntohl(auth_stat));\t/* status */\n\tgoto sendit;\n\nerr_bad_prog:\n\tdprintk(\"svc: unknown program %d\\n\", prog);\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_UNAVAIL);\n\tgoto sendit;\n\nerr_bad_vers:\n\tsvc_printk(rqstp, \"unknown version (%d for prog %d, %s)\\n\",\n\t\t       vers, prog, progp->pg_name);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROG_MISMATCH);\n\tsvc_putnl(resv, progp->pg_lovers);\n\tsvc_putnl(resv, progp->pg_hivers);\n\tgoto sendit;\n\nerr_bad_proc:\n\tsvc_printk(rqstp, \"unknown procedure (%d)\\n\", proc);\n\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, RPC_PROC_UNAVAIL);\n\tgoto sendit;\n\nerr_garbage:\n\tsvc_printk(rqstp, \"failed to decode args\\n\");\n\n\trpc_stat = rpc_garbage_args;\nerr_bad:\n\tserv->sv_stats->rpcbadfmt++;\n\tsvc_putnl(resv, ntohl(rpc_stat));\n\tgoto sendit;\n}\n\n/*\n * Process the RPC request.\n */\nint\nsvc_process(struct svc_rqst *rqstp)\n{\n\tstruct kvec\t\t*argv = &rqstp->rq_arg.head[0];\n\tstruct kvec\t\t*resv = &rqstp->rq_res.head[0];\n\tstruct svc_serv\t\t*serv = rqstp->rq_server;\n\tu32\t\t\tdir;\n\n\t/*\n\t * Setup response xdr_buf.\n\t * Initially it has just one page\n\t */\n\trqstp->rq_next_page = &rqstp->rq_respages[1];\n\tresv->iov_base = page_address(rqstp->rq_respages[0]);\n\tresv->iov_len = 0;\n\trqstp->rq_res.pages = rqstp->rq_respages + 1;\n\trqstp->rq_res.len = 0;\n\trqstp->rq_res.page_base = 0;\n\trqstp->rq_res.page_len = 0;\n\trqstp->rq_res.buflen = PAGE_SIZE;\n\trqstp->rq_res.tail[0].iov_base = NULL;\n\trqstp->rq_res.tail[0].iov_len = 0;\n\n\tdir  = svc_getnl(argv);\n\tif (dir != 0) {\n\t\t/* direction != CALL */\n\t\tsvc_printk(rqstp, \"bad direction %d, dropping request\\n\", dir);\n\t\tserv->sv_stats->rpcbadfmt++;\n\t\tgoto out_drop;\n\t}\n\n\t/* Returns 1 for send, 0 for drop */\n\tif (likely(svc_process_common(rqstp, argv, resv))) {\n\t\tint ret = svc_send(rqstp);\n\n\t\ttrace_svc_process(rqstp, ret);\n\t\treturn ret;\n\t}\nout_drop:\n\ttrace_svc_process(rqstp, 0);\n\tsvc_drop(rqstp);\n\treturn 0;\n}\nEXPORT_SYMBOL_GPL(svc_process);\n\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\n/*\n * Process a backchannel RPC request that arrived over an existing\n * outbound connection\n */\nint\nbc_svc_process(struct svc_serv *serv, struct rpc_rqst *req,\n\t       struct svc_rqst *rqstp)\n{\n\tstruct kvec\t*argv = &rqstp->rq_arg.head[0];\n\tstruct kvec\t*resv = &rqstp->rq_res.head[0];\n\tstruct rpc_task *task;\n\tint proc_error;\n\tint error;\n\n\tdprintk(\"svc: %s(%p)\\n\", __func__, req);\n\n\t/* Build the svc_rqst used by the common processing routine */\n\trqstp->rq_xprt = serv->sv_bc_xprt;\n\trqstp->rq_xid = req->rq_xid;\n\trqstp->rq_prot = req->rq_xprt->prot;\n\trqstp->rq_server = serv;\n\n\trqstp->rq_addrlen = sizeof(req->rq_xprt->addr);\n\tmemcpy(&rqstp->rq_addr, &req->rq_xprt->addr, rqstp->rq_addrlen);\n\tmemcpy(&rqstp->rq_arg, &req->rq_rcv_buf, sizeof(rqstp->rq_arg));\n\tmemcpy(&rqstp->rq_res, &req->rq_snd_buf, sizeof(rqstp->rq_res));\n\n\t/* Adjust the argument buffer length */\n\trqstp->rq_arg.len = req->rq_private_buf.len;\n\tif (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len) {\n\t\trqstp->rq_arg.head[0].iov_len = rqstp->rq_arg.len;\n\t\trqstp->rq_arg.page_len = 0;\n\t} else if (rqstp->rq_arg.len <= rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len)\n\t\trqstp->rq_arg.page_len = rqstp->rq_arg.len -\n\t\t\trqstp->rq_arg.head[0].iov_len;\n\telse\n\t\trqstp->rq_arg.len = rqstp->rq_arg.head[0].iov_len +\n\t\t\trqstp->rq_arg.page_len;\n\n\t/* reset result send buffer \"put\" position */\n\tresv->iov_len = 0;\n\n\t/*\n\t * Skip the next two words because they've already been\n\t * processed in the transport\n\t */\n\tsvc_getu32(argv);\t/* XID */\n\tsvc_getnl(argv);\t/* CALLDIR */\n\n\t/* Parse and execute the bc call */\n\tproc_error = svc_process_common(rqstp, argv, resv);\n\n\tatomic_inc(&req->rq_xprt->bc_free_slots);\n\tif (!proc_error) {\n\t\t/* Processing error: drop the request */\n\t\txprt_free_bc_request(req);\n\t\treturn 0;\n\t}\n\n\t/* Finally, send the reply synchronously */\n\tmemcpy(&req->rq_snd_buf, &rqstp->rq_res, sizeof(req->rq_snd_buf));\n\ttask = rpc_run_bc_task(req);\n\tif (IS_ERR(task)) {\n\t\terror = PTR_ERR(task);\n\t\tgoto out;\n\t}\n\n\tWARN_ON_ONCE(atomic_read(&task->tk_count) != 1);\n\terror = task->tk_status;\n\trpc_put_task(task);\n\nout:\n\tdprintk(\"svc: %s(), error=%d\\n\", __func__, error);\n\treturn error;\n}\nEXPORT_SYMBOL_GPL(bc_svc_process);\n#endif /* CONFIG_SUNRPC_BACKCHANNEL */\n\n/*\n * Return (transport-specific) limit on the rpc payload.\n */\nu32 svc_max_payload(const struct svc_rqst *rqstp)\n{\n\tu32 max = rqstp->rq_xprt->xpt_class->xcl_max_payload;\n\n\tif (rqstp->rq_server->sv_max_payload < max)\n\t\tmax = rqstp->rq_server->sv_max_payload;\n\treturn max;\n}\nEXPORT_SYMBOL_GPL(svc_max_payload);\n", "obj-$(CONFIG_SUNRPC_XPRT_RDMA) += rpcrdma.o\n\nrpcrdma-y := transport.o rpc_rdma.o verbs.o \\\n\tfmr_ops.o frwr_ops.o \\\n\tsvc_rdma.o svc_rdma_backchannel.o svc_rdma_transport.o \\\n\tsvc_rdma_marshal.o svc_rdma_sendto.o svc_rdma_recvfrom.o \\\n\tsvc_rdma_rw.o module.o\nrpcrdma-$(CONFIG_SUNRPC_BACKCHANNEL) += backchannel.o\n", "/*\n * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n#include <linux/slab.h>\n#include <linux/fs.h>\n#include <linux/sysctl.h>\n#include <linux/workqueue.h>\n#include <linux/sunrpc/clnt.h>\n#include <linux/sunrpc/sched.h>\n#include <linux/sunrpc/svc_rdma.h>\n#include \"xprt_rdma.h\"\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\n/* RPC/RDMA parameters */\nunsigned int svcrdma_ord = RPCRDMA_ORD;\nstatic unsigned int min_ord = 1;\nstatic unsigned int max_ord = 4096;\nunsigned int svcrdma_max_requests = RPCRDMA_MAX_REQUESTS;\nunsigned int svcrdma_max_bc_requests = RPCRDMA_MAX_BC_REQUESTS;\nstatic unsigned int min_max_requests = 4;\nstatic unsigned int max_max_requests = 16384;\nunsigned int svcrdma_max_req_size = RPCRDMA_DEF_INLINE_THRESH;\nstatic unsigned int min_max_inline = RPCRDMA_DEF_INLINE_THRESH;\nstatic unsigned int max_max_inline = RPCRDMA_MAX_INLINE_THRESH;\n\natomic_t rdma_stat_recv;\natomic_t rdma_stat_read;\natomic_t rdma_stat_write;\natomic_t rdma_stat_sq_starve;\natomic_t rdma_stat_rq_starve;\natomic_t rdma_stat_rq_poll;\natomic_t rdma_stat_rq_prod;\natomic_t rdma_stat_sq_poll;\natomic_t rdma_stat_sq_prod;\n\nstruct workqueue_struct *svc_rdma_wq;\n\n/*\n * This function implements reading and resetting an atomic_t stat\n * variable through read/write to a proc file. Any write to the file\n * resets the associated statistic to zero. Any read returns it's\n * current value.\n */\nstatic int read_reset_stat(struct ctl_table *table, int write,\n\t\t\t   void __user *buffer, size_t *lenp,\n\t\t\t   loff_t *ppos)\n{\n\tatomic_t *stat = (atomic_t *)table->data;\n\n\tif (!stat)\n\t\treturn -EINVAL;\n\n\tif (write)\n\t\tatomic_set(stat, 0);\n\telse {\n\t\tchar str_buf[32];\n\t\tchar *data;\n\t\tint len = snprintf(str_buf, 32, \"%d\\n\", atomic_read(stat));\n\t\tif (len >= 32)\n\t\t\treturn -EFAULT;\n\t\tlen = strlen(str_buf);\n\t\tif (*ppos > len) {\n\t\t\t*lenp = 0;\n\t\t\treturn 0;\n\t\t}\n\t\tdata = &str_buf[*ppos];\n\t\tlen -= *ppos;\n\t\tif (len > *lenp)\n\t\t\tlen = *lenp;\n\t\tif (len && copy_to_user(buffer, str_buf, len))\n\t\t\treturn -EFAULT;\n\t\t*lenp = len;\n\t\t*ppos += len;\n\t}\n\treturn 0;\n}\n\nstatic struct ctl_table_header *svcrdma_table_header;\nstatic struct ctl_table svcrdma_parm_table[] = {\n\t{\n\t\t.procname\t= \"max_requests\",\n\t\t.data\t\t= &svcrdma_max_requests,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_max_requests,\n\t\t.extra2\t\t= &max_max_requests\n\t},\n\t{\n\t\t.procname\t= \"max_req_size\",\n\t\t.data\t\t= &svcrdma_max_req_size,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_max_inline,\n\t\t.extra2\t\t= &max_max_inline\n\t},\n\t{\n\t\t.procname\t= \"max_outbound_read_requests\",\n\t\t.data\t\t= &svcrdma_ord,\n\t\t.maxlen\t\t= sizeof(unsigned int),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= proc_dointvec_minmax,\n\t\t.extra1\t\t= &min_ord,\n\t\t.extra2\t\t= &max_ord,\n\t},\n\n\t{\n\t\t.procname\t= \"rdma_stat_read\",\n\t\t.data\t\t= &rdma_stat_read,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_recv\",\n\t\t.data\t\t= &rdma_stat_recv,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_write\",\n\t\t.data\t\t= &rdma_stat_write,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_sq_starve\",\n\t\t.data\t\t= &rdma_stat_sq_starve,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_rq_starve\",\n\t\t.data\t\t= &rdma_stat_rq_starve,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_rq_poll\",\n\t\t.data\t\t= &rdma_stat_rq_poll,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_rq_prod\",\n\t\t.data\t\t= &rdma_stat_rq_prod,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_sq_poll\",\n\t\t.data\t\t= &rdma_stat_sq_poll,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{\n\t\t.procname\t= \"rdma_stat_sq_prod\",\n\t\t.data\t\t= &rdma_stat_sq_prod,\n\t\t.maxlen\t\t= sizeof(atomic_t),\n\t\t.mode\t\t= 0644,\n\t\t.proc_handler\t= read_reset_stat,\n\t},\n\t{ },\n};\n\nstatic struct ctl_table svcrdma_table[] = {\n\t{\n\t\t.procname\t= \"svc_rdma\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= svcrdma_parm_table\n\t},\n\t{ },\n};\n\nstatic struct ctl_table svcrdma_root_table[] = {\n\t{\n\t\t.procname\t= \"sunrpc\",\n\t\t.mode\t\t= 0555,\n\t\t.child\t\t= svcrdma_table\n\t},\n\t{ },\n};\n\nvoid svc_rdma_cleanup(void)\n{\n\tdprintk(\"SVCRDMA Module Removed, deregister RPC RDMA transport\\n\");\n\tdestroy_workqueue(svc_rdma_wq);\n\tif (svcrdma_table_header) {\n\t\tunregister_sysctl_table(svcrdma_table_header);\n\t\tsvcrdma_table_header = NULL;\n\t}\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\n\tsvc_unreg_xprt_class(&svc_rdma_bc_class);\n#endif\n\tsvc_unreg_xprt_class(&svc_rdma_class);\n}\n\nint svc_rdma_init(void)\n{\n\tdprintk(\"SVCRDMA Module Init, register RPC RDMA transport\\n\");\n\tdprintk(\"\\tsvcrdma_ord      : %d\\n\", svcrdma_ord);\n\tdprintk(\"\\tmax_requests     : %u\\n\", svcrdma_max_requests);\n\tdprintk(\"\\tmax_bc_requests  : %u\\n\", svcrdma_max_bc_requests);\n\tdprintk(\"\\tmax_inline       : %d\\n\", svcrdma_max_req_size);\n\n\tsvc_rdma_wq = alloc_workqueue(\"svc_rdma\", 0, 0);\n\tif (!svc_rdma_wq)\n\t\treturn -ENOMEM;\n\n\tif (!svcrdma_table_header)\n\t\tsvcrdma_table_header =\n\t\t\tregister_sysctl_table(svcrdma_root_table);\n\n\t/* Register RDMA with the SVC transport switch */\n\tsvc_reg_xprt_class(&svc_rdma_class);\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\n\tsvc_reg_xprt_class(&svc_rdma_bc_class);\n#endif\n\treturn 0;\n}\n", "/*\n * Copyright (c) 2015 Oracle.  All rights reserved.\n *\n * Support for backward direction RPCs on RPC/RDMA (server-side).\n */\n\n#include <linux/module.h>\n#include <linux/sunrpc/svc_rdma.h>\n#include \"xprt_rdma.h\"\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\n#undef SVCRDMA_BACKCHANNEL_DEBUG\n\n/**\n * svc_rdma_handle_bc_reply - Process incoming backchannel reply\n * @xprt: controlling backchannel transport\n * @rdma_resp: pointer to incoming transport header\n * @rcvbuf: XDR buffer into which to decode the reply\n *\n * Returns:\n *\t%0 if @rcvbuf is filled in, xprt_complete_rqst called,\n *\t%-EAGAIN if server should call ->recvfrom again.\n */\nint svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,\n\t\t\t     struct xdr_buf *rcvbuf)\n{\n\tstruct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);\n\tstruct kvec *dst, *src = &rcvbuf->head[0];\n\tstruct rpc_rqst *req;\n\tunsigned long cwnd;\n\tu32 credits;\n\tsize_t len;\n\t__be32 xid;\n\t__be32 *p;\n\tint ret;\n\n\tp = (__be32 *)src->iov_base;\n\tlen = src->iov_len;\n\txid = *rdma_resp;\n\n#ifdef SVCRDMA_BACKCHANNEL_DEBUG\n\tpr_info(\"%s: xid=%08x, length=%zu\\n\",\n\t\t__func__, be32_to_cpu(xid), len);\n\tpr_info(\"%s: RPC/RDMA: %*ph\\n\",\n\t\t__func__, (int)RPCRDMA_HDRLEN_MIN, rdma_resp);\n\tpr_info(\"%s:      RPC: %*ph\\n\",\n\t\t__func__, (int)len, p);\n#endif\n\n\tret = -EAGAIN;\n\tif (src->iov_len < 24)\n\t\tgoto out_shortreply;\n\n\tspin_lock_bh(&xprt->transport_lock);\n\treq = xprt_lookup_rqst(xprt, xid);\n\tif (!req)\n\t\tgoto out_notfound;\n\n\tdst = &req->rq_private_buf.head[0];\n\tmemcpy(&req->rq_private_buf, &req->rq_rcv_buf, sizeof(struct xdr_buf));\n\tif (dst->iov_len < len)\n\t\tgoto out_unlock;\n\tmemcpy(dst->iov_base, p, len);\n\n\tcredits = be32_to_cpup(rdma_resp + 2);\n\tif (credits == 0)\n\t\tcredits = 1;\t/* don't deadlock */\n\telse if (credits > r_xprt->rx_buf.rb_bc_max_requests)\n\t\tcredits = r_xprt->rx_buf.rb_bc_max_requests;\n\n\tcwnd = xprt->cwnd;\n\txprt->cwnd = credits << RPC_CWNDSHIFT;\n\tif (xprt->cwnd > cwnd)\n\t\txprt_release_rqst_cong(req->rq_task);\n\n\tret = 0;\n\txprt_complete_rqst(req->rq_task, rcvbuf->len);\n\trcvbuf->len = 0;\n\nout_unlock:\n\tspin_unlock_bh(&xprt->transport_lock);\nout:\n\treturn ret;\n\nout_shortreply:\n\tdprintk(\"svcrdma: short bc reply: xprt=%p, len=%zu\\n\",\n\t\txprt, src->iov_len);\n\tgoto out;\n\nout_notfound:\n\tdprintk(\"svcrdma: unrecognized bc reply: xprt=%p, xid=%08x\\n\",\n\t\txprt, be32_to_cpu(xid));\n\n\tgoto out_unlock;\n}\n\n/* Send a backwards direction RPC call.\n *\n * Caller holds the connection's mutex and has already marshaled\n * the RPC/RDMA request.\n *\n * This is similar to svc_rdma_send_reply_msg, but takes a struct\n * rpc_rqst instead, does not support chunks, and avoids blocking\n * memory allocation.\n *\n * XXX: There is still an opportunity to block in svc_rdma_send()\n * if there are no SQ entries to post the Send. This may occur if\n * the adapter has a small maximum SQ depth.\n */\nstatic int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,\n\t\t\t      struct rpc_rqst *rqst)\n{\n\tstruct svc_rdma_op_ctxt *ctxt;\n\tint ret;\n\n\tctxt = svc_rdma_get_context(rdma);\n\n\t/* rpcrdma_bc_send_request builds the transport header and\n\t * the backchannel RPC message in the same buffer. Thus only\n\t * one SGE is needed to send both.\n\t */\n\tret = svc_rdma_map_reply_hdr(rdma, ctxt, rqst->rq_buffer,\n\t\t\t\t     rqst->rq_snd_buf.len);\n\tif (ret < 0)\n\t\tgoto out_err;\n\n\tret = svc_rdma_repost_recv(rdma, GFP_NOIO);\n\tif (ret)\n\t\tgoto out_err;\n\n\tret = svc_rdma_post_send_wr(rdma, ctxt, 1, 0);\n\tif (ret)\n\t\tgoto out_unmap;\n\nout_err:\n\tdprintk(\"svcrdma: %s returns %d\\n\", __func__, ret);\n\treturn ret;\n\nout_unmap:\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 1);\n\tret = -EIO;\n\tgoto out_err;\n}\n\n/* Server-side transport endpoint wants a whole page for its send\n * buffer. The client RPC code constructs the RPC header in this\n * buffer before it invokes ->send_request.\n */\nstatic int\nxprt_rdma_bc_allocate(struct rpc_task *task)\n{\n\tstruct rpc_rqst *rqst = task->tk_rqstp;\n\tsize_t size = rqst->rq_callsize;\n\tstruct page *page;\n\n\tif (size > PAGE_SIZE) {\n\t\tWARN_ONCE(1, \"svcrdma: large bc buffer request (size %zu)\\n\",\n\t\t\t  size);\n\t\treturn -EINVAL;\n\t}\n\n\t/* svc_rdma_sendto releases this page */\n\tpage = alloc_page(RPCRDMA_DEF_GFP);\n\tif (!page)\n\t\treturn -ENOMEM;\n\trqst->rq_buffer = page_address(page);\n\n\trqst->rq_rbuffer = kmalloc(rqst->rq_rcvsize, RPCRDMA_DEF_GFP);\n\tif (!rqst->rq_rbuffer) {\n\t\tput_page(page);\n\t\treturn -ENOMEM;\n\t}\n\treturn 0;\n}\n\nstatic void\nxprt_rdma_bc_free(struct rpc_task *task)\n{\n\tstruct rpc_rqst *rqst = task->tk_rqstp;\n\n\tkfree(rqst->rq_rbuffer);\n}\n\nstatic int\nrpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)\n{\n\tstruct rpc_xprt *xprt = rqst->rq_xprt;\n\tstruct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);\n\t__be32 *p;\n\tint rc;\n\n\t/* Space in the send buffer for an RPC/RDMA header is reserved\n\t * via xprt->tsh_size.\n\t */\n\tp = rqst->rq_buffer;\n\t*p++ = rqst->rq_xid;\n\t*p++ = rpcrdma_version;\n\t*p++ = cpu_to_be32(r_xprt->rx_buf.rb_bc_max_requests);\n\t*p++ = rdma_msg;\n\t*p++ = xdr_zero;\n\t*p++ = xdr_zero;\n\t*p   = xdr_zero;\n\n#ifdef SVCRDMA_BACKCHANNEL_DEBUG\n\tpr_info(\"%s: %*ph\\n\", __func__, 64, rqst->rq_buffer);\n#endif\n\n\trc = svc_rdma_bc_sendto(rdma, rqst);\n\tif (rc)\n\t\tgoto drop_connection;\n\treturn rc;\n\ndrop_connection:\n\tdprintk(\"svcrdma: failed to send bc call\\n\");\n\txprt_disconnect_done(xprt);\n\treturn -ENOTCONN;\n}\n\n/* Send an RPC call on the passive end of a transport\n * connection.\n */\nstatic int\nxprt_rdma_bc_send_request(struct rpc_task *task)\n{\n\tstruct rpc_rqst *rqst = task->tk_rqstp;\n\tstruct svc_xprt *sxprt = rqst->rq_xprt->bc_xprt;\n\tstruct svcxprt_rdma *rdma;\n\tint ret;\n\n\tdprintk(\"svcrdma: sending bc call with xid: %08x\\n\",\n\t\tbe32_to_cpu(rqst->rq_xid));\n\n\tif (!mutex_trylock(&sxprt->xpt_mutex)) {\n\t\trpc_sleep_on(&sxprt->xpt_bc_pending, task, NULL);\n\t\tif (!mutex_trylock(&sxprt->xpt_mutex))\n\t\t\treturn -EAGAIN;\n\t\trpc_wake_up_queued_task(&sxprt->xpt_bc_pending, task);\n\t}\n\n\tret = -ENOTCONN;\n\trdma = container_of(sxprt, struct svcxprt_rdma, sc_xprt);\n\tif (!test_bit(XPT_DEAD, &sxprt->xpt_flags))\n\t\tret = rpcrdma_bc_send_request(rdma, rqst);\n\n\tmutex_unlock(&sxprt->xpt_mutex);\n\n\tif (ret < 0)\n\t\treturn ret;\n\treturn 0;\n}\n\nstatic void\nxprt_rdma_bc_close(struct rpc_xprt *xprt)\n{\n\tdprintk(\"svcrdma: %s: xprt %p\\n\", __func__, xprt);\n}\n\nstatic void\nxprt_rdma_bc_put(struct rpc_xprt *xprt)\n{\n\tdprintk(\"svcrdma: %s: xprt %p\\n\", __func__, xprt);\n\n\txprt_free(xprt);\n\tmodule_put(THIS_MODULE);\n}\n\nstatic struct rpc_xprt_ops xprt_rdma_bc_procs = {\n\t.reserve_xprt\t\t= xprt_reserve_xprt_cong,\n\t.release_xprt\t\t= xprt_release_xprt_cong,\n\t.alloc_slot\t\t= xprt_alloc_slot,\n\t.release_request\t= xprt_release_rqst_cong,\n\t.buf_alloc\t\t= xprt_rdma_bc_allocate,\n\t.buf_free\t\t= xprt_rdma_bc_free,\n\t.send_request\t\t= xprt_rdma_bc_send_request,\n\t.set_retrans_timeout\t= xprt_set_retrans_timeout_def,\n\t.close\t\t\t= xprt_rdma_bc_close,\n\t.destroy\t\t= xprt_rdma_bc_put,\n\t.print_stats\t\t= xprt_rdma_print_stats\n};\n\nstatic const struct rpc_timeout xprt_rdma_bc_timeout = {\n\t.to_initval = 60 * HZ,\n\t.to_maxval = 60 * HZ,\n};\n\n/* It shouldn't matter if the number of backchannel session slots\n * doesn't match the number of RPC/RDMA credits. That just means\n * one or the other will have extra slots that aren't used.\n */\nstatic struct rpc_xprt *\nxprt_setup_rdma_bc(struct xprt_create *args)\n{\n\tstruct rpc_xprt *xprt;\n\tstruct rpcrdma_xprt *new_xprt;\n\n\tif (args->addrlen > sizeof(xprt->addr)) {\n\t\tdprintk(\"RPC:       %s: address too large\\n\", __func__);\n\t\treturn ERR_PTR(-EBADF);\n\t}\n\n\txprt = xprt_alloc(args->net, sizeof(*new_xprt),\n\t\t\t  RPCRDMA_MAX_BC_REQUESTS,\n\t\t\t  RPCRDMA_MAX_BC_REQUESTS);\n\tif (!xprt) {\n\t\tdprintk(\"RPC:       %s: couldn't allocate rpc_xprt\\n\",\n\t\t\t__func__);\n\t\treturn ERR_PTR(-ENOMEM);\n\t}\n\n\txprt->timeout = &xprt_rdma_bc_timeout;\n\txprt_set_bound(xprt);\n\txprt_set_connected(xprt);\n\txprt->bind_timeout = RPCRDMA_BIND_TO;\n\txprt->reestablish_timeout = RPCRDMA_INIT_REEST_TO;\n\txprt->idle_timeout = RPCRDMA_IDLE_DISC_TO;\n\n\txprt->prot = XPRT_TRANSPORT_BC_RDMA;\n\txprt->tsh_size = RPCRDMA_HDRLEN_MIN / sizeof(__be32);\n\txprt->ops = &xprt_rdma_bc_procs;\n\n\tmemcpy(&xprt->addr, args->dstaddr, args->addrlen);\n\txprt->addrlen = args->addrlen;\n\txprt_rdma_format_addresses(xprt, (struct sockaddr *)&xprt->addr);\n\txprt->resvport = 0;\n\n\txprt->max_payload = xprt_rdma_max_inline_read;\n\n\tnew_xprt = rpcx_to_rdmax(xprt);\n\tnew_xprt->rx_buf.rb_bc_max_requests = xprt->max_reqs;\n\n\txprt_get(xprt);\n\targs->bc_xprt->xpt_bc_xprt = xprt;\n\txprt->bc_xprt = args->bc_xprt;\n\n\tif (!try_module_get(THIS_MODULE))\n\t\tgoto out_fail;\n\n\t/* Final put for backchannel xprt is in __svc_rdma_free */\n\txprt_get(xprt);\n\treturn xprt;\n\nout_fail:\n\txprt_rdma_free_addresses(xprt);\n\targs->bc_xprt->xpt_bc_xprt = NULL;\n\targs->bc_xprt->xpt_bc_xps = NULL;\n\txprt_put(xprt);\n\txprt_free(xprt);\n\treturn ERR_PTR(-EINVAL);\n}\n\nstruct xprt_class xprt_rdma_bc = {\n\t.list\t\t\t= LIST_HEAD_INIT(xprt_rdma_bc.list),\n\t.name\t\t\t= \"rdma backchannel\",\n\t.owner\t\t\t= THIS_MODULE,\n\t.ident\t\t\t= XPRT_TRANSPORT_BC_RDMA,\n\t.setup\t\t\t= xprt_setup_rdma_bc,\n};\n", "/*\n * Copyright (c) 2016 Oracle. All rights reserved.\n * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n#include <linux/sunrpc/xdr.h>\n#include <linux/sunrpc/debug.h>\n#include <asm/unaligned.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <linux/sunrpc/svc_rdma.h>\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\nstatic __be32 *xdr_check_read_list(__be32 *p, __be32 *end)\n{\n\t__be32 *next;\n\n\twhile (*p++ != xdr_zero) {\n\t\tnext = p + rpcrdma_readchunk_maxsz - 1;\n\t\tif (next > end)\n\t\t\treturn NULL;\n\t\tp = next;\n\t}\n\treturn p;\n}\n\nstatic __be32 *xdr_check_write_list(__be32 *p, __be32 *end)\n{\n\t__be32 *next;\n\n\twhile (*p++ != xdr_zero) {\n\t\tnext = p + 1 + be32_to_cpup(p) * rpcrdma_segment_maxsz;\n\t\tif (next > end)\n\t\t\treturn NULL;\n\t\tp = next;\n\t}\n\treturn p;\n}\n\nstatic __be32 *xdr_check_reply_chunk(__be32 *p, __be32 *end)\n{\n\t__be32 *next;\n\n\tif (*p++ != xdr_zero) {\n\t\tnext = p + 1 + be32_to_cpup(p) * rpcrdma_segment_maxsz;\n\t\tif (next > end)\n\t\t\treturn NULL;\n\t\tp = next;\n\t}\n\treturn p;\n}\n\n/**\n * svc_rdma_xdr_decode_req - Parse incoming RPC-over-RDMA header\n * @rq_arg: Receive buffer\n *\n * On entry, xdr->head[0].iov_base points to first byte in the\n * RPC-over-RDMA header.\n *\n * On successful exit, head[0] points to first byte past the\n * RPC-over-RDMA header. For RDMA_MSG, this is the RPC message.\n * The length of the RPC-over-RDMA header is returned.\n */\nint svc_rdma_xdr_decode_req(struct xdr_buf *rq_arg)\n{\n\t__be32 *p, *end, *rdma_argp;\n\tunsigned int hdr_len;\n\n\t/* Verify that there's enough bytes for header + something */\n\tif (rq_arg->len <= RPCRDMA_HDRLEN_ERR)\n\t\tgoto out_short;\n\n\trdma_argp = rq_arg->head[0].iov_base;\n\tif (*(rdma_argp + 1) != rpcrdma_version)\n\t\tgoto out_version;\n\n\tswitch (*(rdma_argp + 3)) {\n\tcase rdma_msg:\n\tcase rdma_nomsg:\n\t\tbreak;\n\n\tcase rdma_done:\n\t\tgoto out_drop;\n\n\tcase rdma_error:\n\t\tgoto out_drop;\n\n\tdefault:\n\t\tgoto out_proc;\n\t}\n\n\tend = (__be32 *)((unsigned long)rdma_argp + rq_arg->len);\n\tp = xdr_check_read_list(rdma_argp + 4, end);\n\tif (!p)\n\t\tgoto out_inval;\n\tp = xdr_check_write_list(p, end);\n\tif (!p)\n\t\tgoto out_inval;\n\tp = xdr_check_reply_chunk(p, end);\n\tif (!p)\n\t\tgoto out_inval;\n\tif (p > end)\n\t\tgoto out_inval;\n\n\trq_arg->head[0].iov_base = p;\n\thdr_len = (unsigned long)p - (unsigned long)rdma_argp;\n\trq_arg->head[0].iov_len -= hdr_len;\n\treturn hdr_len;\n\nout_short:\n\tdprintk(\"svcrdma: header too short = %d\\n\", rq_arg->len);\n\treturn -EINVAL;\n\nout_version:\n\tdprintk(\"svcrdma: bad xprt version: %u\\n\",\n\t\tbe32_to_cpup(rdma_argp + 1));\n\treturn -EPROTONOSUPPORT;\n\nout_drop:\n\tdprintk(\"svcrdma: dropping RDMA_DONE/ERROR message\\n\");\n\treturn 0;\n\nout_proc:\n\tdprintk(\"svcrdma: bad rdma procedure (%u)\\n\",\n\t\tbe32_to_cpup(rdma_argp + 3));\n\treturn -EINVAL;\n\nout_inval:\n\tdprintk(\"svcrdma: failed to parse transport header\\n\");\n\treturn -EINVAL;\n}\n", "/*\n * Copyright (c) 2014 Open Grid Computing, Inc. All rights reserved.\n * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n#include <linux/sunrpc/debug.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <linux/spinlock.h>\n#include <asm/unaligned.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n#include <linux/sunrpc/svc_rdma.h>\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\n/*\n * Replace the pages in the rq_argpages array with the pages from the SGE in\n * the RDMA_RECV completion. The SGL should contain full pages up until the\n * last one.\n */\nstatic void rdma_build_arg_xdr(struct svc_rqst *rqstp,\n\t\t\t       struct svc_rdma_op_ctxt *ctxt,\n\t\t\t       u32 byte_count)\n{\n\tstruct rpcrdma_msg *rmsgp;\n\tstruct page *page;\n\tu32 bc;\n\tint sge_no;\n\n\t/* Swap the page in the SGE with the page in argpages */\n\tpage = ctxt->pages[0];\n\tput_page(rqstp->rq_pages[0]);\n\trqstp->rq_pages[0] = page;\n\n\t/* Set up the XDR head */\n\trqstp->rq_arg.head[0].iov_base = page_address(page);\n\trqstp->rq_arg.head[0].iov_len =\n\t\tmin_t(size_t, byte_count, ctxt->sge[0].length);\n\trqstp->rq_arg.len = byte_count;\n\trqstp->rq_arg.buflen = byte_count;\n\n\t/* Compute bytes past head in the SGL */\n\tbc = byte_count - rqstp->rq_arg.head[0].iov_len;\n\n\t/* If data remains, store it in the pagelist */\n\trqstp->rq_arg.page_len = bc;\n\trqstp->rq_arg.page_base = 0;\n\n\t/* RDMA_NOMSG: RDMA READ data should land just after RDMA RECV data */\n\trmsgp = (struct rpcrdma_msg *)rqstp->rq_arg.head[0].iov_base;\n\tif (rmsgp->rm_type == rdma_nomsg)\n\t\trqstp->rq_arg.pages = &rqstp->rq_pages[0];\n\telse\n\t\trqstp->rq_arg.pages = &rqstp->rq_pages[1];\n\n\tsge_no = 1;\n\twhile (bc && sge_no < ctxt->count) {\n\t\tpage = ctxt->pages[sge_no];\n\t\tput_page(rqstp->rq_pages[sge_no]);\n\t\trqstp->rq_pages[sge_no] = page;\n\t\tbc -= min_t(u32, bc, ctxt->sge[sge_no].length);\n\t\trqstp->rq_arg.buflen += ctxt->sge[sge_no].length;\n\t\tsge_no++;\n\t}\n\trqstp->rq_respages = &rqstp->rq_pages[sge_no];\n\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\n\t/* If not all pages were used from the SGL, free the remaining ones */\n\tbc = sge_no;\n\twhile (sge_no < ctxt->count) {\n\t\tpage = ctxt->pages[sge_no++];\n\t\tput_page(page);\n\t}\n\tctxt->count = bc;\n\n\t/* Set up tail */\n\trqstp->rq_arg.tail[0].iov_base = NULL;\n\trqstp->rq_arg.tail[0].iov_len = 0;\n}\n\n/* Issue an RDMA_READ using the local lkey to map the data sink */\nint rdma_read_chunk_lcl(struct svcxprt_rdma *xprt,\n\t\t\tstruct svc_rqst *rqstp,\n\t\t\tstruct svc_rdma_op_ctxt *head,\n\t\t\tint *page_no,\n\t\t\tu32 *page_offset,\n\t\t\tu32 rs_handle,\n\t\t\tu32 rs_length,\n\t\t\tu64 rs_offset,\n\t\t\tbool last)\n{\n\tstruct ib_rdma_wr read_wr;\n\tint pages_needed = PAGE_ALIGN(*page_offset + rs_length) >> PAGE_SHIFT;\n\tstruct svc_rdma_op_ctxt *ctxt = svc_rdma_get_context(xprt);\n\tint ret, read, pno;\n\tu32 pg_off = *page_offset;\n\tu32 pg_no = *page_no;\n\n\tctxt->direction = DMA_FROM_DEVICE;\n\tctxt->read_hdr = head;\n\tpages_needed = min_t(int, pages_needed, xprt->sc_max_sge_rd);\n\tread = min_t(int, (pages_needed << PAGE_SHIFT) - *page_offset,\n\t\t     rs_length);\n\n\tfor (pno = 0; pno < pages_needed; pno++) {\n\t\tint len = min_t(int, rs_length, PAGE_SIZE - pg_off);\n\n\t\thead->arg.pages[pg_no] = rqstp->rq_arg.pages[pg_no];\n\t\thead->arg.page_len += len;\n\n\t\thead->arg.len += len;\n\t\tif (!pg_off)\n\t\t\thead->count++;\n\t\trqstp->rq_respages = &rqstp->rq_arg.pages[pg_no+1];\n\t\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\t\tctxt->sge[pno].addr =\n\t\t\tib_dma_map_page(xprt->sc_cm_id->device,\n\t\t\t\t\thead->arg.pages[pg_no], pg_off,\n\t\t\t\t\tPAGE_SIZE - pg_off,\n\t\t\t\t\tDMA_FROM_DEVICE);\n\t\tret = ib_dma_mapping_error(xprt->sc_cm_id->device,\n\t\t\t\t\t   ctxt->sge[pno].addr);\n\t\tif (ret)\n\t\t\tgoto err;\n\t\tsvc_rdma_count_mappings(xprt, ctxt);\n\n\t\tctxt->sge[pno].lkey = xprt->sc_pd->local_dma_lkey;\n\t\tctxt->sge[pno].length = len;\n\t\tctxt->count++;\n\n\t\t/* adjust offset and wrap to next page if needed */\n\t\tpg_off += len;\n\t\tif (pg_off == PAGE_SIZE) {\n\t\t\tpg_off = 0;\n\t\t\tpg_no++;\n\t\t}\n\t\trs_length -= len;\n\t}\n\n\tif (last && rs_length == 0)\n\t\tset_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\n\telse\n\t\tclear_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\n\n\tmemset(&read_wr, 0, sizeof(read_wr));\n\tctxt->cqe.done = svc_rdma_wc_read;\n\tread_wr.wr.wr_cqe = &ctxt->cqe;\n\tread_wr.wr.opcode = IB_WR_RDMA_READ;\n\tread_wr.wr.send_flags = IB_SEND_SIGNALED;\n\tread_wr.rkey = rs_handle;\n\tread_wr.remote_addr = rs_offset;\n\tread_wr.wr.sg_list = ctxt->sge;\n\tread_wr.wr.num_sge = pages_needed;\n\n\tret = svc_rdma_send(xprt, &read_wr.wr);\n\tif (ret) {\n\t\tpr_err(\"svcrdma: Error %d posting RDMA_READ\\n\", ret);\n\t\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\t\tgoto err;\n\t}\n\n\t/* return current location in page array */\n\t*page_no = pg_no;\n\t*page_offset = pg_off;\n\tret = read;\n\tatomic_inc(&rdma_stat_read);\n\treturn ret;\n err:\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 0);\n\treturn ret;\n}\n\n/* Issue an RDMA_READ using an FRMR to map the data sink */\nint rdma_read_chunk_frmr(struct svcxprt_rdma *xprt,\n\t\t\t struct svc_rqst *rqstp,\n\t\t\t struct svc_rdma_op_ctxt *head,\n\t\t\t int *page_no,\n\t\t\t u32 *page_offset,\n\t\t\t u32 rs_handle,\n\t\t\t u32 rs_length,\n\t\t\t u64 rs_offset,\n\t\t\t bool last)\n{\n\tstruct ib_rdma_wr read_wr;\n\tstruct ib_send_wr inv_wr;\n\tstruct ib_reg_wr reg_wr;\n\tu8 key;\n\tint nents = PAGE_ALIGN(*page_offset + rs_length) >> PAGE_SHIFT;\n\tstruct svc_rdma_op_ctxt *ctxt = svc_rdma_get_context(xprt);\n\tstruct svc_rdma_fastreg_mr *frmr = svc_rdma_get_frmr(xprt);\n\tint ret, read, pno, dma_nents, n;\n\tu32 pg_off = *page_offset;\n\tu32 pg_no = *page_no;\n\n\tif (IS_ERR(frmr))\n\t\treturn -ENOMEM;\n\n\tctxt->direction = DMA_FROM_DEVICE;\n\tctxt->frmr = frmr;\n\tnents = min_t(unsigned int, nents, xprt->sc_frmr_pg_list_len);\n\tread = min_t(int, (nents << PAGE_SHIFT) - *page_offset, rs_length);\n\n\tfrmr->direction = DMA_FROM_DEVICE;\n\tfrmr->access_flags = (IB_ACCESS_LOCAL_WRITE|IB_ACCESS_REMOTE_WRITE);\n\tfrmr->sg_nents = nents;\n\n\tfor (pno = 0; pno < nents; pno++) {\n\t\tint len = min_t(int, rs_length, PAGE_SIZE - pg_off);\n\n\t\thead->arg.pages[pg_no] = rqstp->rq_arg.pages[pg_no];\n\t\thead->arg.page_len += len;\n\t\thead->arg.len += len;\n\t\tif (!pg_off)\n\t\t\thead->count++;\n\n\t\tsg_set_page(&frmr->sg[pno], rqstp->rq_arg.pages[pg_no],\n\t\t\t    len, pg_off);\n\n\t\trqstp->rq_respages = &rqstp->rq_arg.pages[pg_no+1];\n\t\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\n\t\t/* adjust offset and wrap to next page if needed */\n\t\tpg_off += len;\n\t\tif (pg_off == PAGE_SIZE) {\n\t\t\tpg_off = 0;\n\t\t\tpg_no++;\n\t\t}\n\t\trs_length -= len;\n\t}\n\n\tif (last && rs_length == 0)\n\t\tset_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\n\telse\n\t\tclear_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags);\n\n\tdma_nents = ib_dma_map_sg(xprt->sc_cm_id->device,\n\t\t\t\t  frmr->sg, frmr->sg_nents,\n\t\t\t\t  frmr->direction);\n\tif (!dma_nents) {\n\t\tpr_err(\"svcrdma: failed to dma map sg %p\\n\",\n\t\t       frmr->sg);\n\t\treturn -ENOMEM;\n\t}\n\n\tn = ib_map_mr_sg(frmr->mr, frmr->sg, frmr->sg_nents, NULL, PAGE_SIZE);\n\tif (unlikely(n != frmr->sg_nents)) {\n\t\tpr_err(\"svcrdma: failed to map mr %p (%d/%d elements)\\n\",\n\t\t       frmr->mr, n, frmr->sg_nents);\n\t\treturn n < 0 ? n : -EINVAL;\n\t}\n\n\t/* Bump the key */\n\tkey = (u8)(frmr->mr->lkey & 0x000000FF);\n\tib_update_fast_reg_key(frmr->mr, ++key);\n\n\tctxt->sge[0].addr = frmr->mr->iova;\n\tctxt->sge[0].lkey = frmr->mr->lkey;\n\tctxt->sge[0].length = frmr->mr->length;\n\tctxt->count = 1;\n\tctxt->read_hdr = head;\n\n\t/* Prepare REG WR */\n\tctxt->reg_cqe.done = svc_rdma_wc_reg;\n\treg_wr.wr.wr_cqe = &ctxt->reg_cqe;\n\treg_wr.wr.opcode = IB_WR_REG_MR;\n\treg_wr.wr.send_flags = IB_SEND_SIGNALED;\n\treg_wr.wr.num_sge = 0;\n\treg_wr.mr = frmr->mr;\n\treg_wr.key = frmr->mr->lkey;\n\treg_wr.access = frmr->access_flags;\n\treg_wr.wr.next = &read_wr.wr;\n\n\t/* Prepare RDMA_READ */\n\tmemset(&read_wr, 0, sizeof(read_wr));\n\tctxt->cqe.done = svc_rdma_wc_read;\n\tread_wr.wr.wr_cqe = &ctxt->cqe;\n\tread_wr.wr.send_flags = IB_SEND_SIGNALED;\n\tread_wr.rkey = rs_handle;\n\tread_wr.remote_addr = rs_offset;\n\tread_wr.wr.sg_list = ctxt->sge;\n\tread_wr.wr.num_sge = 1;\n\tif (xprt->sc_dev_caps & SVCRDMA_DEVCAP_READ_W_INV) {\n\t\tread_wr.wr.opcode = IB_WR_RDMA_READ_WITH_INV;\n\t\tread_wr.wr.ex.invalidate_rkey = ctxt->frmr->mr->lkey;\n\t} else {\n\t\tread_wr.wr.opcode = IB_WR_RDMA_READ;\n\t\tread_wr.wr.next = &inv_wr;\n\t\t/* Prepare invalidate */\n\t\tmemset(&inv_wr, 0, sizeof(inv_wr));\n\t\tctxt->inv_cqe.done = svc_rdma_wc_inv;\n\t\tinv_wr.wr_cqe = &ctxt->inv_cqe;\n\t\tinv_wr.opcode = IB_WR_LOCAL_INV;\n\t\tinv_wr.send_flags = IB_SEND_SIGNALED | IB_SEND_FENCE;\n\t\tinv_wr.ex.invalidate_rkey = frmr->mr->lkey;\n\t}\n\n\t/* Post the chain */\n\tret = svc_rdma_send(xprt, &reg_wr.wr);\n\tif (ret) {\n\t\tpr_err(\"svcrdma: Error %d posting RDMA_READ\\n\", ret);\n\t\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\t\tgoto err;\n\t}\n\n\t/* return current location in page array */\n\t*page_no = pg_no;\n\t*page_offset = pg_off;\n\tret = read;\n\tatomic_inc(&rdma_stat_read);\n\treturn ret;\n err:\n\tsvc_rdma_put_context(ctxt, 0);\n\tsvc_rdma_put_frmr(xprt, frmr);\n\treturn ret;\n}\n\nstatic unsigned int\nrdma_rcl_chunk_count(struct rpcrdma_read_chunk *ch)\n{\n\tunsigned int count;\n\n\tfor (count = 0; ch->rc_discrim != xdr_zero; ch++)\n\t\tcount++;\n\treturn count;\n}\n\n/* If there was additional inline content, append it to the end of arg.pages.\n * Tail copy has to be done after the reader function has determined how many\n * pages are needed for RDMA READ.\n */\nstatic int\nrdma_copy_tail(struct svc_rqst *rqstp, struct svc_rdma_op_ctxt *head,\n\t       u32 position, u32 byte_count, u32 page_offset, int page_no)\n{\n\tchar *srcp, *destp;\n\n\tsrcp = head->arg.head[0].iov_base + position;\n\tbyte_count = head->arg.head[0].iov_len - position;\n\tif (byte_count > PAGE_SIZE) {\n\t\tdprintk(\"svcrdma: large tail unsupported\\n\");\n\t\treturn 0;\n\t}\n\n\t/* Fit as much of the tail on the current page as possible */\n\tif (page_offset != PAGE_SIZE) {\n\t\tdestp = page_address(rqstp->rq_arg.pages[page_no]);\n\t\tdestp += page_offset;\n\t\twhile (byte_count--) {\n\t\t\t*destp++ = *srcp++;\n\t\t\tpage_offset++;\n\t\t\tif (page_offset == PAGE_SIZE && byte_count)\n\t\t\t\tgoto more;\n\t\t}\n\t\tgoto done;\n\t}\n\nmore:\n\t/* Fit the rest on the next page */\n\tpage_no++;\n\tdestp = page_address(rqstp->rq_arg.pages[page_no]);\n\twhile (byte_count--)\n\t\t*destp++ = *srcp++;\n\n\trqstp->rq_respages = &rqstp->rq_arg.pages[page_no+1];\n\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\ndone:\n\tbyte_count = head->arg.head[0].iov_len - position;\n\thead->arg.page_len += byte_count;\n\thead->arg.len += byte_count;\n\thead->arg.buflen += byte_count;\n\treturn 1;\n}\n\n/* Returns the address of the first read chunk or <nul> if no read chunk\n * is present\n */\nstatic struct rpcrdma_read_chunk *\nsvc_rdma_get_read_chunk(struct rpcrdma_msg *rmsgp)\n{\n\tstruct rpcrdma_read_chunk *ch =\n\t\t(struct rpcrdma_read_chunk *)&rmsgp->rm_body.rm_chunks[0];\n\n\tif (ch->rc_discrim == xdr_zero)\n\t\treturn NULL;\n\treturn ch;\n}\n\nstatic int rdma_read_chunks(struct svcxprt_rdma *xprt,\n\t\t\t    struct rpcrdma_msg *rmsgp,\n\t\t\t    struct svc_rqst *rqstp,\n\t\t\t    struct svc_rdma_op_ctxt *head)\n{\n\tint page_no, ret;\n\tstruct rpcrdma_read_chunk *ch;\n\tu32 handle, page_offset, byte_count;\n\tu32 position;\n\tu64 rs_offset;\n\tbool last;\n\n\t/* If no read list is present, return 0 */\n\tch = svc_rdma_get_read_chunk(rmsgp);\n\tif (!ch)\n\t\treturn 0;\n\n\tif (rdma_rcl_chunk_count(ch) > RPCSVC_MAXPAGES)\n\t\treturn -EINVAL;\n\n\t/* The request is completed when the RDMA_READs complete. The\n\t * head context keeps all the pages that comprise the\n\t * request.\n\t */\n\thead->arg.head[0] = rqstp->rq_arg.head[0];\n\thead->arg.tail[0] = rqstp->rq_arg.tail[0];\n\thead->hdr_count = head->count;\n\thead->arg.page_base = 0;\n\thead->arg.page_len = 0;\n\thead->arg.len = rqstp->rq_arg.len;\n\thead->arg.buflen = rqstp->rq_arg.buflen;\n\n\t/* RDMA_NOMSG: RDMA READ data should land just after RDMA RECV data */\n\tposition = be32_to_cpu(ch->rc_position);\n\tif (position == 0) {\n\t\thead->arg.pages = &head->pages[0];\n\t\tpage_offset = head->byte_len;\n\t} else {\n\t\thead->arg.pages = &head->pages[head->count];\n\t\tpage_offset = 0;\n\t}\n\n\tret = 0;\n\tpage_no = 0;\n\tfor (; ch->rc_discrim != xdr_zero; ch++) {\n\t\tif (be32_to_cpu(ch->rc_position) != position)\n\t\t\tgoto err;\n\n\t\thandle = be32_to_cpu(ch->rc_target.rs_handle),\n\t\tbyte_count = be32_to_cpu(ch->rc_target.rs_length);\n\t\txdr_decode_hyper((__be32 *)&ch->rc_target.rs_offset,\n\t\t\t\t &rs_offset);\n\n\t\twhile (byte_count > 0) {\n\t\t\tlast = (ch + 1)->rc_discrim == xdr_zero;\n\t\t\tret = xprt->sc_reader(xprt, rqstp, head,\n\t\t\t\t\t      &page_no, &page_offset,\n\t\t\t\t\t      handle, byte_count,\n\t\t\t\t\t      rs_offset, last);\n\t\t\tif (ret < 0)\n\t\t\t\tgoto err;\n\t\t\tbyte_count -= ret;\n\t\t\trs_offset += ret;\n\t\t\thead->arg.buflen += ret;\n\t\t}\n\t}\n\n\t/* Read list may need XDR round-up (see RFC 5666, s. 3.7) */\n\tif (page_offset & 3) {\n\t\tu32 pad = 4 - (page_offset & 3);\n\n\t\thead->arg.tail[0].iov_len += pad;\n\t\thead->arg.len += pad;\n\t\thead->arg.buflen += pad;\n\t\tpage_offset += pad;\n\t}\n\n\tret = 1;\n\tif (position && position < head->arg.head[0].iov_len)\n\t\tret = rdma_copy_tail(rqstp, head, position,\n\t\t\t\t     byte_count, page_offset, page_no);\n\thead->arg.head[0].iov_len = position;\n\thead->position = position;\n\n err:\n\t/* Detach arg pages. svc_recv will replenish them */\n\tfor (page_no = 0;\n\t     &rqstp->rq_pages[page_no] < rqstp->rq_respages; page_no++)\n\t\trqstp->rq_pages[page_no] = NULL;\n\n\treturn ret;\n}\n\nstatic void rdma_read_complete(struct svc_rqst *rqstp,\n\t\t\t       struct svc_rdma_op_ctxt *head)\n{\n\tint page_no;\n\n\t/* Copy RPC pages */\n\tfor (page_no = 0; page_no < head->count; page_no++) {\n\t\tput_page(rqstp->rq_pages[page_no]);\n\t\trqstp->rq_pages[page_no] = head->pages[page_no];\n\t}\n\n\t/* Adjustments made for RDMA_NOMSG type requests */\n\tif (head->position == 0) {\n\t\tif (head->arg.len <= head->sge[0].length) {\n\t\t\thead->arg.head[0].iov_len = head->arg.len -\n\t\t\t\t\t\t\thead->byte_len;\n\t\t\thead->arg.page_len = 0;\n\t\t} else {\n\t\t\thead->arg.head[0].iov_len = head->sge[0].length -\n\t\t\t\t\t\t\t\thead->byte_len;\n\t\t\thead->arg.page_len = head->arg.len -\n\t\t\t\t\t\thead->sge[0].length;\n\t\t}\n\t}\n\n\t/* Point rq_arg.pages past header */\n\trqstp->rq_arg.pages = &rqstp->rq_pages[head->hdr_count];\n\trqstp->rq_arg.page_len = head->arg.page_len;\n\trqstp->rq_arg.page_base = head->arg.page_base;\n\n\t/* rq_respages starts after the last arg page */\n\trqstp->rq_respages = &rqstp->rq_pages[page_no];\n\trqstp->rq_next_page = rqstp->rq_respages + 1;\n\n\t/* Rebuild rq_arg head and tail. */\n\trqstp->rq_arg.head[0] = head->arg.head[0];\n\trqstp->rq_arg.tail[0] = head->arg.tail[0];\n\trqstp->rq_arg.len = head->arg.len;\n\trqstp->rq_arg.buflen = head->arg.buflen;\n}\n\nstatic void svc_rdma_send_error(struct svcxprt_rdma *xprt,\n\t\t\t\t__be32 *rdma_argp, int status)\n{\n\tstruct svc_rdma_op_ctxt *ctxt;\n\t__be32 *p, *err_msgp;\n\tunsigned int length;\n\tstruct page *page;\n\tint ret;\n\n\tret = svc_rdma_repost_recv(xprt, GFP_KERNEL);\n\tif (ret)\n\t\treturn;\n\n\tpage = alloc_page(GFP_KERNEL);\n\tif (!page)\n\t\treturn;\n\terr_msgp = page_address(page);\n\n\tp = err_msgp;\n\t*p++ = *rdma_argp;\n\t*p++ = *(rdma_argp + 1);\n\t*p++ = xprt->sc_fc_credits;\n\t*p++ = rdma_error;\n\tif (status == -EPROTONOSUPPORT) {\n\t\t*p++ = err_vers;\n\t\t*p++ = rpcrdma_version;\n\t\t*p++ = rpcrdma_version;\n\t} else {\n\t\t*p++ = err_chunk;\n\t}\n\tlength = (unsigned long)p - (unsigned long)err_msgp;\n\n\t/* Map transport header; no RPC message payload */\n\tctxt = svc_rdma_get_context(xprt);\n\tret = svc_rdma_map_reply_hdr(xprt, ctxt, err_msgp, length);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: Error %d mapping send for protocol error\\n\",\n\t\t\tret);\n\t\treturn;\n\t}\n\n\tret = svc_rdma_post_send_wr(xprt, ctxt, 1, 0);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: Error %d posting send for protocol error\\n\",\n\t\t\tret);\n\t\tsvc_rdma_unmap_dma(ctxt);\n\t\tsvc_rdma_put_context(ctxt, 1);\n\t}\n}\n\n/* By convention, backchannel calls arrive via rdma_msg type\n * messages, and never populate the chunk lists. This makes\n * the RPC/RDMA header small and fixed in size, so it is\n * straightforward to check the RPC header's direction field.\n */\nstatic bool svc_rdma_is_backchannel_reply(struct svc_xprt *xprt,\n\t\t\t\t\t  __be32 *rdma_resp)\n{\n\t__be32 *p;\n\n\tif (!xprt->xpt_bc_xprt)\n\t\treturn false;\n\n\tp = rdma_resp + 3;\n\tif (*p++ != rdma_msg)\n\t\treturn false;\n\n\tif (*p++ != xdr_zero)\n\t\treturn false;\n\tif (*p++ != xdr_zero)\n\t\treturn false;\n\tif (*p++ != xdr_zero)\n\t\treturn false;\n\n\t/* XID sanity */\n\tif (*p++ != *rdma_resp)\n\t\treturn false;\n\t/* call direction */\n\tif (*p == cpu_to_be32(RPC_CALL))\n\t\treturn false;\n\n\treturn true;\n}\n\n/*\n * Set up the rqstp thread context to point to the RQ buffer. If\n * necessary, pull additional data from the client with an RDMA_READ\n * request.\n */\nint svc_rdma_recvfrom(struct svc_rqst *rqstp)\n{\n\tstruct svc_xprt *xprt = rqstp->rq_xprt;\n\tstruct svcxprt_rdma *rdma_xprt =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tstruct svc_rdma_op_ctxt *ctxt = NULL;\n\tstruct rpcrdma_msg *rmsgp;\n\tint ret = 0;\n\n\tdprintk(\"svcrdma: rqstp=%p\\n\", rqstp);\n\n\tspin_lock(&rdma_xprt->sc_rq_dto_lock);\n\tif (!list_empty(&rdma_xprt->sc_read_complete_q)) {\n\t\tctxt = list_first_entry(&rdma_xprt->sc_read_complete_q,\n\t\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\t\tlist_del(&ctxt->list);\n\t\tspin_unlock(&rdma_xprt->sc_rq_dto_lock);\n\t\trdma_read_complete(rqstp, ctxt);\n\t\tgoto complete;\n\t} else if (!list_empty(&rdma_xprt->sc_rq_dto_q)) {\n\t\tctxt = list_first_entry(&rdma_xprt->sc_rq_dto_q,\n\t\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\t\tlist_del(&ctxt->list);\n\t} else {\n\t\tatomic_inc(&rdma_stat_rq_starve);\n\t\tclear_bit(XPT_DATA, &xprt->xpt_flags);\n\t\tctxt = NULL;\n\t}\n\tspin_unlock(&rdma_xprt->sc_rq_dto_lock);\n\tif (!ctxt) {\n\t\t/* This is the EAGAIN path. The svc_recv routine will\n\t\t * return -EAGAIN, the nfsd thread will go to call into\n\t\t * svc_recv again and we shouldn't be on the active\n\t\t * transport list\n\t\t */\n\t\tif (test_bit(XPT_CLOSE, &xprt->xpt_flags))\n\t\t\tgoto defer;\n\t\tgoto out;\n\t}\n\tdprintk(\"svcrdma: processing ctxt=%p on xprt=%p, rqstp=%p\\n\",\n\t\tctxt, rdma_xprt, rqstp);\n\tatomic_inc(&rdma_stat_recv);\n\n\t/* Build up the XDR from the receive buffers. */\n\trdma_build_arg_xdr(rqstp, ctxt, ctxt->byte_len);\n\n\t/* Decode the RDMA header. */\n\trmsgp = (struct rpcrdma_msg *)rqstp->rq_arg.head[0].iov_base;\n\tret = svc_rdma_xdr_decode_req(&rqstp->rq_arg);\n\tif (ret < 0)\n\t\tgoto out_err;\n\tif (ret == 0)\n\t\tgoto out_drop;\n\trqstp->rq_xprt_hlen = ret;\n\n\tif (svc_rdma_is_backchannel_reply(xprt, &rmsgp->rm_xid)) {\n\t\tret = svc_rdma_handle_bc_reply(xprt->xpt_bc_xprt,\n\t\t\t\t\t       &rmsgp->rm_xid,\n\t\t\t\t\t       &rqstp->rq_arg);\n\t\tsvc_rdma_put_context(ctxt, 0);\n\t\tif (ret)\n\t\t\tgoto repost;\n\t\treturn ret;\n\t}\n\n\t/* Read read-list data. */\n\tret = rdma_read_chunks(rdma_xprt, rmsgp, rqstp, ctxt);\n\tif (ret > 0) {\n\t\t/* read-list posted, defer until data received from client. */\n\t\tgoto defer;\n\t} else if (ret < 0) {\n\t\t/* Post of read-list failed, free context. */\n\t\tsvc_rdma_put_context(ctxt, 1);\n\t\treturn 0;\n\t}\n\ncomplete:\n\tret = rqstp->rq_arg.head[0].iov_len\n\t\t+ rqstp->rq_arg.page_len\n\t\t+ rqstp->rq_arg.tail[0].iov_len;\n\tsvc_rdma_put_context(ctxt, 0);\n out:\n\tdprintk(\"svcrdma: ret=%d, rq_arg.len=%u, \"\n\t\t\"rq_arg.head[0].iov_base=%p, rq_arg.head[0].iov_len=%zd\\n\",\n\t\tret, rqstp->rq_arg.len,\n\t\trqstp->rq_arg.head[0].iov_base,\n\t\trqstp->rq_arg.head[0].iov_len);\n\trqstp->rq_prot = IPPROTO_MAX;\n\tsvc_xprt_copy_addrs(rqstp, xprt);\n\treturn ret;\n\nout_err:\n\tsvc_rdma_send_error(rdma_xprt, &rmsgp->rm_xid, ret);\n\tsvc_rdma_put_context(ctxt, 0);\n\treturn 0;\n\ndefer:\n\treturn 0;\n\nout_drop:\n\tsvc_rdma_put_context(ctxt, 1);\nrepost:\n\treturn svc_rdma_repost_recv(rdma_xprt, GFP_KERNEL);\n}\n", "/*\n * Copyright (c) 2016 Oracle.  All rights reserved.\n *\n * Use the core R/W API to move RPC-over-RDMA Read and Write chunks.\n */\n\n#include <linux/sunrpc/rpc_rdma.h>\n#include <linux/sunrpc/svc_rdma.h>\n#include <linux/sunrpc/debug.h>\n\n#include <rdma/rw.h>\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\n/* Each R/W context contains state for one chain of RDMA Read or\n * Write Work Requests.\n *\n * Each WR chain handles a single contiguous server-side buffer,\n * because scatterlist entries after the first have to start on\n * page alignment. xdr_buf iovecs cannot guarantee alignment.\n *\n * Each WR chain handles only one R_key. Each RPC-over-RDMA segment\n * from a client may contain a unique R_key, so each WR chain moves\n * up to one segment at a time.\n *\n * The scatterlist makes this data structure over 4KB in size. To\n * make it less likely to fail, and to handle the allocation for\n * smaller I/O requests without disabling bottom-halves, these\n * contexts are created on demand, but cached and reused until the\n * controlling svcxprt_rdma is destroyed.\n */\nstruct svc_rdma_rw_ctxt {\n\tstruct list_head\trw_list;\n\tstruct rdma_rw_ctx\trw_ctx;\n\tint\t\t\trw_nents;\n\tstruct sg_table\t\trw_sg_table;\n\tstruct scatterlist\trw_first_sgl[0];\n};\n\nstatic inline struct svc_rdma_rw_ctxt *\nsvc_rdma_next_ctxt(struct list_head *list)\n{\n\treturn list_first_entry_or_null(list, struct svc_rdma_rw_ctxt,\n\t\t\t\t\trw_list);\n}\n\nstatic struct svc_rdma_rw_ctxt *\nsvc_rdma_get_rw_ctxt(struct svcxprt_rdma *rdma, unsigned int sges)\n{\n\tstruct svc_rdma_rw_ctxt *ctxt;\n\n\tspin_lock(&rdma->sc_rw_ctxt_lock);\n\n\tctxt = svc_rdma_next_ctxt(&rdma->sc_rw_ctxts);\n\tif (ctxt) {\n\t\tlist_del(&ctxt->rw_list);\n\t\tspin_unlock(&rdma->sc_rw_ctxt_lock);\n\t} else {\n\t\tspin_unlock(&rdma->sc_rw_ctxt_lock);\n\t\tctxt = kmalloc(sizeof(*ctxt) +\n\t\t\t       SG_CHUNK_SIZE * sizeof(struct scatterlist),\n\t\t\t       GFP_KERNEL);\n\t\tif (!ctxt)\n\t\t\tgoto out;\n\t\tINIT_LIST_HEAD(&ctxt->rw_list);\n\t}\n\n\tctxt->rw_sg_table.sgl = ctxt->rw_first_sgl;\n\tif (sg_alloc_table_chained(&ctxt->rw_sg_table, sges,\n\t\t\t\t   ctxt->rw_sg_table.sgl)) {\n\t\tkfree(ctxt);\n\t\tctxt = NULL;\n\t}\nout:\n\treturn ctxt;\n}\n\nstatic void svc_rdma_put_rw_ctxt(struct svcxprt_rdma *rdma,\n\t\t\t\t struct svc_rdma_rw_ctxt *ctxt)\n{\n\tsg_free_table_chained(&ctxt->rw_sg_table, true);\n\n\tspin_lock(&rdma->sc_rw_ctxt_lock);\n\tlist_add(&ctxt->rw_list, &rdma->sc_rw_ctxts);\n\tspin_unlock(&rdma->sc_rw_ctxt_lock);\n}\n\n/**\n * svc_rdma_destroy_rw_ctxts - Free accumulated R/W contexts\n * @rdma: transport about to be destroyed\n *\n */\nvoid svc_rdma_destroy_rw_ctxts(struct svcxprt_rdma *rdma)\n{\n\tstruct svc_rdma_rw_ctxt *ctxt;\n\n\twhile ((ctxt = svc_rdma_next_ctxt(&rdma->sc_rw_ctxts)) != NULL) {\n\t\tlist_del(&ctxt->rw_list);\n\t\tkfree(ctxt);\n\t}\n}\n\n/* A chunk context tracks all I/O for moving one Read or Write\n * chunk. This is a a set of rdma_rw's that handle data movement\n * for all segments of one chunk.\n *\n * These are small, acquired with a single allocator call, and\n * no more than one is needed per chunk. They are allocated on\n * demand, and not cached.\n */\nstruct svc_rdma_chunk_ctxt {\n\tstruct ib_cqe\t\tcc_cqe;\n\tstruct svcxprt_rdma\t*cc_rdma;\n\tstruct list_head\tcc_rwctxts;\n\tint\t\t\tcc_sqecount;\n\tenum dma_data_direction cc_dir;\n};\n\nstatic void svc_rdma_cc_init(struct svcxprt_rdma *rdma,\n\t\t\t     struct svc_rdma_chunk_ctxt *cc,\n\t\t\t     enum dma_data_direction dir)\n{\n\tcc->cc_rdma = rdma;\n\tsvc_xprt_get(&rdma->sc_xprt);\n\n\tINIT_LIST_HEAD(&cc->cc_rwctxts);\n\tcc->cc_sqecount = 0;\n\tcc->cc_dir = dir;\n}\n\nstatic void svc_rdma_cc_release(struct svc_rdma_chunk_ctxt *cc)\n{\n\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n\tstruct svc_rdma_rw_ctxt *ctxt;\n\n\twhile ((ctxt = svc_rdma_next_ctxt(&cc->cc_rwctxts)) != NULL) {\n\t\tlist_del(&ctxt->rw_list);\n\n\t\trdma_rw_ctx_destroy(&ctxt->rw_ctx, rdma->sc_qp,\n\t\t\t\t    rdma->sc_port_num, ctxt->rw_sg_table.sgl,\n\t\t\t\t    ctxt->rw_nents, cc->cc_dir);\n\t\tsvc_rdma_put_rw_ctxt(rdma, ctxt);\n\t}\n\tsvc_xprt_put(&rdma->sc_xprt);\n}\n\n/* State for sending a Write or Reply chunk.\n *  - Tracks progress of writing one chunk over all its segments\n *  - Stores arguments for the SGL constructor functions\n */\nstruct svc_rdma_write_info {\n\t/* write state of this chunk */\n\tunsigned int\t\twi_seg_off;\n\tunsigned int\t\twi_seg_no;\n\tunsigned int\t\twi_nsegs;\n\t__be32\t\t\t*wi_segs;\n\n\t/* SGL constructor arguments */\n\tstruct xdr_buf\t\t*wi_xdr;\n\tunsigned char\t\t*wi_base;\n\tunsigned int\t\twi_next_off;\n\n\tstruct svc_rdma_chunk_ctxt\twi_cc;\n};\n\nstatic struct svc_rdma_write_info *\nsvc_rdma_write_info_alloc(struct svcxprt_rdma *rdma, __be32 *chunk)\n{\n\tstruct svc_rdma_write_info *info;\n\n\tinfo = kmalloc(sizeof(*info), GFP_KERNEL);\n\tif (!info)\n\t\treturn info;\n\n\tinfo->wi_seg_off = 0;\n\tinfo->wi_seg_no = 0;\n\tinfo->wi_nsegs = be32_to_cpup(++chunk);\n\tinfo->wi_segs = ++chunk;\n\tsvc_rdma_cc_init(rdma, &info->wi_cc, DMA_TO_DEVICE);\n\treturn info;\n}\n\nstatic void svc_rdma_write_info_free(struct svc_rdma_write_info *info)\n{\n\tsvc_rdma_cc_release(&info->wi_cc);\n\tkfree(info);\n}\n\n/**\n * svc_rdma_write_done - Write chunk completion\n * @cq: controlling Completion Queue\n * @wc: Work Completion\n *\n * Pages under I/O are freed by a subsequent Send completion.\n */\nstatic void svc_rdma_write_done(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_chunk_ctxt *cc =\n\t\t\tcontainer_of(cqe, struct svc_rdma_chunk_ctxt, cc_cqe);\n\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n\tstruct svc_rdma_write_info *info =\n\t\t\tcontainer_of(cc, struct svc_rdma_write_info, wi_cc);\n\n\tatomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);\n\twake_up(&rdma->sc_send_wait);\n\n\tif (unlikely(wc->status != IB_WC_SUCCESS)) {\n\t\tset_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);\n\t\tif (wc->status != IB_WC_WR_FLUSH_ERR)\n\t\t\tpr_err(\"svcrdma: write ctx: %s (%u/0x%x)\\n\",\n\t\t\t       ib_wc_status_msg(wc->status),\n\t\t\t       wc->status, wc->vendor_err);\n\t}\n\n\tsvc_rdma_write_info_free(info);\n}\n\n/* This function sleeps when the transport's Send Queue is congested.\n *\n * Assumptions:\n * - If ib_post_send() succeeds, only one completion is expected,\n *   even if one or more WRs are flushed. This is true when posting\n *   an rdma_rw_ctx or when posting a single signaled WR.\n */\nstatic int svc_rdma_post_chunk_ctxt(struct svc_rdma_chunk_ctxt *cc)\n{\n\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n\tstruct svc_xprt *xprt = &rdma->sc_xprt;\n\tstruct ib_send_wr *first_wr, *bad_wr;\n\tstruct list_head *tmp;\n\tstruct ib_cqe *cqe;\n\tint ret;\n\n\tfirst_wr = NULL;\n\tcqe = &cc->cc_cqe;\n\tlist_for_each(tmp, &cc->cc_rwctxts) {\n\t\tstruct svc_rdma_rw_ctxt *ctxt;\n\n\t\tctxt = list_entry(tmp, struct svc_rdma_rw_ctxt, rw_list);\n\t\tfirst_wr = rdma_rw_ctx_wrs(&ctxt->rw_ctx, rdma->sc_qp,\n\t\t\t\t\t   rdma->sc_port_num, cqe, first_wr);\n\t\tcqe = NULL;\n\t}\n\n\tdo {\n\t\tif (atomic_sub_return(cc->cc_sqecount,\n\t\t\t\t      &rdma->sc_sq_avail) > 0) {\n\t\t\tret = ib_post_send(rdma->sc_qp, first_wr, &bad_wr);\n\t\t\tif (ret)\n\t\t\t\tbreak;\n\t\t\treturn 0;\n\t\t}\n\n\t\tatomic_inc(&rdma_stat_sq_starve);\n\t\tatomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);\n\t\twait_event(rdma->sc_send_wait,\n\t\t\t   atomic_read(&rdma->sc_sq_avail) > cc->cc_sqecount);\n\t} while (1);\n\n\tpr_err(\"svcrdma: ib_post_send failed (%d)\\n\", ret);\n\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\n\t/* If even one was posted, there will be a completion. */\n\tif (bad_wr != first_wr)\n\t\treturn 0;\n\n\tatomic_add(cc->cc_sqecount, &rdma->sc_sq_avail);\n\twake_up(&rdma->sc_send_wait);\n\treturn -ENOTCONN;\n}\n\n/* Build and DMA-map an SGL that covers one kvec in an xdr_buf\n */\nstatic void svc_rdma_vec_to_sg(struct svc_rdma_write_info *info,\n\t\t\t       unsigned int len,\n\t\t\t       struct svc_rdma_rw_ctxt *ctxt)\n{\n\tstruct scatterlist *sg = ctxt->rw_sg_table.sgl;\n\n\tsg_set_buf(&sg[0], info->wi_base, len);\n\tinfo->wi_base += len;\n\n\tctxt->rw_nents = 1;\n}\n\n/* Build and DMA-map an SGL that covers part of an xdr_buf's pagelist.\n */\nstatic void svc_rdma_pagelist_to_sg(struct svc_rdma_write_info *info,\n\t\t\t\t    unsigned int remaining,\n\t\t\t\t    struct svc_rdma_rw_ctxt *ctxt)\n{\n\tunsigned int sge_no, sge_bytes, page_off, page_no;\n\tstruct xdr_buf *xdr = info->wi_xdr;\n\tstruct scatterlist *sg;\n\tstruct page **page;\n\n\tpage_off = (info->wi_next_off + xdr->page_base) & ~PAGE_MASK;\n\tpage_no = (info->wi_next_off + xdr->page_base) >> PAGE_SHIFT;\n\tpage = xdr->pages + page_no;\n\tinfo->wi_next_off += remaining;\n\tsg = ctxt->rw_sg_table.sgl;\n\tsge_no = 0;\n\tdo {\n\t\tsge_bytes = min_t(unsigned int, remaining,\n\t\t\t\t  PAGE_SIZE - page_off);\n\t\tsg_set_page(sg, *page, sge_bytes, page_off);\n\n\t\tremaining -= sge_bytes;\n\t\tsg = sg_next(sg);\n\t\tpage_off = 0;\n\t\tsge_no++;\n\t\tpage++;\n\t} while (remaining);\n\n\tctxt->rw_nents = sge_no;\n}\n\n/* Construct RDMA Write WRs to send a portion of an xdr_buf containing\n * an RPC Reply.\n */\nstatic int\nsvc_rdma_build_writes(struct svc_rdma_write_info *info,\n\t\t      void (*constructor)(struct svc_rdma_write_info *info,\n\t\t\t\t\t  unsigned int len,\n\t\t\t\t\t  struct svc_rdma_rw_ctxt *ctxt),\n\t\t      unsigned int remaining)\n{\n\tstruct svc_rdma_chunk_ctxt *cc = &info->wi_cc;\n\tstruct svcxprt_rdma *rdma = cc->cc_rdma;\n\tstruct svc_rdma_rw_ctxt *ctxt;\n\t__be32 *seg;\n\tint ret;\n\n\tcc->cc_cqe.done = svc_rdma_write_done;\n\tseg = info->wi_segs + info->wi_seg_no * rpcrdma_segment_maxsz;\n\tdo {\n\t\tunsigned int write_len;\n\t\tu32 seg_length, seg_handle;\n\t\tu64 seg_offset;\n\n\t\tif (info->wi_seg_no >= info->wi_nsegs)\n\t\t\tgoto out_overflow;\n\n\t\tseg_handle = be32_to_cpup(seg);\n\t\tseg_length = be32_to_cpup(seg + 1);\n\t\txdr_decode_hyper(seg + 2, &seg_offset);\n\t\tseg_offset += info->wi_seg_off;\n\n\t\twrite_len = min(remaining, seg_length - info->wi_seg_off);\n\t\tctxt = svc_rdma_get_rw_ctxt(rdma,\n\t\t\t\t\t    (write_len >> PAGE_SHIFT) + 2);\n\t\tif (!ctxt)\n\t\t\tgoto out_noctx;\n\n\t\tconstructor(info, write_len, ctxt);\n\t\tret = rdma_rw_ctx_init(&ctxt->rw_ctx, rdma->sc_qp,\n\t\t\t\t       rdma->sc_port_num, ctxt->rw_sg_table.sgl,\n\t\t\t\t       ctxt->rw_nents, 0, seg_offset,\n\t\t\t\t       seg_handle, DMA_TO_DEVICE);\n\t\tif (ret < 0)\n\t\t\tgoto out_initerr;\n\n\t\tlist_add(&ctxt->rw_list, &cc->cc_rwctxts);\n\t\tcc->cc_sqecount += ret;\n\t\tif (write_len == seg_length - info->wi_seg_off) {\n\t\t\tseg += 4;\n\t\t\tinfo->wi_seg_no++;\n\t\t\tinfo->wi_seg_off = 0;\n\t\t} else {\n\t\t\tinfo->wi_seg_off += write_len;\n\t\t}\n\t\tremaining -= write_len;\n\t} while (remaining);\n\n\treturn 0;\n\nout_overflow:\n\tdprintk(\"svcrdma: inadequate space in Write chunk (%u)\\n\",\n\t\tinfo->wi_nsegs);\n\treturn -E2BIG;\n\nout_noctx:\n\tdprintk(\"svcrdma: no R/W ctxs available\\n\");\n\treturn -ENOMEM;\n\nout_initerr:\n\tsvc_rdma_put_rw_ctxt(rdma, ctxt);\n\tpr_err(\"svcrdma: failed to map pagelist (%d)\\n\", ret);\n\treturn -EIO;\n}\n\n/* Send one of an xdr_buf's kvecs by itself. To send a Reply\n * chunk, the whole RPC Reply is written back to the client.\n * This function writes either the head or tail of the xdr_buf\n * containing the Reply.\n */\nstatic int svc_rdma_send_xdr_kvec(struct svc_rdma_write_info *info,\n\t\t\t\t  struct kvec *vec)\n{\n\tinfo->wi_base = vec->iov_base;\n\treturn svc_rdma_build_writes(info, svc_rdma_vec_to_sg,\n\t\t\t\t     vec->iov_len);\n}\n\n/* Send an xdr_buf's page list by itself. A Write chunk is\n * just the page list. a Reply chunk is the head, page list,\n * and tail. This function is shared between the two types\n * of chunk.\n */\nstatic int svc_rdma_send_xdr_pagelist(struct svc_rdma_write_info *info,\n\t\t\t\t      struct xdr_buf *xdr)\n{\n\tinfo->wi_xdr = xdr;\n\tinfo->wi_next_off = 0;\n\treturn svc_rdma_build_writes(info, svc_rdma_pagelist_to_sg,\n\t\t\t\t     xdr->page_len);\n}\n\n/**\n * svc_rdma_send_write_chunk - Write all segments in a Write chunk\n * @rdma: controlling RDMA transport\n * @wr_ch: Write chunk provided by client\n * @xdr: xdr_buf containing the data payload\n *\n * Returns a non-negative number of bytes the chunk consumed, or\n *\t%-E2BIG if the payload was larger than the Write chunk,\n *\t%-ENOMEM if rdma_rw context pool was exhausted,\n *\t%-ENOTCONN if posting failed (connection is lost),\n *\t%-EIO if rdma_rw initialization failed (DMA mapping, etc).\n */\nint svc_rdma_send_write_chunk(struct svcxprt_rdma *rdma, __be32 *wr_ch,\n\t\t\t      struct xdr_buf *xdr)\n{\n\tstruct svc_rdma_write_info *info;\n\tint ret;\n\n\tif (!xdr->page_len)\n\t\treturn 0;\n\n\tinfo = svc_rdma_write_info_alloc(rdma, wr_ch);\n\tif (!info)\n\t\treturn -ENOMEM;\n\n\tret = svc_rdma_send_xdr_pagelist(info, xdr);\n\tif (ret < 0)\n\t\tgoto out_err;\n\n\tret = svc_rdma_post_chunk_ctxt(&info->wi_cc);\n\tif (ret < 0)\n\t\tgoto out_err;\n\treturn xdr->page_len;\n\nout_err:\n\tsvc_rdma_write_info_free(info);\n\treturn ret;\n}\n\n/**\n * svc_rdma_send_reply_chunk - Write all segments in the Reply chunk\n * @rdma: controlling RDMA transport\n * @rp_ch: Reply chunk provided by client\n * @writelist: true if client provided a Write list\n * @xdr: xdr_buf containing an RPC Reply\n *\n * Returns a non-negative number of bytes the chunk consumed, or\n *\t%-E2BIG if the payload was larger than the Reply chunk,\n *\t%-ENOMEM if rdma_rw context pool was exhausted,\n *\t%-ENOTCONN if posting failed (connection is lost),\n *\t%-EIO if rdma_rw initialization failed (DMA mapping, etc).\n */\nint svc_rdma_send_reply_chunk(struct svcxprt_rdma *rdma, __be32 *rp_ch,\n\t\t\t      bool writelist, struct xdr_buf *xdr)\n{\n\tstruct svc_rdma_write_info *info;\n\tint consumed, ret;\n\n\tinfo = svc_rdma_write_info_alloc(rdma, rp_ch);\n\tif (!info)\n\t\treturn -ENOMEM;\n\n\tret = svc_rdma_send_xdr_kvec(info, &xdr->head[0]);\n\tif (ret < 0)\n\t\tgoto out_err;\n\tconsumed = xdr->head[0].iov_len;\n\n\t/* Send the page list in the Reply chunk only if the\n\t * client did not provide Write chunks.\n\t */\n\tif (!writelist && xdr->page_len) {\n\t\tret = svc_rdma_send_xdr_pagelist(info, xdr);\n\t\tif (ret < 0)\n\t\t\tgoto out_err;\n\t\tconsumed += xdr->page_len;\n\t}\n\n\tif (xdr->tail[0].iov_len) {\n\t\tret = svc_rdma_send_xdr_kvec(info, &xdr->tail[0]);\n\t\tif (ret < 0)\n\t\t\tgoto out_err;\n\t\tconsumed += xdr->tail[0].iov_len;\n\t}\n\n\tret = svc_rdma_post_chunk_ctxt(&info->wi_cc);\n\tif (ret < 0)\n\t\tgoto out_err;\n\treturn consumed;\n\nout_err:\n\tsvc_rdma_write_info_free(info);\n\treturn ret;\n}\n", "/*\n * Copyright (c) 2016 Oracle. All rights reserved.\n * Copyright (c) 2014 Open Grid Computing, Inc. All rights reserved.\n * Copyright (c) 2005-2006 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n/* Operation\n *\n * The main entry point is svc_rdma_sendto. This is called by the\n * RPC server when an RPC Reply is ready to be transmitted to a client.\n *\n * The passed-in svc_rqst contains a struct xdr_buf which holds an\n * XDR-encoded RPC Reply message. sendto must construct the RPC-over-RDMA\n * transport header, post all Write WRs needed for this Reply, then post\n * a Send WR conveying the transport header and the RPC message itself to\n * the client.\n *\n * svc_rdma_sendto must fully transmit the Reply before returning, as\n * the svc_rqst will be recycled as soon as sendto returns. Remaining\n * resources referred to by the svc_rqst are also recycled at that time.\n * Therefore any resources that must remain longer must be detached\n * from the svc_rqst and released later.\n *\n * Page Management\n *\n * The I/O that performs Reply transmission is asynchronous, and may\n * complete well after sendto returns. Thus pages under I/O must be\n * removed from the svc_rqst before sendto returns.\n *\n * The logic here depends on Send Queue and completion ordering. Since\n * the Send WR is always posted last, it will always complete last. Thus\n * when it completes, it is guaranteed that all previous Write WRs have\n * also completed.\n *\n * Write WRs are constructed and posted. Each Write segment gets its own\n * svc_rdma_rw_ctxt, allowing the Write completion handler to find and\n * DMA-unmap the pages under I/O for that Write segment. The Write\n * completion handler does not release any pages.\n *\n * When the Send WR is constructed, it also gets its own svc_rdma_op_ctxt.\n * The ownership of all of the Reply's pages are transferred into that\n * ctxt, the Send WR is posted, and sendto returns.\n *\n * The svc_rdma_op_ctxt is presented when the Send WR completes. The\n * Send completion handler finally releases the Reply's pages.\n *\n * This mechanism also assumes that completions on the transport's Send\n * Completion Queue do not run in parallel. Otherwise a Write completion\n * and Send completion running at the same time could release pages that\n * are still DMA-mapped.\n *\n * Error Handling\n *\n * - If the Send WR is posted successfully, it will either complete\n *   successfully, or get flushed. Either way, the Send completion\n *   handler releases the Reply's pages.\n * - If the Send WR cannot be not posted, the forward path releases\n *   the Reply's pages.\n *\n * This handles the case, without the use of page reference counting,\n * where two different Write segments send portions of the same page.\n */\n\n#include <linux/sunrpc/debug.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <linux/spinlock.h>\n#include <asm/unaligned.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n#include <linux/sunrpc/svc_rdma.h>\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\nstatic u32 xdr_padsize(u32 len)\n{\n\treturn (len & 3) ? (4 - (len & 3)) : 0;\n}\n\n/* Returns length of transport header, in bytes.\n */\nstatic unsigned int svc_rdma_reply_hdr_len(__be32 *rdma_resp)\n{\n\tunsigned int nsegs;\n\t__be32 *p;\n\n\tp = rdma_resp;\n\n\t/* RPC-over-RDMA V1 replies never have a Read list. */\n\tp += rpcrdma_fixed_maxsz + 1;\n\n\t/* Skip Write list. */\n\twhile (*p++ != xdr_zero) {\n\t\tnsegs = be32_to_cpup(p++);\n\t\tp += nsegs * rpcrdma_segment_maxsz;\n\t}\n\n\t/* Skip Reply chunk. */\n\tif (*p++ != xdr_zero) {\n\t\tnsegs = be32_to_cpup(p++);\n\t\tp += nsegs * rpcrdma_segment_maxsz;\n\t}\n\n\treturn (unsigned long)p - (unsigned long)rdma_resp;\n}\n\n/* One Write chunk is copied from Call transport header to Reply\n * transport header. Each segment's length field is updated to\n * reflect number of bytes consumed in the segment.\n *\n * Returns number of segments in this chunk.\n */\nstatic unsigned int xdr_encode_write_chunk(__be32 *dst, __be32 *src,\n\t\t\t\t\t   unsigned int remaining)\n{\n\tunsigned int i, nsegs;\n\tu32 seg_len;\n\n\t/* Write list discriminator */\n\t*dst++ = *src++;\n\n\t/* number of segments in this chunk */\n\tnsegs = be32_to_cpup(src);\n\t*dst++ = *src++;\n\n\tfor (i = nsegs; i; i--) {\n\t\t/* segment's RDMA handle */\n\t\t*dst++ = *src++;\n\n\t\t/* bytes returned in this segment */\n\t\tseg_len = be32_to_cpu(*src);\n\t\tif (remaining >= seg_len) {\n\t\t\t/* entire segment was consumed */\n\t\t\t*dst = *src;\n\t\t\tremaining -= seg_len;\n\t\t} else {\n\t\t\t/* segment only partly filled */\n\t\t\t*dst = cpu_to_be32(remaining);\n\t\t\tremaining = 0;\n\t\t}\n\t\tdst++; src++;\n\n\t\t/* segment's RDMA offset */\n\t\t*dst++ = *src++;\n\t\t*dst++ = *src++;\n\t}\n\n\treturn nsegs;\n}\n\n/* The client provided a Write list in the Call message. Fill in\n * the segments in the first Write chunk in the Reply's transport\n * header with the number of bytes consumed in each segment.\n * Remaining chunks are returned unused.\n *\n * Assumptions:\n *  - Client has provided only one Write chunk\n */\nstatic void svc_rdma_xdr_encode_write_list(__be32 *rdma_resp, __be32 *wr_ch,\n\t\t\t\t\t   unsigned int consumed)\n{\n\tunsigned int nsegs;\n\t__be32 *p, *q;\n\n\t/* RPC-over-RDMA V1 replies never have a Read list. */\n\tp = rdma_resp + rpcrdma_fixed_maxsz + 1;\n\n\tq = wr_ch;\n\twhile (*q != xdr_zero) {\n\t\tnsegs = xdr_encode_write_chunk(p, q, consumed);\n\t\tq += 2 + nsegs * rpcrdma_segment_maxsz;\n\t\tp += 2 + nsegs * rpcrdma_segment_maxsz;\n\t\tconsumed = 0;\n\t}\n\n\t/* Terminate Write list */\n\t*p++ = xdr_zero;\n\n\t/* Reply chunk discriminator; may be replaced later */\n\t*p = xdr_zero;\n}\n\n/* The client provided a Reply chunk in the Call message. Fill in\n * the segments in the Reply chunk in the Reply message with the\n * number of bytes consumed in each segment.\n *\n * Assumptions:\n * - Reply can always fit in the provided Reply chunk\n */\nstatic void svc_rdma_xdr_encode_reply_chunk(__be32 *rdma_resp, __be32 *rp_ch,\n\t\t\t\t\t    unsigned int consumed)\n{\n\t__be32 *p;\n\n\t/* Find the Reply chunk in the Reply's xprt header.\n\t * RPC-over-RDMA V1 replies never have a Read list.\n\t */\n\tp = rdma_resp + rpcrdma_fixed_maxsz + 1;\n\n\t/* Skip past Write list */\n\twhile (*p++ != xdr_zero)\n\t\tp += 1 + be32_to_cpup(p) * rpcrdma_segment_maxsz;\n\n\txdr_encode_write_chunk(p, rp_ch, consumed);\n}\n\n/* Parse the RPC Call's transport header.\n */\nstatic void svc_rdma_get_write_arrays(__be32 *rdma_argp,\n\t\t\t\t      __be32 **write, __be32 **reply)\n{\n\t__be32 *p;\n\n\tp = rdma_argp + rpcrdma_fixed_maxsz;\n\n\t/* Read list */\n\twhile (*p++ != xdr_zero)\n\t\tp += 5;\n\n\t/* Write list */\n\tif (*p != xdr_zero) {\n\t\t*write = p;\n\t\twhile (*p++ != xdr_zero)\n\t\t\tp += 1 + be32_to_cpu(*p) * 4;\n\t} else {\n\t\t*write = NULL;\n\t\tp++;\n\t}\n\n\t/* Reply chunk */\n\tif (*p != xdr_zero)\n\t\t*reply = p;\n\telse\n\t\t*reply = NULL;\n}\n\n/* RPC-over-RDMA Version One private extension: Remote Invalidation.\n * Responder's choice: requester signals it can handle Send With\n * Invalidate, and responder chooses one rkey to invalidate.\n *\n * Find a candidate rkey to invalidate when sending a reply.  Picks the\n * first R_key it finds in the chunk lists.\n *\n * Returns zero if RPC's chunk lists are empty.\n */\nstatic u32 svc_rdma_get_inv_rkey(__be32 *rdma_argp,\n\t\t\t\t __be32 *wr_lst, __be32 *rp_ch)\n{\n\t__be32 *p;\n\n\tp = rdma_argp + rpcrdma_fixed_maxsz;\n\tif (*p != xdr_zero)\n\t\tp += 2;\n\telse if (wr_lst && be32_to_cpup(wr_lst + 1))\n\t\tp = wr_lst + 2;\n\telse if (rp_ch && be32_to_cpup(rp_ch + 1))\n\t\tp = rp_ch + 2;\n\telse\n\t\treturn 0;\n\treturn be32_to_cpup(p);\n}\n\n/* ib_dma_map_page() is used here because svc_rdma_dma_unmap()\n * is used during completion to DMA-unmap this memory, and\n * it uses ib_dma_unmap_page() exclusively.\n */\nstatic int svc_rdma_dma_map_buf(struct svcxprt_rdma *rdma,\n\t\t\t\tstruct svc_rdma_op_ctxt *ctxt,\n\t\t\t\tunsigned int sge_no,\n\t\t\t\tunsigned char *base,\n\t\t\t\tunsigned int len)\n{\n\tunsigned long offset = (unsigned long)base & ~PAGE_MASK;\n\tstruct ib_device *dev = rdma->sc_cm_id->device;\n\tdma_addr_t dma_addr;\n\n\tdma_addr = ib_dma_map_page(dev, virt_to_page(base),\n\t\t\t\t   offset, len, DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(dev, dma_addr))\n\t\treturn -EIO;\n\n\tctxt->sge[sge_no].addr = dma_addr;\n\tctxt->sge[sge_no].length = len;\n\tctxt->sge[sge_no].lkey = rdma->sc_pd->local_dma_lkey;\n\tsvc_rdma_count_mappings(rdma, ctxt);\n\treturn 0;\n}\n\nstatic int svc_rdma_dma_map_page(struct svcxprt_rdma *rdma,\n\t\t\t\t struct svc_rdma_op_ctxt *ctxt,\n\t\t\t\t unsigned int sge_no,\n\t\t\t\t struct page *page,\n\t\t\t\t unsigned int offset,\n\t\t\t\t unsigned int len)\n{\n\tstruct ib_device *dev = rdma->sc_cm_id->device;\n\tdma_addr_t dma_addr;\n\n\tdma_addr = ib_dma_map_page(dev, page, offset, len, DMA_TO_DEVICE);\n\tif (ib_dma_mapping_error(dev, dma_addr))\n\t\treturn -EIO;\n\n\tctxt->sge[sge_no].addr = dma_addr;\n\tctxt->sge[sge_no].length = len;\n\tctxt->sge[sge_no].lkey = rdma->sc_pd->local_dma_lkey;\n\tsvc_rdma_count_mappings(rdma, ctxt);\n\treturn 0;\n}\n\n/**\n * svc_rdma_map_reply_hdr - DMA map the transport header buffer\n * @rdma: controlling transport\n * @ctxt: op_ctxt for the Send WR\n * @rdma_resp: buffer containing transport header\n * @len: length of transport header\n *\n * Returns:\n *\t%0 if the header is DMA mapped,\n *\t%-EIO if DMA mapping failed.\n */\nint svc_rdma_map_reply_hdr(struct svcxprt_rdma *rdma,\n\t\t\t   struct svc_rdma_op_ctxt *ctxt,\n\t\t\t   __be32 *rdma_resp,\n\t\t\t   unsigned int len)\n{\n\tctxt->direction = DMA_TO_DEVICE;\n\tctxt->pages[0] = virt_to_page(rdma_resp);\n\tctxt->count = 1;\n\treturn svc_rdma_dma_map_page(rdma, ctxt, 0, ctxt->pages[0], 0, len);\n}\n\n/* Load the xdr_buf into the ctxt's sge array, and DMA map each\n * element as it is added.\n *\n * Returns the number of sge elements loaded on success, or\n * a negative errno on failure.\n */\nstatic int svc_rdma_map_reply_msg(struct svcxprt_rdma *rdma,\n\t\t\t\t  struct svc_rdma_op_ctxt *ctxt,\n\t\t\t\t  struct xdr_buf *xdr, __be32 *wr_lst)\n{\n\tunsigned int len, sge_no, remaining, page_off;\n\tstruct page **ppages;\n\tunsigned char *base;\n\tu32 xdr_pad;\n\tint ret;\n\n\tsge_no = 1;\n\n\tret = svc_rdma_dma_map_buf(rdma, ctxt, sge_no++,\n\t\t\t\t   xdr->head[0].iov_base,\n\t\t\t\t   xdr->head[0].iov_len);\n\tif (ret < 0)\n\t\treturn ret;\n\n\t/* If a Write chunk is present, the xdr_buf's page list\n\t * is not included inline. However the Upper Layer may\n\t * have added XDR padding in the tail buffer, and that\n\t * should not be included inline.\n\t */\n\tif (wr_lst) {\n\t\tbase = xdr->tail[0].iov_base;\n\t\tlen = xdr->tail[0].iov_len;\n\t\txdr_pad = xdr_padsize(xdr->page_len);\n\n\t\tif (len && xdr_pad) {\n\t\t\tbase += xdr_pad;\n\t\t\tlen -= xdr_pad;\n\t\t}\n\n\t\tgoto tail;\n\t}\n\n\tppages = xdr->pages + (xdr->page_base >> PAGE_SHIFT);\n\tpage_off = xdr->page_base & ~PAGE_MASK;\n\tremaining = xdr->page_len;\n\twhile (remaining) {\n\t\tlen = min_t(u32, PAGE_SIZE - page_off, remaining);\n\n\t\tret = svc_rdma_dma_map_page(rdma, ctxt, sge_no++,\n\t\t\t\t\t    *ppages++, page_off, len);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\n\t\tremaining -= len;\n\t\tpage_off = 0;\n\t}\n\n\tbase = xdr->tail[0].iov_base;\n\tlen = xdr->tail[0].iov_len;\ntail:\n\tif (len) {\n\t\tret = svc_rdma_dma_map_buf(rdma, ctxt, sge_no++, base, len);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\treturn sge_no - 1;\n}\n\n/* The svc_rqst and all resources it owns are released as soon as\n * svc_rdma_sendto returns. Transfer pages under I/O to the ctxt\n * so they are released by the Send completion handler.\n */\nstatic void svc_rdma_save_io_pages(struct svc_rqst *rqstp,\n\t\t\t\t   struct svc_rdma_op_ctxt *ctxt)\n{\n\tint i, pages = rqstp->rq_next_page - rqstp->rq_respages;\n\n\tctxt->count += pages;\n\tfor (i = 0; i < pages; i++) {\n\t\tctxt->pages[i + 1] = rqstp->rq_respages[i];\n\t\trqstp->rq_respages[i] = NULL;\n\t}\n\trqstp->rq_next_page = rqstp->rq_respages + 1;\n}\n\n/**\n * svc_rdma_post_send_wr - Set up and post one Send Work Request\n * @rdma: controlling transport\n * @ctxt: op_ctxt for transmitting the Send WR\n * @num_sge: number of SGEs to send\n * @inv_rkey: R_key argument to Send With Invalidate, or zero\n *\n * Returns:\n *\t%0 if the Send* was posted successfully,\n *\t%-ENOTCONN if the connection was lost or dropped,\n *\t%-EINVAL if there was a problem with the Send we built,\n *\t%-ENOMEM if ib_post_send failed.\n */\nint svc_rdma_post_send_wr(struct svcxprt_rdma *rdma,\n\t\t\t  struct svc_rdma_op_ctxt *ctxt, int num_sge,\n\t\t\t  u32 inv_rkey)\n{\n\tstruct ib_send_wr *send_wr = &ctxt->send_wr;\n\n\tdprintk(\"svcrdma: posting Send WR with %u sge(s)\\n\", num_sge);\n\n\tsend_wr->next = NULL;\n\tctxt->cqe.done = svc_rdma_wc_send;\n\tsend_wr->wr_cqe = &ctxt->cqe;\n\tsend_wr->sg_list = ctxt->sge;\n\tsend_wr->num_sge = num_sge;\n\tsend_wr->send_flags = IB_SEND_SIGNALED;\n\tif (inv_rkey) {\n\t\tsend_wr->opcode = IB_WR_SEND_WITH_INV;\n\t\tsend_wr->ex.invalidate_rkey = inv_rkey;\n\t} else {\n\t\tsend_wr->opcode = IB_WR_SEND;\n\t}\n\n\treturn svc_rdma_send(rdma, send_wr);\n}\n\n/* Prepare the portion of the RPC Reply that will be transmitted\n * via RDMA Send. The RPC-over-RDMA transport header is prepared\n * in sge[0], and the RPC xdr_buf is prepared in following sges.\n *\n * Depending on whether a Write list or Reply chunk is present,\n * the server may send all, a portion of, or none of the xdr_buf.\n * In the latter case, only the transport header (sge[0]) is\n * transmitted.\n *\n * RDMA Send is the last step of transmitting an RPC reply. Pages\n * involved in the earlier RDMA Writes are here transferred out\n * of the rqstp and into the ctxt's page array. These pages are\n * DMA unmapped by each Write completion, but the subsequent Send\n * completion finally releases these pages.\n *\n * Assumptions:\n * - The Reply's transport header will never be larger than a page.\n */\nstatic int svc_rdma_send_reply_msg(struct svcxprt_rdma *rdma,\n\t\t\t\t   __be32 *rdma_argp, __be32 *rdma_resp,\n\t\t\t\t   struct svc_rqst *rqstp,\n\t\t\t\t   __be32 *wr_lst, __be32 *rp_ch)\n{\n\tstruct svc_rdma_op_ctxt *ctxt;\n\tu32 inv_rkey;\n\tint ret;\n\n\tdprintk(\"svcrdma: sending %s reply: head=%zu, pagelen=%u, tail=%zu\\n\",\n\t\t(rp_ch ? \"RDMA_NOMSG\" : \"RDMA_MSG\"),\n\t\trqstp->rq_res.head[0].iov_len,\n\t\trqstp->rq_res.page_len,\n\t\trqstp->rq_res.tail[0].iov_len);\n\n\tctxt = svc_rdma_get_context(rdma);\n\n\tret = svc_rdma_map_reply_hdr(rdma, ctxt, rdma_resp,\n\t\t\t\t     svc_rdma_reply_hdr_len(rdma_resp));\n\tif (ret < 0)\n\t\tgoto err;\n\n\tif (!rp_ch) {\n\t\tret = svc_rdma_map_reply_msg(rdma, ctxt,\n\t\t\t\t\t     &rqstp->rq_res, wr_lst);\n\t\tif (ret < 0)\n\t\t\tgoto err;\n\t}\n\n\tsvc_rdma_save_io_pages(rqstp, ctxt);\n\n\tinv_rkey = 0;\n\tif (rdma->sc_snd_w_inv)\n\t\tinv_rkey = svc_rdma_get_inv_rkey(rdma_argp, wr_lst, rp_ch);\n\tret = svc_rdma_post_send_wr(rdma, ctxt, 1 + ret, inv_rkey);\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tpr_err(\"svcrdma: failed to post Send WR (%d)\\n\", ret);\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 1);\n\treturn ret;\n}\n\n/* Given the client-provided Write and Reply chunks, the server was not\n * able to form a complete reply. Return an RDMA_ERROR message so the\n * client can retire this RPC transaction. As above, the Send completion\n * routine releases payload pages that were part of a previous RDMA Write.\n *\n * Remote Invalidation is skipped for simplicity.\n */\nstatic int svc_rdma_send_error_msg(struct svcxprt_rdma *rdma,\n\t\t\t\t   __be32 *rdma_resp, struct svc_rqst *rqstp)\n{\n\tstruct svc_rdma_op_ctxt *ctxt;\n\t__be32 *p;\n\tint ret;\n\n\tctxt = svc_rdma_get_context(rdma);\n\n\t/* Replace the original transport header with an\n\t * RDMA_ERROR response. XID etc are preserved.\n\t */\n\tp = rdma_resp + 3;\n\t*p++ = rdma_error;\n\t*p   = err_chunk;\n\n\tret = svc_rdma_map_reply_hdr(rdma, ctxt, rdma_resp, 20);\n\tif (ret < 0)\n\t\tgoto err;\n\n\tsvc_rdma_save_io_pages(rqstp, ctxt);\n\n\tret = svc_rdma_post_send_wr(rdma, ctxt, 1 + ret, 0);\n\tif (ret)\n\t\tgoto err;\n\n\treturn 0;\n\nerr:\n\tpr_err(\"svcrdma: failed to post Send WR (%d)\\n\", ret);\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 1);\n\treturn ret;\n}\n\nvoid svc_rdma_prep_reply_hdr(struct svc_rqst *rqstp)\n{\n}\n\n/**\n * svc_rdma_sendto - Transmit an RPC reply\n * @rqstp: processed RPC request, reply XDR already in ::rq_res\n *\n * Any resources still associated with @rqstp are released upon return.\n * If no reply message was possible, the connection is closed.\n *\n * Returns:\n *\t%0 if an RPC reply has been successfully posted,\n *\t%-ENOMEM if a resource shortage occurred (connection is lost),\n *\t%-ENOTCONN if posting failed (connection is lost).\n */\nint svc_rdma_sendto(struct svc_rqst *rqstp)\n{\n\tstruct svc_xprt *xprt = rqstp->rq_xprt;\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\t__be32 *p, *rdma_argp, *rdma_resp, *wr_lst, *rp_ch;\n\tstruct xdr_buf *xdr = &rqstp->rq_res;\n\tstruct page *res_page;\n\tint ret;\n\n\t/* Find the call's chunk lists to decide how to send the reply.\n\t * Receive places the Call's xprt header at the start of page 0.\n\t */\n\trdma_argp = page_address(rqstp->rq_pages[0]);\n\tsvc_rdma_get_write_arrays(rdma_argp, &wr_lst, &rp_ch);\n\n\tdprintk(\"svcrdma: preparing response for XID 0x%08x\\n\",\n\t\tbe32_to_cpup(rdma_argp));\n\n\t/* Create the RDMA response header. xprt->xpt_mutex,\n\t * acquired in svc_send(), serializes RPC replies. The\n\t * code path below that inserts the credit grant value\n\t * into each transport header runs only inside this\n\t * critical section.\n\t */\n\tret = -ENOMEM;\n\tres_page = alloc_page(GFP_KERNEL);\n\tif (!res_page)\n\t\tgoto err0;\n\trdma_resp = page_address(res_page);\n\n\tp = rdma_resp;\n\t*p++ = *rdma_argp;\n\t*p++ = *(rdma_argp + 1);\n\t*p++ = rdma->sc_fc_credits;\n\t*p++ = rp_ch ? rdma_nomsg : rdma_msg;\n\n\t/* Start with empty chunks */\n\t*p++ = xdr_zero;\n\t*p++ = xdr_zero;\n\t*p   = xdr_zero;\n\n\tif (wr_lst) {\n\t\t/* XXX: Presume the client sent only one Write chunk */\n\t\tret = svc_rdma_send_write_chunk(rdma, wr_lst, xdr);\n\t\tif (ret < 0)\n\t\t\tgoto err2;\n\t\tsvc_rdma_xdr_encode_write_list(rdma_resp, wr_lst, ret);\n\t}\n\tif (rp_ch) {\n\t\tret = svc_rdma_send_reply_chunk(rdma, rp_ch, wr_lst, xdr);\n\t\tif (ret < 0)\n\t\t\tgoto err2;\n\t\tsvc_rdma_xdr_encode_reply_chunk(rdma_resp, rp_ch, ret);\n\t}\n\n\tret = svc_rdma_post_recv(rdma, GFP_KERNEL);\n\tif (ret)\n\t\tgoto err1;\n\tret = svc_rdma_send_reply_msg(rdma, rdma_argp, rdma_resp, rqstp,\n\t\t\t\t      wr_lst, rp_ch);\n\tif (ret < 0)\n\t\tgoto err0;\n\treturn 0;\n\n err2:\n\tif (ret != -E2BIG)\n\t\tgoto err1;\n\n\tret = svc_rdma_post_recv(rdma, GFP_KERNEL);\n\tif (ret)\n\t\tgoto err1;\n\tret = svc_rdma_send_error_msg(rdma, rdma_resp, rqstp);\n\tif (ret < 0)\n\t\tgoto err0;\n\treturn 0;\n\n err1:\n\tput_page(res_page);\n err0:\n\tpr_err(\"svcrdma: Could not send reply, err=%d. Closing transport.\\n\",\n\t       ret);\n\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\treturn -ENOTCONN;\n}\n", "/*\n * Copyright (c) 2014 Open Grid Computing, Inc. All rights reserved.\n * Copyright (c) 2005-2007 Network Appliance, Inc. All rights reserved.\n *\n * This software is available to you under a choice of one of two\n * licenses.  You may choose to be licensed under the terms of the GNU\n * General Public License (GPL) Version 2, available from the file\n * COPYING in the main directory of this source tree, or the BSD-type\n * license below:\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n *\n *      Redistributions of source code must retain the above copyright\n *      notice, this list of conditions and the following disclaimer.\n *\n *      Redistributions in binary form must reproduce the above\n *      copyright notice, this list of conditions and the following\n *      disclaimer in the documentation and/or other materials provided\n *      with the distribution.\n *\n *      Neither the name of the Network Appliance, Inc. nor the names of\n *      its contributors may be used to endorse or promote products\n *      derived from this software without specific prior written\n *      permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Author: Tom Tucker <tom@opengridcomputing.com>\n */\n\n#include <linux/sunrpc/svc_xprt.h>\n#include <linux/sunrpc/addr.h>\n#include <linux/sunrpc/debug.h>\n#include <linux/sunrpc/rpc_rdma.h>\n#include <linux/interrupt.h>\n#include <linux/sched.h>\n#include <linux/slab.h>\n#include <linux/spinlock.h>\n#include <linux/workqueue.h>\n#include <rdma/ib_verbs.h>\n#include <rdma/rdma_cm.h>\n#include <linux/sunrpc/svc_rdma.h>\n#include <linux/export.h>\n#include \"xprt_rdma.h\"\n\n#define RPCDBG_FACILITY\tRPCDBG_SVCXPRT\n\nstatic struct svcxprt_rdma *rdma_create_xprt(struct svc_serv *, int);\nstatic struct svc_xprt *svc_rdma_create(struct svc_serv *serv,\n\t\t\t\t\tstruct net *net,\n\t\t\t\t\tstruct sockaddr *sa, int salen,\n\t\t\t\t\tint flags);\nstatic struct svc_xprt *svc_rdma_accept(struct svc_xprt *xprt);\nstatic void svc_rdma_release_rqst(struct svc_rqst *);\nstatic void svc_rdma_detach(struct svc_xprt *xprt);\nstatic void svc_rdma_free(struct svc_xprt *xprt);\nstatic int svc_rdma_has_wspace(struct svc_xprt *xprt);\nstatic int svc_rdma_secure_port(struct svc_rqst *);\nstatic void svc_rdma_kill_temp_xprt(struct svc_xprt *);\n\nstatic struct svc_xprt_ops svc_rdma_ops = {\n\t.xpo_create = svc_rdma_create,\n\t.xpo_recvfrom = svc_rdma_recvfrom,\n\t.xpo_sendto = svc_rdma_sendto,\n\t.xpo_release_rqst = svc_rdma_release_rqst,\n\t.xpo_detach = svc_rdma_detach,\n\t.xpo_free = svc_rdma_free,\n\t.xpo_prep_reply_hdr = svc_rdma_prep_reply_hdr,\n\t.xpo_has_wspace = svc_rdma_has_wspace,\n\t.xpo_accept = svc_rdma_accept,\n\t.xpo_secure_port = svc_rdma_secure_port,\n\t.xpo_kill_temp_xprt = svc_rdma_kill_temp_xprt,\n};\n\nstruct svc_xprt_class svc_rdma_class = {\n\t.xcl_name = \"rdma\",\n\t.xcl_owner = THIS_MODULE,\n\t.xcl_ops = &svc_rdma_ops,\n\t.xcl_max_payload = RPCSVC_MAXPAYLOAD_RDMA,\n\t.xcl_ident = XPRT_TRANSPORT_RDMA,\n};\n\n#if defined(CONFIG_SUNRPC_BACKCHANNEL)\nstatic struct svc_xprt *svc_rdma_bc_create(struct svc_serv *, struct net *,\n\t\t\t\t\t   struct sockaddr *, int, int);\nstatic void svc_rdma_bc_detach(struct svc_xprt *);\nstatic void svc_rdma_bc_free(struct svc_xprt *);\n\nstatic struct svc_xprt_ops svc_rdma_bc_ops = {\n\t.xpo_create = svc_rdma_bc_create,\n\t.xpo_detach = svc_rdma_bc_detach,\n\t.xpo_free = svc_rdma_bc_free,\n\t.xpo_prep_reply_hdr = svc_rdma_prep_reply_hdr,\n\t.xpo_secure_port = svc_rdma_secure_port,\n};\n\nstruct svc_xprt_class svc_rdma_bc_class = {\n\t.xcl_name = \"rdma-bc\",\n\t.xcl_owner = THIS_MODULE,\n\t.xcl_ops = &svc_rdma_bc_ops,\n\t.xcl_max_payload = (1024 - RPCRDMA_HDRLEN_MIN)\n};\n\nstatic struct svc_xprt *svc_rdma_bc_create(struct svc_serv *serv,\n\t\t\t\t\t   struct net *net,\n\t\t\t\t\t   struct sockaddr *sa, int salen,\n\t\t\t\t\t   int flags)\n{\n\tstruct svcxprt_rdma *cma_xprt;\n\tstruct svc_xprt *xprt;\n\n\tcma_xprt = rdma_create_xprt(serv, 0);\n\tif (!cma_xprt)\n\t\treturn ERR_PTR(-ENOMEM);\n\txprt = &cma_xprt->sc_xprt;\n\n\tsvc_xprt_init(net, &svc_rdma_bc_class, xprt, serv);\n\tset_bit(XPT_CONG_CTRL, &xprt->xpt_flags);\n\tserv->sv_bc_xprt = xprt;\n\n\tdprintk(\"svcrdma: %s(%p)\\n\", __func__, xprt);\n\treturn xprt;\n}\n\nstatic void svc_rdma_bc_detach(struct svc_xprt *xprt)\n{\n\tdprintk(\"svcrdma: %s(%p)\\n\", __func__, xprt);\n}\n\nstatic void svc_rdma_bc_free(struct svc_xprt *xprt)\n{\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\n\tdprintk(\"svcrdma: %s(%p)\\n\", __func__, xprt);\n\tif (xprt)\n\t\tkfree(rdma);\n}\n#endif\t/* CONFIG_SUNRPC_BACKCHANNEL */\n\nstatic struct svc_rdma_op_ctxt *alloc_ctxt(struct svcxprt_rdma *xprt,\n\t\t\t\t\t   gfp_t flags)\n{\n\tstruct svc_rdma_op_ctxt *ctxt;\n\n\tctxt = kmalloc(sizeof(*ctxt), flags);\n\tif (ctxt) {\n\t\tctxt->xprt = xprt;\n\t\tINIT_LIST_HEAD(&ctxt->list);\n\t}\n\treturn ctxt;\n}\n\nstatic bool svc_rdma_prealloc_ctxts(struct svcxprt_rdma *xprt)\n{\n\tunsigned int i;\n\n\t/* Each RPC/RDMA credit can consume a number of send\n\t * and receive WQEs. One ctxt is allocated for each.\n\t */\n\ti = xprt->sc_sq_depth + xprt->sc_rq_depth;\n\n\twhile (i--) {\n\t\tstruct svc_rdma_op_ctxt *ctxt;\n\n\t\tctxt = alloc_ctxt(xprt, GFP_KERNEL);\n\t\tif (!ctxt) {\n\t\t\tdprintk(\"svcrdma: No memory for RDMA ctxt\\n\");\n\t\t\treturn false;\n\t\t}\n\t\tlist_add(&ctxt->list, &xprt->sc_ctxts);\n\t}\n\treturn true;\n}\n\nstruct svc_rdma_op_ctxt *svc_rdma_get_context(struct svcxprt_rdma *xprt)\n{\n\tstruct svc_rdma_op_ctxt *ctxt = NULL;\n\n\tspin_lock(&xprt->sc_ctxt_lock);\n\txprt->sc_ctxt_used++;\n\tif (list_empty(&xprt->sc_ctxts))\n\t\tgoto out_empty;\n\n\tctxt = list_first_entry(&xprt->sc_ctxts,\n\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\tlist_del(&ctxt->list);\n\tspin_unlock(&xprt->sc_ctxt_lock);\n\nout:\n\tctxt->count = 0;\n\tctxt->mapped_sges = 0;\n\tctxt->frmr = NULL;\n\treturn ctxt;\n\nout_empty:\n\t/* Either pre-allocation missed the mark, or send\n\t * queue accounting is broken.\n\t */\n\tspin_unlock(&xprt->sc_ctxt_lock);\n\n\tctxt = alloc_ctxt(xprt, GFP_NOIO);\n\tif (ctxt)\n\t\tgoto out;\n\n\tspin_lock(&xprt->sc_ctxt_lock);\n\txprt->sc_ctxt_used--;\n\tspin_unlock(&xprt->sc_ctxt_lock);\n\tWARN_ONCE(1, \"svcrdma: empty RDMA ctxt list?\\n\");\n\treturn NULL;\n}\n\nvoid svc_rdma_unmap_dma(struct svc_rdma_op_ctxt *ctxt)\n{\n\tstruct svcxprt_rdma *xprt = ctxt->xprt;\n\tstruct ib_device *device = xprt->sc_cm_id->device;\n\tu32 lkey = xprt->sc_pd->local_dma_lkey;\n\tunsigned int i;\n\n\tfor (i = 0; i < ctxt->mapped_sges; i++) {\n\t\t/*\n\t\t * Unmap the DMA addr in the SGE if the lkey matches\n\t\t * the local_dma_lkey, otherwise, ignore it since it is\n\t\t * an FRMR lkey and will be unmapped later when the\n\t\t * last WR that uses it completes.\n\t\t */\n\t\tif (ctxt->sge[i].lkey == lkey)\n\t\t\tib_dma_unmap_page(device,\n\t\t\t\t\t    ctxt->sge[i].addr,\n\t\t\t\t\t    ctxt->sge[i].length,\n\t\t\t\t\t    ctxt->direction);\n\t}\n\tctxt->mapped_sges = 0;\n}\n\nvoid svc_rdma_put_context(struct svc_rdma_op_ctxt *ctxt, int free_pages)\n{\n\tstruct svcxprt_rdma *xprt = ctxt->xprt;\n\tint i;\n\n\tif (free_pages)\n\t\tfor (i = 0; i < ctxt->count; i++)\n\t\t\tput_page(ctxt->pages[i]);\n\n\tspin_lock(&xprt->sc_ctxt_lock);\n\txprt->sc_ctxt_used--;\n\tlist_add(&ctxt->list, &xprt->sc_ctxts);\n\tspin_unlock(&xprt->sc_ctxt_lock);\n}\n\nstatic void svc_rdma_destroy_ctxts(struct svcxprt_rdma *xprt)\n{\n\twhile (!list_empty(&xprt->sc_ctxts)) {\n\t\tstruct svc_rdma_op_ctxt *ctxt;\n\n\t\tctxt = list_first_entry(&xprt->sc_ctxts,\n\t\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\t\tlist_del(&ctxt->list);\n\t\tkfree(ctxt);\n\t}\n}\n\n/* QP event handler */\nstatic void qp_event_handler(struct ib_event *event, void *context)\n{\n\tstruct svc_xprt *xprt = context;\n\n\tswitch (event->event) {\n\t/* These are considered benign events */\n\tcase IB_EVENT_PATH_MIG:\n\tcase IB_EVENT_COMM_EST:\n\tcase IB_EVENT_SQ_DRAINED:\n\tcase IB_EVENT_QP_LAST_WQE_REACHED:\n\t\tdprintk(\"svcrdma: QP event %s (%d) received for QP=%p\\n\",\n\t\t\tib_event_msg(event->event), event->event,\n\t\t\tevent->element.qp);\n\t\tbreak;\n\t/* These are considered fatal events */\n\tcase IB_EVENT_PATH_MIG_ERR:\n\tcase IB_EVENT_QP_FATAL:\n\tcase IB_EVENT_QP_REQ_ERR:\n\tcase IB_EVENT_QP_ACCESS_ERR:\n\tcase IB_EVENT_DEVICE_FATAL:\n\tdefault:\n\t\tdprintk(\"svcrdma: QP ERROR event %s (%d) received for QP=%p, \"\n\t\t\t\"closing transport\\n\",\n\t\t\tib_event_msg(event->event), event->event,\n\t\t\tevent->element.qp);\n\t\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\t\tbreak;\n\t}\n}\n\n/**\n * svc_rdma_wc_receive - Invoked by RDMA provider for each polled Receive WC\n * @cq:        completion queue\n * @wc:        completed WR\n *\n */\nstatic void svc_rdma_wc_receive(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct svcxprt_rdma *xprt = cq->cq_context;\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\n\t/* WARNING: Only wc->wr_cqe and wc->status are reliable */\n\tctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\n\tsvc_rdma_unmap_dma(ctxt);\n\n\tif (wc->status != IB_WC_SUCCESS)\n\t\tgoto flushed;\n\n\t/* All wc fields are now known to be valid */\n\tctxt->byte_len = wc->byte_len;\n\tspin_lock(&xprt->sc_rq_dto_lock);\n\tlist_add_tail(&ctxt->list, &xprt->sc_rq_dto_q);\n\tspin_unlock(&xprt->sc_rq_dto_lock);\n\n\tset_bit(XPT_DATA, &xprt->sc_xprt.xpt_flags);\n\tif (test_bit(RDMAXPRT_CONN_PENDING, &xprt->sc_flags))\n\t\tgoto out;\n\tsvc_xprt_enqueue(&xprt->sc_xprt);\n\tgoto out;\n\nflushed:\n\tif (wc->status != IB_WC_WR_FLUSH_ERR)\n\t\tpr_warn(\"svcrdma: receive: %s (%u/0x%x)\\n\",\n\t\t\tib_wc_status_msg(wc->status),\n\t\t\twc->status, wc->vendor_err);\n\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\tsvc_rdma_put_context(ctxt, 1);\n\nout:\n\tsvc_xprt_put(&xprt->sc_xprt);\n}\n\nstatic void svc_rdma_send_wc_common(struct svcxprt_rdma *xprt,\n\t\t\t\t    struct ib_wc *wc,\n\t\t\t\t    const char *opname)\n{\n\tif (wc->status != IB_WC_SUCCESS)\n\t\tgoto err;\n\nout:\n\tatomic_inc(&xprt->sc_sq_avail);\n\twake_up(&xprt->sc_send_wait);\n\treturn;\n\nerr:\n\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\tif (wc->status != IB_WC_WR_FLUSH_ERR)\n\t\tpr_err(\"svcrdma: %s: %s (%u/0x%x)\\n\",\n\t\t       opname, ib_wc_status_msg(wc->status),\n\t\t       wc->status, wc->vendor_err);\n\tgoto out;\n}\n\nstatic void svc_rdma_send_wc_common_put(struct ib_cq *cq, struct ib_wc *wc,\n\t\t\t\t\tconst char *opname)\n{\n\tstruct svcxprt_rdma *xprt = cq->cq_context;\n\n\tsvc_rdma_send_wc_common(xprt, wc, opname);\n\tsvc_xprt_put(&xprt->sc_xprt);\n}\n\n/**\n * svc_rdma_wc_send - Invoked by RDMA provider for each polled Send WC\n * @cq:        completion queue\n * @wc:        completed WR\n *\n */\nvoid svc_rdma_wc_send(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\n\tsvc_rdma_send_wc_common_put(cq, wc, \"send\");\n\n\tctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 1);\n}\n\n/**\n * svc_rdma_wc_reg - Invoked by RDMA provider for each polled FASTREG WC\n * @cq:        completion queue\n * @wc:        completed WR\n *\n */\nvoid svc_rdma_wc_reg(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tsvc_rdma_send_wc_common_put(cq, wc, \"fastreg\");\n}\n\n/**\n * svc_rdma_wc_read - Invoked by RDMA provider for each polled Read WC\n * @cq:        completion queue\n * @wc:        completed WR\n *\n */\nvoid svc_rdma_wc_read(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tstruct svcxprt_rdma *xprt = cq->cq_context;\n\tstruct ib_cqe *cqe = wc->wr_cqe;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\n\tsvc_rdma_send_wc_common(xprt, wc, \"read\");\n\n\tctxt = container_of(cqe, struct svc_rdma_op_ctxt, cqe);\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_frmr(xprt, ctxt->frmr);\n\n\tif (test_bit(RDMACTXT_F_LAST_CTXT, &ctxt->flags)) {\n\t\tstruct svc_rdma_op_ctxt *read_hdr;\n\n\t\tread_hdr = ctxt->read_hdr;\n\t\tspin_lock(&xprt->sc_rq_dto_lock);\n\t\tlist_add_tail(&read_hdr->list,\n\t\t\t      &xprt->sc_read_complete_q);\n\t\tspin_unlock(&xprt->sc_rq_dto_lock);\n\n\t\tset_bit(XPT_DATA, &xprt->sc_xprt.xpt_flags);\n\t\tsvc_xprt_enqueue(&xprt->sc_xprt);\n\t}\n\n\tsvc_rdma_put_context(ctxt, 0);\n\tsvc_xprt_put(&xprt->sc_xprt);\n}\n\n/**\n * svc_rdma_wc_inv - Invoked by RDMA provider for each polled LOCAL_INV WC\n * @cq:        completion queue\n * @wc:        completed WR\n *\n */\nvoid svc_rdma_wc_inv(struct ib_cq *cq, struct ib_wc *wc)\n{\n\tsvc_rdma_send_wc_common_put(cq, wc, \"localInv\");\n}\n\nstatic struct svcxprt_rdma *rdma_create_xprt(struct svc_serv *serv,\n\t\t\t\t\t     int listener)\n{\n\tstruct svcxprt_rdma *cma_xprt = kzalloc(sizeof *cma_xprt, GFP_KERNEL);\n\n\tif (!cma_xprt)\n\t\treturn NULL;\n\tsvc_xprt_init(&init_net, &svc_rdma_class, &cma_xprt->sc_xprt, serv);\n\tINIT_LIST_HEAD(&cma_xprt->sc_accept_q);\n\tINIT_LIST_HEAD(&cma_xprt->sc_rq_dto_q);\n\tINIT_LIST_HEAD(&cma_xprt->sc_read_complete_q);\n\tINIT_LIST_HEAD(&cma_xprt->sc_frmr_q);\n\tINIT_LIST_HEAD(&cma_xprt->sc_ctxts);\n\tINIT_LIST_HEAD(&cma_xprt->sc_rw_ctxts);\n\tinit_waitqueue_head(&cma_xprt->sc_send_wait);\n\n\tspin_lock_init(&cma_xprt->sc_lock);\n\tspin_lock_init(&cma_xprt->sc_rq_dto_lock);\n\tspin_lock_init(&cma_xprt->sc_frmr_q_lock);\n\tspin_lock_init(&cma_xprt->sc_ctxt_lock);\n\tspin_lock_init(&cma_xprt->sc_rw_ctxt_lock);\n\n\t/*\n\t * Note that this implies that the underlying transport support\n\t * has some form of congestion control (see RFC 7530 section 3.1\n\t * paragraph 2). For now, we assume that all supported RDMA\n\t * transports are suitable here.\n\t */\n\tset_bit(XPT_CONG_CTRL, &cma_xprt->sc_xprt.xpt_flags);\n\n\tif (listener)\n\t\tset_bit(XPT_LISTENER, &cma_xprt->sc_xprt.xpt_flags);\n\n\treturn cma_xprt;\n}\n\nint svc_rdma_post_recv(struct svcxprt_rdma *xprt, gfp_t flags)\n{\n\tstruct ib_recv_wr recv_wr, *bad_recv_wr;\n\tstruct svc_rdma_op_ctxt *ctxt;\n\tstruct page *page;\n\tdma_addr_t pa;\n\tint sge_no;\n\tint buflen;\n\tint ret;\n\n\tctxt = svc_rdma_get_context(xprt);\n\tbuflen = 0;\n\tctxt->direction = DMA_FROM_DEVICE;\n\tctxt->cqe.done = svc_rdma_wc_receive;\n\tfor (sge_no = 0; buflen < xprt->sc_max_req_size; sge_no++) {\n\t\tif (sge_no >= xprt->sc_max_sge) {\n\t\t\tpr_err(\"svcrdma: Too many sges (%d)\\n\", sge_no);\n\t\t\tgoto err_put_ctxt;\n\t\t}\n\t\tpage = alloc_page(flags);\n\t\tif (!page)\n\t\t\tgoto err_put_ctxt;\n\t\tctxt->pages[sge_no] = page;\n\t\tpa = ib_dma_map_page(xprt->sc_cm_id->device,\n\t\t\t\t     page, 0, PAGE_SIZE,\n\t\t\t\t     DMA_FROM_DEVICE);\n\t\tif (ib_dma_mapping_error(xprt->sc_cm_id->device, pa))\n\t\t\tgoto err_put_ctxt;\n\t\tsvc_rdma_count_mappings(xprt, ctxt);\n\t\tctxt->sge[sge_no].addr = pa;\n\t\tctxt->sge[sge_no].length = PAGE_SIZE;\n\t\tctxt->sge[sge_no].lkey = xprt->sc_pd->local_dma_lkey;\n\t\tctxt->count = sge_no + 1;\n\t\tbuflen += PAGE_SIZE;\n\t}\n\trecv_wr.next = NULL;\n\trecv_wr.sg_list = &ctxt->sge[0];\n\trecv_wr.num_sge = ctxt->count;\n\trecv_wr.wr_cqe = &ctxt->cqe;\n\n\tsvc_xprt_get(&xprt->sc_xprt);\n\tret = ib_post_recv(xprt->sc_qp, &recv_wr, &bad_recv_wr);\n\tif (ret) {\n\t\tsvc_rdma_unmap_dma(ctxt);\n\t\tsvc_rdma_put_context(ctxt, 1);\n\t\tsvc_xprt_put(&xprt->sc_xprt);\n\t}\n\treturn ret;\n\n err_put_ctxt:\n\tsvc_rdma_unmap_dma(ctxt);\n\tsvc_rdma_put_context(ctxt, 1);\n\treturn -ENOMEM;\n}\n\nint svc_rdma_repost_recv(struct svcxprt_rdma *xprt, gfp_t flags)\n{\n\tint ret = 0;\n\n\tret = svc_rdma_post_recv(xprt, flags);\n\tif (ret) {\n\t\tpr_err(\"svcrdma: could not post a receive buffer, err=%d.\\n\",\n\t\t       ret);\n\t\tpr_err(\"svcrdma: closing transport %p.\\n\", xprt);\n\t\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\t\tret = -ENOTCONN;\n\t}\n\treturn ret;\n}\n\nstatic void\nsvc_rdma_parse_connect_private(struct svcxprt_rdma *newxprt,\n\t\t\t       struct rdma_conn_param *param)\n{\n\tconst struct rpcrdma_connect_private *pmsg = param->private_data;\n\n\tif (pmsg &&\n\t    pmsg->cp_magic == rpcrdma_cmp_magic &&\n\t    pmsg->cp_version == RPCRDMA_CMP_VERSION) {\n\t\tnewxprt->sc_snd_w_inv = pmsg->cp_flags &\n\t\t\t\t\tRPCRDMA_CMP_F_SND_W_INV_OK;\n\n\t\tdprintk(\"svcrdma: client send_size %u, recv_size %u \"\n\t\t\t\"remote inv %ssupported\\n\",\n\t\t\trpcrdma_decode_buffer_size(pmsg->cp_send_size),\n\t\t\trpcrdma_decode_buffer_size(pmsg->cp_recv_size),\n\t\t\tnewxprt->sc_snd_w_inv ? \"\" : \"un\");\n\t}\n}\n\n/*\n * This function handles the CONNECT_REQUEST event on a listening\n * endpoint. It is passed the cma_id for the _new_ connection. The context in\n * this cma_id is inherited from the listening cma_id and is the svc_xprt\n * structure for the listening endpoint.\n *\n * This function creates a new xprt for the new connection and enqueues it on\n * the accept queue for the listent xprt. When the listen thread is kicked, it\n * will call the recvfrom method on the listen xprt which will accept the new\n * connection.\n */\nstatic void handle_connect_req(struct rdma_cm_id *new_cma_id,\n\t\t\t       struct rdma_conn_param *param)\n{\n\tstruct svcxprt_rdma *listen_xprt = new_cma_id->context;\n\tstruct svcxprt_rdma *newxprt;\n\tstruct sockaddr *sa;\n\n\t/* Create a new transport */\n\tnewxprt = rdma_create_xprt(listen_xprt->sc_xprt.xpt_server, 0);\n\tif (!newxprt) {\n\t\tdprintk(\"svcrdma: failed to create new transport\\n\");\n\t\treturn;\n\t}\n\tnewxprt->sc_cm_id = new_cma_id;\n\tnew_cma_id->context = newxprt;\n\tdprintk(\"svcrdma: Creating newxprt=%p, cm_id=%p, listenxprt=%p\\n\",\n\t\tnewxprt, newxprt->sc_cm_id, listen_xprt);\n\tsvc_rdma_parse_connect_private(newxprt, param);\n\n\t/* Save client advertised inbound read limit for use later in accept. */\n\tnewxprt->sc_ord = param->initiator_depth;\n\n\t/* Set the local and remote addresses in the transport */\n\tsa = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.dst_addr;\n\tsvc_xprt_set_remote(&newxprt->sc_xprt, sa, svc_addr_len(sa));\n\tsa = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.src_addr;\n\tsvc_xprt_set_local(&newxprt->sc_xprt, sa, svc_addr_len(sa));\n\n\t/*\n\t * Enqueue the new transport on the accept queue of the listening\n\t * transport\n\t */\n\tspin_lock_bh(&listen_xprt->sc_lock);\n\tlist_add_tail(&newxprt->sc_accept_q, &listen_xprt->sc_accept_q);\n\tspin_unlock_bh(&listen_xprt->sc_lock);\n\n\tset_bit(XPT_CONN, &listen_xprt->sc_xprt.xpt_flags);\n\tsvc_xprt_enqueue(&listen_xprt->sc_xprt);\n}\n\n/*\n * Handles events generated on the listening endpoint. These events will be\n * either be incoming connect requests or adapter removal  events.\n */\nstatic int rdma_listen_handler(struct rdma_cm_id *cma_id,\n\t\t\t       struct rdma_cm_event *event)\n{\n\tstruct svcxprt_rdma *xprt = cma_id->context;\n\tint ret = 0;\n\n\tswitch (event->event) {\n\tcase RDMA_CM_EVENT_CONNECT_REQUEST:\n\t\tdprintk(\"svcrdma: Connect request on cma_id=%p, xprt = %p, \"\n\t\t\t\"event = %s (%d)\\n\", cma_id, cma_id->context,\n\t\t\trdma_event_msg(event->event), event->event);\n\t\thandle_connect_req(cma_id, &event->param.conn);\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\t/* Accept complete */\n\t\tdprintk(\"svcrdma: Connection completed on LISTEN xprt=%p, \"\n\t\t\t\"cm_id=%p\\n\", xprt, cma_id);\n\t\tbreak;\n\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\t\tdprintk(\"svcrdma: Device removal xprt=%p, cm_id=%p\\n\",\n\t\t\txprt, cma_id);\n\t\tif (xprt)\n\t\t\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\t\tbreak;\n\n\tdefault:\n\t\tdprintk(\"svcrdma: Unexpected event on listening endpoint %p, \"\n\t\t\t\"event = %s (%d)\\n\", cma_id,\n\t\t\trdma_event_msg(event->event), event->event);\n\t\tbreak;\n\t}\n\n\treturn ret;\n}\n\nstatic int rdma_cma_handler(struct rdma_cm_id *cma_id,\n\t\t\t    struct rdma_cm_event *event)\n{\n\tstruct svc_xprt *xprt = cma_id->context;\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tswitch (event->event) {\n\tcase RDMA_CM_EVENT_ESTABLISHED:\n\t\t/* Accept complete */\n\t\tsvc_xprt_get(xprt);\n\t\tdprintk(\"svcrdma: Connection completed on DTO xprt=%p, \"\n\t\t\t\"cm_id=%p\\n\", xprt, cma_id);\n\t\tclear_bit(RDMAXPRT_CONN_PENDING, &rdma->sc_flags);\n\t\tsvc_xprt_enqueue(xprt);\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DISCONNECTED:\n\t\tdprintk(\"svcrdma: Disconnect on DTO xprt=%p, cm_id=%p\\n\",\n\t\t\txprt, cma_id);\n\t\tif (xprt) {\n\t\t\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\t\t\tsvc_xprt_enqueue(xprt);\n\t\t\tsvc_xprt_put(xprt);\n\t\t}\n\t\tbreak;\n\tcase RDMA_CM_EVENT_DEVICE_REMOVAL:\n\t\tdprintk(\"svcrdma: Device removal cma_id=%p, xprt = %p, \"\n\t\t\t\"event = %s (%d)\\n\", cma_id, xprt,\n\t\t\trdma_event_msg(event->event), event->event);\n\t\tif (xprt) {\n\t\t\tset_bit(XPT_CLOSE, &xprt->xpt_flags);\n\t\t\tsvc_xprt_enqueue(xprt);\n\t\t\tsvc_xprt_put(xprt);\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tdprintk(\"svcrdma: Unexpected event on DTO endpoint %p, \"\n\t\t\t\"event = %s (%d)\\n\", cma_id,\n\t\t\trdma_event_msg(event->event), event->event);\n\t\tbreak;\n\t}\n\treturn 0;\n}\n\n/*\n * Create a listening RDMA service endpoint.\n */\nstatic struct svc_xprt *svc_rdma_create(struct svc_serv *serv,\n\t\t\t\t\tstruct net *net,\n\t\t\t\t\tstruct sockaddr *sa, int salen,\n\t\t\t\t\tint flags)\n{\n\tstruct rdma_cm_id *listen_id;\n\tstruct svcxprt_rdma *cma_xprt;\n\tint ret;\n\n\tdprintk(\"svcrdma: Creating RDMA socket\\n\");\n\tif ((sa->sa_family != AF_INET) && (sa->sa_family != AF_INET6)) {\n\t\tdprintk(\"svcrdma: Address family %d is not supported.\\n\", sa->sa_family);\n\t\treturn ERR_PTR(-EAFNOSUPPORT);\n\t}\n\tcma_xprt = rdma_create_xprt(serv, 1);\n\tif (!cma_xprt)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tlisten_id = rdma_create_id(&init_net, rdma_listen_handler, cma_xprt,\n\t\t\t\t   RDMA_PS_TCP, IB_QPT_RC);\n\tif (IS_ERR(listen_id)) {\n\t\tret = PTR_ERR(listen_id);\n\t\tdprintk(\"svcrdma: rdma_create_id failed = %d\\n\", ret);\n\t\tgoto err0;\n\t}\n\n\t/* Allow both IPv4 and IPv6 sockets to bind a single port\n\t * at the same time.\n\t */\n#if IS_ENABLED(CONFIG_IPV6)\n\tret = rdma_set_afonly(listen_id, 1);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: rdma_set_afonly failed = %d\\n\", ret);\n\t\tgoto err1;\n\t}\n#endif\n\tret = rdma_bind_addr(listen_id, sa);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: rdma_bind_addr failed = %d\\n\", ret);\n\t\tgoto err1;\n\t}\n\tcma_xprt->sc_cm_id = listen_id;\n\n\tret = rdma_listen(listen_id, RPCRDMA_LISTEN_BACKLOG);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: rdma_listen failed = %d\\n\", ret);\n\t\tgoto err1;\n\t}\n\n\t/*\n\t * We need to use the address from the cm_id in case the\n\t * caller specified 0 for the port number.\n\t */\n\tsa = (struct sockaddr *)&cma_xprt->sc_cm_id->route.addr.src_addr;\n\tsvc_xprt_set_local(&cma_xprt->sc_xprt, sa, salen);\n\n\treturn &cma_xprt->sc_xprt;\n\n err1:\n\trdma_destroy_id(listen_id);\n err0:\n\tkfree(cma_xprt);\n\treturn ERR_PTR(ret);\n}\n\nstatic struct svc_rdma_fastreg_mr *rdma_alloc_frmr(struct svcxprt_rdma *xprt)\n{\n\tstruct ib_mr *mr;\n\tstruct scatterlist *sg;\n\tstruct svc_rdma_fastreg_mr *frmr;\n\tu32 num_sg;\n\n\tfrmr = kmalloc(sizeof(*frmr), GFP_KERNEL);\n\tif (!frmr)\n\t\tgoto err;\n\n\tnum_sg = min_t(u32, RPCSVC_MAXPAGES, xprt->sc_frmr_pg_list_len);\n\tmr = ib_alloc_mr(xprt->sc_pd, IB_MR_TYPE_MEM_REG, num_sg);\n\tif (IS_ERR(mr))\n\t\tgoto err_free_frmr;\n\n\tsg = kcalloc(RPCSVC_MAXPAGES, sizeof(*sg), GFP_KERNEL);\n\tif (!sg)\n\t\tgoto err_free_mr;\n\n\tsg_init_table(sg, RPCSVC_MAXPAGES);\n\n\tfrmr->mr = mr;\n\tfrmr->sg = sg;\n\tINIT_LIST_HEAD(&frmr->frmr_list);\n\treturn frmr;\n\n err_free_mr:\n\tib_dereg_mr(mr);\n err_free_frmr:\n\tkfree(frmr);\n err:\n\treturn ERR_PTR(-ENOMEM);\n}\n\nstatic void rdma_dealloc_frmr_q(struct svcxprt_rdma *xprt)\n{\n\tstruct svc_rdma_fastreg_mr *frmr;\n\n\twhile (!list_empty(&xprt->sc_frmr_q)) {\n\t\tfrmr = list_entry(xprt->sc_frmr_q.next,\n\t\t\t\t  struct svc_rdma_fastreg_mr, frmr_list);\n\t\tlist_del_init(&frmr->frmr_list);\n\t\tkfree(frmr->sg);\n\t\tib_dereg_mr(frmr->mr);\n\t\tkfree(frmr);\n\t}\n}\n\nstruct svc_rdma_fastreg_mr *svc_rdma_get_frmr(struct svcxprt_rdma *rdma)\n{\n\tstruct svc_rdma_fastreg_mr *frmr = NULL;\n\n\tspin_lock(&rdma->sc_frmr_q_lock);\n\tif (!list_empty(&rdma->sc_frmr_q)) {\n\t\tfrmr = list_entry(rdma->sc_frmr_q.next,\n\t\t\t\t  struct svc_rdma_fastreg_mr, frmr_list);\n\t\tlist_del_init(&frmr->frmr_list);\n\t\tfrmr->sg_nents = 0;\n\t}\n\tspin_unlock(&rdma->sc_frmr_q_lock);\n\tif (frmr)\n\t\treturn frmr;\n\n\treturn rdma_alloc_frmr(rdma);\n}\n\nvoid svc_rdma_put_frmr(struct svcxprt_rdma *rdma,\n\t\t       struct svc_rdma_fastreg_mr *frmr)\n{\n\tif (frmr) {\n\t\tib_dma_unmap_sg(rdma->sc_cm_id->device,\n\t\t\t\tfrmr->sg, frmr->sg_nents, frmr->direction);\n\t\tspin_lock(&rdma->sc_frmr_q_lock);\n\t\tWARN_ON_ONCE(!list_empty(&frmr->frmr_list));\n\t\tlist_add(&frmr->frmr_list, &rdma->sc_frmr_q);\n\t\tspin_unlock(&rdma->sc_frmr_q_lock);\n\t}\n}\n\n/*\n * This is the xpo_recvfrom function for listening endpoints. Its\n * purpose is to accept incoming connections. The CMA callback handler\n * has already created a new transport and attached it to the new CMA\n * ID.\n *\n * There is a queue of pending connections hung on the listening\n * transport. This queue contains the new svc_xprt structure. This\n * function takes svc_xprt structures off the accept_q and completes\n * the connection.\n */\nstatic struct svc_xprt *svc_rdma_accept(struct svc_xprt *xprt)\n{\n\tstruct svcxprt_rdma *listen_rdma;\n\tstruct svcxprt_rdma *newxprt = NULL;\n\tstruct rdma_conn_param conn_param;\n\tstruct rpcrdma_connect_private pmsg;\n\tstruct ib_qp_init_attr qp_attr;\n\tstruct ib_device *dev;\n\tstruct sockaddr *sap;\n\tunsigned int i;\n\tint ret = 0;\n\n\tlisten_rdma = container_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tclear_bit(XPT_CONN, &xprt->xpt_flags);\n\t/* Get the next entry off the accept list */\n\tspin_lock_bh(&listen_rdma->sc_lock);\n\tif (!list_empty(&listen_rdma->sc_accept_q)) {\n\t\tnewxprt = list_entry(listen_rdma->sc_accept_q.next,\n\t\t\t\t     struct svcxprt_rdma, sc_accept_q);\n\t\tlist_del_init(&newxprt->sc_accept_q);\n\t}\n\tif (!list_empty(&listen_rdma->sc_accept_q))\n\t\tset_bit(XPT_CONN, &listen_rdma->sc_xprt.xpt_flags);\n\tspin_unlock_bh(&listen_rdma->sc_lock);\n\tif (!newxprt)\n\t\treturn NULL;\n\n\tdprintk(\"svcrdma: newxprt from accept queue = %p, cm_id=%p\\n\",\n\t\tnewxprt, newxprt->sc_cm_id);\n\n\tdev = newxprt->sc_cm_id->device;\n\tnewxprt->sc_port_num = newxprt->sc_cm_id->port_num;\n\n\t/* Qualify the transport resource defaults with the\n\t * capabilities of this particular device */\n\tnewxprt->sc_max_sge = min((size_t)dev->attrs.max_sge,\n\t\t\t\t  (size_t)RPCSVC_MAXPAGES);\n\tnewxprt->sc_max_sge_rd = min_t(size_t, dev->attrs.max_sge_rd,\n\t\t\t\t       RPCSVC_MAXPAGES);\n\tnewxprt->sc_max_req_size = svcrdma_max_req_size;\n\tnewxprt->sc_max_requests = min_t(u32, dev->attrs.max_qp_wr,\n\t\t\t\t\t svcrdma_max_requests);\n\tnewxprt->sc_fc_credits = cpu_to_be32(newxprt->sc_max_requests);\n\tnewxprt->sc_max_bc_requests = min_t(u32, dev->attrs.max_qp_wr,\n\t\t\t\t\t    svcrdma_max_bc_requests);\n\tnewxprt->sc_rq_depth = newxprt->sc_max_requests +\n\t\t\t       newxprt->sc_max_bc_requests;\n\tnewxprt->sc_sq_depth = newxprt->sc_rq_depth;\n\tatomic_set(&newxprt->sc_sq_avail, newxprt->sc_sq_depth);\n\n\tif (!svc_rdma_prealloc_ctxts(newxprt))\n\t\tgoto errout;\n\n\t/*\n\t * Limit ORD based on client limit, local device limit, and\n\t * configured svcrdma limit.\n\t */\n\tnewxprt->sc_ord = min_t(size_t, dev->attrs.max_qp_rd_atom, newxprt->sc_ord);\n\tnewxprt->sc_ord = min_t(size_t,\tsvcrdma_ord, newxprt->sc_ord);\n\n\tnewxprt->sc_pd = ib_alloc_pd(dev, 0);\n\tif (IS_ERR(newxprt->sc_pd)) {\n\t\tdprintk(\"svcrdma: error creating PD for connect request\\n\");\n\t\tgoto errout;\n\t}\n\tnewxprt->sc_sq_cq = ib_alloc_cq(dev, newxprt, newxprt->sc_sq_depth,\n\t\t\t\t\t0, IB_POLL_WORKQUEUE);\n\tif (IS_ERR(newxprt->sc_sq_cq)) {\n\t\tdprintk(\"svcrdma: error creating SQ CQ for connect request\\n\");\n\t\tgoto errout;\n\t}\n\tnewxprt->sc_rq_cq = ib_alloc_cq(dev, newxprt, newxprt->sc_rq_depth,\n\t\t\t\t\t0, IB_POLL_WORKQUEUE);\n\tif (IS_ERR(newxprt->sc_rq_cq)) {\n\t\tdprintk(\"svcrdma: error creating RQ CQ for connect request\\n\");\n\t\tgoto errout;\n\t}\n\n\tmemset(&qp_attr, 0, sizeof qp_attr);\n\tqp_attr.event_handler = qp_event_handler;\n\tqp_attr.qp_context = &newxprt->sc_xprt;\n\tqp_attr.port_num = newxprt->sc_cm_id->port_num;\n\tqp_attr.cap.max_rdma_ctxs = newxprt->sc_max_requests;\n\tqp_attr.cap.max_send_wr = newxprt->sc_sq_depth;\n\tqp_attr.cap.max_recv_wr = newxprt->sc_rq_depth;\n\tqp_attr.cap.max_send_sge = newxprt->sc_max_sge;\n\tqp_attr.cap.max_recv_sge = newxprt->sc_max_sge;\n\tqp_attr.sq_sig_type = IB_SIGNAL_REQ_WR;\n\tqp_attr.qp_type = IB_QPT_RC;\n\tqp_attr.send_cq = newxprt->sc_sq_cq;\n\tqp_attr.recv_cq = newxprt->sc_rq_cq;\n\tdprintk(\"svcrdma: newxprt->sc_cm_id=%p, newxprt->sc_pd=%p\\n\",\n\t\tnewxprt->sc_cm_id, newxprt->sc_pd);\n\tdprintk(\"    cap.max_send_wr = %d, cap.max_recv_wr = %d\\n\",\n\t\tqp_attr.cap.max_send_wr, qp_attr.cap.max_recv_wr);\n\tdprintk(\"    cap.max_send_sge = %d, cap.max_recv_sge = %d\\n\",\n\t\tqp_attr.cap.max_send_sge, qp_attr.cap.max_recv_sge);\n\n\tret = rdma_create_qp(newxprt->sc_cm_id, newxprt->sc_pd, &qp_attr);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: failed to create QP, ret=%d\\n\", ret);\n\t\tgoto errout;\n\t}\n\tnewxprt->sc_qp = newxprt->sc_cm_id->qp;\n\n\t/*\n\t * Use the most secure set of MR resources based on the\n\t * transport type and available memory management features in\n\t * the device. Here's the table implemented below:\n\t *\n\t *\t\tFast\tGlobal\tDMA\tRemote WR\n\t *\t\tReg\tLKEY\tMR\tAccess\n\t *\t\tSup'd\tSup'd\tNeeded\tNeeded\n\t *\n\t * IWARP\tN\tN\tY\tY\n\t *\t\tN\tY\tY\tY\n\t *\t\tY\tN\tY\tN\n\t *\t\tY\tY\tN\t-\n\t *\n\t * IB\t\tN\tN\tY\tN\n\t *\t\tN\tY\tN\t-\n\t *\t\tY\tN\tY\tN\n\t *\t\tY\tY\tN\t-\n\t *\n\t * NB:\tiWARP requires remote write access for the data sink\n\t *\tof an RDMA_READ. IB does not.\n\t */\n\tnewxprt->sc_reader = rdma_read_chunk_lcl;\n\tif (dev->attrs.device_cap_flags & IB_DEVICE_MEM_MGT_EXTENSIONS) {\n\t\tnewxprt->sc_frmr_pg_list_len =\n\t\t\tdev->attrs.max_fast_reg_page_list_len;\n\t\tnewxprt->sc_dev_caps |= SVCRDMA_DEVCAP_FAST_REG;\n\t\tnewxprt->sc_reader = rdma_read_chunk_frmr;\n\t} else\n\t\tnewxprt->sc_snd_w_inv = false;\n\n\t/*\n\t * Determine if a DMA MR is required and if so, what privs are required\n\t */\n\tif (!rdma_protocol_iwarp(dev, newxprt->sc_cm_id->port_num) &&\n\t    !rdma_ib_or_roce(dev, newxprt->sc_cm_id->port_num))\n\t\tgoto errout;\n\n\tif (rdma_protocol_iwarp(dev, newxprt->sc_cm_id->port_num))\n\t\tnewxprt->sc_dev_caps |= SVCRDMA_DEVCAP_READ_W_INV;\n\n\t/* Post receive buffers */\n\tfor (i = 0; i < newxprt->sc_max_requests; i++) {\n\t\tret = svc_rdma_post_recv(newxprt, GFP_KERNEL);\n\t\tif (ret) {\n\t\t\tdprintk(\"svcrdma: failure posting receive buffers\\n\");\n\t\t\tgoto errout;\n\t\t}\n\t}\n\n\t/* Swap out the handler */\n\tnewxprt->sc_cm_id->event_handler = rdma_cma_handler;\n\n\t/* Construct RDMA-CM private message */\n\tpmsg.cp_magic = rpcrdma_cmp_magic;\n\tpmsg.cp_version = RPCRDMA_CMP_VERSION;\n\tpmsg.cp_flags = 0;\n\tpmsg.cp_send_size = pmsg.cp_recv_size =\n\t\trpcrdma_encode_buffer_size(newxprt->sc_max_req_size);\n\n\t/* Accept Connection */\n\tset_bit(RDMAXPRT_CONN_PENDING, &newxprt->sc_flags);\n\tmemset(&conn_param, 0, sizeof conn_param);\n\tconn_param.responder_resources = 0;\n\tconn_param.initiator_depth = newxprt->sc_ord;\n\tconn_param.private_data = &pmsg;\n\tconn_param.private_data_len = sizeof(pmsg);\n\tret = rdma_accept(newxprt->sc_cm_id, &conn_param);\n\tif (ret) {\n\t\tdprintk(\"svcrdma: failed to accept new connection, ret=%d\\n\",\n\t\t       ret);\n\t\tgoto errout;\n\t}\n\n\tdprintk(\"svcrdma: new connection %p accepted:\\n\", newxprt);\n\tsap = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.src_addr;\n\tdprintk(\"    local address   : %pIS:%u\\n\", sap, rpc_get_port(sap));\n\tsap = (struct sockaddr *)&newxprt->sc_cm_id->route.addr.dst_addr;\n\tdprintk(\"    remote address  : %pIS:%u\\n\", sap, rpc_get_port(sap));\n\tdprintk(\"    max_sge         : %d\\n\", newxprt->sc_max_sge);\n\tdprintk(\"    max_sge_rd      : %d\\n\", newxprt->sc_max_sge_rd);\n\tdprintk(\"    sq_depth        : %d\\n\", newxprt->sc_sq_depth);\n\tdprintk(\"    max_requests    : %d\\n\", newxprt->sc_max_requests);\n\tdprintk(\"    ord             : %d\\n\", newxprt->sc_ord);\n\n\treturn &newxprt->sc_xprt;\n\n errout:\n\tdprintk(\"svcrdma: failure accepting new connection rc=%d.\\n\", ret);\n\t/* Take a reference in case the DTO handler runs */\n\tsvc_xprt_get(&newxprt->sc_xprt);\n\tif (newxprt->sc_qp && !IS_ERR(newxprt->sc_qp))\n\t\tib_destroy_qp(newxprt->sc_qp);\n\trdma_destroy_id(newxprt->sc_cm_id);\n\t/* This call to put will destroy the transport */\n\tsvc_xprt_put(&newxprt->sc_xprt);\n\treturn NULL;\n}\n\nstatic void svc_rdma_release_rqst(struct svc_rqst *rqstp)\n{\n}\n\n/*\n * When connected, an svc_xprt has at least two references:\n *\n * - A reference held by the cm_id between the ESTABLISHED and\n *   DISCONNECTED events. If the remote peer disconnected first, this\n *   reference could be gone.\n *\n * - A reference held by the svc_recv code that called this function\n *   as part of close processing.\n *\n * At a minimum one references should still be held.\n */\nstatic void svc_rdma_detach(struct svc_xprt *xprt)\n{\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tdprintk(\"svc: svc_rdma_detach(%p)\\n\", xprt);\n\n\t/* Disconnect and flush posted WQE */\n\trdma_disconnect(rdma->sc_cm_id);\n}\n\nstatic void __svc_rdma_free(struct work_struct *work)\n{\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(work, struct svcxprt_rdma, sc_work);\n\tstruct svc_xprt *xprt = &rdma->sc_xprt;\n\n\tdprintk(\"svcrdma: %s(%p)\\n\", __func__, rdma);\n\n\tif (rdma->sc_qp && !IS_ERR(rdma->sc_qp))\n\t\tib_drain_qp(rdma->sc_qp);\n\n\t/* We should only be called from kref_put */\n\tif (kref_read(&xprt->xpt_ref) != 0)\n\t\tpr_err(\"svcrdma: sc_xprt still in use? (%d)\\n\",\n\t\t       kref_read(&xprt->xpt_ref));\n\n\t/*\n\t * Destroy queued, but not processed read completions. Note\n\t * that this cleanup has to be done before destroying the\n\t * cm_id because the device ptr is needed to unmap the dma in\n\t * svc_rdma_put_context.\n\t */\n\twhile (!list_empty(&rdma->sc_read_complete_q)) {\n\t\tstruct svc_rdma_op_ctxt *ctxt;\n\t\tctxt = list_first_entry(&rdma->sc_read_complete_q,\n\t\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\t\tlist_del(&ctxt->list);\n\t\tsvc_rdma_put_context(ctxt, 1);\n\t}\n\n\t/* Destroy queued, but not processed recv completions */\n\twhile (!list_empty(&rdma->sc_rq_dto_q)) {\n\t\tstruct svc_rdma_op_ctxt *ctxt;\n\t\tctxt = list_first_entry(&rdma->sc_rq_dto_q,\n\t\t\t\t\tstruct svc_rdma_op_ctxt, list);\n\t\tlist_del(&ctxt->list);\n\t\tsvc_rdma_put_context(ctxt, 1);\n\t}\n\n\t/* Warn if we leaked a resource or under-referenced */\n\tif (rdma->sc_ctxt_used != 0)\n\t\tpr_err(\"svcrdma: ctxt still in use? (%d)\\n\",\n\t\t       rdma->sc_ctxt_used);\n\n\t/* Final put of backchannel client transport */\n\tif (xprt->xpt_bc_xprt) {\n\t\txprt_put(xprt->xpt_bc_xprt);\n\t\txprt->xpt_bc_xprt = NULL;\n\t}\n\n\trdma_dealloc_frmr_q(rdma);\n\tsvc_rdma_destroy_rw_ctxts(rdma);\n\tsvc_rdma_destroy_ctxts(rdma);\n\n\t/* Destroy the QP if present (not a listener) */\n\tif (rdma->sc_qp && !IS_ERR(rdma->sc_qp))\n\t\tib_destroy_qp(rdma->sc_qp);\n\n\tif (rdma->sc_sq_cq && !IS_ERR(rdma->sc_sq_cq))\n\t\tib_free_cq(rdma->sc_sq_cq);\n\n\tif (rdma->sc_rq_cq && !IS_ERR(rdma->sc_rq_cq))\n\t\tib_free_cq(rdma->sc_rq_cq);\n\n\tif (rdma->sc_pd && !IS_ERR(rdma->sc_pd))\n\t\tib_dealloc_pd(rdma->sc_pd);\n\n\t/* Destroy the CM ID */\n\trdma_destroy_id(rdma->sc_cm_id);\n\n\tkfree(rdma);\n}\n\nstatic void svc_rdma_free(struct svc_xprt *xprt)\n{\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\tINIT_WORK(&rdma->sc_work, __svc_rdma_free);\n\tqueue_work(svc_rdma_wq, &rdma->sc_work);\n}\n\nstatic int svc_rdma_has_wspace(struct svc_xprt *xprt)\n{\n\tstruct svcxprt_rdma *rdma =\n\t\tcontainer_of(xprt, struct svcxprt_rdma, sc_xprt);\n\n\t/*\n\t * If there are already waiters on the SQ,\n\t * return false.\n\t */\n\tif (waitqueue_active(&rdma->sc_send_wait))\n\t\treturn 0;\n\n\t/* Otherwise return true. */\n\treturn 1;\n}\n\nstatic int svc_rdma_secure_port(struct svc_rqst *rqstp)\n{\n\treturn 1;\n}\n\nstatic void svc_rdma_kill_temp_xprt(struct svc_xprt *xprt)\n{\n}\n\nint svc_rdma_send(struct svcxprt_rdma *xprt, struct ib_send_wr *wr)\n{\n\tstruct ib_send_wr *bad_wr, *n_wr;\n\tint wr_count;\n\tint i;\n\tint ret;\n\n\tif (test_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags))\n\t\treturn -ENOTCONN;\n\n\twr_count = 1;\n\tfor (n_wr = wr->next; n_wr; n_wr = n_wr->next)\n\t\twr_count++;\n\n\t/* If the SQ is full, wait until an SQ entry is available */\n\twhile (1) {\n\t\tif ((atomic_sub_return(wr_count, &xprt->sc_sq_avail) < 0)) {\n\t\t\tatomic_inc(&rdma_stat_sq_starve);\n\n\t\t\t/* Wait until SQ WR available if SQ still full */\n\t\t\tatomic_add(wr_count, &xprt->sc_sq_avail);\n\t\t\twait_event(xprt->sc_send_wait,\n\t\t\t\t   atomic_read(&xprt->sc_sq_avail) > wr_count);\n\t\t\tif (test_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags))\n\t\t\t\treturn -ENOTCONN;\n\t\t\tcontinue;\n\t\t}\n\t\t/* Take a transport ref for each WR posted */\n\t\tfor (i = 0; i < wr_count; i++)\n\t\t\tsvc_xprt_get(&xprt->sc_xprt);\n\n\t\t/* Bump used SQ WR count and post */\n\t\tret = ib_post_send(xprt->sc_qp, wr, &bad_wr);\n\t\tif (ret) {\n\t\t\tset_bit(XPT_CLOSE, &xprt->sc_xprt.xpt_flags);\n\t\t\tfor (i = 0; i < wr_count; i ++)\n\t\t\t\tsvc_xprt_put(&xprt->sc_xprt);\n\t\t\tdprintk(\"svcrdma: failed to post SQ WR rc=%d\\n\", ret);\n\t\t\tdprintk(\"    sc_sq_avail=%d, sc_sq_depth=%d\\n\",\n\t\t\t\tatomic_read(&xprt->sc_sq_avail),\n\t\t\t\txprt->sc_sq_depth);\n\t\t\twake_up(&xprt->sc_send_wait);\n\t\t}\n\t\tbreak;\n\t}\n\treturn ret;\n}\n"], "buggy_code_start_loc": [134, 873, 79, 337, 1262, 1915, 2834, 259, 96, 145, 339, 49, 24, 54, 704, 7, 61, 15, 169, 560, 1, 1, 272], "buggy_code_end_loc": [275, 882, 284, 602, 1263, 1937, 4215, 408, 180, 145, 476, 250, 52, 54, 777, 8, 252, 156, 258, 690, 2, 713, 1253], "fixing_code_start_loc": [135, 873, 79, 336, 1262, 1914, 2834, 260, 97, 146, 339, 50, 25, 55, 705, 7, 61, 15, 168, 561, 1, 2, 271], "fixing_code_end_loc": [276, 882, 292, 613, 1264, 1924, 4221, 415, 196, 149, 477, 228, 54, 56, 835, 8, 249, 144, 168, 743, 513, 698, 1156], "type": "CWE-404", "message": "The NFSv4 implementation in the Linux kernel through 4.11.1 allows local users to cause a denial of service (resource consumption) by leveraging improper channel callback shutdown when unmounting an NFSv4 filesystem, aka a \"module reference and kernel daemon\" leak.", "other": {"cve": {"id": "CVE-2017-9059", "sourceIdentifier": "cve@mitre.org", "published": "2017-05-18T06:29:00.700", "lastModified": "2019-10-03T00:03:26.223", "vulnStatus": "Modified", "descriptions": [{"lang": "en", "value": "The NFSv4 implementation in the Linux kernel through 4.11.1 allows local users to cause a denial of service (resource consumption) by leveraging improper channel callback shutdown when unmounting an NFSv4 filesystem, aka a \"module reference and kernel daemon\" leak."}, {"lang": "es", "value": "La implementaci\u00f3n NFSv4 en el kernel de Linux hasta versi\u00f3n 4.11.1, permite a los usuarios locales causar una denegaci\u00f3n de servicio (consumo de recursos) para aprovechar el apagado inapropiado de la devoluci\u00f3n de llamada de canal al desmontar un sistema de archivos NFSv4, tambi\u00e9n se conoce como una perdida del \"module reference and kernel daemon\"."}], "metrics": {"cvssMetricV30": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "3.0", "vectorString": "CVSS:3.0/AV:L/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H", "attackVector": "LOCAL", "attackComplexity": "LOW", "privilegesRequired": "LOW", "userInteraction": "NONE", "scope": "UNCHANGED", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "HIGH", "baseScore": 5.5, "baseSeverity": "MEDIUM"}, "exploitabilityScore": 1.8, "impactScore": 3.6}], "cvssMetricV2": [{"source": "nvd@nist.gov", "type": "Primary", "cvssData": {"version": "2.0", "vectorString": "AV:L/AC:L/Au:N/C:N/I:N/A:C", "accessVector": "LOCAL", "accessComplexity": "LOW", "authentication": "NONE", "confidentialityImpact": "NONE", "integrityImpact": "NONE", "availabilityImpact": "COMPLETE", "baseScore": 4.9}, "baseSeverity": "MEDIUM", "exploitabilityScore": 3.9, "impactScore": 6.9, "acInsufInfo": false, "obtainAllPrivilege": false, "obtainUserPrivilege": false, "obtainOtherPrivilege": false, "userInteractionRequired": false}]}, "weaknesses": [{"source": "nvd@nist.gov", "type": "Primary", "description": [{"lang": "en", "value": "CWE-404"}]}], "configurations": [{"nodes": [{"operator": "OR", "negate": false, "cpeMatch": [{"vulnerable": true, "criteria": "cpe:2.3:o:linux:linux_kernel:*:*:*:*:*:*:*:*", "versionEndIncluding": "4.11.1", "matchCriteriaId": "41FE3927-B113-4AAB-AC33-5DA02D19F524"}]}]}], "references": [{"url": "http://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=c70422f760c120480fee4de6c38804c72aa26bc1", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "http://www.securityfocus.com/bid/98551", "source": "cve@mitre.org"}, {"url": "https://bugzilla.redhat.com/show_bug.cgi?id=1451386", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch"]}, {"url": "https://github.com/torvalds/linux/commit/c70422f760c120480fee4de6c38804c72aa26bc1", "source": "cve@mitre.org", "tags": ["Issue Tracking", "Patch", "Third Party Advisory"]}, {"url": "https://www.spinics.net/lists/linux-nfs/msg63334.html", "source": "cve@mitre.org", "tags": ["Mailing List", "Patch", "Third Party Advisory"]}]}, "github_commit_url": "https://github.com/torvalds/linux/commit/c70422f760c120480fee4de6c38804c72aa26bc1"}}